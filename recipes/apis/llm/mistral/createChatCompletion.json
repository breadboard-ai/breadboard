{"edges":[{"from":"fetch-4","to":"output-3","out":"response","in":"api_json_response"},{"from":"createFetchParameters","to":"fetch-4","out":"*","in":""},{"from":"input-1","to":"APIInput","out":"graph","in":"graph"},{"from":"input-1","to":"createFetchParameters","out":"item","in":"item"},{"from":"toAPIInputs","to":"createFetchParameters","out":"api_inputs","in":"api_inputs"},{"from":"APIInput","to":"toAPIInputs","out":"*","in":""}],"nodes":[{"id":"output-3","type":"output","configuration":{"schema":{"type":"object","properties":{"api_json_response":{"title":"response","description":"The response from the fetch request","type":["string","object"]}},"required":["api_json_response"]}}},{"id":"fetch-4","type":"fetch","configuration":{}},{"id":"createFetchParameters","type":"invoke","configuration":{"path":"#createFetchParameters"}},{"id":"input-1","type":"input","configuration":{"schema":{"type":"object","properties":{"graph":{"title":"graph","description":"The graph descriptor of the board to invoke.","type":"object"},"item":{"type":"string","title":"item"}},"required":["graph","item"]}}},{"id":"toAPIInputs","type":"invoke","configuration":{"path":"#toAPIInputs"}},{"id":"APIInput","type":"invoke","configuration":{}}],"kits":[],"graphs":{"createFetchParameters":{"edges":[{"from":"createFetchParameters-input","to":"createFetchParameters-run","out":"*"},{"from":"createFetchParameters-run","to":"createFetchParameters-output","out":"*"}],"nodes":[{"id":"createFetchParameters-input","type":"input","configuration":{}},{"id":"createFetchParameters-run","type":"runJavascript","configuration":{"code":"function createFetchParameters({ item, api_inputs }) {\n            const { method, parameters, secrets, requestBody, info } = item;\n            let { url } = item;\n            const queryStringParameters = {};\n            if (typeof api_inputs == \"string\") {\n                api_inputs = JSON.parse(api_inputs);\n            }\n            if (parameters != undefined &&\n                parameters.length > 0 &&\n                api_inputs == undefined) {\n                throw new Error(`Missing input for parameters ${JSON.stringify(parameters)}`);\n            }\n            for (const param of parameters) {\n                if (api_inputs && param.name in api_inputs == false && param.required) {\n                    throw new Error(`Missing required parameter ${param.name}`);\n                }\n                if (api_inputs && param.name in api_inputs == false) {\n                    // Parameter is not required and not in input, so we can skip it.\n                    continue;\n                }\n                if (param.in == \"path\") {\n                    // Replace the path parameter with the value from the input.\n                    url = url.replace(`{${param.name}}`, api_inputs[param.name]);\n                }\n                if (param.in == \"query\") {\n                    queryStringParameters[param.name] = api_inputs[param.name];\n                }\n            }\n            // // If the method is POST or PUT, then we need to add the requestBody to the body.\n            // We are going to want to add in the secret somehow\n            const headers = {};\n            // Create the query string\n            const queryString = Object.entries(queryStringParameters)\n                .map(([key, value]) => {\n                return `${key}=${value}`;\n            })\n                .join(\"&\");\n            if (queryString.length > 0) {\n                url = `${url}?${queryString}`;\n            }\n            // Many APIs will require an authentication token but they don't define it in the Open API spec. If the user has provided a secret, we will use that.\n            if (secrets != undefined && secrets[1].scheme == \"bearer\") {\n                const envKey = `${item.info.title\n                    .replace(/[^a-zA-Z0-9]+/g, \"_\")\n                    .toUpperCase()}_KEY`;\n                const envValue = api_inputs[envKey];\n                headers[\"Authorization\"] = `Bearer ${envValue}`;\n            }\n            let body = undefined;\n            if (requestBody) {\n                // We know the method needs a request Body.\n                // Find the first input that matches the valid required schema of the API.\n                let requestContentType;\n                // We can only handle JSON\n                if (\"requestBody\" in api_inputs) {\n                    body =\n                        typeof api_inputs[\"requestBody\"] == \"string\"\n                            ? JSON.parse(api_inputs[\"requestBody\"])\n                            : api_inputs[\"requestBody\"];\n                    requestContentType = \"application/json\";\n                }\n                if (body == undefined) {\n                    throw new Error(`Missing required request body for ${JSON.stringify(requestBody)}`);\n                }\n                headers[\"Content-Type\"] = requestContentType;\n            }\n            return { url, method, headers, body, queryString };\n        }","name":"createFetchParameters","raw":true}},{"id":"createFetchParameters-output","type":"output","configuration":{}}]},"toAPIInputs":{"edges":[{"from":"toAPIInputs-input","to":"toAPIInputs-run","out":"*"},{"from":"toAPIInputs-run","to":"toAPIInputs-output","out":"*"}],"nodes":[{"id":"toAPIInputs-input","type":"input","configuration":{}},{"id":"toAPIInputs-run","type":"runJavascript","configuration":{"code":"function toAPIInputs(item) {\n            return { api_inputs: item };\n        }","name":"toAPIInputs","raw":true}},{"id":"toAPIInputs-output","type":"output","configuration":{}}]}},"args":{"item":{"operationId":"createChatCompletion","url":"https://api.mistral.ai/v1/chat/completions","method":"POST","summary":"Create Chat Completions","parameters":[],"requestBody":{"application/json":{"schema":{"type":"object","properties":{"model":{"description":"ID of the model to use. You can use the [List Available Models](/api#operation/listModels) API to see all of your available models, or see our [Model overview](/models) for model descriptions.\n","type":"string","example":"mistral-tiny"},"messages":{"description":"The prompt(s) to generate completions for, encoded as a list of dict with role and content. The first prompt role should be `user` or `system`.\n","type":"array","items":{"type":"object","properties":{"role":{"type":"string","enum":["system","user","assistant"]},"content":{"type":"string"}}},"example":[{"role":"user","content":"What is the best French cheese?"}]},"temperature":{"type":"number","minimum":0,"maximum":1,"default":0.7,"example":0.7,"nullable":true,"description":"What sampling temperature to use, between 0.0 and 1.0. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n\nWe generally recommend altering this or `top_p` but not both.\n"},"top_p":{"type":"number","minimum":0,"maximum":1,"default":1,"example":1,"nullable":true,"description":"Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n\nWe generally recommend altering this or `temperature` but not both.\n"},"max_tokens":{"type":"integer","minimum":0,"default":null,"example":16,"nullable":true,"description":"The maximum number of tokens to generate in the completion.\n\nThe token count of your prompt plus `max_tokens` cannot exceed the model's context length. \n"},"stream":{"type":"boolean","default":false,"nullable":true,"description":"Whether to stream back partial progress. If set, tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.\n"},"safe_prompt":{"type":"boolean","default":false,"description":"Whether to inject a safety prompt before all conversations.\n"},"random_seed":{"type":"integer","default":null,"description":"The seed to use for random sampling. If set, different calls will generate deterministic results.\n"}},"required":["model","messages"],"description":"Request POST data (format: application/json)"}}},"info":{"title":"Mistral AI API","description":"Chat Completion and Embeddings APIs","version":"0.0.1"}},"graph":{"title":"API Inputs for createChatCompletion","url":"#","nodes":[{"id":"output","type":"output"},{"id":"input-requestBody","type":"input","configuration":{"schema":{"type":"object","properties":{"requestBody":{"type":"object","title":"requestBody","description":"The request body for the API call (JSON)"}}}}}],"edges":[{"from":"input-requestBody","out":"requestBody","to":"output","in":"requestBody"}]}}}