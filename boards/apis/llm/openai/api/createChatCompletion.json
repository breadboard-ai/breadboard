{"edges":[{"from":"fetch-4","to":"output-3","out":"response","in":"api_json_response"},{"from":"createFetchParameters","to":"fetch-4","out":"*","in":""},{"from":"input-1","to":"APIInput","out":"*","in":""},{"from":"input-1","to":"APIInput","out":"graph","in":"graph"},{"from":"input-1","to":"createFetchParameters","out":"item","in":"item"},{"from":"toAPIInputs","to":"createFetchParameters","out":"api_inputs","in":"api_inputs"},{"from":"APIInput","to":"toAPIInputs","out":"*","in":""}],"nodes":[{"id":"output-3","type":"output","configuration":{"schema":{"type":"object","properties":{"api_json_response":{"title":"response","description":"The response from the fetch request","type":["string","object"]}},"required":["api_json_response"]}}},{"id":"fetch-4","type":"fetch","configuration":{}},{"id":"createFetchParameters","type":"invoke","configuration":{"path":"#createFetchParameters"}},{"id":"input-1","type":"input","configuration":{"schema":{"type":"object","properties":{"graph":{"title":"graph","description":"The graph descriptor of the board to invoke.","type":"object"},"item":{"type":"string","title":"item"}},"required":["graph","item"]}}},{"id":"toAPIInputs","type":"invoke","configuration":{"path":"#toAPIInputs"}},{"id":"APIInput","type":"invoke","configuration":{}}],"kits":[],"graphs":{"createFetchParameters":{"edges":[{"from":"createFetchParameters-input","to":"createFetchParameters-run","out":"*"},{"from":"createFetchParameters-run","to":"createFetchParameters-output","out":"*"}],"nodes":[{"id":"createFetchParameters-input","type":"input","configuration":{}},{"id":"createFetchParameters-run","type":"runJavascript","configuration":{"code":"function createFetchParameters({ item, api_inputs }) {\n            const { method, parameters, secrets, requestBody } = item;\n            let { url } = item;\n            const queryStringParameters = {};\n            if (typeof api_inputs == \"string\") {\n                api_inputs = JSON.parse(api_inputs);\n            }\n            if (parameters != undefined &&\n                parameters.length > 0 &&\n                api_inputs == undefined) {\n                throw new Error(`Missing input for parameters ${JSON.stringify(parameters)}`);\n            }\n            for (const param of parameters) {\n                if (api_inputs && param.name in api_inputs == false && param.required) {\n                    throw new Error(`Missing required parameter ${param.name}`);\n                }\n                if (api_inputs && param.name in api_inputs == false) {\n                    // Parameter is not required and not in input, so we can skip it.\n                    continue;\n                }\n                if (param.in == \"path\") {\n                    // Replace the path parameter with the value from the input.\n                    url = url.replace(`{${param.name}}`, api_inputs[param.name]);\n                }\n                if (param.in == \"query\") {\n                    queryStringParameters[param.name] = api_inputs[param.name];\n                }\n            }\n            // If the method is POST or PUT, then we need to add the requestBody to the body.\n            // We are going to want to add in the secret somehow\n            const headers = {};\n            // Create the query string\n            const queryString = Object.entries(queryStringParameters)\n                .map(([key, value]) => {\n                return `${key}=${value}`;\n            })\n                .join(\"&\");\n            if (queryString.length > 0) {\n                url = `${url}?${queryString}`;\n            }\n            // Many APIs will require an authentication token but they don't define it in the Open API spec.\n            if (secrets != undefined && secrets[1].scheme == \"bearer\") {\n                const envKey = `${item.info.title\n                    .replace(/[^a-zA-Z0-9]+/g, \"_\")\n                    .toUpperCase()}_KEY`;\n                const envValue = api_inputs[envKey];\n                headers[\"Authorization\"] = `Bearer ${envValue}`;\n            }\n            let body = undefined;\n            if (requestBody) {\n                // We know the method needs a request Body.\n                // Find the first input that matches the valid required schema of the API.\n                let requestContentType;\n                // We can only handle JSON\n                if (\"requestBody\" in api_inputs) {\n                    body =\n                        typeof api_inputs[\"requestBody\"] == \"string\"\n                            ? JSON.parse(api_inputs[\"requestBody\"])\n                            : api_inputs[\"requestBody\"];\n                    requestContentType = \"application/json\";\n                }\n                if (body == undefined) {\n                    throw new Error(`Missing required request body for ${JSON.stringify(requestBody)}`);\n                }\n                headers[\"Content-Type\"] = requestContentType;\n            }\n            return { url, method, headers, body, queryString };\n        }","name":"createFetchParameters","raw":true}},{"id":"createFetchParameters-output","type":"output","configuration":{}}]},"toAPIInputs":{"edges":[{"from":"toAPIInputs-input","to":"toAPIInputs-run","out":"*"},{"from":"toAPIInputs-run","to":"toAPIInputs-output","out":"*"}],"nodes":[{"id":"toAPIInputs-input","type":"input","configuration":{}},{"id":"toAPIInputs-run","type":"runJavascript","configuration":{"code":"function toAPIInputs(item) {\n            return { api_inputs: item };\n        }","name":"toAPIInputs","raw":true}},{"id":"toAPIInputs-output","type":"output","configuration":{}}]}},"args":{"item":{"operationId":"createChatCompletion","url":"https://api.openai.com/v1/chat/completions","method":"POST","summary":"Creates a model response for the given chat conversation.","parameters":[],"requestBody":{"application/json":{"schema":{"type":"object","properties":{"messages":{"description":"A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).","type":"array","minItems":1,"items":{"$ref":"#/components/schemas/ChatCompletionRequestMessage"}},"model":{"description":"ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API.","example":"gpt-3.5-turbo","anyOf":[{"type":"string"},{"type":"string","enum":["gpt-4-0125-preview","gpt-4-turbo-preview","gpt-4-1106-preview","gpt-4-vision-preview","gpt-4","gpt-4-0314","gpt-4-0613","gpt-4-32k","gpt-4-32k-0314","gpt-4-32k-0613","gpt-3.5-turbo","gpt-3.5-turbo-16k","gpt-3.5-turbo-0301","gpt-3.5-turbo-0613","gpt-3.5-turbo-1106","gpt-3.5-turbo-16k-0613"]}],"x-oaiTypeLabel":"string"},"frequency_penalty":{"type":"number","default":0,"minimum":-2,"maximum":2,"nullable":true,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n\n[See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)\n"},"logit_bias":{"type":"object","x-oaiTypeLabel":"map","default":null,"nullable":true,"additionalProperties":{"type":"integer"},"description":"Modify the likelihood of specified tokens appearing in the completion.\n\nAccepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n"},"logprobs":{"description":"Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`. This option is currently not available on the `gpt-4-vision-preview` model.","type":"boolean","default":false,"nullable":true},"top_logprobs":{"description":"An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.","type":"integer","minimum":0,"maximum":5,"nullable":true},"max_tokens":{"description":"The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.\n\nThe total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.\n","type":"integer","nullable":true},"n":{"type":"integer","minimum":1,"maximum":128,"default":1,"example":1,"nullable":true,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs."},"presence_penalty":{"type":"number","default":0,"minimum":-2,"maximum":2,"nullable":true,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n\n[See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)\n"},"response_format":{"type":"object","description":"An object specifying the format that the model must output. Compatible with [GPT-4 Turbo](/docs/models/gpt-4-and-gpt-4-turbo) and `gpt-3.5-turbo-1106`.\n\nSetting to `{ \"type\": \"json_object\" }` enables JSON mode, which guarantees the message the model generates is valid JSON.\n\n**Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if `finish_reason=\"length\"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.\n","properties":{"type":{"type":"string","enum":["text","json_object"],"example":"json_object","default":"text","description":"Must be one of `text` or `json_object`."}}},"seed":{"type":"integer","minimum":-9223372036854776000,"maximum":9223372036854776000,"nullable":true,"description":"This feature is in Beta.\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.\nDeterminism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.\n","x-oaiMeta":{"beta":true}},"stop":{"description":"Up to 4 sequences where the API will stop generating further tokens.\n","default":null,"oneOf":[{"type":"string","nullable":true},{"type":"array","minItems":1,"maxItems":4,"items":{"type":"string"}}]},"stream":{"description":"If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\n","type":"boolean","nullable":true,"default":false},"temperature":{"type":"number","minimum":0,"maximum":2,"default":1,"example":1,"nullable":true,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n\nWe generally recommend altering this or `top_p` but not both.\n"},"top_p":{"type":"number","minimum":0,"maximum":1,"default":1,"example":1,"nullable":true,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n\nWe generally recommend altering this or `temperature` but not both.\n"},"tools":{"type":"array","description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for.\n","items":{"$ref":"#/components/schemas/ChatCompletionTool"}},"tool_choice":{"$ref":"#/components/schemas/ChatCompletionToolChoiceOption"},"user":{"type":"string","example":"user-1234","description":"A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).\n"},"function_call":{"deprecated":true,"description":"Deprecated in favor of `tool_choice`.\n\nControls which (if any) function is called by the model.\n`none` means the model will not call a function and instead generates a message.\n`auto` means the model can pick between generating a message or calling a function.\nSpecifying a particular function via `{\"name\": \"my_function\"}` forces the model to call that function.\n\n`none` is the default when no functions are present. `auto` is the default if functions are present.\n","oneOf":[{"type":"string","description":"`none` means the model will not call a function and instead generates a message. `auto` means the model can pick between generating a message or calling a function.\n","enum":["none","auto"]},{"$ref":"#/components/schemas/ChatCompletionFunctionCallOption"}],"x-oaiExpandable":true},"functions":{"deprecated":true,"description":"Deprecated in favor of `tools`.\n\nA list of functions the model may generate JSON inputs for.\n","type":"array","minItems":1,"maxItems":128,"items":{"$ref":"#/components/schemas/ChatCompletionFunctions"}}},"required":["model","messages"],"description":"Request POST data (format: application/json)"}}},"secrets":["ApiKeyAuth",{"type":"http","scheme":"bearer"}],"info":{"title":"OpenAI API","description":"The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.","version":"2.0.0","termsOfService":"https://openai.com/policies/terms-of-use","contact":{"name":"OpenAI Support","url":"https://help.openai.com/"},"license":{"name":"MIT","url":"https://github.com/openai/openai-openapi/blob/master/LICENSE"}}},"graph":{"title":"API Inputs for createChatCompletion","url":"#","nodes":[{"id":"input","type":"input","configuration":{"schema":{"type":"object","properties":{}}}},{"id":"output","type":"output"},{"id":"input-requestBody","type":"input","configuration":{"schema":{"type":"object","properties":{"requestBody":{"type":"object","title":"requestBody","description":"The request body for the API call (JSON)"}}}}},{"id":"input-secrets","type":"secrets","configuration":{"keys":["OPENAI_API_KEY"]}}],"edges":[{"from":"input-requestBody","out":"requestBody","to":"output","in":"requestBody"},{"from":"input-secrets","out":"OPENAI_API_KEY","to":"output","in":"OPENAI_API_KEY"}]}}}