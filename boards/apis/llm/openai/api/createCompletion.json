{"edges":[{"from":"fetch-4","to":"output-3","out":"response","in":"api_json_response"},{"from":"createFetchParameters","to":"fetch-4","out":"*","in":""},{"from":"input-1","to":"APIInput","out":"*","in":""},{"from":"input-1","to":"APIInput","out":"graph","in":"graph"},{"from":"input-1","to":"createFetchParameters","out":"item","in":"item"},{"from":"toAPIInputs","to":"createFetchParameters","out":"api_inputs","in":"api_inputs"},{"from":"APIInput","to":"toAPIInputs","out":"*","in":""}],"nodes":[{"id":"output-3","type":"output","configuration":{"schema":{"type":"object","properties":{"api_json_response":{"title":"response","description":"The response from the fetch request","type":["string","object"]}},"required":["api_json_response"]}}},{"id":"fetch-4","type":"fetch","configuration":{}},{"id":"createFetchParameters","type":"invoke","configuration":{"path":"#createFetchParameters"}},{"id":"input-1","type":"input","configuration":{"schema":{"type":"object","properties":{"graph":{"title":"graph","description":"The graph descriptor of the board to invoke.","type":"object"},"item":{"type":"string","title":"item"}},"required":["graph","item"]}}},{"id":"toAPIInputs","type":"invoke","configuration":{"path":"#toAPIInputs"}},{"id":"APIInput","type":"invoke","configuration":{}}],"kits":[],"graphs":{"createFetchParameters":{"edges":[{"from":"createFetchParameters-input","to":"createFetchParameters-run","out":"*"},{"from":"createFetchParameters-run","to":"createFetchParameters-output","out":"*"}],"nodes":[{"id":"createFetchParameters-input","type":"input","configuration":{}},{"id":"createFetchParameters-run","type":"runJavascript","configuration":{"code":"function createFetchParameters({ item, api_inputs }) {\n            const { method, parameters, secrets, requestBody } = item;\n            let { url } = item;\n            const queryStringParameters = {};\n            if (typeof api_inputs == \"string\") {\n                api_inputs = JSON.parse(api_inputs);\n            }\n            if (parameters != undefined &&\n                parameters.length > 0 &&\n                api_inputs == undefined) {\n                throw new Error(`Missing input for parameters ${JSON.stringify(parameters)}`);\n            }\n            for (const param of parameters) {\n                if (api_inputs && param.name in api_inputs == false && param.required) {\n                    throw new Error(`Missing required parameter ${param.name}`);\n                }\n                if (api_inputs && param.name in api_inputs == false) {\n                    // Parameter is not required and not in input, so we can skip it.\n                    continue;\n                }\n                if (param.in == \"path\") {\n                    // Replace the path parameter with the value from the input.\n                    url = url.replace(`{${param.name}}`, api_inputs[param.name]);\n                }\n                if (param.in == \"query\") {\n                    queryStringParameters[param.name] = api_inputs[param.name];\n                }\n            }\n            // If the method is POST or PUT, then we need to add the requestBody to the body.\n            // We are going to want to add in the secret somehow\n            const headers = {};\n            // Create the query string\n            const queryString = Object.entries(queryStringParameters)\n                .map(([key, value]) => {\n                return `${key}=${value}`;\n            })\n                .join(\"&\");\n            if (queryString.length > 0) {\n                url = `${url}?${queryString}`;\n            }\n            // Many APIs will require an authentication token but they don't define it in the Open API spec.\n            if (secrets != undefined && secrets[1].scheme == \"bearer\") {\n                const envKey = `${item.info.title\n                    .replace(/[^a-zA-Z0-9]+/g, \"_\")\n                    .toUpperCase()}_KEY`;\n                const envValue = api_inputs[envKey];\n                headers[\"Authorization\"] = `Bearer ${envValue}`;\n            }\n            let body = undefined;\n            if (requestBody) {\n                // We know the method needs a request Body.\n                // Find the first input that matches the valid required schema of the API.\n                let requestContentType;\n                // We can only handle JSON\n                if (\"requestBody\" in api_inputs) {\n                    body =\n                        typeof api_inputs[\"requestBody\"] == \"string\"\n                            ? JSON.parse(api_inputs[\"requestBody\"])\n                            : api_inputs[\"requestBody\"];\n                    requestContentType = \"application/json\";\n                }\n                if (body == undefined) {\n                    throw new Error(`Missing required request body for ${JSON.stringify(requestBody)}`);\n                }\n                headers[\"Content-Type\"] = requestContentType;\n            }\n            return { url, method, headers, body, queryString };\n        }","name":"createFetchParameters","raw":true}},{"id":"createFetchParameters-output","type":"output","configuration":{}}]},"toAPIInputs":{"edges":[{"from":"toAPIInputs-input","to":"toAPIInputs-run","out":"*"},{"from":"toAPIInputs-run","to":"toAPIInputs-output","out":"*"}],"nodes":[{"id":"toAPIInputs-input","type":"input","configuration":{}},{"id":"toAPIInputs-run","type":"runJavascript","configuration":{"code":"function toAPIInputs(item) {\n            return { api_inputs: item };\n        }","name":"toAPIInputs","raw":true}},{"id":"toAPIInputs-output","type":"output","configuration":{}}]}},"args":{"item":{"operationId":"createCompletion","url":"https://api.openai.com/v1/completions","method":"POST","summary":"Creates a completion for the provided prompt and parameters.","parameters":[],"requestBody":{"application/json":{"schema":{"type":"object","properties":{"model":{"description":"ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](/docs/models/overview) for descriptions of them.\n","anyOf":[{"type":"string"},{"type":"string","enum":["gpt-3.5-turbo-instruct","davinci-002","babbage-002"]}],"x-oaiTypeLabel":"string"},"prompt":{"description":"The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.\n\nNote that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document.\n","default":"<|endoftext|>","nullable":true,"oneOf":[{"type":"string","default":"","example":"This is a test."},{"type":"array","items":{"type":"string","default":"","example":"This is a test."}},{"type":"array","minItems":1,"items":{"type":"integer"},"example":"[1212, 318, 257, 1332, 13]"},{"type":"array","minItems":1,"items":{"type":"array","minItems":1,"items":{"type":"integer"}},"example":"[[1212, 318, 257, 1332, 13]]"}]},"best_of":{"type":"integer","default":1,"minimum":0,"maximum":20,"nullable":true,"description":"Generates `best_of` completions server-side and returns the \"best\" (the one with the highest log probability per token). Results cannot be streamed.\n\nWhen used with `n`, `best_of` controls the number of candidate completions and `n` specifies how many to return â€“ `best_of` must be greater than `n`.\n\n**Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.\n"},"echo":{"type":"boolean","default":false,"nullable":true,"description":"Echo back the prompt in addition to the completion\n"},"frequency_penalty":{"type":"number","default":0,"minimum":-2,"maximum":2,"nullable":true,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n\n[See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)\n"},"logit_bias":{"type":"object","x-oaiTypeLabel":"map","default":null,"nullable":true,"additionalProperties":{"type":"integer"},"description":"Modify the likelihood of specified tokens appearing in the completion.\n\nAccepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n\nAs an example, you can pass `{\"50256\": -100}` to prevent the <|endoftext|> token from being generated.\n"},"logprobs":{"type":"integer","minimum":0,"maximum":5,"default":null,"nullable":true,"description":"Include the log probabilities on the `logprobs` most likely output tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.\n\nThe maximum value for `logprobs` is 5.\n"},"max_tokens":{"type":"integer","minimum":0,"default":16,"example":16,"nullable":true,"description":"The maximum number of [tokens](/tokenizer) that can be generated in the completion.\n\nThe token count of your prompt plus `max_tokens` cannot exceed the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.\n"},"n":{"type":"integer","minimum":1,"maximum":128,"default":1,"example":1,"nullable":true,"description":"How many completions to generate for each prompt.\n\n**Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.\n"},"presence_penalty":{"type":"number","default":0,"minimum":-2,"maximum":2,"nullable":true,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n\n[See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)\n"},"seed":{"type":"integer","minimum":-9223372036854776000,"maximum":9223372036854776000,"nullable":true,"description":"If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.\n\nDeterminism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.\n"},"stop":{"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\n","default":null,"nullable":true,"oneOf":[{"type":"string","default":"<|endoftext|>","example":"\n","nullable":true},{"type":"array","minItems":1,"maxItems":4,"items":{"type":"string","example":"[\"\\n\"]"}}]},"stream":{"description":"Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\n","type":"boolean","nullable":true,"default":false},"suffix":{"description":"The suffix that comes after a completion of inserted text.","default":null,"nullable":true,"type":"string","example":"test."},"temperature":{"type":"number","minimum":0,"maximum":2,"default":1,"example":1,"nullable":true,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n\nWe generally recommend altering this or `top_p` but not both.\n"},"top_p":{"type":"number","minimum":0,"maximum":1,"default":1,"example":1,"nullable":true,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n\nWe generally recommend altering this or `temperature` but not both.\n"},"user":{"type":"string","example":"user-1234","description":"A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).\n"}},"required":["model","prompt"],"description":"Request POST data (format: application/json)"}}},"secrets":["ApiKeyAuth",{"type":"http","scheme":"bearer"}],"info":{"title":"OpenAI API","description":"The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.","version":"2.0.0","termsOfService":"https://openai.com/policies/terms-of-use","contact":{"name":"OpenAI Support","url":"https://help.openai.com/"},"license":{"name":"MIT","url":"https://github.com/openai/openai-openapi/blob/master/LICENSE"}}},"graph":{"title":"API Inputs for createCompletion","url":"#","nodes":[{"id":"input","type":"input","configuration":{"schema":{"type":"object","properties":{}}}},{"id":"output","type":"output"},{"id":"input-requestBody","type":"input","configuration":{"schema":{"type":"object","properties":{"requestBody":{"type":"object","title":"requestBody","description":"The request body for the API call (JSON)"}}}}},{"id":"input-secrets","type":"secrets","configuration":{"keys":["OPENAI_API_KEY"]}}],"edges":[{"from":"input-requestBody","out":"requestBody","to":"output","in":"requestBody"},{"from":"input-secrets","out":"OPENAI_API_KEY","to":"output","in":"OPENAI_API_KEY"}]}}}