{
  "title": "Autonaming",
  "description": "",
  "version": "0.0.1",
  "nodes": [
    {
      "id": "input",
      "type": "input",
      "metadata": {
        "title": "Input"
      }
    },
    {
      "id": "run-module",
      "type": "runModule",
      "configuration": {
        "$module": "main"
      },
      "metadata": {
        "title": "Autonaming"
      }
    },
    {
      "id": "output",
      "type": "output",
      "metadata": {
        "title": "Output"
      }
    }
  ],
  "edges": [
    {
      "from": "input",
      "to": "run-module",
      "out": "*",
      "in": ""
    },
    {
      "from": "run-module",
      "to": "output",
      "out": "*",
      "in": ""
    }
  ],
  "main": "main",
  "modules": {
    "main": {
      "code": "/**\n * @fileoverview Add a description for your module here.\n */\nimport { GeminiPrompt } from \"./gemini-prompt\";\nimport { defaultSafetySettings } from \"./gemini\";\nimport { err, ok, llm } from \"./utils\";\nexport { invoke as default, describe };\nconst NODES_RESULT_SCHEMA = {\n    nodes: {\n        type: \"array\",\n        items: {\n            type: \"object\",\n            properties: {\n                id: {\n                    type: \"string\",\n                    description: \"node id (the value of the 'id' property)\",\n                },\n                currentTitle: {\n                    type: \"string\",\n                    description: \"Current title of the node\",\n                },\n                suggestedTitle: {\n                    type: \"string\",\n                    description: \"Suggested title of the node, verb-first, action oriented\",\n                },\n                suggestedDescription: {\n                    type: \"string\",\n                    description: \"Suggested description of the node\",\n                },\n                reasoning: {\n                    type: \"string\",\n                    description: \"The reasoning behind the suggested change\",\n                },\n                note: {\n                    type: \"string\",\n                    description: \"Additional notes, if any\",\n                },\n            },\n            required: [\n                \"currentTitle\",\n                \"suggestedTitle\",\n                \"suggestedDescription\",\n                \"reasoning\",\n            ],\n        },\n    },\n};\nfunction resultSchema(hasNodes) {\n    return {\n        type: \"object\",\n        properties: {\n            analysis: {\n                type: \"string\",\n                description: \"Overall analysis of the JSON structure\",\n            },\n            suggestions: {\n                type: \"object\",\n                description: \"The naming suggestions and the reasoning behind them\",\n                properties: {\n                    graph: {\n                        type: \"object\",\n                        description: \"Suggestions for the graph title and description\",\n                        properties: {\n                            currentTitle: {\n                                type: \"string\",\n                                description: \"Current title of the graph\",\n                            },\n                            currentTitleAndDescriptionMatch: {\n                                type: \"number\",\n                                description: \"How much the curent title and description match the graph structure\" +\n                                    \"already (0 - no match at all, 100 - perfect match)\",\n                            },\n                            suggestedTitle: {\n                                type: \"string\",\n                                description: \"Suggested title of the graph\",\n                            },\n                            suggestedDescription: {\n                                type: \"string\",\n                                description: \"Suggested description of the graph\",\n                            },\n                            reasoning: {\n                                type: \"string\",\n                                description: \"The reasoning behind the suggested change\",\n                            },\n                            note: {\n                                type: \"string\",\n                                description: \"Additional notes, if any\",\n                            },\n                        },\n                        required: [\"currentTitle\", \"suggestedTitle\", \"reasoning\"],\n                    },\n                    ...(hasNodes ? NODES_RESULT_SCHEMA : {}),\n                    rationaleForChanges: {\n                        type: \"string\",\n                        description: \"The rationale for changes -- how and why do they improve the graph?\",\n                    },\n                    otherConsiderations: {\n                        type: \"string\",\n                        description: \"Other considerations and items of note related to the graph and the suggested changes\",\n                    },\n                },\n                required: [\"graph\", ...(hasNodes ? [\"nodes\"] : [])],\n            },\n        },\n    };\n}\nfunction getArguments(context) {\n    const part = context?.at(-1)?.parts?.at(0);\n    if (!(part && \"json\" in part)) {\n        return err(`Invalid input arguments`);\n    }\n    return part.json;\n}\nconst FOR_NODES = `For node titles and descriptions:\n- Either or both the title and description may be completely stale and no longer relevant. \n  Use what is in \"configuration\" property to suggest new titles/descriptions and not anchor on\n  the title/description current values.\n- When creating a title or description for a node, make sure to only describe what this nodes does\n  and avoid adding context from other nodes, because the other nodes may change over time. We want\n  to ensure that the node's title and description stay independent of other nodes.`;\nasync function invoke({ context }) {\n    // TODO: Make the autonamer aware of \"no-nodes\" renames.\n    const args = getArguments(context);\n    if (!ok(args))\n        return args;\n    console.log(\"ARGS\", args);\n    const { graph, graphId, nodes } = args;\n    // Reset title and description for graph, to reduce the chance of\n    // LLM anchoring on old title.\n    // graph.title = \"Untitled Graph\";\n    // graph.description = \"\";\n    // Remove any intent/instruction bits that might have been in the BGL.\n    if (graph.metadata) {\n        delete graph.metadata.instruction;\n        delete graph.metadata.intent;\n    }\n    // TODO: For subgraphs, add a clause about \"context\" input,\n    // so that the description is useful for the Introducer.\n    let graphTitleInstruction = \"Each title must be short and to the point\";\n    if (graphId) {\n        graph.parameters = {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"string\",\n                },\n            },\n        };\n        graphTitleInstruction =\n            \"Each title must be verb-first, action oriented, short and to the point\";\n    }\n    const nodeMap = new Map(graph.nodes.map((node) => {\n        return [node.id, node];\n    }));\n    const finalNodes = [];\n    for (const id of nodes) {\n        const descriptor = nodeMap.get(id);\n        if (!descriptor ||\n            !descriptor.configuration ||\n            Object.keys(descriptor.configuration).length === 0 ||\n            descriptor.metadata?.userModified) {\n            continue;\n        }\n        finalNodes.push(id);\n        // Reset titles and descriptions of affected nodes to reduce\n        // LLM anchoring on old title.\n        if (!descriptor.metadata)\n            continue;\n        descriptor.metadata.title = \"Untitled Node\";\n        descriptor.metadata.description = \"\";\n    }\n    const hasNodes = finalNodes.length > 0;\n    const alsoNodes = hasNodes\n        ? `, \nand the nodes with these Ids:\n\n${finalNodes.map((id) => {\n            return `- ${id}`;\n        })}\n`\n        : \"\";\n    const naming = await new GeminiPrompt({\n        body: {\n            contents: [\n                llm `\nAnalyze the provided JSON structure below and provide suggestions for titles and descriptions for graph the itself${alsoNodes}\n\nWhen analyzing, make sure to evaluate how much the current title/description match the structure of the graph already,\nwith 0 being no match at all, and 100 being a perfect match.\n\nGraph:\n\n\\`\\`\\`json\n${JSON.stringify(args.graph)}\n\\`\\`\\`\n\nImportant:\n- Both the title and description must be accurate and specific. Avoid making them generic or general-purpose.\n- Either or both the title and description may be completely stale and no longer relevant. When suggesting new graph title/description,\n  rely on the graph structure and contents, not the current title/description values.\n- The descriptions must be detailed, yet concise, suitable for a JSON schema description used in LLM function calls. If provided, use the\n  \"parameters\" property describe the input parameters of the function call.\n- ${graphTitleInstruction}, so that it could be visible even in a small space on mobile UI.\n- The audience who will look at the titles and descriptions will be non-technical, so avoid jargon and \n  don't mention graphs or JSON, etc.\n\n${hasNodes ? FOR_NODES : \"\"}\n\n`.asContent(),\n            ],\n            safetySettings: defaultSafetySettings(),\n            generationConfig: {\n                responseMimeType: \"application/json\",\n                responseSchema: resultSchema(hasNodes),\n            },\n        },\n    }).invoke();\n    if (!ok(naming))\n        return naming;\n    return { context: naming.all };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "main",
        "source": {
          "code": "/**\n * @fileoverview Add a description for your module here.\n */\n\nimport { GeminiPrompt } from \"./gemini-prompt\";\nimport { type GeminiSchema, defaultSafetySettings } from \"./gemini\";\nimport { err, ok, llm } from \"./utils\";\n\nexport { invoke as default, describe };\n\nexport type AutonamingResult = {\n  analysis: string;\n  suggestions: {\n    graph: {\n      currentTitle: string;\n      currentTitleAndDescriptionMatch: number;\n      suggestedTitle: string;\n      suggestedDescription: string;\n      reasoning: string;\n      note?: string;\n    };\n    nodes: {\n      id: string;\n      currentTitle: string;\n      suggestedTitle: string;\n      suggestedDescription: string;\n      reasoning: string;\n      note?: string;\n    }[];\n  };\n  rationaleForChanges: string;\n  otherConsiderations?: string;\n};\n\ntype Inputs = {\n  context: LLMContent[];\n};\n\ntype Outputs = {\n  context: LLMContent[];\n};\n\ntype PartialNodeDescriptor = {\n  id: string;\n  configuration: Record<string, object>;\n  metadata?: {\n    title?: string;\n    description?: string;\n    userModified?: boolean;\n  };\n};\n\nconst NODES_RESULT_SCHEMA: GeminiSchema[\"properties\"] = {\n  nodes: {\n    type: \"array\",\n    items: {\n      type: \"object\",\n      properties: {\n        id: {\n          type: \"string\",\n          description: \"node id (the value of the 'id' property)\",\n        },\n        currentTitle: {\n          type: \"string\",\n          description: \"Current title of the node\",\n        },\n        suggestedTitle: {\n          type: \"string\",\n          description:\n            \"Suggested title of the node, verb-first, action oriented\",\n        },\n        suggestedDescription: {\n          type: \"string\",\n          description: \"Suggested description of the node\",\n        },\n        reasoning: {\n          type: \"string\",\n          description: \"The reasoning behind the suggested change\",\n        },\n        note: {\n          type: \"string\",\n          description: \"Additional notes, if any\",\n        },\n      },\n      required: [\n        \"currentTitle\",\n        \"suggestedTitle\",\n        \"suggestedDescription\",\n        \"reasoning\",\n      ],\n    },\n  },\n};\n\nfunction resultSchema(hasNodes: boolean): GeminiSchema {\n  return {\n    type: \"object\",\n    properties: {\n      analysis: {\n        type: \"string\",\n        description: \"Overall analysis of the JSON structure\",\n      },\n      suggestions: {\n        type: \"object\",\n        description: \"The naming suggestions and the reasoning behind them\",\n        properties: {\n          graph: {\n            type: \"object\",\n            description: \"Suggestions for the graph title and description\",\n            properties: {\n              currentTitle: {\n                type: \"string\",\n                description: \"Current title of the graph\",\n              },\n              currentTitleAndDescriptionMatch: {\n                type: \"number\",\n                description:\n                  \"How much the curent title and description match the graph structure\" +\n                  \"already (0 - no match at all, 100 - perfect match)\",\n              },\n              suggestedTitle: {\n                type: \"string\",\n                description: \"Suggested title of the graph\",\n              },\n              suggestedDescription: {\n                type: \"string\",\n                description: \"Suggested description of the graph\",\n              },\n              reasoning: {\n                type: \"string\",\n                description: \"The reasoning behind the suggested change\",\n              },\n              note: {\n                type: \"string\",\n                description: \"Additional notes, if any\",\n              },\n            },\n            required: [\"currentTitle\", \"suggestedTitle\", \"reasoning\"],\n          },\n          ...(hasNodes ? NODES_RESULT_SCHEMA : {}),\n          rationaleForChanges: {\n            type: \"string\",\n            description:\n              \"The rationale for changes -- how and why do they improve the graph?\",\n          },\n          otherConsiderations: {\n            type: \"string\",\n            description:\n              \"Other considerations and items of note related to the graph and the suggested changes\",\n          },\n        },\n        required: [\"graph\", ...(hasNodes ? [\"nodes\"] : [])],\n      },\n    },\n  };\n}\n\n/**\n * Represents an edge in a graph.\n */\nexport type Edge = {\n  /**\n   * The node that the edge is coming from.\n   */\n  from: string;\n\n  /**\n   * The node that the edge is going to.\n   */\n  to: string;\n\n  /**\n   * The input of the `to` node. If this value is undefined, then\n   * the then no data is passed as output of the `from` node.\n   */\n  in?: string;\n\n  /**\n   * The output of the `from` node. If this value is \"*\", then all outputs\n   * of the `from` node are passed to the `to` node. If this value is undefined,\n   * then no data is passed to any inputs of the `to` node.\n   */\n  out?: string;\n\n  /**\n   * If true, this edge is optional: the data that passes through it is not\n   * considered a required input to the node.\n   */\n  optional?: boolean;\n\n  /**\n   * If true, this edge acts as a constant: the data that passes through it\n   * remains available even after the node has consumed it.\n   */\n  constant?: boolean;\n};\n\ntype PartialGraphDescriptor = {\n  title?: string;\n  metadata?: {\n    intent?: string;\n    instruction?: string;\n    userModified?: string;\n  };\n  parameters?: {\n    type: \"object\";\n    properties: {\n      context: {\n        type: \"string\";\n      };\n    };\n  };\n  description?: string;\n  nodes: PartialNodeDescriptor[];\n  edges: Edge[];\n};\n\ntype Arguments = {\n  graph: PartialGraphDescriptor;\n  graphId: string;\n  nodes: string[];\n};\n\nfunction getArguments(context?: LLMContent[]): Outcome<Arguments> {\n  const part = context?.at(-1)?.parts?.at(0);\n  if (!(part && \"json\" in part)) {\n    return err(`Invalid input arguments`);\n  }\n  return part.json as Arguments;\n}\n\nconst FOR_NODES = `For node titles and descriptions:\n- Either or both the title and description may be completely stale and no longer relevant. \n  Use what is in \"configuration\" property to suggest new titles/descriptions and not anchor on\n  the title/description current values.\n- When creating a title or description for a node, make sure to only describe what this nodes does\n  and avoid adding context from other nodes, because the other nodes may change over time. We want\n  to ensure that the node's title and description stay independent of other nodes.`;\n\nasync function invoke({ context }: Inputs): Promise<Outcome<Outputs>> {\n  // TODO: Make the autonamer aware of \"no-nodes\" renames.\n\n  const args = getArguments(context);\n  if (!ok(args)) return args;\n  console.log(\"ARGS\", args);\n  const { graph, graphId, nodes } = args;\n\n  // Reset title and description for graph, to reduce the chance of\n  // LLM anchoring on old title.\n  // graph.title = \"Untitled Graph\";\n  // graph.description = \"\";\n\n  // Remove any intent/instruction bits that might have been in the BGL.\n  if (graph.metadata) {\n    delete graph.metadata.instruction;\n    delete graph.metadata.intent;\n  }\n\n  // TODO: For subgraphs, add a clause about \"context\" input,\n  // so that the description is useful for the Introducer.\n\n  let graphTitleInstruction = \"Each title must be short and to the point\";\n\n  if (graphId) {\n    graph.parameters = {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"string\",\n        },\n      },\n    };\n    graphTitleInstruction =\n      \"Each title must be verb-first, action oriented, short and to the point\";\n  }\n\n  const nodeMap = new Map(\n    graph.nodes.map((node) => {\n      return [node.id, node];\n    })\n  );\n  const finalNodes: string[] = [];\n  for (const id of nodes) {\n    const descriptor = nodeMap.get(id);\n    if (\n      !descriptor ||\n      !descriptor.configuration ||\n      Object.keys(descriptor.configuration).length === 0 ||\n      descriptor.metadata?.userModified\n    ) {\n      continue;\n    }\n    finalNodes.push(id);\n    // Reset titles and descriptions of affected nodes to reduce\n    // LLM anchoring on old title.\n    if (!descriptor.metadata) continue;\n    descriptor.metadata.title = \"Untitled Node\";\n    descriptor.metadata.description = \"\";\n  }\n\n  const hasNodes = finalNodes.length > 0;\n\n  const alsoNodes = hasNodes\n    ? `, \nand the nodes with these Ids:\n\n${finalNodes.map((id) => {\n  return `- ${id}`;\n})}\n`\n    : \"\";\n\n  const naming = await new GeminiPrompt({\n    body: {\n      contents: [\n        llm`\nAnalyze the provided JSON structure below and provide suggestions for titles and descriptions for graph the itself${alsoNodes}\n\nWhen analyzing, make sure to evaluate how much the current title/description match the structure of the graph already,\nwith 0 being no match at all, and 100 being a perfect match.\n\nGraph:\n\n\\`\\`\\`json\n${JSON.stringify(args.graph)}\n\\`\\`\\`\n\nImportant:\n- Both the title and description must be accurate and specific. Avoid making them generic or general-purpose.\n- Either or both the title and description may be completely stale and no longer relevant. When suggesting new graph title/description,\n  rely on the graph structure and contents, not the current title/description values.\n- The descriptions must be detailed, yet concise, suitable for a JSON schema description used in LLM function calls. If provided, use the\n  \"parameters\" property describe the input parameters of the function call.\n- ${graphTitleInstruction}, so that it could be visible even in a small space on mobile UI.\n- The audience who will look at the titles and descriptions will be non-technical, so avoid jargon and \n  don't mention graphs or JSON, etc.\n\n${hasNodes ? FOR_NODES : \"\"}\n\n`.asContent(),\n      ],\n      safetySettings: defaultSafetySettings(),\n      generationConfig: {\n        responseMimeType: \"application/json\",\n        responseSchema: resultSchema(hasNodes),\n      },\n    },\n  }).invoke();\n  if (!ok(naming)) return naming;\n  return { context: naming.all };\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Add a description for your module here.",
        "runnable": true
      }
    },
    "common": {
      "code": "/**\n * @fileoverview Common types and code\n */\n",
      "metadata": {
        "title": "common",
        "source": {
          "code": "/**\n * @fileoverview Common types and code\n */\n\nexport type UserInput = LLMContent;\n\nexport type Params = {\n  [key: `p-z-${string}`]: unknown;\n};\n\nexport type DescriberResult = {\n  title?: string;\n  description?: string;\n  inputSchema?: Schema;\n  outputSchema?: Schema;\n};\n\nexport type AgentInputs = {\n  /**\n   * Whether (true) or not (false) the agent is allowed to chat with user.\n   */\n  chat: boolean;\n  /**\n   * The incoming conversation context.\n   */\n  context: LLMContent[];\n  /**\n   * Accumulated work context. This is the internal conversation, a result\n   * of talking with the user, for instance.\n   * This context is discarded at the end of interacting with the agent.\n   */\n  work: LLMContent[];\n  /**\n   * Agent's job description.\n   */\n  description?: LLMContent;\n  /**\n   * Last work product.\n   */\n  last?: LLMContent;\n  /**\n   * Type of the task.\n   */\n  type: \"introduction\" | \"work\";\n  /**\n   * The board URL of the model\n   */\n  model: string;\n  /**\n   * The default model that is passed along by the manager\n   */\n  defaultModel: string;\n  /**\n   * The tools that the worker can use\n   */\n  tools?: string[];\n  /**\n   * params\n   */\n  params: Params;\n};\n\nexport type AgentContext = AgentInputs & {\n  /**\n   * A unique identifier for the session.\n   * Currently used to have a persistent part separator across conversation context\n   */\n  id: string;\n  /**\n   * Accumulating list of user inputs\n   */\n  userInputs: UserInput[];\n  /**\n   * Indicator that the user ended chat.\n   */\n  userEndedChat: boolean;\n};\n\nexport type DescribeInputs = {\n  inputs: {\n    description: LLMContent | undefined;\n  };\n};\n",
          "language": "typescript"
        },
        "description": "Common types and code",
        "runnable": false
      }
    },
    "utils": {
      "code": "/**\n * @fileoverview Common utils for manipulating LLM Content and other relevant types.\n */\nexport { isLLMContent, isLLMContentArray, toLLMContent, toInlineData, toLLMContentInline, toText, contentToJSON, defaultLLMContent, endsWithRole, addUserTurn, isEmpty, llm, };\nexport { ok, err };\nfunction ok(o) {\n    return !(o && typeof o === \"object\" && \"$error\" in o);\n}\nfunction err($error) {\n    return { $error };\n}\nfunction mergeTextParts(parts) {\n    const merged = [];\n    let text = \"\";\n    for (const part of parts) {\n        if (\"text\" in part) {\n            text += part.text;\n        }\n        else {\n            if (text) {\n                merged.push({ text });\n            }\n            text = \"\";\n            merged.push(part);\n        }\n    }\n    if (text) {\n        merged.push({ text });\n    }\n    return merged;\n}\nclass LLMTemplate {\n    strings;\n    values;\n    constructor(strings, values) {\n        this.strings = strings;\n        this.values = values;\n    }\n    asParts() {\n        return mergeTextParts(this.strings.flatMap((s, i) => {\n            let text = s;\n            const value = this.values.at(i);\n            if (value == undefined) {\n                return { text };\n            }\n            else if (typeof value === \"string\") {\n                text += value;\n                return { text };\n            }\n            else if (value instanceof LLMTemplate) {\n                return value.asParts();\n            }\n            else if (isLLMContent(value)) {\n                return [{ text }, ...value.parts];\n            }\n            else {\n                text += JSON.stringify(value);\n                return { text };\n            }\n        }));\n    }\n    asContent() {\n        const parts = this.asParts();\n        return { parts, role: \"user\" };\n    }\n}\nfunction llm(strings, ...values) {\n    return new LLMTemplate(strings, values);\n}\n/**\n * Copied from @google-labs/breadboard\n */\nfunction isLLMContent(nodeValue) {\n    if (typeof nodeValue !== \"object\" || !nodeValue)\n        return false;\n    if (nodeValue === null || nodeValue === undefined)\n        return false;\n    if (\"role\" in nodeValue && nodeValue.role === \"$metadata\") {\n        return true;\n    }\n    return \"parts\" in nodeValue && Array.isArray(nodeValue.parts);\n}\nfunction isLLMContentArray(nodeValue) {\n    if (!Array.isArray(nodeValue))\n        return false;\n    if (nodeValue.length === 0)\n        return true;\n    return isLLMContent(nodeValue.at(-1));\n}\nfunction toLLMContent(text, role = \"user\") {\n    return { parts: [{ text }], role };\n}\nfunction endsWithRole(c, role) {\n    const last = c.at(-1);\n    if (!last)\n        return false;\n    return last.role === role;\n}\nfunction isEmpty(c) {\n    if (!c.parts.length)\n        return true;\n    for (const part of c.parts) {\n        if (\"text\" in part) {\n            if (part.text.trim().length > 0)\n                return false;\n        }\n        else {\n            return false;\n        }\n    }\n    return true;\n}\nfunction toText(c) {\n    if (isLLMContent(c)) {\n        return contentToText(c);\n    }\n    const last = c.at(-1);\n    if (!last)\n        return \"\";\n    return contentToText(last).trim();\n    function contentToText(content) {\n        return content.parts\n            .map((part) => (\"text\" in part ? part.text : \"\"))\n            .join(\"\\n\\n\");\n    }\n}\nfunction contentToJSON(content) {\n    const part = content?.parts?.at(0);\n    if (!part || !(\"text\" in part)) {\n        throw new Error(\"Invalid response from Gemini\");\n    }\n    return JSON.parse(part.text);\n}\nfunction defaultLLMContent() {\n    return JSON.stringify({\n        parts: [{ text: \"\" }],\n        role: \"user\",\n    });\n}\nfunction addUserTurn(content, context) {\n    context ??= [];\n    const isString = typeof content === \"string\";\n    if (!endsWithRole(context, \"user\")) {\n        return [...context, isString ? toLLMContent(content) : content];\n    }\n    const last = context.at(-1);\n    if (isString) {\n        last.parts.push({ text: content });\n    }\n    else {\n        last.parts.push(...content.parts);\n    }\n    return context;\n}\nfunction toLLMContentInline(mimetype, value, role = \"user\") {\n    return {\n        parts: [\n            {\n                inlineData: {\n                    mimeType: mimetype,\n                    data: value,\n                },\n            },\n        ],\n        role,\n    };\n}\nfunction toInlineData(c) {\n    if (isLLMContent(c)) {\n        return contentToInlineData(c);\n    }\n    const last = c.at(-1);\n    if (!last)\n        return null;\n    return contentToInlineData(last);\n    function contentToInlineData(content) {\n        const part = content.parts.at(-1);\n        if (!part)\n            return \"\";\n        return \"inlineData\" in part && part.inlineData ? part.inlineData : null;\n    }\n}\n",
      "metadata": {
        "title": "utils",
        "source": {
          "code": "/**\n * @fileoverview Common utils for manipulating LLM Content and other relevant types.\n */\nexport {\n  isLLMContent,\n  isLLMContentArray,\n  toLLMContent,\n  toInlineData,\n  toLLMContentInline,\n  toText,\n  contentToJSON,\n  defaultLLMContent,\n  endsWithRole,\n  addUserTurn,\n  isEmpty,\n  llm,\n};\n\nexport { ok, err };\n\nfunction ok<T>(o: Outcome<T>): o is T {\n  return !(o && typeof o === \"object\" && \"$error\" in o);\n}\n\nfunction err($error: string) {\n  return { $error };\n}\n\nfunction mergeTextParts(parts: LLMContent[\"parts\"]): LLMContent[\"parts\"] {\n  const merged: LLMContent[\"parts\"] = [];\n  let text: string = \"\";\n  for (const part of parts) {\n    if (\"text\" in part) {\n      text += part.text;\n    } else {\n      if (text) {\n        merged.push({ text });\n      }\n      text = \"\";\n      merged.push(part);\n    }\n  }\n  if (text) {\n    merged.push({ text });\n  }\n  return merged;\n}\n\nclass LLMTemplate {\n  constructor(\n    public readonly strings: TemplateStringsArray,\n    public readonly values: unknown[]\n  ) {}\n\n  asParts(): LLMContent[\"parts\"] {\n    return mergeTextParts(\n      this.strings.flatMap((s, i) => {\n        let text = s;\n        const value = this.values.at(i);\n        if (value == undefined) {\n          return { text };\n        } else if (typeof value === \"string\") {\n          text += value;\n          return { text };\n        } else if (value instanceof LLMTemplate) {\n          return value.asParts();\n        } else if (isLLMContent(value)) {\n          return [{ text }, ...value.parts];\n        } else {\n          text += JSON.stringify(value);\n          return { text };\n        }\n      })\n    );\n  }\n\n  asContent(): LLMContent {\n    const parts = this.asParts();\n    return { parts, role: \"user\" };\n  }\n}\n\nfunction llm(strings: TemplateStringsArray, ...values: unknown[]): LLMTemplate {\n  return new LLMTemplate(strings, values);\n}\n\n/**\n * Copied from @google-labs/breadboard\n */\nfunction isLLMContent(nodeValue: unknown): nodeValue is LLMContent {\n  if (typeof nodeValue !== \"object\" || !nodeValue) return false;\n  if (nodeValue === null || nodeValue === undefined) return false;\n\n  if (\"role\" in nodeValue && nodeValue.role === \"$metadata\") {\n    return true;\n  }\n\n  return \"parts\" in nodeValue && Array.isArray(nodeValue.parts);\n}\n\nfunction isLLMContentArray(nodeValue: unknown): nodeValue is LLMContent[] {\n  if (!Array.isArray(nodeValue)) return false;\n  if (nodeValue.length === 0) return true;\n  return isLLMContent(nodeValue.at(-1));\n}\n\nfunction toLLMContent(\n  text: string,\n  role: LLMContent[\"role\"] = \"user\"\n): LLMContent {\n  return { parts: [{ text }], role };\n}\n\nfunction endsWithRole(c: LLMContent[], role: \"user\" | \"model\"): boolean {\n  const last = c.at(-1);\n  if (!last) return false;\n  return last.role === role;\n}\n\nfunction isEmpty(c: LLMContent): boolean {\n  if (!c.parts.length) return true;\n  for (const part of c.parts) {\n    if (\"text\" in part) {\n      if (part.text.trim().length > 0) return false;\n    } else {\n      return false;\n    }\n  }\n  return true;\n}\n\nfunction toText(c: LLMContent | LLMContent[]): string {\n  if (isLLMContent(c)) {\n    return contentToText(c);\n  }\n  const last = c.at(-1);\n  if (!last) return \"\";\n  return contentToText(last).trim();\n\n  function contentToText(content: LLMContent) {\n    return content.parts\n      .map((part) => (\"text\" in part ? part.text : \"\"))\n      .join(\"\\n\\n\");\n  }\n}\n\nfunction contentToJSON<T>(content?: LLMContent): T {\n  const part = content?.parts?.at(0);\n  if (!part || !(\"text\" in part)) {\n    throw new Error(\"Invalid response from Gemini\");\n  }\n  return JSON.parse(part.text) as T;\n}\n\nfunction defaultLLMContent(): string {\n  return JSON.stringify({\n    parts: [{ text: \"\" }],\n    role: \"user\",\n  } satisfies LLMContent);\n}\n\nfunction addUserTurn(content: string | LLMContent, context?: LLMContent[]) {\n  context ??= [];\n  const isString = typeof content === \"string\";\n  if (!endsWithRole(context, \"user\")) {\n    return [...context, isString ? toLLMContent(content) : content];\n  }\n  const last = context.at(-1)!;\n  if (isString) {\n    last.parts.push({ text: content });\n  } else {\n    last.parts.push(...content.parts);\n  }\n  return context;\n}\n\nfunction toLLMContentInline(\n  mimetype: string,\n  value: string,\n  role: LLMContent[\"role\"] = \"user\"\n): LLMContent {\n  return {\n    parts: [\n      {\n        inlineData: {\n          mimeType: mimetype,\n          data: value,\n        },\n      },\n    ],\n    role,\n  };\n}\n\nfunction toInlineData(c: LLMContent | LLMContent[]) {\n  if (isLLMContent(c)) {\n    return contentToInlineData(c);\n  }\n  const last = c.at(-1);\n  if (!last) return null;\n  return contentToInlineData(last);\n\n  function contentToInlineData(content: LLMContent) {\n    const part = content.parts.at(-1);\n    if (!part) return \"\";\n    return \"inlineData\" in part && part.inlineData ? part.inlineData : null;\n  }\n}\n",
          "language": "typescript"
        },
        "description": "Common utils for manipulating LLM Content and other relevant types.",
        "runnable": false
      }
    },
    "gemini": {
      "code": "/**\n * @fileoverview Gemini Model Family.\n */\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\nimport output from \"@output\";\nimport { ok, err, isLLMContentArray } from \"./utils\";\nconst defaultSafetySettings = () => [\n    {\n        category: \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        threshold: \"OFF\",\n    },\n    {\n        category: \"HARM_CATEGORY_HARASSMENT\",\n        threshold: \"OFF\",\n    },\n    {\n        category: \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        threshold: \"OFF\",\n    },\n];\nasync function endpointURL(model) {\n    const $metadata = {\n        title: \"Get GEMINI_KEY\",\n        description: \"Getting GEMINI_KEY from secrets\",\n    };\n    const key = await secrets({ $metadata, keys: [\"GEMINI_KEY\"] });\n    return `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${key.GEMINI_KEY}`;\n}\nexport { invoke as default, describe, defaultSafetySettings };\nconst VALID_MODALITIES = [\"Text\", \"Text and Image\", \"Audio\"];\nconst MODELS = [\n    \"gemini-2.0-flash\",\n    \"gemini-2.0-flash-001\",\n    \"gemini-2.0-flash-lite-preview-02-05\",\n    \"gemini-2.0-flash-exp\",\n    \"gemini-2.0-flash-thinking-exp\",\n    \"gemini-2.0-flash-thinking-exp-01-21\",\n    \"gemini-2.0-pro-exp-02-05\",\n    \"gemini-1.5-flash-latest\",\n    \"gemini-1.5-pro-latest\",\n    \"gemini-exp-1206\",\n    \"gemini-exp-1121\",\n    \"learnlm-1.5-pro-experimental\",\n    \"gemini-1.5-pro-001\",\n    \"gemini-1.5-pro-002\",\n    \"gemini-1.5-pro-exp-0801\",\n    \"gemini-1.5-pro-exp-0827\",\n    \"gemini-1.5-flash-001\",\n    \"gemini-1.5-flash-002\",\n    \"gemini-1.5-flash-8b-exp-0924\",\n    \"gemini-1.5-flash-8b-exp-0827\",\n    \"gemini-1.5-flash-exp-0827\",\n];\nconst NO_RETRY_CODES = [400, 429, 404];\n/**\n * Modifies the body to remove any\n * Breadboard-specific extensions to LLM Content\n */\nfunction conformBody(body) {\n    return {\n        ...body,\n        contents: body.contents.map((content) => {\n            return {\n                ...content,\n                parts: content.parts.map((part) => {\n                    if (\"json\" in part) {\n                        return { text: JSON.stringify(part.json) };\n                    }\n                    return part;\n                }),\n            };\n        }),\n    };\n}\nasync function callAPI(retries, model, body, $metadata) {\n    let $error = \"Unknown error\";\n    while (retries) {\n        const result = await fetch({\n            $metadata,\n            url: await endpointURL(model),\n            method: \"POST\",\n            body: conformBody(body),\n        });\n        if (!ok(result)) {\n            // Fetch is a bit weird, because it returns various props\n            // along with the `$error`. Let's handle that here.\n            const { status, $error: errObject } = result;\n            if (!status) {\n                // This is not an error response, presume fatal error.\n                return { $error };\n            }\n            $error = maybeExtractError(errObject);\n            if (NO_RETRY_CODES.includes(status)) {\n                return { $error };\n            }\n        }\n        else {\n            const outputs = result.response;\n            const candidate = outputs.candidates?.at(0);\n            if (!candidate) {\n                return err(\"Unable to get a good response from Gemini\");\n            }\n            if (\"content\" in candidate) {\n                return outputs;\n            }\n            if (candidate.finishReason === \"IMAGE_SAFETY\") {\n                return err(\"The response candidate content was flagged for image safety reasons.\");\n            }\n        }\n        retries--;\n    }\n    return { $error };\n}\nfunction maybeExtractError(e) {\n    try {\n        const parsed = JSON.parse(e);\n        return parsed.error.message;\n    }\n    catch (error) {\n        return e;\n    }\n}\nfunction isEmptyLLMContent(content) {\n    if (!content || !content.parts || content.parts.length === 0)\n        return true;\n    return content.parts.every((part) => {\n        if (\"text\" in part) {\n            return !part.text?.trim();\n        }\n        return true;\n    });\n}\nfunction addModality(body, modality) {\n    if (!modality)\n        return;\n    switch (modality) {\n        case \"Text\":\n            // No change, defaults.\n            break;\n        case \"Text and Image\":\n            body.generationConfig ??= {};\n            body.generationConfig.responseModalities = [\"TEXT\", \"IMAGE\"];\n            break;\n        case \"Audio\":\n            body.generationConfig ??= {};\n            body.generationConfig.responseModalities = [\"AUDIO\"];\n            break;\n    }\n}\nfunction constructBody(context = [], systemInstruction, prompt, modality) {\n    const contents = [...context];\n    if (!isEmptyLLMContent(prompt)) {\n        contents.push(prompt);\n    }\n    const body = {\n        contents,\n        safetySettings: defaultSafetySettings(),\n    };\n    const canHaveSystemInstruction = modality === \"Text\";\n    if (!isEmptyLLMContent(systemInstruction) && canHaveSystemInstruction) {\n        body.systemInstruction = systemInstruction;\n    }\n    addModality(body, modality);\n    return body;\n}\nfunction augmentBody(body, systemInstruction, prompt, modality) {\n    if (!body.systemInstruction || !isEmptyLLMContent(systemInstruction)) {\n        body.systemInstruction = systemInstruction;\n    }\n    if (!isEmptyLLMContent(prompt)) {\n        body.contents = [...body.contents, prompt];\n    }\n    addModality(body, modality);\n    return body;\n}\nfunction validateInputs(inputs) {\n    if (\"body\" in inputs) {\n        return;\n    }\n    if (inputs.context) {\n        const { context } = inputs;\n        if (!Array.isArray(context)) {\n            return err(\"Incoming context must be an array.\");\n        }\n        if (!isLLMContentArray(context)) {\n            return err(\"Malformed incoming context\");\n        }\n        return;\n    }\n    return err(\"Either body or context is required\");\n}\nasync function invoke(inputs) {\n    const validatingInputs = validateInputs(inputs);\n    if (!ok(validatingInputs)) {\n        return validatingInputs;\n    }\n    let { model } = inputs;\n    if (!model) {\n        model = MODELS[0];\n    }\n    const { context, systemInstruction, prompt, modality, body, $metadata } = inputs;\n    // TODO: Make this configurable.\n    const retries = 5;\n    if (!(\"body\" in inputs)) {\n        // Public API is being used.\n        // Behave as if we're wired in.\n        const result = await callAPI(retries, model, constructBody(context, systemInstruction, prompt, modality));\n        if (!ok(result)) {\n            return result;\n        }\n        const content = result.candidates.at(0)?.content;\n        if (!content) {\n            return err(\"Unable to get a good response from Gemini\");\n        }\n        return { context: [...context, content] };\n    }\n    else {\n        // Private API is being used.\n        // Behave as if we're being invoked.\n        return callAPI(retries, model, augmentBody(body, systemInstruction, prompt, modality), $metadata);\n    }\n}\nasync function describe({ inputs }) {\n    const canHaveModalities = inputs.model === \"gemini-2.0-flash-exp\";\n    const canHaveSystemInstruction = !canHaveModalities || (canHaveModalities && inputs.modality == \"Text\");\n    const maybeAddSystemInstruction = canHaveSystemInstruction\n        ? {\n            systemInstruction: {\n                type: \"object\",\n                behavior: [\"llm-content\", \"config\"],\n                title: \"System Instruction\",\n                default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n                description: \"(Optional) Give the model additional context on what to do,\" +\n                    \"like specific rules/guidelines to adhere to or specify behavior\" +\n                    \"separate from the provided context\",\n            },\n        }\n        : {};\n    const maybeAddModalities = canHaveModalities\n        ? {\n            modality: {\n                type: \"string\",\n                enum: [...VALID_MODALITIES],\n                title: \"Output Modality\",\n                behavior: [\"config\"],\n                description: \"(Optional) Tell the model what kind of output you're looking for.\",\n            },\n        }\n        : {};\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                model: {\n                    type: \"string\",\n                    behavior: [\"config\"],\n                    title: \"Model Name\",\n                    enum: MODELS,\n                    default: MODELS[0],\n                },\n                prompt: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\"],\n                    title: \"Prompt\",\n                    default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n                    description: \"(Optional) A prompt. Will be added to the end of the the conversation context.\",\n                },\n                ...maybeAddSystemInstruction,\n                ...maybeAddModalities,\n                context: {\n                    type: \"array\",\n                    items: {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                    },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                    },\n                    title: \"Context out\",\n                },\n            },\n        },\n        metadata: {\n            icon: \"generative\",\n        },\n    };\n}\n",
      "metadata": {
        "title": "Gemini",
        "source": {
          "code": "/**\n * @fileoverview Gemini Model Family.\n */\n\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\nimport output from \"@output\";\n\nimport { ok, err, isLLMContentArray } from \"./utils\";\n\nconst defaultSafetySettings = (): SafetySetting[] => [\n  {\n    category: \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n    threshold: \"OFF\",\n  },\n  {\n    category: \"HARM_CATEGORY_HARASSMENT\",\n    threshold: \"OFF\",\n  },\n  {\n    category: \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n    threshold: \"OFF\",\n  },\n];\n\nasync function endpointURL(model: string): Promise<string> {\n  const $metadata = {\n    title: \"Get GEMINI_KEY\",\n    description: \"Getting GEMINI_KEY from secrets\",\n  };\n  const key = await secrets({ $metadata, keys: [\"GEMINI_KEY\"] });\n  return `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${key.GEMINI_KEY}`;\n}\n\nexport { invoke as default, describe, defaultSafetySettings };\n\nconst VALID_MODALITIES = [\"Text\", \"Text and Image\", \"Audio\"] as const;\ntype ValidModalities = (typeof VALID_MODALITIES)[number];\n\nexport type HarmBlockThreshold =\n  // Content with NEGLIGIBLE will be allowed.\n  | \"BLOCK_LOW_AND_ABOVE\"\n  // Content with NEGLIGIBLE and LOW will be allowed.\n  | \"BLOCK_MEDIUM_AND_ABOVE\"\n  // Content with NEGLIGIBLE, LOW, and MEDIUM will be allowed.\n  | \"BLOCK_ONLY_HIGH\"\n  // All content will be allowed.\n  | \"BLOCK_NONE\"\n  // Turn off the safety filter.\n  | \"OFF\";\n\nexport type HarmCategory =\n  // Gemini - Harassment content\n  | \"HARM_CATEGORY_HARASSMENT\"\n  //\tGemini - Hate speech and content.\n  | \"HARM_CATEGORY_HATE_SPEECH\"\n  // Gemini - Sexually explicit content.\n  | \"HARM_CATEGORY_SEXUALLY_EXPLICIT\"\n  // \tGemini - Dangerous content.\n  | \"HARM_CATEGORY_DANGEROUS_CONTENT\"\n  // Gemini - Content that may be used to harm civic integrity.\n  | \"HARM_CATEGORY_CIVIC_INTEGRITY\";\n\nexport type GeminiSchema = {\n  type: \"string\" | \"number\" | \"integer\" | \"boolean\" | \"object\" | \"array\";\n  format?: string;\n  description?: string;\n  nullable?: boolean;\n  enum?: string[];\n  maxItems?: string;\n  minItems?: string;\n  properties?: Record<string, GeminiSchema>;\n  required?: string[];\n  items?: GeminiSchema;\n};\n\nexport type Modality = \"TEXT\" | \"IMAGE\" | \"AUDIO\";\n\nexport type GenerationConfig = {\n  responseMimeType?: \"text/plain\" | \"application/json\" | \"text/x.enum\";\n  responseSchema?: GeminiSchema;\n  responseModalities?: Modality[];\n};\n\nexport type SafetySetting = {\n  category: HarmCategory;\n  threshold: HarmBlockThreshold;\n};\n\nexport type Metadata = {\n  title?: string;\n  description?: string;\n};\n\nexport type GeminiBody = {\n  contents: LLMContent[];\n  tools?: Tool[];\n  toolConfig?: ToolConfig;\n  systemInstruction?: LLMContent;\n  safetySettings?: SafetySetting[];\n  generationConfig?: GenerationConfig;\n};\n\nexport type GeminiInputs = {\n  // The wireable/configurable properties.\n  model?: string;\n  context?: LLMContent[];\n  systemInstruction?: LLMContent;\n  prompt?: LLMContent;\n  modality?: ValidModalities;\n  // The \"private API\" properties\n  $metadata?: Metadata;\n  body: GeminiBody;\n};\n\nexport type Tool = {\n  functionDeclarations?: FunctionDeclaration[];\n  googleSearch?: {};\n  codeExecution?: CodeExecution[];\n};\n\nexport type ToolConfig = {\n  functionCallingConfig?: FunctionCallingConfig;\n};\n\nexport type FunctionCallingConfig = {\n  mode?: \"MODE_UNSPECIFIED\" | \"AUTO\" | \"ANY\" | \"NONE\";\n  allowedFunctionNames?: string[];\n};\n\nexport type FunctionDeclaration = {\n  name: string;\n  description: string;\n  parameters?: GeminiSchema;\n};\n\nexport type CodeExecution = {\n  // Type contains no fields.\n};\n\nexport type FinishReason =\n  // Natural stop point of the model or provided stop sequence.\n  | \"STOP\"\n  // The maximum number of tokens as specified in the request was reached.\n  | \"MAX_TOKENS\"\n  // The response candidate content was flagged for safety reasons.\n  | \"SAFETY\"\n  // The response candidate content was flagged for image safety reasons.\n  | \"IMAGE_SAFETY\"\n  // The response candidate content was flagged for recitation reasons.\n  | \"RECITATION\"\n  // The response candidate content was flagged for using an unsupported language.\n  | \"LANGUAGE\"\n  // Unknown reason.\n  | \"OTHER\"\n  // Token generation stopped because the content contains forbidden terms.\n  | \"BLOCKLIST\"\n  // Token generation stopped for potentially containing prohibited content.\n  | \"PROHIBITED_CONTENT\"\n  // Token generation stopped because the content potentially contains Sensitive Personally Identifiable Information (SPII).\n  | \"SPII\"\n  // The function call generated by the model is invalid.\n  | \"MALFORMED_FUNCTION_CALL\";\n\nexport type GroundingMetadata = {\n  groundingChunks: {\n    web: {\n      uri: string;\n      title: string;\n    };\n  }[];\n  groundingSupports: {\n    groundingChunkIndices: number[];\n    confidenceScores: number[];\n    segment: {\n      partIndex: number;\n      startIndex: number;\n      endIndex: number;\n      text: string;\n    };\n  };\n  webSearchQueries: string[];\n  searchEntryPoint: {\n    renderedContent: string;\n    /**\n     * Base64 encoded JSON representing array of <search term, search url> tuple.\n     * A base64-encoded string.\n     */\n    sdkBlob: string;\n  };\n  retrievalMetadata: {\n    googleSearchDynamicRetrievalScore: number;\n  };\n};\n\nexport type Candidate = {\n  content?: LLMContent;\n  finishReason?: FinishReason;\n  safetyRatings?: SafetySetting[];\n  tokenOutput: number;\n  groundingMetadata: GroundingMetadata;\n};\n\nexport type GeminiAPIOutputs = {\n  candidates: Candidate[];\n};\n\nexport type GeminiOutputs =\n  | GeminiAPIOutputs\n  | {\n      context: LLMContent[];\n    };\n\nconst MODELS: readonly string[] = [\n  \"gemini-2.0-flash\",\n  \"gemini-2.0-flash-001\",\n  \"gemini-2.0-flash-lite-preview-02-05\",\n  \"gemini-2.0-flash-exp\",\n  \"gemini-2.0-flash-thinking-exp\",\n  \"gemini-2.0-flash-thinking-exp-01-21\",\n  \"gemini-2.0-pro-exp-02-05\",\n  \"gemini-1.5-flash-latest\",\n  \"gemini-1.5-pro-latest\",\n  \"gemini-exp-1206\",\n  \"gemini-exp-1121\",\n  \"learnlm-1.5-pro-experimental\",\n  \"gemini-1.5-pro-001\",\n  \"gemini-1.5-pro-002\",\n  \"gemini-1.5-pro-exp-0801\",\n  \"gemini-1.5-pro-exp-0827\",\n  \"gemini-1.5-flash-001\",\n  \"gemini-1.5-flash-002\",\n  \"gemini-1.5-flash-8b-exp-0924\",\n  \"gemini-1.5-flash-8b-exp-0827\",\n  \"gemini-1.5-flash-exp-0827\",\n];\n\nconst NO_RETRY_CODES: readonly number[] = [400, 429, 404];\n\ntype FetchErrorResponse = {\n  $error: string;\n  status: number;\n  statusText: string;\n  contentType: string;\n  responseHeaders: Record<string, string>;\n};\n\n/**\n * Using\n * `{\"error\":{\"code\":400,\"message\":\"Invalid JSON paylo…'contents[0].parts[0]': Cannot find field.\"}]}]}\n * as template for this type.\n */\ntype GeminiError = {\n  error: {\n    code: number;\n    details: {\n      type: string;\n      fieldViolations: {\n        description: string;\n        field: string;\n      }[];\n    }[];\n    message: string;\n    status: string;\n  };\n};\n\n/**\n * Modifies the body to remove any\n * Breadboard-specific extensions to LLM Content\n */\nfunction conformBody(body: GeminiBody): GeminiBody {\n  return {\n    ...body,\n    contents: body.contents.map((content) => {\n      return {\n        ...content,\n        parts: content.parts.map((part) => {\n          if (\"json\" in part) {\n            return { text: JSON.stringify(part.json) };\n          }\n          return part;\n        }),\n      };\n    }),\n  };\n}\n\nasync function callAPI(\n  retries: number,\n  model: string,\n  body: GeminiBody,\n  $metadata?: Metadata\n): Promise<Outcome<GeminiAPIOutputs>> {\n  let $error: string = \"Unknown error\";\n  while (retries) {\n    const result = await fetch({\n      $metadata,\n      url: await endpointURL(model),\n      method: \"POST\",\n      body: conformBody(body),\n    });\n    if (!ok(result)) {\n      // Fetch is a bit weird, because it returns various props\n      // along with the `$error`. Let's handle that here.\n      const { status, $error: errObject } = result as FetchErrorResponse;\n      if (!status) {\n        // This is not an error response, presume fatal error.\n        return { $error };\n      }\n      $error = maybeExtractError(errObject);\n      if (NO_RETRY_CODES.includes(status)) {\n        return { $error };\n      }\n    } else {\n      const outputs = result.response as GeminiAPIOutputs;\n      const candidate = outputs.candidates?.at(0);\n      if (!candidate) {\n        return err(\"Unable to get a good response from Gemini\");\n      }\n      if (\"content\" in candidate) {\n        return outputs;\n      }\n      if (candidate.finishReason === \"IMAGE_SAFETY\") {\n        return err(\n          \"The response candidate content was flagged for image safety reasons.\"\n        );\n      }\n    }\n    retries--;\n  }\n  return { $error };\n}\n\nfunction maybeExtractError(e: string): string {\n  try {\n    const parsed = JSON.parse(e) as GeminiError;\n    return parsed.error.message;\n  } catch (error) {\n    return e;\n  }\n}\n\nfunction isEmptyLLMContent(content?: LLMContent): content is undefined {\n  if (!content || !content.parts || content.parts.length === 0) return true;\n  return content.parts.every((part) => {\n    if (\"text\" in part) {\n      return !part.text?.trim();\n    }\n    return true;\n  });\n}\n\nfunction addModality(body: GeminiBody, modality?: ValidModalities) {\n  if (!modality) return;\n  switch (modality) {\n    case \"Text\":\n      // No change, defaults.\n      break;\n    case \"Text and Image\":\n      body.generationConfig ??= {};\n      body.generationConfig.responseModalities = [\"TEXT\", \"IMAGE\"];\n      break;\n    case \"Audio\":\n      body.generationConfig ??= {};\n      body.generationConfig.responseModalities = [\"AUDIO\"];\n      break;\n  }\n}\n\nfunction constructBody(\n  context: LLMContent[] = [],\n  systemInstruction?: LLMContent,\n  prompt?: LLMContent,\n  modality?: ValidModalities\n): GeminiBody {\n  const contents = [...context];\n  if (!isEmptyLLMContent(prompt)) {\n    contents.push(prompt);\n  }\n  const body: GeminiBody = {\n    contents,\n    safetySettings: defaultSafetySettings(),\n  };\n  const canHaveSystemInstruction = modality === \"Text\";\n  if (!isEmptyLLMContent(systemInstruction) && canHaveSystemInstruction) {\n    body.systemInstruction = systemInstruction;\n  }\n  addModality(body, modality);\n  return body;\n}\n\nfunction augmentBody(\n  body: GeminiBody,\n  systemInstruction?: LLMContent,\n  prompt?: LLMContent,\n  modality?: ValidModalities\n): GeminiBody {\n  if (!body.systemInstruction || !isEmptyLLMContent(systemInstruction)) {\n    body.systemInstruction = systemInstruction;\n  }\n  if (!isEmptyLLMContent(prompt)) {\n    body.contents = [...body.contents, prompt];\n  }\n  addModality(body, modality);\n  return body;\n}\n\nfunction validateInputs(inputs: GeminiInputs): Outcome<void> {\n  if (\"body\" in (inputs as object)) {\n    return;\n  }\n  if (inputs.context) {\n    const { context } = inputs;\n    if (!Array.isArray(context)) {\n      return err(\"Incoming context must be an array.\");\n    }\n    if (!isLLMContentArray(context)) {\n      return err(\"Malformed incoming context\");\n    }\n    return;\n  }\n  return err(\"Either body or context is required\");\n}\n\nasync function invoke(inputs: GeminiInputs): Promise<Outcome<GeminiOutputs>> {\n  const validatingInputs = validateInputs(inputs);\n  if (!ok(validatingInputs)) {\n    return validatingInputs;\n  }\n  let { model } = inputs;\n  if (!model) {\n    model = MODELS[0];\n  }\n  const { context, systemInstruction, prompt, modality, body, $metadata } =\n    inputs;\n  // TODO: Make this configurable.\n  const retries = 5;\n  if (!(\"body\" in inputs)) {\n    // Public API is being used.\n    // Behave as if we're wired in.\n    const result = await callAPI(\n      retries,\n      model,\n      constructBody(context, systemInstruction, prompt, modality)\n    );\n    if (!ok(result)) {\n      return result;\n    }\n    const content = result.candidates.at(0)?.content;\n    if (!content) {\n      return err(\"Unable to get a good response from Gemini\");\n    }\n    return { context: [...context!, content] };\n  } else {\n    // Private API is being used.\n    // Behave as if we're being invoked.\n    return callAPI(\n      retries,\n      model,\n      augmentBody(body, systemInstruction, prompt, modality),\n      $metadata\n    );\n  }\n}\n\ntype DescribeInputs = {\n  inputs: {\n    modality?: ValidModalities;\n    model: string;\n  };\n};\n\nasync function describe({ inputs }: DescribeInputs) {\n  const canHaveModalities = inputs.model === \"gemini-2.0-flash-exp\";\n  const canHaveSystemInstruction =\n    !canHaveModalities || (canHaveModalities && inputs.modality == \"Text\");\n  const maybeAddSystemInstruction: Schema[\"properties\"] =\n    canHaveSystemInstruction\n      ? {\n          systemInstruction: {\n            type: \"object\",\n            behavior: [\"llm-content\", \"config\"],\n            title: \"System Instruction\",\n            default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n            description:\n              \"(Optional) Give the model additional context on what to do,\" +\n              \"like specific rules/guidelines to adhere to or specify behavior\" +\n              \"separate from the provided context\",\n          },\n        }\n      : {};\n  const maybeAddModalities: Schema[\"properties\"] = canHaveModalities\n    ? {\n        modality: {\n          type: \"string\",\n          enum: [...VALID_MODALITIES],\n          title: \"Output Modality\",\n          behavior: [\"config\"],\n          description:\n            \"(Optional) Tell the model what kind of output you're looking for.\",\n        },\n      }\n    : {};\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        model: {\n          type: \"string\",\n          behavior: [\"config\"],\n          title: \"Model Name\",\n          enum: MODELS as string[],\n          default: MODELS[0],\n        },\n        prompt: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\"],\n          title: \"Prompt\",\n          default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n          description:\n            \"(Optional) A prompt. Will be added to the end of the the conversation context.\",\n        },\n        ...maybeAddSystemInstruction,\n        ...maybeAddModalities,\n        context: {\n          type: \"array\",\n          items: {\n            type: \"object\",\n            behavior: [\"llm-content\"],\n          },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: {\n            type: \"object\",\n            behavior: [\"llm-content\"],\n          },\n          title: \"Context out\",\n        },\n      },\n    } satisfies Schema,\n    metadata: {\n      icon: \"generative\",\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Gemini Model Family.",
        "runnable": false
      }
    },
    "tool-manager": {
      "code": "/**\n * @fileoverview Add a description for your module here.\n */\nimport describeGraph from \"@describe\";\nimport invokeGraph from \"@invoke\";\nimport { ok, err } from \"./utils\";\nimport {} from \"./common\";\nimport {} from \"./gemini\";\nimport { ArgumentNameGenerator } from \"./introducer\";\nexport { ToolManager };\nclass ToolManager {\n    #hasSearch = false;\n    tools = new Map();\n    errors = [];\n    #convertSchemas(schema) {\n        return toGeminiSchema(schema);\n        function toGeminiSchema(schema) {\n            switch (schema.type) {\n                case \"object\": {\n                    if (schema.behavior?.includes(\"llm-content\")) {\n                        return {\n                            type: \"string\",\n                            description: schema.description || schema.title,\n                        };\n                    }\n                    if (!schema.properties) {\n                        return { type: \"object\" };\n                    }\n                    return {\n                        type: \"object\",\n                        properties: Object.fromEntries(Object.entries(schema.properties).map(([name, schema]) => {\n                            return [name, toGeminiSchema(schema)];\n                        })),\n                    };\n                }\n                case \"array\": {\n                    const items = schema.items;\n                    if (items.behavior?.includes(\"llm-content\")) {\n                        return {\n                            type: \"string\",\n                            description: schema.description,\n                        };\n                    }\n                    return {\n                        type: \"array\",\n                        items: toGeminiSchema(schema.items),\n                    };\n                }\n                default: {\n                    const geminiSchema = { ...schema };\n                    delete geminiSchema.format;\n                    delete geminiSchema.behavior;\n                    delete geminiSchema.examples;\n                    delete geminiSchema.default;\n                    delete geminiSchema.transient;\n                    if (!geminiSchema.description) {\n                        geminiSchema.description = geminiSchema.title;\n                    }\n                    delete geminiSchema.title;\n                    return geminiSchema;\n                }\n            }\n        }\n    }\n    #toName(title) {\n        return title ? title.replace(/\\W/g, \"_\") : \"function\";\n    }\n    addSearch() {\n        this.#hasSearch = true;\n    }\n    /**\n     * Try to ask the step to introduce itself if the input schema is empty\n     * or if it has a \"context\" LLMContent[] input port.\n     */\n    #shouldTryIntroduction(description) {\n        if (!description.inputSchema?.properties)\n            return true;\n        const context = description.inputSchema.properties[\"context\"];\n        if (!context)\n            return false;\n        if (context.type === \"array\" && context.items) {\n            return !!context.items.behavior?.includes(\"llm-content\");\n        }\n        return false;\n    }\n    async #tryIntroduction(describerResult, url) {\n        const { title, description } = describerResult;\n        if (!title || !description)\n            return err(`Subgraph must have title and description.`);\n        const introducing = await new ArgumentNameGenerator(title, description).invoke();\n        if (!ok(introducing))\n            return introducing;\n        const introResult = introducing;\n        console.log(\"INTRO RESULT\", describerResult, introResult);\n        return {\n            ...introResult,\n            title: describerResult.title || introResult.title,\n            description: describerResult.description || introResult.title,\n        };\n    }\n    async addTool(url) {\n        let description = (await describeGraph({\n            url,\n        }));\n        let passContext = false;\n        if (!ok(description))\n            return description;\n        if (this.#shouldTryIntroduction(description)) {\n            const describing = await this.#tryIntroduction(description, url);\n            if (!ok(describing))\n                return describing;\n            description = describing;\n            passContext = true;\n        }\n        const name = this.#toName(description.title);\n        const functionDeclaration = {\n            name,\n            description: description.description || \"\",\n        };\n        const parameters = this.#convertSchemas(description.inputSchema);\n        if (parameters.properties) {\n            functionDeclaration.parameters = parameters;\n        }\n        this.tools.set(name, { tool: functionDeclaration, url, passContext });\n        return description.title || name;\n    }\n    async initialize(tools) {\n        if (!tools) {\n            return true;\n        }\n        let hasInvalidTools = false;\n        for (const tool of tools) {\n            const url = typeof tool === \"string\" ? tool : tool.url;\n            const description = (await describeGraph({\n                url,\n            }));\n            if (!ok(description)) {\n                this.errors.push(description.$error);\n                // Invalid tool, skip\n                hasInvalidTools = true;\n                continue;\n            }\n            const parameters = this.#convertSchemas(description.inputSchema);\n            const name = this.#toName(description.title);\n            const functionDeclaration = {\n                name,\n                description: description.description || \"\",\n                parameters,\n            };\n            this.tools.set(name, {\n                tool: functionDeclaration,\n                url,\n                passContext: false,\n            });\n        }\n        return !hasInvalidTools;\n    }\n    async processResponse(response, callTool) {\n        for (const part of response.parts) {\n            if (\"functionCall\" in part) {\n                const { args, name } = part.functionCall;\n                const handle = this.tools.get(name);\n                if (handle) {\n                    const { url, passContext } = handle;\n                    await callTool(url, part.functionCall.args, passContext);\n                }\n            }\n        }\n    }\n    hasTools() {\n        return this.tools.size !== 0;\n    }\n    list() {\n        const declaration = {};\n        const entries = [...this.tools.entries()];\n        if (entries.length !== 0) {\n            declaration.functionDeclarations = entries.map(([, value]) => value.tool);\n        }\n        if (this.#hasSearch) {\n            declaration.googleSearch = {};\n        }\n        if (Object.keys(declaration).length === 0)\n            return [];\n        return [declaration];\n    }\n}\n",
      "metadata": {
        "title": "tool-manager",
        "source": {
          "code": "/**\n * @fileoverview Add a description for your module here.\n */\n\nimport describeGraph from \"@describe\";\nimport invokeGraph from \"@invoke\";\nimport { ok, err } from \"./utils\";\nimport { type DescriberResult } from \"./common\";\nimport {\n  type FunctionDeclaration,\n  type GeminiSchema,\n  type Tool,\n} from \"./gemini\";\nimport { ArgumentNameGenerator } from \"./introducer\";\n\nexport type CallToolCallback = (\n  tool: string,\n  args: object,\n  passContext?: boolean\n) => Promise<void>;\n\nexport type ToolHandle = {\n  tool: FunctionDeclaration;\n  url: string;\n  passContext: boolean;\n};\n\nexport type ToolDescriptor =\n  | string\n  | {\n      kind: \"board\";\n      url: string;\n    };\n\nexport { ToolManager };\n\nclass ToolManager {\n  #hasSearch = false;\n  tools: Map<string, ToolHandle> = new Map();\n  errors: string[] = [];\n\n  #convertSchemas(schema: Schema): GeminiSchema {\n    return toGeminiSchema(schema);\n\n    function toGeminiSchema(schema: Schema): GeminiSchema {\n      switch (schema.type) {\n        case \"object\": {\n          if (schema.behavior?.includes(\"llm-content\")) {\n            return {\n              type: \"string\",\n              description: schema.description || schema.title,\n            };\n          }\n          if (!schema.properties) {\n            return { type: \"object\" };\n          }\n          return {\n            type: \"object\",\n            properties: Object.fromEntries(\n              Object.entries(schema.properties).map(([name, schema]) => {\n                return [name, toGeminiSchema(schema)];\n              })\n            ),\n          };\n        }\n        case \"array\": {\n          const items = schema.items as Schema;\n          if (items.behavior?.includes(\"llm-content\")) {\n            return {\n              type: \"string\",\n              description: schema.description,\n            };\n          }\n          return {\n            type: \"array\",\n            items: toGeminiSchema(schema.items as Schema),\n          };\n        }\n        default: {\n          const geminiSchema = { ...schema };\n          delete geminiSchema.format;\n          delete geminiSchema.behavior;\n          delete geminiSchema.examples;\n          delete geminiSchema.default;\n          delete geminiSchema.transient;\n          if (!geminiSchema.description) {\n            geminiSchema.description = geminiSchema.title;\n          }\n          delete geminiSchema.title;\n          return geminiSchema as GeminiSchema;\n        }\n      }\n    }\n  }\n\n  #toName(title?: string) {\n    return title ? title.replace(/\\W/g, \"_\") : \"function\";\n  }\n\n  addSearch() {\n    this.#hasSearch = true;\n  }\n\n  /**\n   * Try to ask the step to introduce itself if the input schema is empty\n   * or if it has a \"context\" LLMContent[] input port.\n   */\n  #shouldTryIntroduction(description: DescriberResult): boolean {\n    if (!description.inputSchema?.properties) return true;\n    const context = description.inputSchema.properties[\"context\"];\n    if (!context) return false;\n    if (context.type === \"array\" && context.items) {\n      return !!(context.items as Schema).behavior?.includes(\"llm-content\");\n    }\n    return false;\n  }\n\n  async #tryIntroduction(\n    describerResult: DescriberResult,\n    url: string\n  ): Promise<Outcome<DescriberResult>> {\n    const { title, description } = describerResult;\n    if (!title || !description)\n      return err(`Subgraph must have title and description.`);\n    const introducing = await new ArgumentNameGenerator(\n      title,\n      description\n    ).invoke();\n    if (!ok(introducing)) return introducing;\n    const introResult = introducing as DescriberResult;\n    console.log(\"INTRO RESULT\", describerResult, introResult);\n    return {\n      ...introResult,\n      title: describerResult.title || introResult.title,\n      description: describerResult.description || introResult.title,\n    };\n  }\n\n  async addTool(url: string): Promise<Outcome<string>> {\n    let description = (await describeGraph({\n      url,\n    })) as Outcome<DescriberResult>;\n    let passContext = false;\n    if (!ok(description)) return description;\n    if (this.#shouldTryIntroduction(description)) {\n      const describing = await this.#tryIntroduction(description, url);\n      if (!ok(describing)) return describing;\n      description = describing;\n      passContext = true;\n    }\n    const name = this.#toName(description.title);\n    const functionDeclaration: FunctionDeclaration = {\n      name,\n      description: description.description || \"\",\n    };\n    const parameters = this.#convertSchemas(description.inputSchema!);\n    if (parameters.properties) {\n      functionDeclaration.parameters = parameters;\n    }\n    this.tools.set(name, { tool: functionDeclaration, url, passContext });\n    return description.title || name;\n  }\n\n  async initialize(tools?: ToolDescriptor[]): Promise<boolean> {\n    if (!tools) {\n      return true;\n    }\n    let hasInvalidTools = false;\n    for (const tool of tools) {\n      const url = typeof tool === \"string\" ? tool : tool.url;\n      const description = (await describeGraph({\n        url,\n      })) as Outcome<DescriberResult>;\n      if (!ok(description)) {\n        this.errors.push(description.$error);\n        // Invalid tool, skip\n        hasInvalidTools = true;\n        continue;\n      }\n      const parameters = this.#convertSchemas(description.inputSchema!);\n      const name = this.#toName(description.title);\n      const functionDeclaration = {\n        name,\n        description: description.description || \"\",\n        parameters,\n      };\n      this.tools.set(name, {\n        tool: functionDeclaration,\n        url,\n        passContext: false,\n      });\n    }\n    return !hasInvalidTools;\n  }\n\n  async processResponse(response: LLMContent, callTool: CallToolCallback) {\n    for (const part of response.parts) {\n      if (\"functionCall\" in part) {\n        const { args, name } = part.functionCall;\n        const handle = this.tools.get(name);\n        if (handle) {\n          const { url, passContext } = handle;\n          await callTool(url, part.functionCall.args, passContext);\n        }\n      }\n    }\n  }\n\n  hasTools(): boolean {\n    return this.tools.size !== 0;\n  }\n\n  list(): Tool[] {\n    const declaration: Tool = {};\n    const entries = [...this.tools.entries()];\n    if (entries.length !== 0) {\n      declaration.functionDeclarations = entries.map(([, value]) => value.tool);\n    }\n    if (this.#hasSearch) {\n      declaration.googleSearch = {};\n    }\n    if (Object.keys(declaration).length === 0) return [];\n    return [declaration];\n  }\n}\n",
          "language": "typescript"
        },
        "description": "Add a description for your module here.",
        "runnable": false
      }
    },
    "template": {
      "code": "/**\n * @fileoverview Handles templated content\n */\nexport { invoke as default, describe, Template };\nimport {} from \"./common\";\nimport { ok, err, isLLMContent, isLLMContentArray } from \"./utils\";\nimport readFile from \"@read\";\nfunction unique(params) {\n    return Array.from(new Set(params));\n}\nfunction isTool(param) {\n    return param.type === \"tool\" && !!param.path;\n}\nfunction isIn(param) {\n    return param.type === \"in\" && !!param.path;\n}\nfunction isAsset(param) {\n    return param.type === \"asset\" && !!param.path;\n}\nfunction isParamPart(param) {\n    return isTool(param) || isIn(param) || isAsset(param);\n}\nconst PARSING_REGEX = /{(?<json>{(?:.*?)})}/gim;\nclass Template {\n    template;\n    #parts;\n    #role;\n    constructor(template) {\n        this.template = template;\n        if (!template) {\n            this.#role = \"user\";\n            this.#parts = [];\n            return;\n        }\n        this.#parts = this.#splitToTemplateParts(template);\n        this.#role = template.role;\n    }\n    #mergeTextParts(parts) {\n        const merged = [];\n        for (const part of parts) {\n            if (\"text\" in part) {\n                const last = merged[merged.length - 1];\n                if (last && \"text\" in last) {\n                    last.text += part.text;\n                }\n                else {\n                    merged.push(part);\n                }\n            }\n            else {\n                merged.push(part);\n            }\n        }\n        return merged;\n    }\n    /**\n     * Takes an LLM Content and splits it further into parts where\n     * each {{param}} substitution is a separate part.\n     */\n    #splitToTemplateParts(content) {\n        const parts = [];\n        for (const part of content.parts) {\n            if (!(\"text\" in part)) {\n                parts.push(part);\n                continue;\n            }\n            const matches = part.text.matchAll(PARSING_REGEX);\n            let start = 0;\n            for (const match of matches) {\n                const json = match.groups?.json;\n                const op = match.groups?.op;\n                const arg = match.groups?.arg;\n                const end = match.index;\n                if (end > start) {\n                    parts.push({ text: part.text.slice(start, end) });\n                }\n                if (json) {\n                    let maybeTemplatePart;\n                    try {\n                        maybeTemplatePart = JSON.parse(json);\n                        if (isParamPart(maybeTemplatePart)) {\n                            parts.push(maybeTemplatePart);\n                        }\n                        else {\n                            maybeTemplatePart = null;\n                        }\n                    }\n                    catch (e) {\n                        // do nothing\n                    }\n                    finally {\n                        if (!maybeTemplatePart) {\n                            parts.push({ text: part.text.slice(end, end + match[0].length) });\n                        }\n                    }\n                }\n                start = end + match[0].length;\n            }\n            if (start < part.text.length) {\n                parts.push({ text: part.text.slice(start) });\n            }\n        }\n        return parts;\n    }\n    #getLastNonMetadata(value) {\n        const content = value;\n        for (let i = content.length - 1; i >= 0; i--) {\n            if (content[i].role !== \"$metadata\") {\n                return content[i];\n            }\n        }\n        return null;\n    }\n    async #replaceParam(param, params, whenTool) {\n        if (isIn(param)) {\n            const { type, title: name, path } = param;\n            const paramName = `p-z-${path}`;\n            if (paramName in params) {\n                return params[paramName];\n            }\n            return name;\n        }\n        else if (isAsset(param)) {\n            const path = `/assets/${param.path}`;\n            const reading = await readFile({ path });\n            if (!ok(reading)) {\n                return err(`Unable to find asset \"${param.title}\"`);\n            }\n            return reading.data;\n        }\n        else if (isTool(param)) {\n            return await whenTool(param);\n        }\n        return null;\n    }\n    async substitute(params, whenTool) {\n        const replaced = [];\n        for (const part of this.#parts) {\n            if (\"type\" in part) {\n                const value = await this.#replaceParam(part, params, whenTool);\n                if (value === null) {\n                    // Ignore if null.\n                    continue;\n                }\n                else if (!ok(value)) {\n                    return value;\n                }\n                else if (typeof value === \"string\") {\n                    replaced.push({ text: value });\n                }\n                else if (isLLMContent(value)) {\n                    replaced.push(...value.parts);\n                }\n                else if (isLLMContentArray(value)) {\n                    const last = this.#getLastNonMetadata(value);\n                    if (last) {\n                        replaced.push(...last.parts);\n                    }\n                }\n                else {\n                    replaced.push({ text: JSON.stringify(value) });\n                }\n            }\n            else {\n                replaced.push(part);\n            }\n        }\n        const parts = this.#mergeTextParts(replaced);\n        return { parts, role: this.#role };\n    }\n    #toId(param) {\n        return `p-z-${param}`;\n    }\n    #toTitle(id) {\n        const spaced = id?.replace(/[_-]/g, \" \");\n        return ((spaced?.at(0)?.toUpperCase() ?? \"\") +\n            (spaced?.slice(1)?.toLowerCase() ?? \"\"));\n    }\n    #forEachParam(handler) {\n        for (const part of this.#parts) {\n            if (\"type\" in part) {\n                handler(part);\n            }\n        }\n    }\n    requireds() {\n        const required = [];\n        let hasValues = false;\n        this.#forEachParam((param) => {\n            if (!isIn(param))\n                return;\n            hasValues = true;\n            required.push(this.#toId(param.title));\n        });\n        return hasValues ? { required } : {};\n    }\n    schemas() {\n        const result = [];\n        this.#forEachParam((param) => {\n            const name = param.title;\n            const id = this.#toId(param.path);\n            if (!isIn(param))\n                return;\n            result.push([\n                id,\n                {\n                    title: this.#toTitle(name),\n                    description: `The value to substitute for the parameter \"${name}\"`,\n                    type: \"object\",\n                    behavior: [\"llm-content\"],\n                },\n            ]);\n        });\n        return Object.fromEntries(result);\n    }\n    static part(part) {\n        return `{${JSON.stringify(part)}}`;\n    }\n}\n/**\n * API for test harness\n */\nfunction fromTestParams(params) {\n    return Object.fromEntries(Object.entries(params).map(([key, value]) => {\n        return [`p-z-${key}`, value];\n    }));\n}\n/**\n * Only used for testing.\n */\nasync function invoke({ inputs: { content, params }, }) {\n    const template = new Template(content);\n    const result = await template.substitute(fromTestParams(params), async (params) => {\n        return params.path;\n    });\n    if (!ok(result)) {\n        return result;\n    }\n    return { outputs: result };\n}\n/**\n * Only used for testing.\n */\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: { inputs: { type: \"object\", title: \"Test inputs\" } },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: { outputs: { type: \"object\", title: \"Test outputs\" } },\n        },\n    };\n}\n",
      "metadata": {
        "title": "template",
        "source": {
          "code": "/**\n * @fileoverview Handles templated content\n */\n\nexport { invoke as default, describe, Template };\n\nimport { type Params } from \"./common\";\nimport { ok, err, isLLMContent, isLLMContentArray } from \"./utils\";\nimport readFile from \"@read\";\n\ntype LLMContentWithMetadata = LLMContent & {\n  $metadata: unknown;\n};\n\nexport type Requireds = {\n  required?: Schema[\"required\"];\n};\n\ntype Location = {\n  part: LLMContent[\"parts\"][0];\n  parts: LLMContent[\"parts\"];\n};\n\nexport type InParamPart = {\n  type: \"in\";\n  path: string;\n  title: string;\n};\n\nexport type ToolParamPart = {\n  type: \"tool\";\n  path: string;\n  title: string;\n};\n\nexport type AssetParamPart = {\n  type: \"asset\";\n  path: string;\n  title: string;\n};\n\nexport type ParamPart = InParamPart | ToolParamPart | AssetParamPart;\n\nexport type TemplatePart = DataPart | ParamPart;\n\nexport type ToolCallback = (param: ToolParamPart) => Promise<Outcome<string>>;\n\nfunction unique<T>(params: T[]): T[] {\n  return Array.from(new Set(params));\n}\n\nfunction isTool(param: ParamPart): param is ToolParamPart {\n  return param.type === \"tool\" && !!param.path;\n}\n\nfunction isIn(param: ParamPart): param is InParamPart {\n  return param.type === \"in\" && !!param.path;\n}\n\nfunction isAsset(param: ParamPart): param is AssetParamPart {\n  return param.type === \"asset\" && !!param.path;\n}\n\nfunction isParamPart(param: ParamPart): param is ParamPart {\n  return isTool(param) || isIn(param) || isAsset(param);\n}\n\nconst PARSING_REGEX = /{(?<json>{(?:.*?)})}/gim;\n\nclass Template {\n  #parts: TemplatePart[];\n  #role: LLMContent[\"role\"];\n\n  constructor(public readonly template: LLMContent | undefined) {\n    if (!template) {\n      this.#role = \"user\";\n      this.#parts = [];\n      return;\n    }\n    this.#parts = this.#splitToTemplateParts(template);\n    this.#role = template.role;\n  }\n\n  #mergeTextParts(parts: TemplatePart[]) {\n    const merged = [];\n    for (const part of parts) {\n      if (\"text\" in part) {\n        const last = merged[merged.length - 1];\n        if (last && \"text\" in last) {\n          last.text += part.text;\n        } else {\n          merged.push(part);\n        }\n      } else {\n        merged.push(part);\n      }\n    }\n    return merged as DataPart[];\n  }\n\n  /**\n   * Takes an LLM Content and splits it further into parts where\n   * each {{param}} substitution is a separate part.\n   */\n  #splitToTemplateParts(content: LLMContent): TemplatePart[] {\n    const parts: TemplatePart[] = [];\n    for (const part of content.parts) {\n      if (!(\"text\" in part)) {\n        parts.push(part);\n        continue;\n      }\n      const matches = part.text.matchAll(PARSING_REGEX);\n      let start = 0;\n      for (const match of matches) {\n        const json = match.groups?.json;\n        const op = match.groups?.op;\n        const arg = match.groups?.arg;\n        const end = match.index;\n        if (end > start) {\n          parts.push({ text: part.text.slice(start, end) });\n        }\n        if (json) {\n          let maybeTemplatePart;\n          try {\n            maybeTemplatePart = JSON.parse(json);\n            if (isParamPart(maybeTemplatePart)) {\n              parts.push(maybeTemplatePart);\n            } else {\n              maybeTemplatePart = null;\n            }\n          } catch (e) {\n            // do nothing\n          } finally {\n            if (!maybeTemplatePart) {\n              parts.push({ text: part.text.slice(end, end + match[0].length) });\n            }\n          }\n        }\n        start = end + match[0].length;\n      }\n      if (start < part.text.length) {\n        parts.push({ text: part.text.slice(start) });\n      }\n    }\n    return parts;\n  }\n\n  #getLastNonMetadata(value: LLMContent[]): LLMContent | null {\n    const content = value as LLMContentWithMetadata[];\n    for (let i = content.length - 1; i >= 0; i--) {\n      if (content[i].role !== \"$metadata\") {\n        return content[i] as LLMContent;\n      }\n    }\n    return null;\n  }\n\n  async #replaceParam(\n    param: ParamPart,\n    params: Params,\n    whenTool: ToolCallback\n  ): Promise<Outcome<unknown>> {\n    if (isIn(param)) {\n      const { type, title: name, path } = param;\n      const paramName: `p-z-${string}` = `p-z-${path}`;\n      if (paramName in params) {\n        return params[paramName];\n      }\n      return name;\n    } else if (isAsset(param)) {\n      const path: FileSystemPath = `/assets/${param.path}`;\n      const reading = await readFile({ path });\n      if (!ok(reading)) {\n        return err(`Unable to find asset \"${param.title}\"`);\n      }\n      return reading.data;\n    } else if (isTool(param)) {\n      return await whenTool(param);\n    }\n    return null;\n  }\n\n  async substitute(\n    params: Params,\n    whenTool: ToolCallback\n  ): Promise<Outcome<LLMContent>> {\n    const replaced: DataPart[] = [];\n    for (const part of this.#parts) {\n      if (\"type\" in part) {\n        const value = await this.#replaceParam(part, params, whenTool);\n        if (value === null) {\n          // Ignore if null.\n          continue;\n        } else if (!ok(value)) {\n          return value;\n        } else if (typeof value === \"string\") {\n          replaced.push({ text: value });\n        } else if (isLLMContent(value)) {\n          replaced.push(...value.parts);\n        } else if (isLLMContentArray(value)) {\n          const last = this.#getLastNonMetadata(value);\n          if (last) {\n            replaced.push(...last.parts);\n          }\n        } else {\n          replaced.push({ text: JSON.stringify(value) });\n        }\n      } else {\n        replaced.push(part);\n      }\n    }\n    const parts = this.#mergeTextParts(replaced);\n    return { parts, role: this.#role };\n  }\n\n  #toId(param: string) {\n    return `p-z-${param}`;\n  }\n\n  #toTitle(id: string) {\n    const spaced = id?.replace(/[_-]/g, \" \");\n    return (\n      (spaced?.at(0)?.toUpperCase() ?? \"\") +\n      (spaced?.slice(1)?.toLowerCase() ?? \"\")\n    );\n  }\n\n  #forEachParam(handler: (param: ParamPart) => void) {\n    for (const part of this.#parts) {\n      if (\"type\" in part) {\n        handler(part);\n      }\n    }\n  }\n\n  requireds(): Requireds {\n    const required: string[] = [];\n    let hasValues = false;\n    this.#forEachParam((param) => {\n      if (!isIn(param)) return;\n      hasValues = true;\n      required.push(this.#toId(param.title!));\n    });\n    return hasValues ? { required } : {};\n  }\n\n  schemas(): Record<string, Schema> {\n    const result: [string, Schema][] = [];\n    this.#forEachParam((param) => {\n      const name = param.title!;\n      const id = this.#toId(param.path!);\n      if (!isIn(param)) return;\n      result.push([\n        id,\n        {\n          title: this.#toTitle(name),\n          description: `The value to substitute for the parameter \"${name}\"`,\n          type: \"object\",\n          behavior: [\"llm-content\"],\n        },\n      ]);\n    });\n    return Object.fromEntries(result);\n  }\n\n  static part(part: ParamPart) {\n    return `{${JSON.stringify(part)}}`;\n  }\n}\n\n/**\n * API for test harness\n */\n\nfunction fromTestParams(params: Record<string, string>): Params {\n  return Object.fromEntries(\n    Object.entries(params).map(([key, value]) => {\n      return [`p-z-${key}`, value];\n    })\n  );\n}\n\ntype TestInputs = {\n  inputs: { content: LLMContent; params: Record<string, string> };\n};\n\ntype TestOutputs = {\n  outputs: LLMContent;\n};\n\n/**\n * Only used for testing.\n */\nasync function invoke({\n  inputs: { content, params },\n}: TestInputs): Promise<Outcome<TestOutputs>> {\n  const template = new Template(content);\n  const result = await template.substitute(\n    fromTestParams(params),\n    async (params) => {\n      return params.path;\n    }\n  );\n  if (!ok(result)) {\n    return result;\n  }\n  return { outputs: result };\n}\n\n/**\n * Only used for testing.\n */\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: { inputs: { type: \"object\", title: \"Test inputs\" } },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: { outputs: { type: \"object\", title: \"Test outputs\" } },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Handles templated content",
        "runnable": true
      }
    },
    "gemini-prompt": {
      "code": "/**\n * @fileoverview Add a description for your module here.\n */\nimport invokeBoard from \"@invoke\";\nimport gemini, {} from \"./gemini\";\nimport { ToolManager } from \"./tool-manager\";\nimport { ok, err, toLLMContent, addUserTurn } from \"./utils\";\nexport { GeminiPrompt };\nfunction textToJson(content) {\n    return {\n        ...content,\n        parts: content.parts.map((part) => {\n            if (\"text\" in part) {\n                try {\n                    return { json: JSON.parse(part.text) };\n                }\n                catch (e) {\n                    // fall through.\n                }\n            }\n            return part;\n        }),\n    };\n}\nfunction mergeLastParts(contexts) {\n    const parts = [];\n    for (const context of contexts) {\n        const last = context.at(-1);\n        if (!last)\n            continue;\n        if (!last.parts)\n            continue;\n        parts.push(...last.parts);\n    }\n    return {\n        parts,\n        role: \"user\",\n    };\n}\nclass GeminiPrompt {\n    inputs;\n    options;\n    constructor(inputs, options) {\n        this.inputs = inputs;\n        this.options = this.#reconcileOptions(options);\n    }\n    #reconcileOptions(options) {\n        if (!options)\n            return {};\n        if (options instanceof ToolManager) {\n            return { toolManager: options };\n        }\n        return options;\n    }\n    #normalizeArgs(args, passContext) {\n        if (!passContext)\n            return args;\n        return Object.fromEntries(Object.entries(args).map(([name, value]) => {\n            const context = addUserTurn(value, [\n                ...this.inputs.body.contents,\n            ]);\n            return [name, context];\n        }));\n    }\n    async invoke() {\n        const { allowToolErrors, validator } = this.options;\n        const invoking = await gemini(this.inputs);\n        if (!ok(invoking))\n            return invoking;\n        if (\"context\" in invoking) {\n            return err(\"Invalid output from Gemini -- must be candidates\");\n        }\n        const candidate = invoking.candidates.at(0);\n        const content = candidate?.content;\n        if (!content) {\n            return err(\"No content from Gemini\");\n        }\n        if (!content.parts) {\n            return err(`Gemini failed to generate result due to ${candidate.finishReason}`);\n        }\n        const results = [];\n        const errors = [];\n        if (validator) {\n            const validating = validator(content);\n            if (!ok(validating))\n                return validating;\n        }\n        await this.options.toolManager?.processResponse(content, async ($board, args, passContext) => {\n            const callingTool = await invokeBoard({\n                $board,\n                ...this.#normalizeArgs(args, passContext),\n            });\n            if (\"$error\" in callingTool) {\n                errors.push(JSON.stringify(callingTool.$error));\n            }\n            else {\n                if (passContext) {\n                    if (!(\"context\" in callingTool)) {\n                        errors.push(`No \"context\" port in outputs of \"${$board}\"`);\n                    }\n                    else {\n                        results.push(callingTool.context);\n                    }\n                }\n                else {\n                    results.push([toLLMContent(JSON.stringify(callingTool))]);\n                }\n            }\n        });\n        if (errors.length && !allowToolErrors) {\n            return err(`Calling tools generated the following errors: ${errors.join(\",\")}`);\n        }\n        const isJSON = this.inputs.body.generationConfig?.responseMimeType == \"application/json\";\n        const result = isJSON ? [textToJson(content)] : [content];\n        if (results.length) {\n            result.push(mergeLastParts(results));\n        }\n        return { all: result, last: result.at(-1), candidate };\n    }\n}\n",
      "metadata": {
        "title": "gemini-prompt",
        "source": {
          "code": "/**\n * @fileoverview Add a description for your module here.\n */\n\nimport invokeBoard from \"@invoke\";\n\nimport gemini, {\n  type GeminiInputs,\n  type GeminiOutputs,\n  type Candidate,\n} from \"./gemini\";\nimport { ToolManager } from \"./tool-manager\";\nimport { ok, err, toLLMContent, addUserTurn } from \"./utils\";\n\nexport { GeminiPrompt };\n\nfunction textToJson(content: LLMContent): LLMContent {\n  return {\n    ...content,\n    parts: content.parts.map((part) => {\n      if (\"text\" in part) {\n        try {\n          return { json: JSON.parse(part.text) };\n        } catch (e) {\n          // fall through.\n        }\n      }\n      return part;\n    }),\n  };\n}\n\nfunction mergeLastParts(contexts: LLMContent[][]): LLMContent {\n  const parts: DataPart[] = [];\n  for (const context of contexts) {\n    const last = context.at(-1);\n    if (!last) continue;\n    if (!last.parts) continue;\n    parts.push(...last.parts);\n  }\n  return {\n    parts,\n    role: \"user\",\n  };\n}\n\nexport type ValidatorFunction = (response: LLMContent) => Outcome<void>;\n\nexport type GeminiPromptOutput = {\n  last: LLMContent;\n  all: LLMContent[];\n  candidate: Candidate;\n};\n\nexport type GeminiPromptInvokeOptions = GeminiPromptOptions;\n\nexport type GeminiPromptOptions = {\n  allowToolErrors?: boolean;\n  validator?: ValidatorFunction;\n  toolManager?: ToolManager;\n};\nclass GeminiPrompt {\n  readonly options: GeminiPromptOptions;\n\n  constructor(\n    public readonly inputs: GeminiInputs,\n    options?: ToolManager | GeminiPromptOptions\n  ) {\n    this.options = this.#reconcileOptions(options);\n  }\n\n  #reconcileOptions(\n    options?: ToolManager | GeminiPromptOptions\n  ): GeminiPromptOptions {\n    if (!options) return {};\n    if (options instanceof ToolManager) {\n      return { toolManager: options };\n    }\n    return options;\n  }\n\n  #normalizeArgs(args: object, passContext?: boolean) {\n    if (!passContext) return args;\n    return Object.fromEntries(\n      Object.entries(args).map(([name, value]) => {\n        const context = addUserTurn(value as string, [\n          ...this.inputs.body.contents,\n        ]);\n        return [name, context];\n      })\n    );\n  }\n\n  async invoke(): Promise<Outcome<GeminiPromptOutput>> {\n    const { allowToolErrors, validator } = this.options;\n    const invoking = await gemini(this.inputs);\n    if (!ok(invoking)) return invoking;\n    if (\"context\" in invoking) {\n      return err(\"Invalid output from Gemini -- must be candidates\");\n    }\n    const candidate = invoking.candidates.at(0);\n    const content = candidate?.content;\n    if (!content) {\n      return err(\"No content from Gemini\");\n    }\n    if (!content.parts) {\n      return err(\n        `Gemini failed to generate result due to ${candidate.finishReason}`\n      );\n    }\n    const results: LLMContent[][] = [];\n    const errors: string[] = [];\n    if (validator) {\n      const validating = validator(content);\n      if (!ok(validating)) return validating;\n    }\n    await this.options.toolManager?.processResponse(\n      content,\n      async ($board, args, passContext) => {\n        const callingTool = await invokeBoard({\n          $board,\n          ...this.#normalizeArgs(args, passContext),\n        });\n        if (\"$error\" in callingTool) {\n          errors.push(JSON.stringify(callingTool.$error));\n        } else {\n          if (passContext) {\n            if (!(\"context\" in callingTool)) {\n              errors.push(`No \"context\" port in outputs of \"${$board}\"`);\n            } else {\n              results.push(callingTool.context as LLMContent[]);\n            }\n          } else {\n            results.push([toLLMContent(JSON.stringify(callingTool))]);\n          }\n        }\n      }\n    );\n    if (errors.length && !allowToolErrors) {\n      return err(\n        `Calling tools generated the following errors: ${errors.join(\",\")}`\n      );\n    }\n    const isJSON =\n      this.inputs.body.generationConfig?.responseMimeType == \"application/json\";\n    const result = isJSON ? [textToJson(content)] : [content];\n    if (results.length) {\n      result.push(mergeLastParts(results));\n    }\n    return { all: result, last: result.at(-1)!, candidate };\n  }\n}\n",
          "language": "typescript"
        },
        "description": "Add a description for your module here.",
        "runnable": false
      }
    },
    "introducer": {
      "code": "/**\n * @fileoverview Handles introduction of the step.\n */\nimport { defaultSafetySettings } from \"./gemini\";\nimport { toLLMContent, toText, ok, err, llm } from \"./utils\";\nimport { ToolManager } from \"./tool-manager\";\nimport { GeminiPrompt } from \"./gemini-prompt\";\nimport {} from \"./common\";\nexport { Introducer, ArgumentNameGenerator };\nfunction introductionSchema() {\n    return {\n        type: \"object\",\n        properties: {\n            title: {\n                type: \"string\",\n                description: \"The title of the agent\",\n            },\n            abilities: {\n                type: \"string\",\n                description: \"Verb-first, third-person summary of the agent's abilities\",\n            },\n            argument: {\n                type: \"string\",\n                description: \"The description of the single text argument that the agent takes as input\",\n            },\n        },\n    };\n}\nclass ArgumentNameGenerator {\n    title;\n    description;\n    constructor(title, description) {\n        this.title = title;\n        this.description = description;\n    }\n    async invoke() {\n        const naming = await new GeminiPrompt({\n            body: {\n                contents: [this.prompt()],\n                safetySettings: defaultSafetySettings(),\n                generationConfig: {\n                    responseSchema: this.schema(),\n                    responseMimeType: \"application/json\",\n                },\n            },\n        }).invoke();\n        if (!ok(naming))\n            return naming;\n        const result = naming.last.parts.at(0).json;\n        return {\n            title: this.title,\n            description: this.description,\n            inputSchema: {\n                type: \"object\",\n                properties: {\n                    context: {\n                        type: \"string\",\n                        description: result.description,\n                    },\n                },\n            },\n        };\n    }\n    schema() {\n        return {\n            type: \"object\",\n            properties: {\n                description: {\n                    type: \"string\",\n                    description: \"One-sentence description of a function argument\",\n                },\n            },\n        };\n    }\n    prompt() {\n        return llm `\nYou are amazing at describing things. Today, you will be coming up a one-sentence description \nof a function argument.\n\nThe function's title is: ${this.title}\n\nThe function's description is ${this.description}\n\nIt takes a single argument.\n\nCome up with a one-sentence description of this argument based on the title/description,\nwith the aim of using this description in a JSON Schema.\n`.asContent();\n    }\n}\nclass Introducer {\n    instruction;\n    toolManager;\n    constructor(instruction, toolManager) {\n        this.instruction = instruction;\n        this.toolManager = toolManager;\n    }\n    prompt() {\n        const tools = this.toolManager?.list() || [];\n        let toolInstruction = \"You have no access to tools of any kind.\";\n        if (tools.length > 0) {\n            toolInstruction = `You have access to the following tools:\n${tools.map((tool) => JSON.stringify(tool)).join(\"\\n\\n\")}\nMake sure to mention your ability to call these particular tools in your abilities.\n`;\n        }\n        return toLLMContent(`You are a project manager for team of AI agents.\nRight now, your job is to summarize what one AI agent does.\nGiven an agent prompt, you distill it to a brief summary of abilities.\nThis summary will be used to decide when to invoke this agent.\n# AI Agent Prompt\n\\`\\`\\`\n${toText(this.instruction)}\n${toolInstruction}\n\\`\\`\\`\nReply in JSON using the provided schema.\n`);\n    }\n    async invoke() {\n        const introducing = await new GeminiPrompt({\n            body: {\n                contents: [this.prompt()],\n                safetySettings: defaultSafetySettings(),\n                generationConfig: {\n                    responseSchema: introductionSchema(),\n                    responseMimeType: \"application/json\",\n                },\n            },\n        }).invoke();\n        if (!ok(introducing))\n            return introducing;\n        const intro = introducing.last.parts.at(0)\n            .json;\n        return {\n            title: intro.title,\n            description: intro.abilities,\n            inputSchema: {\n                type: \"object\",\n                properties: {\n                    context: {\n                        type: \"string\",\n                        description: intro.argument,\n                    },\n                },\n            },\n        };\n    }\n}\n",
      "metadata": {
        "title": "introducer",
        "source": {
          "code": "/**\n * @fileoverview Handles introduction of the step.\n */\n\nimport { type Tool, defaultSafetySettings, type GeminiSchema } from \"./gemini\";\nimport { toLLMContent, toText, ok, err, llm } from \"./utils\";\nimport { ToolManager } from \"./tool-manager\";\nimport { GeminiPrompt } from \"./gemini-prompt\";\nimport { type DescriberResult } from \"./common\";\n\nexport { Introducer, ArgumentNameGenerator };\n\nexport type IntroPort = {\n  $intro: boolean;\n};\n\ntype Introduction = {\n  title: string;\n  abilities: string;\n  argument: string;\n};\n\nfunction introductionSchema(): GeminiSchema {\n  return {\n    type: \"object\",\n    properties: {\n      title: {\n        type: \"string\",\n        description: \"The title of the agent\",\n      },\n      abilities: {\n        type: \"string\",\n        description:\n          \"Verb-first, third-person summary of the agent's abilities\",\n      },\n      argument: {\n        type: \"string\",\n        description:\n          \"The description of the single text argument that the agent takes as input\",\n      },\n    },\n  };\n}\n\ntype NamingResult = {\n  description: string;\n};\n\nclass ArgumentNameGenerator {\n  constructor(\n    public readonly title: string,\n    public readonly description: string\n  ) {}\n\n  async invoke(): Promise<Outcome<DescriberResult>> {\n    const naming = await new GeminiPrompt({\n      body: {\n        contents: [this.prompt()],\n        safetySettings: defaultSafetySettings(),\n        generationConfig: {\n          responseSchema: this.schema(),\n          responseMimeType: \"application/json\",\n        },\n      },\n    }).invoke();\n    if (!ok(naming)) return naming;\n    const result = (naming.last.parts.at(0) as JSONPart).json as NamingResult;\n\n    return {\n      title: this.title,\n      description: this.description,\n      inputSchema: {\n        type: \"object\",\n        properties: {\n          context: {\n            type: \"string\",\n            description: result.description,\n          },\n        },\n      },\n    };\n  }\n\n  schema(): GeminiSchema {\n    return {\n      type: \"object\",\n      properties: {\n        description: {\n          type: \"string\",\n          description: \"One-sentence description of a function argument\",\n        },\n      },\n    };\n  }\n\n  prompt(): LLMContent {\n    return llm`\nYou are amazing at describing things. Today, you will be coming up a one-sentence description \nof a function argument.\n\nThe function's title is: ${this.title}\n\nThe function's description is ${this.description}\n\nIt takes a single argument.\n\nCome up with a one-sentence description of this argument based on the title/description,\nwith the aim of using this description in a JSON Schema.\n`.asContent();\n  }\n}\n\nclass Introducer {\n  constructor(\n    public readonly instruction: LLMContent,\n    public readonly toolManager?: ToolManager\n  ) {}\n\n  prompt(): LLMContent {\n    const tools = this.toolManager?.list() || [];\n    let toolInstruction = \"You have no access to tools of any kind.\";\n    if (tools.length > 0) {\n      toolInstruction = `You have access to the following tools:\n${tools.map((tool) => JSON.stringify(tool)).join(\"\\n\\n\")}\nMake sure to mention your ability to call these particular tools in your abilities.\n`;\n    }\n    return toLLMContent(`You are a project manager for team of AI agents.\nRight now, your job is to summarize what one AI agent does.\nGiven an agent prompt, you distill it to a brief summary of abilities.\nThis summary will be used to decide when to invoke this agent.\n# AI Agent Prompt\n\\`\\`\\`\n${toText(this.instruction)}\n${toolInstruction}\n\\`\\`\\`\nReply in JSON using the provided schema.\n`);\n  }\n\n  async invoke(): Promise<Outcome<DescriberResult>> {\n    const introducing = await new GeminiPrompt({\n      body: {\n        contents: [this.prompt()],\n        safetySettings: defaultSafetySettings(),\n        generationConfig: {\n          responseSchema: introductionSchema(),\n          responseMimeType: \"application/json\",\n        },\n      },\n    }).invoke();\n    if (!ok(introducing)) return introducing;\n    const intro = (introducing.last.parts.at(0) as JSONPart)\n      .json as Introduction;\n\n    return {\n      title: intro.title,\n      description: intro.abilities,\n      inputSchema: {\n        type: \"object\",\n        properties: {\n          context: {\n            type: \"string\",\n            description: intro.argument,\n          },\n        },\n      },\n    };\n  }\n}\n",
          "language": "typescript"
        },
        "description": "Handles introduction of the step.",
        "runnable": false
      }
    }
  },
  "metadata": {
    "tags": []
  }
}