{
  "title": "Generate Text",
  "description": "",
  "version": "0.0.1",
  "nodes": [],
  "edges": [],
  "metadata": {
    "comments": [
      {
        "id": "comment-cc94afe8",
        "text": "Intentionally Left Blank",
        "metadata": {
          "title": "Comment",
          "visual": {
            "x": 531,
            "y": 374,
            "collapsed": "expanded",
            "outputHeight": 0
          }
        }
      }
    ],
    "visual": {
      "presentation": {
        "themes": {
          "5f3ca599-8fee-46fb-951f-0d47b16a6d56": {
            "themeColors": {
              "primaryColor": "#246db5",
              "secondaryColor": "#5cadff",
              "backgroundColor": "#ffffff",
              "textColor": "#1a1a1a",
              "primaryTextColor": "#ffffff"
            },
            "template": "basic",
            "splashScreen": {
              "storedData": {
                "handle": "/images/app/generic-flow.jpg",
                "mimeType": "image/jpeg"
              }
            }
          }
        },
        "theme": "5f3ca599-8fee-46fb-951f-0d47b16a6d56"
      }
    },
    "tags": [
      "published",
      "tool",
      "component"
    ],
    "userModified": true
  },
  "imports": {
    "a2": {
      "url": "./a2.bgl.json"
    }
  },
  "graphs": {
    "daf082ca-c1aa-4aff-b2c8-abeb984ab66c": {
      "title": "Make Text",
      "description": "Generates text and so much more.",
      "version": "0.0.1",
      "describer": "module:entry",
      "nodes": [
        {
          "type": "output",
          "id": "output",
          "configuration": {
            "schema": {
              "properties": {
                "context": {
                  "type": "array",
                  "title": "Context",
                  "items": {
                    "type": "object",
                    "behavior": [
                      "llm-content"
                    ]
                  },
                  "default": "null"
                }
              },
              "type": "object",
              "required": []
            }
          },
          "metadata": {
            "visual": {
              "x": 720,
              "y": 0,
              "collapsed": "expanded",
              "outputHeight": 44
            }
          }
        },
        {
          "id": "board-f138aa03",
          "type": "#module:entry",
          "metadata": {
            "visual": {
              "x": -46.99999999999966,
              "y": -71.99999999999898,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "entry"
          }
        },
        {
          "id": "board-d340ad8f",
          "type": "#module:main",
          "metadata": {
            "visual": {
              "x": 340,
              "y": 0,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "Generating draft",
            "logLevel": "info"
          },
          "configuration": {}
        },
        {
          "id": "board-1946064a",
          "type": "#module:join",
          "metadata": {
            "visual": {
              "x": 1059.9999999999986,
              "y": -159.99999999999886,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "join"
          }
        },
        {
          "type": "input",
          "id": "input",
          "metadata": {
            "visual": {
              "x": 720.0000000000005,
              "y": 160.00000000000114,
              "collapsed": "advanced",
              "outputHeight": 44
            },
            "title": "Waiting for user feedback",
            "logLevel": "info"
          },
          "configuration": {}
        }
      ],
      "edges": [
        {
          "from": "board-f138aa03",
          "to": "board-d340ad8f",
          "out": "context",
          "in": "context"
        },
        {
          "from": "board-d340ad8f",
          "to": "output",
          "out": "done",
          "in": "context"
        },
        {
          "from": "input",
          "to": "board-1946064a",
          "out": "request",
          "in": "request"
        },
        {
          "from": "board-d340ad8f",
          "to": "input",
          "out": "toInput",
          "in": "schema"
        },
        {
          "from": "board-d340ad8f",
          "to": "board-1946064a",
          "out": "context",
          "in": "context"
        },
        {
          "from": "board-1946064a",
          "to": "board-d340ad8f",
          "out": "context",
          "in": "context"
        }
      ],
      "metadata": {
        "visual": {
          "minimized": false
        },
        "describer": "module:entry",
        "tags": []
      }
    }
  },
  "modules": {
    "chat-tools": {
      "code": "/**\n * @fileoverview Tools for conversational (\"chat\") mode\n */\nvar __classPrivateFieldSet = (this && this.__classPrivateFieldSet) || function (receiver, state, value, kind, f) {\n    if (kind === \"m\") throw new TypeError(\"Private method is not writable\");\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a setter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot write private member to an object whose class did not declare it\");\n    return (kind === \"a\" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value)), value;\n};\nvar __classPrivateFieldGet = (this && this.__classPrivateFieldGet) || function (receiver, state, kind, f) {\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a getter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot read private member from an object whose class did not declare it\");\n    return kind === \"m\" ? f : kind === \"a\" ? f.call(receiver) : f ? f.value : state.get(receiver);\n};\nvar _ChatToolImpl_invoked;\nexport { createDoneTool, createKeepChattingResult, createKeepChattingTool };\nfunction createKeepChattingResult() {\n    return {\n        parts: [{ functionCall: { name: \"User_Asks_For_More_Work\", args: {} } }],\n        role: \"model\",\n    };\n}\nclass ChatToolImpl {\n    constructor(name, description) {\n        this.name = name;\n        this.description = description;\n        _ChatToolImpl_invoked.set(this, false);\n    }\n    reset() {\n        __classPrivateFieldSet(this, _ChatToolImpl_invoked, false, \"f\");\n    }\n    get invoked() {\n        return __classPrivateFieldGet(this, _ChatToolImpl_invoked, \"f\");\n    }\n    declaration() {\n        return {\n            name: this.name,\n            description: this.description,\n            parameters: { type: \"object\" },\n        };\n    }\n    handle() {\n        return {\n            tool: this.declaration(),\n            url: \"\",\n            passContext: false,\n            invoke: async () => {\n                __classPrivateFieldSet(this, _ChatToolImpl_invoked, true, \"f\");\n            },\n        };\n    }\n}\n_ChatToolImpl_invoked = new WeakMap();\nfunction createDoneTool() {\n    return new ChatToolImpl(\"User_Says_Done\", \"Call when the user indicates they are done with the conversation and are ready to move on\");\n}\nfunction createKeepChattingTool() {\n    return new ChatToolImpl(\"User_Asks_For_More_Work\", \"Call when the user asked a question or issued instruction to do more work\");\n}\n"
    },
    "entry": {
      "code": "/**\n * @fileoverview Manages the entry point: describer, passing the inputs, etc.\n */\nimport { readSettings } from \"./a2/settings\";\nimport { Template } from \"./a2/template\";\nimport { defaultLLMContent, ok } from \"./a2/utils\";\nimport { defaultSystemInstruction } from \"./system-instruction\";\nexport { invoke as default, describe };\nasync function invoke({ context, \"p-chat\": chat, \"p-list\": makeList, \"b-system-instruction\": systemInstruction, \"p-model-name\": model = \"\", description, ...params }) {\n    // Make sure it's a boolean.\n    chat = !!chat;\n    context ?? (context = []);\n    const type = \"work\";\n    return {\n        context: {\n            id: Math.random().toString(36).substring(2, 5),\n            chat,\n            makeList,\n            listPath: [],\n            context,\n            userInputs: [],\n            defaultModel: model,\n            model: model,\n            description,\n            type,\n            work: [],\n            userEndedChat: false,\n            params,\n            systemInstruction,\n        },\n    };\n}\nasync function describe({ inputs: { description, \"config$ask-user\": chat }, }) {\n    const settings = await readSettings();\n    const chatSchema = chat ? [\"hint-chat-mode\"] : [];\n    const experimental = ok(settings) && !!settings[\"Show Experimental Components\"];\n    const template = new Template(description);\n    let extra = {};\n    if (experimental) {\n        extra = {\n        // \"p-list\": {\n        //   type: \"boolean\",\n        //   title: \"Make a list\",\n        //   behavior: [\"config\", \"hint-preview\", \"hint-advanced\"],\n        //   icon: \"summarize\",\n        //   description:\n        //     \"When checked, this step will try to create a list as its output. Make sure that the prompt asks for a list of some sort\",\n        // },\n        };\n    }\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                description: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"Prompt\",\n                    description: \"Give the model additional context on what to do, like specific rules/guidelines to adhere to or specify behavior separate from the provided context.\",\n                    default: defaultLLMContent(),\n                },\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                \"p-chat\": {\n                    type: \"boolean\",\n                    title: \"Review with user\",\n                    behavior: [\"config\", \"hint-preview\", \"hint-advanced\"],\n                    icon: \"chat\",\n                    description: \"When checked, this step will chat with the user, asking to review work, requesting additional information, etc.\",\n                },\n                \"b-system-instruction\": {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-advanced\"],\n                    title: \"System Instruction\",\n                    description: \"The system instruction for the model\",\n                    default: JSON.stringify(defaultSystemInstruction()),\n                },\n                \"p-model-name\": {\n                    type: \"string\",\n                    behavior: [\"llm-content\"],\n                    title: \"Model\",\n                    description: \"The specific model version to generate with\",\n                },\n                ...extra,\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\", ...chatSchema],\n            ...template.requireds(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"main-port\", \"hint-text\"],\n                },\n            },\n        },\n        title: \"Make Text\",\n        metadata: {\n            icon: \"generative-text\",\n            tags: [\"quick-access\", \"generative\"],\n            order: 1,\n        },\n    };\n}\n"
    },
    "join": {
      "code": "/**\n * @fileoverview Joins user input and Agent Context\n */\nimport { addContent } from \"./a2/lists\";\nimport { isEmpty } from \"./a2/utils\";\nexport { invoke as default, describe };\nasync function invoke({ context, request }) {\n    context.userEndedChat = isEmpty(request);\n    context.userInputs.push(request);\n    if (!context.userEndedChat) {\n        context.work = addContent(context.work, request);\n    }\n    return { context };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    title: \"Agent Context\",\n                    type: \"object\",\n                },\n                request: {\n                    title: \"User Input\",\n                    type: \"object\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    title: \"Agent Context\",\n                    type: \"object\",\n                },\n            },\n        },\n    };\n}\n"
    },
    "main": {
      "code": "/**\n * @fileoverview Add a description for your module here.\n */\nvar __classPrivateFieldSet = (this && this.__classPrivateFieldSet) || function (receiver, state, value, kind, f) {\n    if (kind === \"m\") throw new TypeError(\"Private method is not writable\");\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a setter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot write private member to an object whose class did not declare it\");\n    return (kind === \"a\" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value)), value;\n};\nvar __classPrivateFieldGet = (this && this.__classPrivateFieldGet) || function (receiver, state, kind, f) {\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a getter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot read private member from an object whose class did not declare it\");\n    return kind === \"m\" ? f : kind === \"a\" ? f.call(receiver) : f ? f.value : state.get(receiver);\n};\nvar _GenerateText_hasTools;\nimport output from \"@output\";\nimport { createDoneTool, createKeepChattingResult, createKeepChattingTool, } from \"./chat-tools\";\nimport { createSystemInstruction } from \"./system-instruction\";\nimport { ArgumentNameGenerator } from \"./a2/introducer\";\nimport { ListExpander, listSchema, toList } from \"./a2/lists\";\nimport { report } from \"./a2/output\";\nimport { Template } from \"./a2/template\";\nimport { ToolManager } from \"./a2/tool-manager\";\nimport { defaultLLMContent, err, ok } from \"./a2/utils\";\nimport { defaultSafetySettings } from \"./a2/gemini\";\nimport { GeminiPrompt } from \"./a2/gemini-prompt\";\nexport { invoke as default, describe };\n/**\n * Maximum amount of function-calling turns that we take before bailing.\n */\nconst MAX_TURN_COUNT = 10;\nclass GenerateText {\n    constructor(caps, sharedContext) {\n        this.caps = caps;\n        this.sharedContext = sharedContext;\n        this.listMode = false;\n        _GenerateText_hasTools.set(this, false);\n        this.invoke = this.invoke.bind(this);\n    }\n    async initialize() {\n        const { sharedContext } = this;\n        const template = new Template(sharedContext.description);\n        const toolManager = new ToolManager(new ArgumentNameGenerator(this.caps));\n        const doneTool = createDoneTool();\n        const keepChattingTool = createKeepChattingTool();\n        const substituting = await template.substitute(sharedContext.params, async ({ path: url, instance }) => toolManager.addTool(url, instance));\n        __classPrivateFieldSet(this, _GenerateText_hasTools, toolManager.hasTools(), \"f\");\n        if (sharedContext.chat) {\n            toolManager.addCustomTool(doneTool.name, doneTool.handle());\n            if (!__classPrivateFieldGet(this, _GenerateText_hasTools, \"f\")) {\n                toolManager.addCustomTool(keepChattingTool.name, keepChattingTool.handle());\n            }\n        }\n        if (!ok(substituting)) {\n            return substituting;\n        }\n        this.description = substituting;\n        this.toolManager = toolManager;\n        this.doneTool = doneTool;\n        this.keepChattingTool = keepChattingTool;\n        this.context = [...sharedContext.context, ...sharedContext.work];\n    }\n    createSystemInstruction(makeList) {\n        return createSystemInstruction(this.sharedContext.systemInstruction, makeList);\n    }\n    addKeepChattingResult(context) {\n        context.push(createKeepChattingResult());\n        return context;\n    }\n    /**\n     * Invokes the text generator.\n     * Significant mode flags:\n     * - chat: boolean -- chat mode is on/off\n     * - tools: boolean -- whether or not has tools\n     * - makeList: boolean -- asked to generate a list\n     * - isList: boolean -- is currently in list mode\n     * - model: string -- the model to generate with\n     */\n    async invoke(description, work, isList) {\n        const { sharedContext } = this;\n        const toolManager = this.toolManager;\n        const doneTool = this.doneTool;\n        const keepChattingTool = this.keepChattingTool;\n        // Disallow making nested lists (for now).\n        const makeList = sharedContext.makeList && !isList;\n        const safetySettings = defaultSafetySettings();\n        const systemInstruction = this.createSystemInstruction(makeList);\n        const tools = toolManager.list();\n        // Unless it's a very first turn, we always supply tools when chatting,\n        // since we add the \"Done\" and \"Keep Chatting\" tools to figure out when\n        // the conversation ends.\n        // In the first turn, we actually create fake \"keep chatting\" result,\n        // to help the LLM get into the rhythm. Like, \"come on, LLM\".\n        const firstTurn = this.firstTurn;\n        const shouldAddTools = (this.chat && !firstTurn) || __classPrivateFieldGet(this, _GenerateText_hasTools, \"f\");\n        const shouldAddFakeResult = this.chat && firstTurn;\n        let product;\n        const context = !shouldAddTools && shouldAddFakeResult\n            ? this.addKeepChattingResult([description])\n            : [description];\n        const contents = [...context, ...work];\n        const inputs = {\n            body: { contents, safetySettings },\n            model: sharedContext.model,\n        };\n        if (shouldAddTools) {\n            inputs.body.tools = [...tools];\n            if (this.toolManager.hasToolDeclarations()) {\n                inputs.body.toolConfig = { functionCallingConfig: { mode: \"ANY\" } };\n            }\n        }\n        else {\n            // When we have tools, the first call will not try to make a list,\n            // because JSON mode and tool-calling are incompatible.\n            if (makeList) {\n                inputs.body.generationConfig = {\n                    responseSchema: listSchema(),\n                    responseMimeType: \"application/json\",\n                };\n            }\n        }\n        inputs.body.systemInstruction = systemInstruction;\n        const prompt = new GeminiPrompt(this.caps, inputs, { toolManager });\n        const result = await prompt.invoke();\n        if (!ok(result))\n            return result;\n        const calledTools = prompt.calledTools || doneTool.invoked || keepChattingTool.invoked;\n        if (calledTools) {\n            if (doneTool.invoked) {\n                return result.last;\n            }\n            const invokedSubgraph = prompt.calledCustomTools;\n            if (invokedSubgraph) {\n                if (makeList && !this.chat) {\n                    // This case might be unusual (making a list of images directly?),\n                    // but handle it for completeness.\n                    // TODO: support this case properly. This seems\n                    const list = toList(result.last);\n                    if (!ok(list))\n                        return list;\n                    product = list;\n                }\n                else {\n                    // Be careful to return subgraph output (which can be media) as-is\n                    // without rewriting/summarizing it with gemini because gemini cannot generate media.\n                    product = result.last;\n                }\n            }\n            else {\n                if (!keepChattingTool.invoked) {\n                    contents.push(...result.all);\n                }\n                const inputs = {\n                    model: sharedContext.model,\n                    body: { contents, systemInstruction, safetySettings },\n                };\n                if (makeList) {\n                    inputs.body.generationConfig = {\n                        responseSchema: listSchema(),\n                        responseMimeType: \"application/json\",\n                    };\n                }\n                else {\n                    if (shouldAddTools) {\n                        // If we added function declarations (or saw a function call request) before, then we need to add them again so\n                        // Gemini isn't confused by the presence of a function call request.\n                        // However, set the mode to NONE so we don't call tools again.\n                        inputs.body.tools = [...tools];\n                        console.log(\"adding tools\");\n                        // Can't set to functionCallingConfig mode to NONE, as that seems to hallucinate tool use.\n                    }\n                }\n                const keepCallingGemini = true;\n                let afterTools = undefined;\n                let turnCount = 0;\n                while (keepCallingGemini) {\n                    if (turnCount > MAX_TURN_COUNT && toolManager.hasToolDeclarations()) {\n                        inputs.body.toolConfig = {\n                            functionCallingConfig: {\n                                mode: \"NONE\",\n                            },\n                        };\n                    }\n                    const nextTurn = new GeminiPrompt(this.caps, inputs, { toolManager });\n                    const nextTurnResult = await nextTurn.invoke();\n                    if (!ok(nextTurnResult))\n                        return nextTurnResult;\n                    if (!nextTurn.calledTools && !nextTurn.calledCustomTools) {\n                        afterTools = nextTurnResult;\n                        break;\n                    }\n                    inputs.body.contents = [\n                        ...inputs.body.contents,\n                        ...nextTurnResult.all,\n                    ];\n                    turnCount++;\n                }\n                if (!afterTools) {\n                    return err(`Invalid state: Somehow, \"afterTools\" is undefined.`, {\n                        origin: \"client\",\n                        kind: \"bug\",\n                    });\n                }\n                if (makeList && !this.chat) {\n                    const list = toList(afterTools.last);\n                    if (!ok(list))\n                        return list;\n                    product = list;\n                }\n                else {\n                    product = afterTools.last;\n                }\n            }\n        }\n        else {\n            if (makeList && !this.chat) {\n                const list = toList(result.last);\n                if (!ok(list))\n                    return list;\n                product = list;\n            }\n            else {\n                product = result.last;\n            }\n        }\n        return product;\n    }\n    get firstTurn() {\n        return this.sharedContext.userInputs.length === 0;\n    }\n    get chat() {\n        // When we are in list mode, disable chat.\n        // Can't have chat inside of a list (yet).\n        return this.sharedContext.chat && !this.listMode;\n    }\n    get doneChatting() {\n        return !!this.doneTool?.invoked;\n    }\n}\n_GenerateText_hasTools = new WeakMap();\nfunction done(result, makeList = false) {\n    if (makeList) {\n        const list = toList(result.at(-1));\n        if (!ok(list))\n            return list;\n        result = [list];\n    }\n    return { done: result };\n}\nasync function keepChatting(sharedContext, result, isList) {\n    const last = result.at(-1);\n    let product = last;\n    if (isList) {\n        const list = toList(last);\n        if (!ok(list))\n            return list;\n        product = list;\n    }\n    await output({\n        schema: {\n            type: \"object\",\n            properties: {\n                \"a-product\": {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"hint-chat-mode\"],\n                    title: \"Draft\",\n                },\n            },\n        },\n        $metadata: {\n            title: \"Writer\",\n            description: \"Asking user\",\n            icon: \"generative-text\",\n        },\n        \"a-product\": product,\n    });\n    const toInput = {\n        type: \"object\",\n        properties: {\n            request: {\n                type: \"object\",\n                title: \"Please provide feedback\",\n                description: \"Provide feedback or click submit to continue\",\n                behavior: [\"transient\", \"llm-content\"],\n                examples: [defaultLLMContent()],\n            },\n        },\n    };\n    return {\n        toInput,\n        context: {\n            ...sharedContext,\n            work: result,\n            last,\n        },\n    };\n}\nasync function invoke({ context }, caps) {\n    if (!context.description) {\n        const msg = \"Please provide a prompt for the step\";\n        await report({\n            actor: \"Text Generator\",\n            name: msg,\n            category: \"Runtime error\",\n            details: `In order to run, I need to have an instruction.`,\n        });\n        return err(msg, { origin: \"client\", kind: \"config\" });\n    }\n    // Check to see if the user ended chat and return early.\n    const { userEndedChat, last } = context;\n    if (userEndedChat) {\n        if (!last) {\n            return err(\"Chat ended without any work\", {\n                origin: \"client\",\n                kind: \"bug\",\n            });\n        }\n        return done([...context.context, last], context.makeList);\n    }\n    const gen = new GenerateText(caps, context);\n    const initializing = await gen.initialize();\n    if (!ok(initializing))\n        return initializing;\n    const expander = new ListExpander(gen.description, gen.context);\n    expander.expand();\n    gen.listMode = expander.list().length > 1;\n    const result = await expander.map(gen.invoke);\n    if (!ok(result))\n        return result;\n    console.log(\"RESULT\", result);\n    if (gen.doneChatting) {\n        // If done tool was invoked, rewind to remove the last interaction\n        // and return that.\n        const previousResult = context.work.at(-2);\n        if (!previousResult) {\n            return err(`Done chatting, but have nothing to pass along to next step.`, { origin: \"client\", kind: \"bug\" });\n        }\n        return done([previousResult], context.makeList);\n    }\n    // Use the gen.chat here, because it will correctly prevent\n    // chat mode when we're in list mode.\n    if (gen.chat && !userEndedChat) {\n        return keepChatting(gen.sharedContext, result, context.makeList);\n    }\n    // Fall through to default response.\n    return done(result);\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                },\n            },\n        },\n    };\n}\n"
    },
    "system-instruction-ts": {
      "code": "/**\n * @fileoverview Helps create a system instruction.\n */\nimport { listPrompt } from \"./a2/lists\";\nimport { llm } from \"./a2/utils\";\nexport { createSystemInstruction };\nfunction defaultSystemInstruction() {\n    return llm `IMPORTANT NOTE: Start directly with the output, do not output any delimiters.\nYou are working as part of an AI system, so no chit-chat and no explainining what you're doing and why.\nDO NOT start with \"Okay\", or \"Alright\" or any preambles.\nJust the output, please.\nTake a Deep Breath, read the instructions again, read the inputs again.\nEach instruction is crucial and must be executed with utmost care and attention to detail.`.asContent();\n}\nfunction createSystemInstruction(existing, makeList) {\n    if (existing) {\n        existing = defaultSystemInstruction();\n    }\n    const builtIn = llm `\n\nToday is ${new Date().toLocaleString(\"en-US\", {\n        month: \"long\",\n        day: \"numeric\",\n        year: \"numeric\",\n        hour: \"numeric\",\n        minute: \"2-digit\",\n    })}\n    \n${existing}`.asContent();\n    if (!makeList)\n        return builtIn;\n    return listPrompt(builtIn);\n}\n"
    },
    "system-instruction": {
      "code": "/**\n * @fileoverview Helps create a system instruction.\n */\nimport { listPrompt } from \"./a2/lists\";\nimport { llm } from \"./a2/utils\";\nexport { createSystemInstruction, defaultSystemInstruction };\nfunction defaultSystemInstruction() {\n    return llm `You are working as part of an AI system, so no chit-chat and no explaining what you're doing and why.\nDO NOT start with \"Okay\", or \"Alright\" or any preambles. Just the output, please.`.asContent();\n}\nfunction createSystemInstruction(existing, makeList) {\n    if (!existing) {\n        existing = defaultSystemInstruction();\n    }\n    const builtIn = llm `\n\nToday is ${new Date().toLocaleString(\"en-US\", {\n        month: \"long\",\n        day: \"numeric\",\n        year: \"numeric\",\n        hour: \"numeric\",\n        minute: \"2-digit\",\n    })}\n    \n${existing}`.asContent();\n    if (!makeList)\n        return builtIn;\n    return listPrompt(builtIn);\n}\n"
    },
    "types": {
      "code": "/**\n * @fileoverview Common type definitions\n */\nexport {};\n"
    }
  },
  "exports": [
    "#module:main"
  ],
  "assets": {
    "@@thumbnail": {
      "metadata": {
        "title": "Thumbnail",
        "type": "file"
      },
      "data": "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjUwIiBoZWlnaHQ9IjIwMCIgdmlld0JveD0iMCAwIDI1MCAyMDAiIGZpbGw9Im5vbmUiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+CiAgICAKICAgICAgPHJlY3QgeD0iMTM5LjA1IgogICAgICAgICAgICAgICAgICAgIHk9Ijg3Ljg5IgogICAgICAgICAgICAgICAgICAgIHdpZHRoPSI0My43NSIKICAgICAgICAgICAgICAgICAgICBoZWlnaHQ9IjI0LjIzIgogICAgICAgICAgICAgICAgICAgIHJ4PSIzLjUiCiAgICAgICAgICAgICAgICAgICAgZmlsbD0id2hpdGUiCiAgICAgICAgICAgICAgICAgICAgc3Ryb2tlPSIjMjBhMjAyIiAvPgo8cmVjdCB4PSIxMC4wMCIKICAgICAgICAgICAgICAgICAgICB5PSI3NS43NyIKICAgICAgICAgICAgICAgICAgICB3aWR0aD0iNDMuNzUiCiAgICAgICAgICAgICAgICAgICAgaGVpZ2h0PSIyNC4yMyIKICAgICAgICAgICAgICAgICAgICByeD0iMy41IgogICAgICAgICAgICAgICAgICAgIGZpbGw9IndoaXRlIgogICAgICAgICAgICAgICAgICAgIHN0cm9rZT0iIzJlOGJlOCIgLz4KPHJlY3QgeD0iNzUuMTEiCiAgICAgICAgICAgICAgICAgICAgeT0iODcuODkiCiAgICAgICAgICAgICAgICAgICAgd2lkdGg9IjQzLjc1IgogICAgICAgICAgICAgICAgICAgIGhlaWdodD0iMjQuMjMiCiAgICAgICAgICAgICAgICAgICAgcng9IjMuNSIKICAgICAgICAgICAgICAgICAgICBmaWxsPSJ3aGl0ZSIKICAgICAgICAgICAgICAgICAgICBzdHJva2U9IiMyZThiZTgiIC8+CjxyZWN0IHg9IjE5Ni4yNSIKICAgICAgICAgICAgICAgICAgICB5PSI2MC45NyIKICAgICAgICAgICAgICAgICAgICB3aWR0aD0iNDMuNzUiCiAgICAgICAgICAgICAgICAgICAgaGVpZ2h0PSIyNC4yMyIKICAgICAgICAgICAgICAgICAgICByeD0iMy41IgogICAgICAgICAgICAgICAgICAgIGZpbGw9IndoaXRlIgogICAgICAgICAgICAgICAgICAgIHN0cm9rZT0iIzJlOGJlOCIgLz4KPHJlY3QgeD0iMTM5LjA1IgogICAgICAgICAgICAgICAgICAgIHk9IjExNC44MSIKICAgICAgICAgICAgICAgICAgICB3aWR0aD0iNDMuNzUiCiAgICAgICAgICAgICAgICAgICAgaGVpZ2h0PSIyNC4yMyIKICAgICAgICAgICAgICAgICAgICByeD0iMy41IgogICAgICAgICAgICAgICAgICAgIGZpbGw9IndoaXRlIgogICAgICAgICAgICAgICAgICAgIHN0cm9rZT0iIzIwYTIwMiIgLz4KICAgIDwvc3ZnPg=="
    }
  }
}