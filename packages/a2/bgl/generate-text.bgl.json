{
  "title": "Generate Text",
  "description": "",
  "version": "0.0.1",
  "nodes": [],
  "edges": [],
  "metadata": {
    "comments": [
      {
        "id": "comment-cc94afe8",
        "text": "Intentionally Left Blank",
        "metadata": {
          "title": "Comment",
          "visual": {
            "x": 531,
            "y": 374,
            "collapsed": "expanded",
            "outputHeight": 0
          }
        }
      }
    ],
    "visual": {
      "presentation": {
        "themes": {
          "5f3ca599-8fee-46fb-951f-0d47b16a6d56": {
            "themeColors": {
              "primaryColor": "#246db5",
              "secondaryColor": "#5cadff",
              "backgroundColor": "#ffffff",
              "textColor": "#1a1a1a",
              "primaryTextColor": "#ffffff"
            },
            "template": "basic",
            "splashScreen": {
              "storedData": {
                "handle": "/images/app/generic-flow.jpg",
                "mimeType": "image/jpeg"
              }
            }
          }
        },
        "theme": "5f3ca599-8fee-46fb-951f-0d47b16a6d56"
      }
    },
    "tags": [
      "published",
      "tool",
      "component"
    ],
    "userModified": true
  },
  "imports": {
    "a2": {
      "url": "./a2.bgl.json"
    }
  },
  "graphs": {
    "daf082ca-c1aa-4aff-b2c8-abeb984ab66c": {
      "title": "Make Text",
      "description": "Generates text and so much more.",
      "version": "0.0.1",
      "describer": "module:entry",
      "nodes": [
        {
          "type": "output",
          "id": "output",
          "configuration": {
            "schema": {
              "properties": {
                "context": {
                  "type": "array",
                  "title": "Context",
                  "items": {
                    "type": "object",
                    "behavior": [
                      "llm-content"
                    ]
                  },
                  "default": "null"
                }
              },
              "type": "object",
              "required": []
            }
          },
          "metadata": {
            "visual": {
              "x": 720,
              "y": 0,
              "collapsed": "expanded",
              "outputHeight": 44
            }
          }
        },
        {
          "id": "board-f138aa03",
          "type": "#module:entry",
          "metadata": {
            "visual": {
              "x": -46.99999999999966,
              "y": -71.99999999999898,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "entry"
          }
        },
        {
          "id": "board-d340ad8f",
          "type": "#module:main",
          "metadata": {
            "visual": {
              "x": 340,
              "y": 0,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "Generating draft",
            "logLevel": "info"
          },
          "configuration": {}
        },
        {
          "id": "board-1946064a",
          "type": "#module:join",
          "metadata": {
            "visual": {
              "x": 1059.9999999999986,
              "y": -159.99999999999886,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "join"
          }
        },
        {
          "type": "input",
          "id": "input",
          "metadata": {
            "visual": {
              "x": 720.0000000000005,
              "y": 160.00000000000114,
              "collapsed": "advanced",
              "outputHeight": 44
            },
            "title": "Waiting for user feedback",
            "logLevel": "info"
          },
          "configuration": {}
        }
      ],
      "edges": [
        {
          "from": "board-f138aa03",
          "to": "board-d340ad8f",
          "out": "context",
          "in": "context"
        },
        {
          "from": "board-d340ad8f",
          "to": "output",
          "out": "done",
          "in": "context"
        },
        {
          "from": "input",
          "to": "board-1946064a",
          "out": "request",
          "in": "request"
        },
        {
          "from": "board-d340ad8f",
          "to": "input",
          "out": "toInput",
          "in": "schema"
        },
        {
          "from": "board-d340ad8f",
          "to": "board-1946064a",
          "out": "context",
          "in": "context"
        },
        {
          "from": "board-1946064a",
          "to": "board-d340ad8f",
          "out": "context",
          "in": "context"
        }
      ],
      "metadata": {
        "visual": {
          "minimized": false
        },
        "describer": "module:entry",
        "tags": []
      }
    }
  },
  "modules": {
    "main": {
      "code": "/**\n * @fileoverview Add a description for your module here.\n */\nimport output from \"@output\";\nimport { createDoneTool, createKeepChattingTool, } from \"./chat-tools\";\nimport { createSystemInstruction } from \"./system-instruction\";\nimport { report } from \"./a2/output\";\nimport { err, ok, defaultLLMContent, llm } from \"./a2/utils\";\nimport { Template } from \"./a2/template\";\nimport { ArgumentNameGenerator } from \"./a2/introducer\";\nimport { ToolManager } from \"./a2/tool-manager\";\nimport { ListExpander, listPrompt, toList, listSchema } from \"./a2/lists\";\nimport { defaultSafetySettings, } from \"./a2/gemini\";\nimport { GeminiPrompt } from \"./a2/gemini-prompt\";\nexport { invoke as default, describe };\nclass GenerateText {\n    sharedContext;\n    toolManager;\n    doneTool;\n    keepChattingTool;\n    description;\n    context;\n    listMode = false;\n    constructor(sharedContext) {\n        this.sharedContext = sharedContext;\n        this.invoke = this.invoke.bind(this);\n    }\n    async initialize() {\n        const { sharedContext } = this;\n        const template = new Template(sharedContext.description);\n        const toolManager = new ToolManager(new ArgumentNameGenerator());\n        const doneTool = createDoneTool();\n        const keepChattingTool = createKeepChattingTool();\n        const substituting = await template.substitute(sharedContext.params, async ({ path: url, instance }) => toolManager.addTool(url, instance));\n        const hasTools = toolManager.hasTools();\n        if (sharedContext.chat) {\n            toolManager.addCustomTool(doneTool.name, doneTool.handle());\n            if (!hasTools) {\n                toolManager.addCustomTool(keepChattingTool.name, keepChattingTool.handle());\n            }\n        }\n        if (!ok(substituting)) {\n            return substituting;\n        }\n        this.description = substituting;\n        this.toolManager = toolManager;\n        this.doneTool = doneTool;\n        this.keepChattingTool = keepChattingTool;\n        this.context = [...sharedContext.context, ...sharedContext.work];\n    }\n    createSystemInstruction(makeList) {\n        return createSystemInstruction(this.sharedContext.systemInstruction, makeList);\n    }\n    /**\n     * Invokes the text generator.\n     * Significant mode flags:\n     * - chat: boolean -- chat mode is on/off\n     * - tools: boolean -- whether or not has tools\n     * - makeList: boolean -- asked to generate a list\n     * - isList: boolean -- is currently in list mode\n     */\n    async invoke(description, work, isList) {\n        const { sharedContext } = this;\n        const toolManager = this.toolManager;\n        const doneTool = this.doneTool;\n        const keepChattingTool = this.keepChattingTool;\n        // Disallow making nested lists (for now).\n        const makeList = sharedContext.makeList && !isList;\n        let product;\n        const contents = [description, ...work];\n        const safetySettings = defaultSafetySettings();\n        const systemInstruction = this.createSystemInstruction(makeList);\n        const tools = toolManager.list();\n        const inputs = { body: { contents, safetySettings } };\n        // We always supply tools when chatting, since we add\n        // the \"Done\" and \"Keep Chatting\" tools to figure out when\n        // the conversation ends.\n        if (this.chat || toolManager.hasTools()) {\n            inputs.body.tools = [...tools];\n            inputs.body.toolConfig = { functionCallingConfig: { mode: \"ANY\" } };\n        }\n        else {\n            inputs.body.systemInstruction = systemInstruction;\n        }\n        // When we have tools, the first call will not try to make a list,\n        // because JSON mode and tool-calling are incompatible.\n        if (makeList && !toolManager.hasTools()) {\n            inputs.body.generationConfig = {\n                responseSchema: listSchema(),\n                responseMimeType: \"application/json\",\n            };\n        }\n        const prompt = new GeminiPrompt(inputs, { toolManager });\n        const result = await prompt.invoke();\n        if (!ok(result))\n            return result;\n        const calledTools = prompt.calledTools || doneTool.invoked || keepChattingTool.invoked;\n        if (calledTools) {\n            if (doneTool.invoked) {\n                return result.last;\n            }\n            if (!keepChattingTool.invoked) {\n                contents.push(...result.all);\n            }\n            const inputs = {\n                body: { contents, systemInstruction, safetySettings },\n            };\n            if (makeList) {\n                inputs.body.generationConfig = {\n                    responseSchema: listSchema(),\n                    responseMimeType: \"application/json\",\n                };\n            }\n            const afterTools = await new GeminiPrompt(inputs).invoke();\n            if (!ok(afterTools))\n                return afterTools;\n            if (makeList && !this.chat) {\n                const list = toList(afterTools.last);\n                if (!ok(list))\n                    return list;\n                product = list;\n            }\n            else {\n                product = afterTools.last;\n            }\n        }\n        else {\n            if (makeList && !this.chat) {\n                const list = toList(result.last);\n                if (!ok(list))\n                    return list;\n                product = list;\n            }\n            else {\n                product = result.last;\n            }\n        }\n        return product;\n    }\n    get chat() {\n        // When we are in list mode, disable chat.\n        // Can't have chat inside of a list (yet).\n        return this.sharedContext.chat && !this.listMode;\n    }\n    get doneChatting() {\n        return !!this.doneTool?.invoked;\n    }\n}\nfunction done(result, makeList = false) {\n    if (makeList) {\n        const list = toList(result.at(-1));\n        if (!ok(list))\n            return list;\n        result = [list];\n    }\n    return { done: result };\n}\nasync function keepChatting(sharedContext, result, isList) {\n    const last = result.at(-1);\n    let product = last;\n    if (isList) {\n        const list = toList(last);\n        if (!ok(list))\n            return list;\n        product = list;\n    }\n    await output({\n        schema: {\n            type: \"object\",\n            properties: {\n                \"a-product\": {\n                    type: \"object\",\n                    behavior: [\"llm-content\"],\n                    title: \"Draft\",\n                },\n            },\n        },\n        $metadata: {\n            title: \"Writer\",\n            description: \"Asking user\",\n            icon: \"generative-text\",\n        },\n        \"a-product\": product,\n    });\n    const toInput = {\n        type: \"object\",\n        properties: {\n            request: {\n                type: \"object\",\n                title: \"Please provide feedback\",\n                description: \"Provide feedback or click submit to continue\",\n                behavior: [\"transient\", \"llm-content\"],\n                examples: [defaultLLMContent()],\n            },\n        },\n    };\n    return {\n        toInput,\n        context: {\n            ...sharedContext,\n            work: result,\n            last,\n        },\n    };\n}\nasync function invoke({ context }) {\n    if (!context.description) {\n        const msg = \"No instruction supplied\";\n        await report({\n            actor: \"Text Generator\",\n            name: msg,\n            category: \"Runtime error\",\n            details: `In order to run, I need to have an instruction.`,\n        });\n        return err(msg);\n    }\n    // Check to see if the user ended chat and return early.\n    const { userEndedChat, userInputs, last } = context;\n    if (userEndedChat) {\n        if (!last) {\n            return err(\"Chat ended without any work\");\n        }\n        return done([...context.context, last], context.makeList);\n    }\n    const gen = new GenerateText(context);\n    const initializing = await gen.initialize();\n    if (!ok(initializing))\n        return initializing;\n    const expander = new ListExpander(gen.description, gen.context);\n    expander.expand();\n    gen.listMode = expander.list().length > 1;\n    const result = await expander.map(gen.invoke);\n    if (!ok(result))\n        return result;\n    console.log(\"RESULT\", result);\n    if (gen.doneChatting) {\n        // If done tool was invoked, rewind to remove the last interaction\n        // and return that.\n        const previousResult = context.work.at(-2);\n        if (!previousResult) {\n            return err(`Done chatting, but have nothing to pass along to next step.`);\n        }\n        return done([previousResult], context.makeList);\n    }\n    // Use the gen.chat here, because it will correctly prevent\n    // chat mode when we're in list mode.\n    if (gen.chat && !userEndedChat) {\n        return keepChatting(gen.sharedContext, result, context.makeList);\n    }\n    // Fall through to default response.\n    return done(result);\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "main",
        "source": {
          "code": "/**\n * @fileoverview Add a description for your module here.\n */\n\nimport output from \"@output\";\n\nimport type { SharedContext } from \"./types\";\nimport {\n  createDoneTool,\n  createKeepChattingTool,\n  type ChatTool,\n} from \"./chat-tools\";\nimport { createSystemInstruction } from \"./system-instruction\";\n\nimport { report } from \"./a2/output\";\nimport { err, ok, defaultLLMContent, llm } from \"./a2/utils\";\nimport { Template } from \"./a2/template\";\nimport { ArgumentNameGenerator } from \"./a2/introducer\";\nimport { ToolManager, type ToolHandle } from \"./a2/tool-manager\";\nimport { ListExpander, listPrompt, toList, listSchema } from \"./a2/lists\";\n\nimport {\n  defaultSafetySettings,\n  type GeminiInputs,\n  type GeminiSchema,\n  type Tool,\n} from \"./a2/gemini\";\nimport { GeminiPrompt } from \"./a2/gemini-prompt\";\n\nexport { invoke as default, describe };\n\ntype Inputs = {\n  context: SharedContext;\n};\n\ntype Outputs = {\n  $error?: string;\n  context?: SharedContext;\n  toInput?: Schema;\n  done?: LLMContent[];\n};\n\nclass GenerateText {\n  private toolManager!: ToolManager;\n  private doneTool!: ChatTool;\n  private keepChattingTool!: ChatTool;\n  public description!: LLMContent;\n  public context!: LLMContent[];\n  public listMode = false;\n\n  constructor(public readonly sharedContext: SharedContext) {\n    this.invoke = this.invoke.bind(this);\n  }\n\n  async initialize(): Promise<Outcome<void>> {\n    const { sharedContext } = this;\n    const template = new Template(sharedContext.description);\n    const toolManager = new ToolManager(new ArgumentNameGenerator());\n    const doneTool = createDoneTool();\n    const keepChattingTool = createKeepChattingTool();\n    const substituting = await template.substitute(\n      sharedContext.params,\n      async ({ path: url, instance }) => toolManager.addTool(url, instance)\n    );\n    const hasTools = toolManager.hasTools();\n    if (sharedContext.chat) {\n      toolManager.addCustomTool(doneTool.name, doneTool.handle());\n      if (!hasTools) {\n        toolManager.addCustomTool(\n          keepChattingTool.name,\n          keepChattingTool.handle()\n        );\n      }\n    }\n    if (!ok(substituting)) {\n      return substituting;\n    }\n    this.description = substituting;\n    this.toolManager = toolManager;\n    this.doneTool = doneTool;\n    this.keepChattingTool = keepChattingTool;\n    this.context = [...sharedContext.context, ...sharedContext.work];\n  }\n\n  createSystemInstruction(makeList: boolean) {\n    return createSystemInstruction(\n      this.sharedContext.systemInstruction,\n      makeList\n    );\n  }\n\n  /**\n   * Invokes the text generator.\n   * Significant mode flags:\n   * - chat: boolean -- chat mode is on/off\n   * - tools: boolean -- whether or not has tools\n   * - makeList: boolean -- asked to generate a list\n   * - isList: boolean -- is currently in list mode\n   */\n  async invoke(\n    description: LLMContent,\n    work: LLMContent[],\n    isList: boolean\n  ): Promise<Outcome<LLMContent>> {\n    const { sharedContext } = this;\n    const toolManager = this.toolManager;\n    const doneTool = this.doneTool;\n    const keepChattingTool = this.keepChattingTool;\n    // Disallow making nested lists (for now).\n    const makeList = sharedContext.makeList && !isList;\n\n    let product: LLMContent;\n    const contents = [description, ...work];\n    const safetySettings = defaultSafetySettings();\n    const systemInstruction = this.createSystemInstruction(makeList);\n    const tools = toolManager.list();\n    const inputs: GeminiInputs = { body: { contents, safetySettings } };\n    // We always supply tools when chatting, since we add\n    // the \"Done\" and \"Keep Chatting\" tools to figure out when\n    // the conversation ends.\n    if (this.chat || toolManager.hasTools()) {\n      inputs.body.tools = [...tools];\n      inputs.body.toolConfig = { functionCallingConfig: { mode: \"ANY\" } };\n    } else {\n      inputs.body.systemInstruction = systemInstruction;\n    }\n    // When we have tools, the first call will not try to make a list,\n    // because JSON mode and tool-calling are incompatible.\n    if (makeList && !toolManager.hasTools()) {\n      inputs.body.generationConfig = {\n        responseSchema: listSchema(),\n        responseMimeType: \"application/json\",\n      };\n    }\n    const prompt = new GeminiPrompt(inputs, { toolManager });\n    const result = await prompt.invoke();\n    if (!ok(result)) return result;\n    const calledTools =\n      prompt.calledTools || doneTool.invoked || keepChattingTool.invoked;\n    if (calledTools) {\n      if (doneTool.invoked) {\n        return result.last;\n      }\n      if (!keepChattingTool.invoked) {\n        contents.push(...result.all);\n      }\n      const inputs: GeminiInputs = {\n        body: { contents, systemInstruction, safetySettings },\n      };\n      if (makeList) {\n        inputs.body.generationConfig = {\n          responseSchema: listSchema(),\n          responseMimeType: \"application/json\",\n        };\n      }\n      const afterTools = await new GeminiPrompt(inputs).invoke();\n      if (!ok(afterTools)) return afterTools;\n      if (makeList && !this.chat) {\n        const list = toList(afterTools.last);\n        if (!ok(list)) return list;\n        product = list;\n      } else {\n        product = afterTools.last;\n      }\n    } else {\n      if (makeList && !this.chat) {\n        const list = toList(result.last);\n        if (!ok(list)) return list;\n        product = list;\n      } else {\n        product = result.last;\n      }\n    }\n\n    return product;\n  }\n\n  get chat(): boolean {\n    // When we are in list mode, disable chat.\n    // Can't have chat inside of a list (yet).\n    return this.sharedContext.chat && !this.listMode;\n  }\n\n  get doneChatting(): boolean {\n    return !!this.doneTool?.invoked;\n  }\n}\n\nfunction done(result: LLMContent[], makeList: boolean = false) {\n  if (makeList) {\n    const list = toList(result.at(-1)!);\n    if (!ok(list)) return list;\n    result = [list];\n  }\n  return { done: result };\n}\n\nasync function keepChatting(\n  sharedContext: SharedContext,\n  result: LLMContent[],\n  isList: boolean\n) {\n  const last = result.at(-1)!;\n  let product = last;\n  if (isList) {\n    const list = toList(last);\n    if (!ok(list)) return list;\n    product = list;\n  }\n  await output({\n    schema: {\n      type: \"object\",\n      properties: {\n        \"a-product\": {\n          type: \"object\",\n          behavior: [\"llm-content\"],\n          title: \"Draft\",\n        },\n      },\n    },\n    $metadata: {\n      title: \"Writer\",\n      description: \"Asking user\",\n      icon: \"generative-text\",\n    },\n    \"a-product\": product,\n  });\n\n  const toInput: Schema = {\n    type: \"object\",\n    properties: {\n      request: {\n        type: \"object\",\n        title: \"Please provide feedback\",\n        description: \"Provide feedback or click submit to continue\",\n        behavior: [\"transient\", \"llm-content\"],\n        examples: [defaultLLMContent()],\n      },\n    },\n  };\n  return {\n    toInput,\n    context: {\n      ...sharedContext,\n      work: result,\n      last,\n    },\n  };\n}\n\nasync function invoke({ context }: Inputs) {\n  if (!context.description) {\n    const msg = \"No instruction supplied\";\n    await report({\n      actor: \"Text Generator\",\n      name: msg,\n      category: \"Runtime error\",\n      details: `In order to run, I need to have an instruction.`,\n    });\n    return err(msg);\n  }\n\n  // Check to see if the user ended chat and return early.\n  const { userEndedChat, userInputs, last } = context;\n  if (userEndedChat) {\n    if (!last) {\n      return err(\"Chat ended without any work\");\n    }\n    return done([...context.context, last], context.makeList);\n  }\n\n  const gen = new GenerateText(context);\n  const initializing = await gen.initialize();\n  if (!ok(initializing)) return initializing;\n\n  const expander = new ListExpander(gen.description, gen.context);\n  expander.expand();\n  gen.listMode = expander.list().length > 1;\n\n  const result = await expander.map(gen.invoke);\n  if (!ok(result)) return result;\n  console.log(\"RESULT\", result);\n  if (gen.doneChatting) {\n    // If done tool was invoked, rewind to remove the last interaction\n    // and return that.\n    const previousResult = context.work.at(-2);\n    if (!previousResult) {\n      return err(`Done chatting, but have nothing to pass along to next step.`);\n    }\n    return done([previousResult], context.makeList);\n  }\n\n  // Use the gen.chat here, because it will correctly prevent\n  // chat mode when we're in list mode.\n  if (gen.chat && !userEndedChat) {\n    return keepChatting(gen.sharedContext, result, context.makeList);\n  }\n\n  // Fall through to default response.\n  return done(result);\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Add a description for your module here.",
        "runnable": false
      }
    },
    "join": {
      "code": "/**\n * @fileoverview Joins user input and Agent Context\n */\nimport {} from \"./a2/common\";\nimport { isEmpty } from \"./a2/utils\";\nimport { addContent } from \"./a2/lists\";\nexport { invoke as default, describe };\nasync function invoke({ context, request }) {\n    context.userEndedChat = isEmpty(request);\n    context.userInputs.push(request);\n    if (!context.userEndedChat) {\n        context.work = addContent(context.work, request);\n    }\n    return { context };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    title: \"Agent Context\",\n                    type: \"object\",\n                },\n                request: {\n                    title: \"User Input\",\n                    type: \"object\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    title: \"Agent Context\",\n                    type: \"object\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "join",
        "source": {
          "code": "/**\n * @fileoverview Joins user input and Agent Context\n */\n\nimport { type AgentContext } from \"./a2/common\";\nimport { isEmpty } from \"./a2/utils\";\nimport { addContent } from \"./a2/lists\";\n\nexport { invoke as default, describe };\n\ntype Inputs = {\n  context: AgentContext;\n  request: LLMContent;\n};\n\ntype Outputs = {\n  context: AgentContext;\n};\n\nasync function invoke({ context, request }: Inputs): Promise<Outputs> {\n  context.userEndedChat = isEmpty(request);\n  context.userInputs.push(request);\n  if (!context.userEndedChat) {\n    context.work = addContent(context.work, request);\n  }\n  return { context };\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          title: \"Agent Context\",\n          type: \"object\",\n        },\n        request: {\n          title: \"User Input\",\n          type: \"object\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          title: \"Agent Context\",\n          type: \"object\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Joins user input and Agent Context",
        "runnable": true
      }
    },
    "entry": {
      "code": "/**\n * @fileoverview Manages the entry point: describer, passing the inputs, etc.\n */\nimport {} from \"./a2/common\";\nimport { ok, toLLMContent, defaultLLMContent } from \"./a2/utils\";\nimport { Template } from \"./a2/template\";\nimport { readSettings } from \"./a2/settings\";\nimport { defaultSystemInstruction } from \"./system-instruction\";\nexport { invoke as default, describe };\nasync function invoke({ context, \"p-chat\": chat, \"p-list\": makeList, \"b-system-instruction\": systemInstruction, description, ...params }) {\n    // Make sure it's a boolean.\n    chat = !!chat;\n    context ??= [];\n    const defaultModel = \"\";\n    const type = \"work\";\n    return {\n        context: {\n            id: Math.random().toString(36).substring(2, 5),\n            chat,\n            makeList,\n            listPath: [],\n            context,\n            userInputs: [],\n            defaultModel,\n            model: \"\",\n            description,\n            type,\n            work: [],\n            userEndedChat: false,\n            params,\n            systemInstruction,\n        },\n    };\n}\nasync function describe({ inputs: { description } }) {\n    const settings = await readSettings();\n    const experimental = ok(settings) && !!settings[\"Show Experimental Components\"];\n    const template = new Template(description);\n    let extra = {};\n    if (experimental) {\n        extra = {\n            \"p-chat\": {\n                type: \"boolean\",\n                title: \"Chat with User\",\n                behavior: [\"config\", \"hint-preview\"],\n                icon: \"chat\",\n                description: \"When checked, this step will chat with the user, asking to review work, requesting additional information, etc.\",\n            },\n            \"p-list\": {\n                type: \"boolean\",\n                title: \"Make a list\",\n                behavior: [\"config\", \"hint-preview\"],\n                icon: \"summarize\",\n                description: \"When checked, this step will try to create a list as its output. Make sure that the prompt asks for a list of some sort\",\n            },\n            \"b-system-instruction\": {\n                type: \"object\",\n                behavior: [\"llm-content\", \"config\", \"hint-advanced\"],\n                title: \"System Instruction\",\n                description: \"The system instruction for the model\",\n                default: JSON.stringify(defaultSystemInstruction()),\n            },\n        };\n    }\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                description: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"Prompt\",\n                    description: \"Give the model additional context on what to do, like specific rules/guidelines to adhere to or specify behavior separate from the provided context.\",\n                    default: defaultLLMContent(),\n                },\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                ...extra,\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"main-port\", \"hint-text\"],\n                },\n            },\n        },\n        title: \"Make Text\",\n        metadata: {\n            icon: \"generative-text\",\n            tags: [\"quick-access\", \"generative\"],\n            order: 1,\n        },\n    };\n}\n",
      "metadata": {
        "title": "Make Text",
        "source": {
          "code": "/**\n * @fileoverview Manages the entry point: describer, passing the inputs, etc.\n */\n\nimport { type Params } from \"./a2/common\";\nimport type { SharedContext } from \"./types\";\nimport { ok, toLLMContent, defaultLLMContent } from \"./a2/utils\";\nimport { Template } from \"./a2/template\";\nimport { readSettings } from \"./a2/settings\";\nimport { defaultSystemInstruction } from \"./system-instruction\";\n\nexport { invoke as default, describe };\n\nexport type EntryInputs = {\n  context: LLMContent[];\n  description: LLMContent;\n  \"p-chat\": boolean;\n  \"p-list\": boolean;\n  \"b-system-instruction\": LLMContent;\n} & Params;\n\nexport type DescribeInputs = {\n  inputs: EntryInputs;\n};\n\ntype Outputs = {\n  context: SharedContext;\n};\n\nasync function invoke({\n  context,\n  \"p-chat\": chat,\n  \"p-list\": makeList,\n  \"b-system-instruction\": systemInstruction,\n  description,\n  ...params\n}: EntryInputs): Promise<Outputs> {\n  // Make sure it's a boolean.\n  chat = !!chat;\n  context ??= [];\n  const defaultModel = \"\";\n  const type = \"work\";\n  return {\n    context: {\n      id: Math.random().toString(36).substring(2, 5),\n      chat,\n      makeList,\n      listPath: [],\n      context,\n      userInputs: [],\n      defaultModel,\n      model: \"\",\n      description,\n      type,\n      work: [],\n      userEndedChat: false,\n      params,\n      systemInstruction,\n    },\n  };\n}\n\nasync function describe({ inputs: { description } }: DescribeInputs) {\n  const settings = await readSettings();\n  const experimental =\n    ok(settings) && !!settings[\"Show Experimental Components\"];\n  const template = new Template(description);\n  let extra: Record<string, Schema> = {};\n  if (experimental) {\n    extra = {\n      \"p-chat\": {\n        type: \"boolean\",\n        title: \"Chat with User\",\n        behavior: [\"config\", \"hint-preview\"],\n        icon: \"chat\",\n        description:\n          \"When checked, this step will chat with the user, asking to review work, requesting additional information, etc.\",\n      },\n      \"p-list\": {\n        type: \"boolean\",\n        title: \"Make a list\",\n        behavior: [\"config\", \"hint-preview\"],\n        icon: \"summarize\",\n        description:\n          \"When checked, this step will try to create a list as its output. Make sure that the prompt asks for a list of some sort\",\n      },\n      \"b-system-instruction\": {\n        type: \"object\",\n        behavior: [\"llm-content\", \"config\", \"hint-advanced\"],\n        title: \"System Instruction\",\n        description: \"The system instruction for the model\",\n        default: JSON.stringify(defaultSystemInstruction()),\n      },\n    };\n  }\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        description: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n          title: \"Prompt\",\n          description:\n            \"Give the model additional context on what to do, like specific rules/guidelines to adhere to or specify behavior separate from the provided context.\",\n          default: defaultLLMContent(),\n        },\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n        ...extra,\n        ...template.schemas(),\n      },\n      behavior: [\"at-wireable\"],\n      ...template.requireds(),\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n          behavior: [\"main-port\", \"hint-text\"],\n        },\n      },\n    } satisfies Schema,\n    title: \"Make Text\",\n    metadata: {\n      icon: \"generative-text\",\n      tags: [\"quick-access\", \"generative\"],\n      order: 1,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Manages the entry point: describer, passing the inputs, etc.",
        "runnable": true
      }
    },
    "types": {
      "code": "/**\n * @fileoverview Common type definitions\n */\n",
      "metadata": {
        "title": "types",
        "source": {
          "code": "/**\n * @fileoverview Common type definitions\n */\n\nimport type { Params } from \"./a2/common\";\n\nexport type GenerateTextInputs = {\n  /**\n   * Whether (true) or not (false) the agent is allowed to chat with user.\n   */\n  chat: boolean;\n  /**\n   * Whether (true) or not (false) to try to turn the output into a list\n   */\n  makeList: boolean;\n  /**\n   * The incoming conversation context.\n   */\n  context: LLMContent[];\n  /**\n   * Accumulated work context. This is the internal conversation, a result\n   * of talking with the user, for instance.\n   * This context is discarded at the end of interacting with the agent.\n   */\n  work: LLMContent[];\n  /**\n   * The index path to the currently processed list.\n   */\n  listPath: number[];\n  /**\n   * Agent's job description.\n   */\n  description?: LLMContent;\n  /**\n   *\n   */\n  systemInstruction?: LLMContent;\n  /**\n   * Last work product.\n   */\n  last?: LLMContent;\n  /**\n   * Type of the task.\n   */\n  type: \"introduction\" | \"work\";\n  /**\n   * The board URL of the model\n   */\n  model: string;\n  /**\n   * The default model that is passed along by the manager\n   */\n  defaultModel: string;\n  /**\n   * params\n   */\n  params: Params;\n};\n\nexport type SharedContext = GenerateTextInputs & {\n  /**\n   * A unique identifier for the session.\n   * Currently used to have a persistent part separator across conversation context\n   */\n  id: string;\n  /**\n   * Accumulating list of user inputs\n   */\n  userInputs: LLMContent[];\n  /**\n   * Indicator that the user ended chat.\n   */\n  userEndedChat: boolean;\n};\n",
          "language": "typescript"
        },
        "description": "Common type definitions",
        "runnable": false
      }
    },
    "chat-tools": {
      "code": "/**\n * @fileoverview Tools for conversational (\"chat\") mode\n */\nexport { createDoneTool, createKeepChattingTool };\nclass ChatToolImpl {\n    name;\n    description;\n    #invoked = false;\n    constructor(name, description) {\n        this.name = name;\n        this.description = description;\n    }\n    reset() {\n        this.#invoked = false;\n    }\n    get invoked() {\n        return this.#invoked;\n    }\n    declaration() {\n        return {\n            name: this.name,\n            description: this.description,\n        };\n    }\n    handle() {\n        return {\n            tool: this.declaration(),\n            url: \"\",\n            passContext: false,\n            invoke: async () => {\n                this.#invoked = true;\n            },\n        };\n    }\n}\nfunction createDoneTool() {\n    return new ChatToolImpl(\"User_Says_Done\", \"Call when the user indicates they are done with the conversation and are ready to move on\");\n}\nfunction createKeepChattingTool() {\n    return new ChatToolImpl(\"User_Asks_For_More_Work\", \"Call when the user asked a question or issued instruction to do more work\");\n}\n",
      "metadata": {
        "title": "chat-tools",
        "source": {
          "code": "/**\n * @fileoverview Tools for conversational (\"chat\") mode\n */\n\nimport type { ToolHandle } from \"./a2/tool-manager\";\n\nexport type ChatTool = {\n  readonly name: string;\n  handle(): ToolHandle;\n  reset(): void;\n  readonly invoked: boolean;\n};\n\nexport { createDoneTool, createKeepChattingTool };\n\nclass ChatToolImpl implements ChatTool {\n  #invoked = false;\n\n  constructor(\n    public readonly name: string,\n    public readonly description: string\n  ) {}\n\n  reset() {\n    this.#invoked = false;\n  }\n\n  get invoked() {\n    return this.#invoked;\n  }\n\n  declaration() {\n    return {\n      name: this.name,\n      description: this.description,\n    };\n  }\n\n  handle(): ToolHandle {\n    return {\n      tool: this.declaration(),\n      url: \"\",\n      passContext: false,\n      invoke: async () => {\n        this.#invoked = true;\n      },\n    };\n  }\n}\n\nfunction createDoneTool(): ChatTool {\n  return new ChatToolImpl(\n    \"User_Says_Done\",\n    \"Call when the user indicates they are done with the conversation and are ready to move on\"\n  );\n}\n\nfunction createKeepChattingTool(): ChatTool {\n  return new ChatToolImpl(\n    \"User_Asks_For_More_Work\",\n    \"Call when the user asked a question or issued instruction to do more work\"\n  );\n}\n",
          "language": "typescript"
        },
        "description": "Tools for conversational (\"chat\") mode",
        "runnable": false
      }
    },
    "system-instruction": {
      "code": "/**\n * @fileoverview Helps create a system instruction.\n */\nimport { llm } from \"./a2/utils\";\nimport { listPrompt } from \"./a2/lists\";\nexport { createSystemInstruction, defaultSystemInstruction };\nfunction defaultSystemInstruction() {\n    return llm `IMPORTANT NOTE: Start directly with the output, do not output any delimiters.\nYou are working as part of an AI system, so no chit-chat and no explaining what you're doing and why.\nDO NOT start with \"Okay\", or \"Alright\" or any preambles.\nJust the output, please.\nTake a Deep Breath, read the instructions again, read the inputs again.\nEach instruction is crucial and must be executed with utmost care and attention to detail.`.asContent();\n}\nfunction createSystemInstruction(existing, makeList) {\n    if (existing) {\n        existing = defaultSystemInstruction();\n    }\n    const builtIn = llm `\n\nToday is ${new Date().toLocaleString(\"en-US\", {\n        month: \"long\",\n        day: \"numeric\",\n        year: \"numeric\",\n        hour: \"numeric\",\n        minute: \"2-digit\",\n    })}\n    \n${existing}`.asContent();\n    if (!makeList)\n        return builtIn;\n    return listPrompt(builtIn);\n}\n",
      "metadata": {
        "title": "system-instruction",
        "source": {
          "code": "/**\n * @fileoverview Helps create a system instruction.\n */\n\nimport { llm } from \"./a2/utils\";\nimport { listPrompt } from \"./a2/lists\";\n\nexport { createSystemInstruction, defaultSystemInstruction };\n\nfunction defaultSystemInstruction(): LLMContent {\n  return llm`IMPORTANT NOTE: Start directly with the output, do not output any delimiters.\nYou are working as part of an AI system, so no chit-chat and no explaining what you're doing and why.\nDO NOT start with \"Okay\", or \"Alright\" or any preambles.\nJust the output, please.\nTake a Deep Breath, read the instructions again, read the inputs again.\nEach instruction is crucial and must be executed with utmost care and attention to detail.`.asContent();\n}\n\nfunction createSystemInstruction(\n  existing: LLMContent | undefined,\n  makeList: boolean\n) {\n  if (existing) {\n    existing = defaultSystemInstruction();\n  }\n  const builtIn = llm`\n\nToday is ${new Date().toLocaleString(\"en-US\", {\n    month: \"long\",\n    day: \"numeric\",\n    year: \"numeric\",\n    hour: \"numeric\",\n    minute: \"2-digit\",\n  })}\n    \n${existing}`.asContent();\n  if (!makeList) return builtIn;\n  return listPrompt(builtIn);\n}\n",
          "language": "typescript"
        },
        "description": "Helps create a system instruction.",
        "runnable": false
      }
    }
  },
  "exports": [
    "#module:main"
  ],
  "assets": {
    "@@thumbnail": {
      "metadata": {
        "title": "Thumbnail",
        "type": "file"
      },
      "data": "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjUwIiBoZWlnaHQ9IjIwMCIgdmlld0JveD0iMCAwIDI1MCAyMDAiIGZpbGw9Im5vbmUiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+CiAgICAKICAgICAgPHJlY3QgeD0iMTM5LjA1IgogICAgICAgICAgICAgICAgICAgIHk9Ijg3Ljg5IgogICAgICAgICAgICAgICAgICAgIHdpZHRoPSI0My43NSIKICAgICAgICAgICAgICAgICAgICBoZWlnaHQ9IjI0LjIzIgogICAgICAgICAgICAgICAgICAgIHJ4PSIzLjUiCiAgICAgICAgICAgICAgICAgICAgZmlsbD0id2hpdGUiCiAgICAgICAgICAgICAgICAgICAgc3Ryb2tlPSIjMjBhMjAyIiAvPgo8cmVjdCB4PSIxMC4wMCIKICAgICAgICAgICAgICAgICAgICB5PSI3NS43NyIKICAgICAgICAgICAgICAgICAgICB3aWR0aD0iNDMuNzUiCiAgICAgICAgICAgICAgICAgICAgaGVpZ2h0PSIyNC4yMyIKICAgICAgICAgICAgICAgICAgICByeD0iMy41IgogICAgICAgICAgICAgICAgICAgIGZpbGw9IndoaXRlIgogICAgICAgICAgICAgICAgICAgIHN0cm9rZT0iIzc3NTdkOSIgLz4KPHJlY3QgeD0iNzUuMTEiCiAgICAgICAgICAgICAgICAgICAgeT0iODcuODkiCiAgICAgICAgICAgICAgICAgICAgd2lkdGg9IjQzLjc1IgogICAgICAgICAgICAgICAgICAgIGhlaWdodD0iMjQuMjMiCiAgICAgICAgICAgICAgICAgICAgcng9IjMuNSIKICAgICAgICAgICAgICAgICAgICBmaWxsPSJ3aGl0ZSIKICAgICAgICAgICAgICAgICAgICBzdHJva2U9IiMyZThiZTgiIC8+CjxyZWN0IHg9IjE5Ni4yNSIKICAgICAgICAgICAgICAgICAgICB5PSI2MC45NyIKICAgICAgICAgICAgICAgICAgICB3aWR0aD0iNDMuNzUiCiAgICAgICAgICAgICAgICAgICAgaGVpZ2h0PSIyNC4yMyIKICAgICAgICAgICAgICAgICAgICByeD0iMy41IgogICAgICAgICAgICAgICAgICAgIGZpbGw9IndoaXRlIgogICAgICAgICAgICAgICAgICAgIHN0cm9rZT0iIzJlOGJlOCIgLz4KPHJlY3QgeD0iMTM5LjA1IgogICAgICAgICAgICAgICAgICAgIHk9IjExNC44MSIKICAgICAgICAgICAgICAgICAgICB3aWR0aD0iNDMuNzUiCiAgICAgICAgICAgICAgICAgICAgaGVpZ2h0PSIyNC4yMyIKICAgICAgICAgICAgICAgICAgICByeD0iMy41IgogICAgICAgICAgICAgICAgICAgIGZpbGw9IndoaXRlIgogICAgICAgICAgICAgICAgICAgIHN0cm9rZT0iIzIwYTIwMiIgLz4KICAgIDwvc3ZnPg=="
    }
  }
}