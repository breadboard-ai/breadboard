{
  "title": "Generate Text",
  "description": "",
  "version": "0.0.1",
  "nodes": [],
  "edges": [],
  "metadata": {
    "comments": [
      {
        "id": "comment-cc94afe8",
        "text": "Intentionally Left Blank",
        "metadata": {
          "title": "Comment",
          "visual": {
            "x": 531,
            "y": 374,
            "collapsed": "expanded",
            "outputHeight": 0
          }
        }
      }
    ],
    "visual": {
      "presentation": {
        "themes": {
          "5f3ca599-8fee-46fb-951f-0d47b16a6d56": {
            "themeColors": {
              "primaryColor": "#246db5",
              "secondaryColor": "#5cadff",
              "backgroundColor": "#ffffff",
              "textColor": "#1a1a1a",
              "primaryTextColor": "#ffffff"
            },
            "template": "basic",
            "splashScreen": {
              "storedData": {
                "handle": "/images/app/generic-flow.jpg",
                "mimeType": "image/jpeg"
              }
            }
          }
        },
        "theme": "5f3ca599-8fee-46fb-951f-0d47b16a6d56"
      }
    },
    "tags": [
      "published",
      "tool",
      "component"
    ],
    "userModified": true
  },
  "imports": {
    "a2": {
      "url": "./a2.bgl.json"
    }
  },
  "graphs": {
    "daf082ca-c1aa-4aff-b2c8-abeb984ab66c": {
      "title": "Make Text",
      "description": "Generates text and so much more.",
      "version": "0.0.1",
      "describer": "module:entry",
      "nodes": [
        {
          "type": "output",
          "id": "output",
          "configuration": {
            "schema": {
              "properties": {
                "context": {
                  "type": "array",
                  "title": "Context",
                  "items": {
                    "type": "object",
                    "behavior": [
                      "llm-content"
                    ]
                  },
                  "default": "null"
                }
              },
              "type": "object",
              "required": []
            }
          },
          "metadata": {
            "visual": {
              "x": 720,
              "y": 0,
              "collapsed": "expanded",
              "outputHeight": 44
            }
          }
        },
        {
          "id": "board-f138aa03",
          "type": "#module:entry",
          "metadata": {
            "visual": {
              "x": -46.99999999999966,
              "y": -71.99999999999898,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "entry"
          }
        },
        {
          "id": "board-d340ad8f",
          "type": "#module:main",
          "metadata": {
            "visual": {
              "x": 340,
              "y": 0,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "Generating draft",
            "logLevel": "info"
          },
          "configuration": {}
        },
        {
          "id": "board-1946064a",
          "type": "#module:join",
          "metadata": {
            "visual": {
              "x": 1059.9999999999986,
              "y": -159.99999999999886,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "join"
          }
        },
        {
          "type": "input",
          "id": "input",
          "metadata": {
            "visual": {
              "x": 720.0000000000005,
              "y": 160.00000000000114,
              "collapsed": "advanced",
              "outputHeight": 44
            },
            "title": "Waiting for user feedback",
            "logLevel": "info"
          },
          "configuration": {}
        }
      ],
      "edges": [
        {
          "from": "board-f138aa03",
          "to": "board-d340ad8f",
          "out": "context",
          "in": "context"
        },
        {
          "from": "board-d340ad8f",
          "to": "output",
          "out": "done",
          "in": "context"
        },
        {
          "from": "input",
          "to": "board-1946064a",
          "out": "request",
          "in": "request"
        },
        {
          "from": "board-d340ad8f",
          "to": "input",
          "out": "toInput",
          "in": "schema"
        },
        {
          "from": "board-d340ad8f",
          "to": "board-1946064a",
          "out": "context",
          "in": "context"
        },
        {
          "from": "board-1946064a",
          "to": "board-d340ad8f",
          "out": "context",
          "in": "context"
        }
      ],
      "metadata": {
        "visual": {
          "minimized": false
        },
        "describer": "module:entry",
        "tags": []
      }
    }
  },
  "modules": {
    "main": {
      "code": "/**\n * @fileoverview Add a description for your module here.\n */\nimport output from \"@output\";\nimport { createDoneTool, createKeepChattingTool } from \"./chat-tools\";\nimport { report } from \"./a2/output\";\nimport { err, ok, defaultLLMContent, llm } from \"./a2/utils\";\nimport { Template } from \"./a2/template\";\nimport { ArgumentNameGenerator } from \"./a2/introducer\";\nimport { ToolManager } from \"./a2/tool-manager\";\nimport { ListExpander, listPrompt, toList, listSchema } from \"./a2/lists\";\nimport { defaultSafetySettings, } from \"./a2/gemini\";\nimport { GeminiPrompt } from \"./a2/gemini-prompt\";\nexport { invoke as default, describe };\nfunction promptOld(description, mode) {\n    const preamble = llm `\n${description}\n\n`;\n    const postamble = `\n\nToday is ${new Date().toLocaleString(\"en-US\", {\n        month: \"long\",\n        day: \"numeric\",\n        year: \"numeric\",\n        hour: \"numeric\",\n        minute: \"2-digit\",\n    })}`;\n    switch (mode) {\n        case \"summarize\":\n            return llm ` \n${preamble}\nSummarize the research results to fulfill the specified task.\n${postamble}`.asContent();\n        case \"call-tools\":\n            return llm `\n${preamble}\nGenerate multiple function calls to fulfill the specified task.\n${postamble}`.asContent();\n        case \"generate\":\n            return llm `\n${preamble}\nProvide the response that fulfills the specified task.\n${postamble}`.asContent();\n    }\n}\nasync function invoke({ context }) {\n    if (!context.description) {\n        const msg = \"No instruction supplied\";\n        await report({\n            actor: \"Text Generator\",\n            name: msg,\n            category: \"Runtime error\",\n            details: `In order to run, I need to have an instruction.`,\n        });\n        return err(msg);\n    }\n    // Check to see if the user ended chat and return early.\n    const { userEndedChat, userInputs, last } = context;\n    if (userEndedChat) {\n        if (!last) {\n            return err(\"Chat ended without any work\");\n        }\n        return {\n            done: [...context.context, last],\n        };\n    }\n    const template = new Template(context.description);\n    const toolManager = new ToolManager(new ArgumentNameGenerator());\n    const doneTool = createDoneTool();\n    const keepChattingTool = createKeepChattingTool();\n    const substituting = await template.substitute(context.params, async ({ path: url, instance }) => toolManager.addTool(url, instance));\n    const hasTools = toolManager.hasTools();\n    if (context.chat) {\n        toolManager.addCustomTool(doneTool.name, doneTool.handle());\n        if (!hasTools) {\n            toolManager.addCustomTool(keepChattingTool.name, keepChattingTool.handle());\n        }\n    }\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    const { work } = context;\n    const mode = \"generate\";\n    const result = await new ListExpander(substituting, work.length > 0 ? work : context.context).map(async (description, work, isList) => {\n        // Disallow making nested lists\n        const makeList = context.makeList && !isList;\n        let product;\n        if (makeList) {\n            // TODO: Make this work as well.\n            const generating = await new GeminiPrompt({\n                body: {\n                    contents: [\n                        ...context.context,\n                        listPrompt(promptOld(description, mode)),\n                    ],\n                    safetySettings: defaultSafetySettings(),\n                    generationConfig: {\n                        responseSchema: listSchema(),\n                        responseMimeType: \"application/json\",\n                    },\n                    tools: toolManager.list(),\n                },\n            }, {\n                toolManager,\n            }).invoke();\n            if (!ok(generating))\n                return generating;\n            const list = toList(generating.last);\n            if (!ok(list))\n                return list;\n            product = list;\n        }\n        else {\n            const work = context.work.length > 0 ? context.work : [description];\n            const contents = [...context.context, ...work];\n            const safetySettings = defaultSafetySettings();\n            const systemInstruction = llm `\nIMPORTANT NOTE: Start directly with the output, do not output any delimiters.\nTake a Deep Breath, read the instructions again, read the inputs again.\nEach instruction is crucial and must be executed with utmost care and attention to detail.`.asContent();\n            const tools = toolManager.list();\n            const inputs = { body: { contents, safetySettings } };\n            if (context.chat) {\n                inputs.body.tools = [...tools];\n            }\n            const prompt = new GeminiPrompt(inputs, { toolManager });\n            const result = await prompt.invoke();\n            if (!ok(result))\n                return result;\n            const calledTools = prompt.calledTools || doneTool.invoked || keepChattingTool.invoked;\n            if (calledTools) {\n                if (doneTool.invoked) {\n                    return result.last;\n                }\n                if (!keepChattingTool.invoked) {\n                    contents.push(...result.all);\n                }\n                const inputs = {\n                    body: { contents, systemInstruction, safetySettings },\n                };\n                const afterTools = await new GeminiPrompt(inputs).invoke();\n                if (!ok(afterTools))\n                    return afterTools;\n                product = afterTools.last;\n            }\n            else {\n                product = result.last;\n            }\n        }\n        return product;\n    });\n    if (!ok(result))\n        return result;\n    // This really needs work, since it will not work with lists\n    // TODO: Listify.\n    if (doneTool.invoked) {\n        // If done tool was invoked, rewind removing the last interaction\n        // and return that.\n        const previousResult = work.at(-2);\n        return previousResult ? { done: [previousResult] } : { done: result };\n    }\n    // 4) Handle chat.\n    if (context.chat) {\n        const last = result.at(-1);\n        await output({\n            schema: {\n                type: \"object\",\n                properties: {\n                    \"a-product\": {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                        title: \"Draft\",\n                    },\n                },\n            },\n            $metadata: {\n                title: \"Writer\",\n                description: \"Asking user\",\n                icon: \"generative-text\",\n            },\n            \"a-product\": last,\n        });\n        const { userInputs } = context;\n        if (!userEndedChat) {\n            const toInput = {\n                type: \"object\",\n                properties: {\n                    request: {\n                        type: \"object\",\n                        title: \"Please provide feedback\",\n                        description: \"Provide feedback or click submit to continue\",\n                        behavior: [\"transient\", \"llm-content\"],\n                        examples: [defaultLLMContent()],\n                    },\n                },\n            };\n            return {\n                toInput,\n                context: {\n                    ...context,\n                    work: result,\n                    last,\n                },\n            };\n        }\n    }\n    // 5) Fall through to default response.\n    return { done: result };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "main",
        "source": {
          "code": "/**\n * @fileoverview Add a description for your module here.\n */\n\nimport output from \"@output\";\n\nimport type { SharedContext } from \"./types\";\nimport { createDoneTool, createKeepChattingTool } from \"./chat-tools\";\n\nimport { report } from \"./a2/output\";\nimport { err, ok, defaultLLMContent, llm } from \"./a2/utils\";\nimport { Template } from \"./a2/template\";\nimport { ArgumentNameGenerator } from \"./a2/introducer\";\nimport { ToolManager, type ToolHandle } from \"./a2/tool-manager\";\nimport { ListExpander, listPrompt, toList, listSchema } from \"./a2/lists\";\n\nimport {\n  defaultSafetySettings,\n  type GeminiInputs,\n  type GeminiSchema,\n  type Tool,\n} from \"./a2/gemini\";\nimport { GeminiPrompt } from \"./a2/gemini-prompt\";\n\nexport { invoke as default, describe };\n\ntype Inputs = {\n  context: SharedContext;\n};\n\ntype Outputs = {\n  $error?: string;\n  context?: SharedContext;\n  toInput?: Schema;\n  done?: LLMContent[];\n};\n\ntype WorkMode = \"generate\" | \"call-tools\" | \"summarize\";\n\nfunction promptOld(description: LLMContent, mode: WorkMode): LLMContent {\n  const preamble = llm`\n${description}\n\n`;\n  const postamble = `\n\nToday is ${new Date().toLocaleString(\"en-US\", {\n    month: \"long\",\n    day: \"numeric\",\n    year: \"numeric\",\n    hour: \"numeric\",\n    minute: \"2-digit\",\n  })}`;\n\n  switch (mode) {\n    case \"summarize\":\n      return llm` \n${preamble}\nSummarize the research results to fulfill the specified task.\n${postamble}`.asContent();\n\n    case \"call-tools\":\n      return llm`\n${preamble}\nGenerate multiple function calls to fulfill the specified task.\n${postamble}`.asContent();\n\n    case \"generate\":\n      return llm`\n${preamble}\nProvide the response that fulfills the specified task.\n${postamble}`.asContent();\n  }\n}\n\nasync function invoke({ context }: Inputs) {\n  if (!context.description) {\n    const msg = \"No instruction supplied\";\n    await report({\n      actor: \"Text Generator\",\n      name: msg,\n      category: \"Runtime error\",\n      details: `In order to run, I need to have an instruction.`,\n    });\n    return err(msg);\n  }\n\n  // Check to see if the user ended chat and return early.\n  const { userEndedChat, userInputs, last } = context;\n  if (userEndedChat) {\n    if (!last) {\n      return err(\"Chat ended without any work\");\n    }\n    return {\n      done: [...context.context, last],\n    };\n  }\n\n  const template = new Template(context.description);\n  const toolManager = new ToolManager(new ArgumentNameGenerator());\n  const doneTool = createDoneTool();\n  const keepChattingTool = createKeepChattingTool();\n  const substituting = await template.substitute(\n    context.params,\n    async ({ path: url, instance }) => toolManager.addTool(url, instance)\n  );\n  const hasTools = toolManager.hasTools();\n  if (context.chat) {\n    toolManager.addCustomTool(doneTool.name, doneTool.handle());\n    if (!hasTools) {\n      toolManager.addCustomTool(\n        keepChattingTool.name,\n        keepChattingTool.handle()\n      );\n    }\n  }\n  if (!ok(substituting)) {\n    return substituting;\n  }\n\n  const { work } = context;\n  const mode = \"generate\";\n  const result = await new ListExpander(\n    substituting,\n    work.length > 0 ? work : context.context\n  ).map(async (description, work, isList) => {\n    // Disallow making nested lists\n    const makeList = context.makeList && !isList;\n\n    let product: LLMContent;\n    if (makeList) {\n      // TODO: Make this work as well.\n      const generating = await new GeminiPrompt(\n        {\n          body: {\n            contents: [\n              ...context.context,\n              listPrompt(promptOld(description, mode)),\n            ],\n            safetySettings: defaultSafetySettings(),\n            generationConfig: {\n              responseSchema: listSchema(),\n              responseMimeType: \"application/json\",\n            },\n            tools: toolManager.list(),\n          },\n        },\n        {\n          toolManager,\n        }\n      ).invoke();\n      if (!ok(generating)) return generating;\n\n      const list = toList(generating.last);\n      if (!ok(list)) return list;\n\n      product = list;\n    } else {\n      const work = context.work.length > 0 ? context.work : [description];\n      const contents = [...context.context, ...work];\n      const safetySettings = defaultSafetySettings();\n      const systemInstruction = llm`\nIMPORTANT NOTE: Start directly with the output, do not output any delimiters.\nTake a Deep Breath, read the instructions again, read the inputs again.\nEach instruction is crucial and must be executed with utmost care and attention to detail.`.asContent();\n      const tools = toolManager.list();\n      const inputs: GeminiInputs = { body: { contents, safetySettings } };\n      if (context.chat) {\n        inputs.body.tools = [...tools];\n      }\n      const prompt = new GeminiPrompt(inputs, { toolManager });\n      const result = await prompt.invoke();\n      if (!ok(result)) return result;\n      const calledTools =\n        prompt.calledTools || doneTool.invoked || keepChattingTool.invoked;\n      if (calledTools) {\n        if (doneTool.invoked) {\n          return result.last;\n        }\n        if (!keepChattingTool.invoked) {\n          contents.push(...result.all);\n        }\n        const inputs: GeminiInputs = {\n          body: { contents, systemInstruction, safetySettings },\n        };\n        const afterTools = await new GeminiPrompt(inputs).invoke();\n        if (!ok(afterTools)) return afterTools;\n        product = afterTools.last;\n      } else {\n        product = result.last;\n      }\n    }\n    return product;\n  });\n  if (!ok(result)) return result;\n  // This really needs work, since it will not work with lists\n  // TODO: Listify.\n  if (doneTool.invoked) {\n    // If done tool was invoked, rewind removing the last interaction\n    // and return that.\n    const previousResult = work.at(-2);\n    return previousResult ? { done: [previousResult] } : { done: result };\n  }\n\n  // 4) Handle chat.\n  if (context.chat) {\n    const last = result.at(-1)!;\n    await output({\n      schema: {\n        type: \"object\",\n        properties: {\n          \"a-product\": {\n            type: \"object\",\n            behavior: [\"llm-content\"],\n            title: \"Draft\",\n          },\n        },\n      },\n      $metadata: {\n        title: \"Writer\",\n        description: \"Asking user\",\n        icon: \"generative-text\",\n      },\n      \"a-product\": last,\n    });\n\n    const { userInputs } = context;\n    if (!userEndedChat) {\n      const toInput: Schema = {\n        type: \"object\",\n        properties: {\n          request: {\n            type: \"object\",\n            title: \"Please provide feedback\",\n            description: \"Provide feedback or click submit to continue\",\n            behavior: [\"transient\", \"llm-content\"],\n            examples: [defaultLLMContent()],\n          },\n        },\n      };\n      return {\n        toInput,\n        context: {\n          ...context,\n          work: result,\n          last,\n        },\n      };\n    }\n  }\n\n  // 5) Fall through to default response.\n  return { done: result };\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Add a description for your module here.",
        "runnable": false
      }
    },
    "join": {
      "code": "/**\n * @fileoverview Joins user input and Agent Context\n */\nimport {} from \"./a2/common\";\nimport { isEmpty } from \"./a2/utils\";\nimport { addContent } from \"./a2/lists\";\nexport { invoke as default, describe };\nasync function invoke({ context, request }) {\n    context.userEndedChat = isEmpty(request);\n    context.userInputs.push(request);\n    if (!context.userEndedChat) {\n        context.work = addContent(context.work, request);\n    }\n    return { context };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    title: \"Agent Context\",\n                    type: \"object\",\n                },\n                request: {\n                    title: \"User Input\",\n                    type: \"object\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    title: \"Agent Context\",\n                    type: \"object\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "join",
        "source": {
          "code": "/**\n * @fileoverview Joins user input and Agent Context\n */\n\nimport { type AgentContext } from \"./a2/common\";\nimport { isEmpty } from \"./a2/utils\";\nimport { addContent } from \"./a2/lists\";\n\nexport { invoke as default, describe };\n\ntype Inputs = {\n  context: AgentContext;\n  request: LLMContent;\n};\n\ntype Outputs = {\n  context: AgentContext;\n};\n\nasync function invoke({ context, request }: Inputs): Promise<Outputs> {\n  context.userEndedChat = isEmpty(request);\n  context.userInputs.push(request);\n  if (!context.userEndedChat) {\n    context.work = addContent(context.work, request);\n  }\n  return { context };\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          title: \"Agent Context\",\n          type: \"object\",\n        },\n        request: {\n          title: \"User Input\",\n          type: \"object\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          title: \"Agent Context\",\n          type: \"object\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Joins user input and Agent Context",
        "runnable": true
      }
    },
    "entry": {
      "code": "/**\n * @fileoverview Manages the entry point: describer, passing the inputs, etc.\n */\nimport {} from \"./a2/common\";\nimport { ok, toLLMContent, defaultLLMContent } from \"./a2/utils\";\nimport { Template } from \"./a2/template\";\nimport { readSettings } from \"./a2/settings\";\nexport { invoke as default, describe };\nasync function invoke({ context, \"p-chat\": chat, \"p-list\": makeList, description, ...params }) {\n    // Make sure it's a boolean.\n    chat = !!chat;\n    context ??= [];\n    const defaultModel = \"\";\n    const type = \"work\";\n    return {\n        context: {\n            id: Math.random().toString(36).substring(2, 5),\n            chat,\n            makeList,\n            listPath: [],\n            context,\n            userInputs: [],\n            defaultModel,\n            model: \"\",\n            description,\n            type,\n            work: [],\n            userEndedChat: false,\n            params,\n        },\n    };\n}\nasync function describe({ inputs: { description } }) {\n    const settings = await readSettings();\n    const experimental = ok(settings) && !!settings[\"Show Experimental Components\"];\n    const template = new Template(description);\n    let extra = {};\n    if (experimental) {\n        extra = {\n            \"p-chat\": {\n                type: \"boolean\",\n                title: \"Chat with User\",\n                behavior: [\"config\", \"hint-preview\"],\n                icon: \"chat\",\n                description: \"When checked, this step will chat with the user, asking to review work, requesting additional information, etc.\",\n            },\n            \"p-list\": {\n                type: \"boolean\",\n                title: \"Make a list\",\n                behavior: [\"config\", \"hint-preview\"],\n                icon: \"summarize\",\n                description: \"When checked, this step will try to create a list as its output. Make sure that the prompt asks for a list of some sort\",\n            },\n        };\n    }\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                description: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"Instruction\",\n                    description: \"Give the model additional context on what to do, like specific rules/guidelines to adhere to or specify behavior separate from the provided context.\",\n                    default: defaultLLMContent(),\n                },\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                ...extra,\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"main-port\", \"hint-text\"],\n                },\n            },\n        },\n        title: \"Make Text\",\n        metadata: {\n            icon: \"generative-text\",\n            tags: [\"quick-access\", \"generative\"],\n            order: 1,\n        },\n    };\n}\n",
      "metadata": {
        "title": "Make Text",
        "source": {
          "code": "/**\n * @fileoverview Manages the entry point: describer, passing the inputs, etc.\n */\n\nimport { type Params } from \"./a2/common\";\nimport type { SharedContext } from \"./types\";\nimport { ok, toLLMContent, defaultLLMContent } from \"./a2/utils\";\nimport { Template } from \"./a2/template\";\nimport { readSettings } from \"./a2/settings\";\n\nexport { invoke as default, describe };\n\nexport type EntryInputs = {\n  context: LLMContent[];\n  description: LLMContent;\n  \"p-chat\": boolean;\n  \"p-list\": boolean;\n} & Params;\n\nexport type DescribeInputs = {\n  inputs: EntryInputs;\n};\n\ntype Outputs = {\n  context: SharedContext;\n};\n\nasync function invoke({\n  context,\n  \"p-chat\": chat,\n  \"p-list\": makeList,\n  description,\n  ...params\n}: EntryInputs): Promise<Outputs> {\n  // Make sure it's a boolean.\n  chat = !!chat;\n  context ??= [];\n  const defaultModel = \"\";\n  const type = \"work\";\n  return {\n    context: {\n      id: Math.random().toString(36).substring(2, 5),\n      chat,\n      makeList,\n      listPath: [],\n      context,\n      userInputs: [],\n      defaultModel,\n      model: \"\",\n      description,\n      type,\n      work: [],\n      userEndedChat: false,\n      params,\n    },\n  };\n}\n\nasync function describe({ inputs: { description } }: DescribeInputs) {\n  const settings = await readSettings();\n  const experimental =\n    ok(settings) && !!settings[\"Show Experimental Components\"];\n  const template = new Template(description);\n  let extra: Record<string, Schema> = {};\n  if (experimental) {\n    extra = {\n      \"p-chat\": {\n        type: \"boolean\",\n        title: \"Chat with User\",\n        behavior: [\"config\", \"hint-preview\"],\n        icon: \"chat\",\n        description:\n          \"When checked, this step will chat with the user, asking to review work, requesting additional information, etc.\",\n      },\n      \"p-list\": {\n        type: \"boolean\",\n        title: \"Make a list\",\n        behavior: [\"config\", \"hint-preview\"],\n        icon: \"summarize\",\n        description:\n          \"When checked, this step will try to create a list as its output. Make sure that the prompt asks for a list of some sort\",\n      },\n    };\n  }\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        description: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n          title: \"Instruction\",\n          description:\n            \"Give the model additional context on what to do, like specific rules/guidelines to adhere to or specify behavior separate from the provided context.\",\n          default: defaultLLMContent(),\n        },\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n        ...extra,\n        ...template.schemas(),\n      },\n      behavior: [\"at-wireable\"],\n      ...template.requireds(),\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n          behavior: [\"main-port\", \"hint-text\"],\n        },\n      },\n    } satisfies Schema,\n    title: \"Make Text\",\n    metadata: {\n      icon: \"generative-text\",\n      tags: [\"quick-access\", \"generative\"],\n      order: 1,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Manages the entry point: describer, passing the inputs, etc.",
        "runnable": true
      }
    },
    "types": {
      "code": "/**\n * @fileoverview Common type definitions\n */\n",
      "metadata": {
        "title": "types",
        "source": {
          "code": "/**\n * @fileoverview Common type definitions\n */\n\nimport type { Params } from \"./a2/common\";\n\nexport type GenerateTextInputs = {\n  /**\n   * Whether (true) or not (false) the agent is allowed to chat with user.\n   */\n  chat: boolean;\n  /**\n   * Whether (true) or not (false) to try to turn the output into a list\n   */\n  makeList: boolean;\n  /**\n   * The incoming conversation context.\n   */\n  context: LLMContent[];\n  /**\n   * Accumulated work context. This is the internal conversation, a result\n   * of talking with the user, for instance.\n   * This context is discarded at the end of interacting with the agent.\n   */\n  work: LLMContent[];\n  /**\n   * The index path to the currently processed list.\n   */\n  listPath: number[];\n  /**\n   * Agent's job description.\n   */\n  description?: LLMContent;\n  /**\n   * Last work product.\n   */\n  last?: LLMContent;\n  /**\n   * Type of the task.\n   */\n  type: \"introduction\" | \"work\";\n  /**\n   * The board URL of the model\n   */\n  model: string;\n  /**\n   * The default model that is passed along by the manager\n   */\n  defaultModel: string;\n  /**\n   * params\n   */\n  params: Params;\n};\n\nexport type SharedContext = GenerateTextInputs & {\n  /**\n   * A unique identifier for the session.\n   * Currently used to have a persistent part separator across conversation context\n   */\n  id: string;\n  /**\n   * Accumulating list of user inputs\n   */\n  userInputs: LLMContent[];\n  /**\n   * Indicator that the user ended chat.\n   */\n  userEndedChat: boolean;\n};\n",
          "language": "typescript"
        },
        "description": "Common type definitions",
        "runnable": false
      }
    },
    "chat-tools": {
      "code": "/**\n * @fileoverview Tools for conversational (\"chat\") mode\n */\nexport { createDoneTool, createKeepChattingTool };\nclass ChatTool {\n    name;\n    description;\n    #invoked = false;\n    constructor(name, description) {\n        this.name = name;\n        this.description = description;\n    }\n    get invoked() {\n        return this.#invoked;\n    }\n    declaration() {\n        return {\n            name: this.name,\n            description: this.description,\n        };\n    }\n    handle() {\n        return {\n            tool: this.declaration(),\n            url: \"\",\n            passContext: false,\n            invoke: async () => {\n                this.#invoked = true;\n            },\n        };\n    }\n}\nfunction createDoneTool() {\n    return new ChatTool(\"User_Says_Done\", \"Call when the user indicates they are done with the conversation and are ready to move on\");\n}\nfunction createKeepChattingTool() {\n    return new ChatTool(\"User_Asks_For_More_Work\", \"Call when the user asked a question or issued instruction to do more work\");\n}\n",
      "metadata": {
        "title": "chat-tools",
        "source": {
          "code": "/**\n * @fileoverview Tools for conversational (\"chat\") mode\n */\n\nimport type { ToolHandle } from \"./a2/tool-manager\";\n\nexport { createDoneTool, createKeepChattingTool };\n\nclass ChatTool {\n  #invoked = false;\n\n  constructor(\n    public readonly name: string,\n    public readonly description: string\n  ) {}\n\n  get invoked() {\n    return this.#invoked;\n  }\n\n  declaration() {\n    return {\n      name: this.name,\n      description: this.description,\n    };\n  }\n\n  handle(): ToolHandle {\n    return {\n      tool: this.declaration(),\n      url: \"\",\n      passContext: false,\n      invoke: async () => {\n        this.#invoked = true;\n      },\n    };\n  }\n}\n\nfunction createDoneTool() {\n  return new ChatTool(\n    \"User_Says_Done\",\n    \"Call when the user indicates they are done with the conversation and are ready to move on\"\n  );\n}\n\nfunction createKeepChattingTool() {\n  return new ChatTool(\n    \"User_Asks_For_More_Work\",\n    \"Call when the user asked a question or issued instruction to do more work\"\n  );\n}\n",
          "language": "typescript"
        },
        "description": "Tools for conversational (\"chat\") mode",
        "runnable": false
      }
    }
  },
  "exports": [
    "#module:main"
  ],
  "assets": {
    "@@thumbnail": {
      "metadata": {
        "title": "Thumbnail",
        "type": "file"
      },
      "data": "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjUwIiBoZWlnaHQ9IjIwMCIgdmlld0JveD0iMCAwIDI1MCAyMDAiIGZpbGw9Im5vbmUiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+CiAgICAKICAgICAgPHJlY3QgeD0iMTM5LjA1IgogICAgICAgICAgICAgICAgICAgIHk9Ijg3Ljg5IgogICAgICAgICAgICAgICAgICAgIHdpZHRoPSI0My43NSIKICAgICAgICAgICAgICAgICAgICBoZWlnaHQ9IjI0LjIzIgogICAgICAgICAgICAgICAgICAgIHJ4PSIzLjUiCiAgICAgICAgICAgICAgICAgICAgZmlsbD0id2hpdGUiCiAgICAgICAgICAgICAgICAgICAgc3Ryb2tlPSIjMjBhMjAyIiAvPgo8cmVjdCB4PSIxMC4wMCIKICAgICAgICAgICAgICAgICAgICB5PSI3NS43NyIKICAgICAgICAgICAgICAgICAgICB3aWR0aD0iNDMuNzUiCiAgICAgICAgICAgICAgICAgICAgaGVpZ2h0PSIyNC4yMyIKICAgICAgICAgICAgICAgICAgICByeD0iMy41IgogICAgICAgICAgICAgICAgICAgIGZpbGw9IndoaXRlIgogICAgICAgICAgICAgICAgICAgIHN0cm9rZT0iIzc3NTdkOSIgLz4KPHJlY3QgeD0iNzUuMTEiCiAgICAgICAgICAgICAgICAgICAgeT0iODcuODkiCiAgICAgICAgICAgICAgICAgICAgd2lkdGg9IjQzLjc1IgogICAgICAgICAgICAgICAgICAgIGhlaWdodD0iMjQuMjMiCiAgICAgICAgICAgICAgICAgICAgcng9IjMuNSIKICAgICAgICAgICAgICAgICAgICBmaWxsPSJ3aGl0ZSIKICAgICAgICAgICAgICAgICAgICBzdHJva2U9IiMyZThiZTgiIC8+CjxyZWN0IHg9IjE5Ni4yNSIKICAgICAgICAgICAgICAgICAgICB5PSI2MC45NyIKICAgICAgICAgICAgICAgICAgICB3aWR0aD0iNDMuNzUiCiAgICAgICAgICAgICAgICAgICAgaGVpZ2h0PSIyNC4yMyIKICAgICAgICAgICAgICAgICAgICByeD0iMy41IgogICAgICAgICAgICAgICAgICAgIGZpbGw9IndoaXRlIgogICAgICAgICAgICAgICAgICAgIHN0cm9rZT0iIzJlOGJlOCIgLz4KPHJlY3QgeD0iMTM5LjA1IgogICAgICAgICAgICAgICAgICAgIHk9IjExNC44MSIKICAgICAgICAgICAgICAgICAgICB3aWR0aD0iNDMuNzUiCiAgICAgICAgICAgICAgICAgICAgaGVpZ2h0PSIyNC4yMyIKICAgICAgICAgICAgICAgICAgICByeD0iMy41IgogICAgICAgICAgICAgICAgICAgIGZpbGw9IndoaXRlIgogICAgICAgICAgICAgICAgICAgIHN0cm9rZT0iIzIwYTIwMiIgLz4KICAgIDwvc3ZnPg=="
    }
  }
}