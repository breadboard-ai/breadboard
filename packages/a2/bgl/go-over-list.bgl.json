{
  "title": "A2 Go Over a List",
  "description": "",
  "version": "0.0.1",
  "nodes": [],
  "edges": [],
  "metadata": {
    "comments": [
      {
        "id": "comment-cc94afe8",
        "text": "Intentionally Left Blank",
        "metadata": {
          "title": "Comment",
          "visual": {
            "x": 531,
            "y": 374,
            "collapsed": "expanded",
            "outputHeight": 0
          }
        }
      }
    ],
    "visual": {
      "presentation": {
        "themes": {
          "5f3ca599-8fee-46fb-951f-0d47b16a6d56": {
            "themeColors": {
              "primaryColor": "#246db5",
              "secondaryColor": "#5cadff",
              "backgroundColor": "#ffffff",
              "textColor": "#1a1a1a",
              "primaryTextColor": "#ffffff"
            },
            "template": "basic",
            "splashScreen": {
              "storedData": {
                "handle": "/images/app/generic-flow.jpg",
                "mimeType": "image/jpeg"
              }
            }
          }
        },
        "theme": "5f3ca599-8fee-46fb-951f-0d47b16a6d56"
      }
    },
    "tags": [
      "published",
      "tool",
      "component"
    ]
  },
  "modules": {
    "main": {
      "code": "/**\n * @fileoverview Break an objective into tasks and then execute them.\n */\nimport { Template } from \"./a2/template\";\nimport { ToolManager } from \"./a2/tool-manager\";\nimport {} from \"./a2/common\";\nimport { ok, err, generateId } from \"./a2/utils\";\nimport { ArgumentNameGenerator } from \"./a2/introducer\";\nimport { Runtime } from \"./runtime\";\nimport { ParallelStrategist } from \"./parallel-strategist\";\nimport { SequentialStrategist } from \"./sequential-strategist\";\nimport { ThinkStrategist } from \"./think-strategist\";\nimport { ConversationalThinkStrategist } from \"./conversational-think-strategist\";\nimport {} from \"./types\";\nimport { fanOutContext } from \"./a2/lists\";\nimport { readSettings } from \"./a2/settings\";\nexport { invoke as default, describe };\nconst STRATEGISTS = [\n    new ParallelStrategist(),\n    new SequentialStrategist(),\n    new ThinkStrategist(),\n    new ConversationalThinkStrategist(),\n];\nfunction findStrategist(name) {\n    if (!name)\n        return STRATEGISTS[0];\n    return STRATEGISTS.find((strategist) => strategist.name === name);\n}\nasync function invoke({ context, plan: objective, strategy, \"z-list\": makeList, ...params }) {\n    const toolManager = new ToolManager(new ArgumentNameGenerator());\n    const template = new Template(objective);\n    const substituting = await template.substitute(params, async ({ path: url, instance }) => toolManager.addTool(url, instance));\n    if (!ok(substituting))\n        return substituting;\n    const strategist = findStrategist(strategy);\n    if (!strategist) {\n        return err(`Unknown strategy: \"${strategy}\"`);\n    }\n    const result = await fanOutContext(substituting, context, async (objective, context, isList) => {\n        const disallowNestedLists = makeList && !isList;\n        const executor = new Runtime(context, toolManager, disallowNestedLists);\n        const executingOne = await executor.executeStrategy(objective, strategist);\n        if (!ok(executingOne))\n            return executingOne;\n        // Disallow making a list when already inside of a make list\n        if (disallowNestedLists) {\n            return {\n                role: \"model\",\n                parts: [\n                    {\n                        id: generateId(),\n                        list: executingOne.map((item) => {\n                            return { content: [item] };\n                        }),\n                    },\n                ],\n            };\n        }\n        const oneContent = {\n            role: \"model\",\n            parts: executingOne.flatMap((item) => {\n                return item.parts;\n            }),\n        };\n        return oneContent;\n    });\n    if (!ok(result))\n        return result;\n    return { context: result };\n}\nasync function describe({ inputs: { plan } }) {\n    const template = new Template(plan);\n    const settings = await readSettings();\n    const experimental = ok(settings) && !!settings[\"Show Experimental Components\"];\n    let extra = {};\n    if (experimental) {\n        extra = {\n            \"z-list\": {\n                type: \"boolean\",\n                title: \"Make a list\",\n                behavior: [\"config\", \"hint-preview\", \"hint-advanced\"],\n                icon: \"summarize\",\n                description: \"When checked, this step will try to create a list as its output. Make sure that the prompt asks for a list of some sort\",\n            },\n        };\n    }\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                plan: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"Objective\",\n                    description: \"Describe what will be turned into a list and then gone over\",\n                },\n                strategy: {\n                    title: \"Strategy\",\n                    description: `How to go over the list: \n\"${STRATEGISTS[0].name}\" is fastest, working in parallel. \n\"${STRATEGISTS[1].name}\" will build on previous work.\n\"${STRATEGISTS[2].name}\" will think after each step adjust the list if necessary`,\n                    type: \"string\",\n                    behavior: [\"config\", \"hint-preview\", \"hint-advanced\"],\n                    enum: STRATEGISTS.map((strategist) => strategist.name),\n                    icon: \"joiner\",\n                    default: STRATEGISTS[0].name,\n                },\n                ...extra,\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Results\",\n                    behavior: [\"main-port\"],\n                },\n            },\n            additionalProperties: false,\n        },\n        title: \"Plan and Execute\",\n        description: \"Break an objective into tasks and then execute them\",\n        metadata: {\n            icon: \"laps\",\n            tags: [\"quick-access\", \"generative\", \"experimental\"],\n            order: 102,\n        },\n    };\n}\n",
      "metadata": {
        "title": "main",
        "source": {
          "code": "/**\n * @fileoverview Break an objective into tasks and then execute them.\n */\nimport { Template } from \"./a2/template\";\nimport { ToolManager } from \"./a2/tool-manager\";\nimport { type Params } from \"./a2/common\";\nimport { ok, err, generateId } from \"./a2/utils\";\nimport { ArgumentNameGenerator } from \"./a2/introducer\";\nimport { Runtime } from \"./runtime\";\nimport { ParallelStrategist } from \"./parallel-strategist\";\nimport { SequentialStrategist } from \"./sequential-strategist\";\nimport { ThinkStrategist } from \"./think-strategist\";\nimport { ConversationalThinkStrategist } from \"./conversational-think-strategist\";\nimport { type Strategist } from \"./types\";\nimport { fanOutContext } from \"./a2/lists\";\nimport { readSettings } from \"./a2/settings\";\n\nexport { invoke as default, describe };\n\ntype Inputs = {\n  context: LLMContent[];\n  plan: LLMContent;\n  strategy: string;\n  \"z-list\": boolean;\n} & Params;\n\ntype Outputs = {\n  context: LLMContent[];\n};\n\nconst STRATEGISTS: Strategist[] = [\n  new ParallelStrategist(),\n  new SequentialStrategist(),\n  new ThinkStrategist(),\n  new ConversationalThinkStrategist(),\n];\n\nfunction findStrategist(name?: string): Strategist | undefined {\n  if (!name) return STRATEGISTS[0];\n  return STRATEGISTS.find((strategist) => strategist.name === name);\n}\n\nasync function invoke({\n  context,\n  plan: objective,\n  strategy,\n  \"z-list\": makeList,\n  ...params\n}: Inputs): Promise<Outcome<Outputs>> {\n  const toolManager = new ToolManager(new ArgumentNameGenerator());\n  const template = new Template(objective);\n  const substituting = await template.substitute(\n    params,\n    async ({ path: url, instance }) => toolManager.addTool(url, instance)\n  );\n  if (!ok(substituting)) return substituting;\n\n  const strategist = findStrategist(strategy);\n  if (!strategist) {\n    return err(`Unknown strategy: \"${strategy}\"`);\n  }\n\n  const result = await fanOutContext(\n    substituting,\n    context,\n    async (objective, context, isList) => {\n      const disallowNestedLists = makeList && !isList;\n      const executor = new Runtime(context, toolManager, disallowNestedLists);\n      const executingOne = await executor.executeStrategy(\n        objective,\n        strategist\n      );\n      if (!ok(executingOne)) return executingOne;\n\n      // Disallow making a list when already inside of a make list\n      if (disallowNestedLists) {\n        return {\n          role: \"model\",\n          parts: [\n            {\n              id: generateId(),\n              list: executingOne.map((item) => {\n                return { content: [item] };\n              }),\n            },\n          ],\n        };\n      }\n\n      const oneContent = {\n        role: \"model\",\n        parts: executingOne.flatMap((item) => {\n          return item.parts;\n        }),\n      };\n\n      return oneContent;\n    }\n  );\n  if (!ok(result)) return result;\n  return { context: result };\n}\n\ntype DescribeInputs = {\n  inputs: {\n    plan: LLMContent;\n  };\n};\n\nasync function describe({ inputs: { plan } }: DescribeInputs) {\n  const template = new Template(plan);\n  const settings = await readSettings();\n  const experimental =\n    ok(settings) && !!settings[\"Show Experimental Components\"];\n  let extra: Record<string, Schema> = {};\n  if (experimental) {\n    extra = {\n      \"z-list\": {\n        type: \"boolean\",\n        title: \"Make a list\",\n        behavior: [\"config\", \"hint-preview\", \"hint-advanced\"],\n        icon: \"summarize\",\n        description:\n          \"When checked, this step will try to create a list as its output. Make sure that the prompt asks for a list of some sort\",\n      },\n    };\n  }\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n        plan: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n          title: \"Objective\",\n          description:\n            \"Describe what will be turned into a list and then gone over\",\n        },\n        strategy: {\n          title: \"Strategy\",\n          description: `How to go over the list: \n\"${STRATEGISTS[0].name}\" is fastest, working in parallel. \n\"${STRATEGISTS[1].name}\" will build on previous work.\n\"${STRATEGISTS[2].name}\" will think after each step adjust the list if necessary`,\n          type: \"string\",\n          behavior: [\"config\", \"hint-preview\", \"hint-advanced\"],\n          enum: STRATEGISTS.map((strategist) => strategist.name),\n          icon: \"joiner\",\n          default: STRATEGISTS[0].name,\n        },\n        ...extra,\n        ...template.schemas(),\n      },\n      behavior: [\"at-wireable\"],\n      ...template.requireds(),\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Results\",\n          behavior: [\"main-port\"],\n        },\n      },\n      additionalProperties: false,\n    } satisfies Schema,\n    title: \"Plan and Execute\",\n    description: \"Break an objective into tasks and then execute them\",\n    metadata: {\n      icon: \"laps\",\n      tags: [\"quick-access\", \"generative\", \"experimental\"],\n      order: 102,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Break an objective into tasks and then execute them.",
        "runnable": true
      }
    },
    "planner-prompt": {
      "code": "/**\n * @fileoverview Contains the planner prompt.\n */\nimport { defaultSafetySettings } from \"./a2/gemini\";\nimport { GeminiPrompt } from \"./a2/gemini-prompt\";\nimport { llm, err } from \"./a2/utils\";\nimport {} from \"./types\";\nexport { plannerPrompt, thinkingPlannerPrompt, getPlan };\nfunction preamble(extraPlannerPrompt) {\n    return `You are a planner. \nYou are to create a precise plan -- a list of tasks -- for a given objective. This plan will be executed by others.\n\n${extraPlannerPrompt}\n\nYour responsibility is to produce a plan that fulfills the objective.\n\nExamine the objective, slow down, take a deep breath.\nNow think: how might you break the objective into tasks that, when all completed, will fulfill the objective?\nNow write out the tasks. Do not add any superflous tasks.\n\nFor each task, also think of a brief label that describes that task and could be used in a UI as a hint to the user.\n\nIf multiple tools are mentioned in the objective, make sure to mention them all in the task so that whoever\nexecutes the task can decide which tool to use.\n`;\n}\nfunction planSchema(organize) {\n    const organizeFlag = (organize\n        ? {\n            summarizeResults: {\n                type: \"boolean\",\n                description: \"Set to true if and only if the objective calls for summarizing results at the end. Set to false otherwise.\",\n            },\n        }\n        : {});\n    const required = [\"thinking\", \"todo\"];\n    if (organize) {\n        required.push(...Object.keys(organizeFlag));\n    }\n    return {\n        type: \"object\",\n        properties: {\n            thinking: {\n                type: \"string\",\n                description: \"Brief reasoning on why these steps are the right steps to fulfill the objective.\",\n            },\n            todo: {\n                type: \"array\",\n                items: {\n                    type: \"object\",\n                    properties: {\n                        task: {\n                            type: \"string\",\n                            description: \"The task description. Use action-oriented language, starting with a verb that fits the task.\",\n                        },\n                        label: {\n                            description: \"Short, precise label for that describes the task.\",\n                            type: \"string\",\n                        },\n                    },\n                    required: [\"task\", \"label\"],\n                },\n            },\n            ...organizeFlag,\n        },\n        required,\n    };\n}\nfunction getPlan(content) {\n    const planPart = content.parts.at(0);\n    if (!planPart || !(\"json\" in planPart)) {\n        // TODO: Error recovery.\n        return err(`Gemini generated invalid plan`);\n    }\n    console.log(\"PLAN\", planPart.json);\n    return planPart.json;\n}\nfunction prependInstruction(text, plan) {\n    return {\n        ...plan,\n        parts: [...plan.parts, { text }],\n    };\n}\nfunction thinkingPlannerPrompt(context, objective, plan, steps, extraPlannerPrompt) {\n    const instruction = llm `\n${preamble(extraPlannerPrompt)}\n\nYour objective is:\n\n\\`\\`\\`\n\n${objective}\n\n\\`\\`\\`\n\nYour original plan was:\n\n\\`\\`\\`json\n${JSON.stringify(plan)}\n\\`\\`\\`\n\nSo far, you've completed these steps:\n\n${steps.map((step) => `- ${step}`).join(\"\\n\")}\n\nUpdate the plan to ensure that the steps that follow achieve the objective.\nPay particular attention to steps that did not complete successfully and strategize\ndifferent approaches and steps that might be necesssary to complete them.\n\nOnly add steps to the plan that still need to be completed.\nDo not return completed steps as part of the updated plan.\nIf no more steps are needed, return no steps.\n`.asContent();\n    const contents = [...context, instruction];\n    return new GeminiPrompt({\n        body: {\n            contents,\n            safetySettings: defaultSafetySettings(),\n            generationConfig: {\n                responseSchema: planSchema(true),\n                responseMimeType: \"application/json\",\n            },\n        },\n    });\n}\nfunction plannerPrompt(context, objective, extraPlannerPrompt, organize) {\n    context ??= [];\n    const instruction = `${preamble(extraPlannerPrompt)}`;\n    const contents = [...context, prependInstruction(instruction, objective)];\n    return new GeminiPrompt({\n        body: {\n            contents,\n            safetySettings: defaultSafetySettings(),\n            generationConfig: {\n                responseSchema: planSchema(organize),\n                responseMimeType: \"application/json\",\n            },\n        },\n    });\n}\n",
      "metadata": {
        "title": "planner-prompt",
        "source": {
          "code": "/**\n * @fileoverview Contains the planner prompt.\n */\n\nimport { type GeminiSchema, defaultSafetySettings } from \"./a2/gemini\";\nimport { GeminiPrompt } from \"./a2/gemini-prompt\";\nimport { llm, err } from \"./a2/utils\";\nimport { type Plan } from \"./types\";\n\nexport { plannerPrompt, thinkingPlannerPrompt, getPlan };\n\nfunction preamble(extraPlannerPrompt: string) {\n  return `You are a planner. \nYou are to create a precise plan -- a list of tasks -- for a given objective. This plan will be executed by others.\n\n${extraPlannerPrompt}\n\nYour responsibility is to produce a plan that fulfills the objective.\n\nExamine the objective, slow down, take a deep breath.\nNow think: how might you break the objective into tasks that, when all completed, will fulfill the objective?\nNow write out the tasks. Do not add any superflous tasks.\n\nFor each task, also think of a brief label that describes that task and could be used in a UI as a hint to the user.\n\nIf multiple tools are mentioned in the objective, make sure to mention them all in the task so that whoever\nexecutes the task can decide which tool to use.\n`;\n}\n\nfunction planSchema(organize?: boolean): GeminiSchema {\n  const organizeFlag = (\n    organize\n      ? {\n          summarizeResults: {\n            type: \"boolean\",\n            description:\n              \"Set to true if and only if the objective calls for summarizing results at the end. Set to false otherwise.\",\n          },\n        }\n      : {}\n  ) as Record<string, GeminiSchema>;\n  const required = [\"thinking\", \"todo\"];\n  if (organize) {\n    required.push(...Object.keys(organizeFlag));\n  }\n  return {\n    type: \"object\",\n    properties: {\n      thinking: {\n        type: \"string\",\n        description:\n          \"Brief reasoning on why these steps are the right steps to fulfill the objective.\",\n      },\n      todo: {\n        type: \"array\",\n        items: {\n          type: \"object\",\n          properties: {\n            task: {\n              type: \"string\",\n              description:\n                \"The task description. Use action-oriented language, starting with a verb that fits the task.\",\n            },\n            label: {\n              description: \"Short, precise label for that describes the task.\",\n              type: \"string\",\n            },\n          },\n          required: [\"task\", \"label\"],\n        },\n      },\n      ...organizeFlag,\n    },\n    required,\n  };\n}\n\nfunction getPlan(content: LLMContent): Outcome<Plan> {\n  const planPart = content.parts.at(0);\n  if (!planPart || !(\"json\" in planPart)) {\n    // TODO: Error recovery.\n    return err(`Gemini generated invalid plan`);\n  }\n  console.log(\"PLAN\", planPart.json);\n  return planPart.json as Plan;\n}\n\nfunction prependInstruction(text: string, plan: LLMContent): LLMContent {\n  return {\n    ...plan,\n    parts: [...plan.parts, { text }],\n  };\n}\n\nfunction thinkingPlannerPrompt(\n  context: LLMContent[],\n  objective: LLMContent,\n  plan: Plan,\n  steps: string[],\n  extraPlannerPrompt: string\n): GeminiPrompt {\n  const instruction = llm`\n${preamble(extraPlannerPrompt)}\n\nYour objective is:\n\n\\`\\`\\`\n\n${objective}\n\n\\`\\`\\`\n\nYour original plan was:\n\n\\`\\`\\`json\n${JSON.stringify(plan)}\n\\`\\`\\`\n\nSo far, you've completed these steps:\n\n${steps.map((step) => `- ${step}`).join(\"\\n\")}\n\nUpdate the plan to ensure that the steps that follow achieve the objective.\nPay particular attention to steps that did not complete successfully and strategize\ndifferent approaches and steps that might be necesssary to complete them.\n\nOnly add steps to the plan that still need to be completed.\nDo not return completed steps as part of the updated plan.\nIf no more steps are needed, return no steps.\n`.asContent();\n\n  const contents = [...context, instruction];\n  return new GeminiPrompt({\n    body: {\n      contents,\n      safetySettings: defaultSafetySettings(),\n      generationConfig: {\n        responseSchema: planSchema(true),\n        responseMimeType: \"application/json\",\n      },\n    },\n  });\n}\n\nfunction plannerPrompt(\n  context: LLMContent[] | undefined,\n  objective: LLMContent,\n  extraPlannerPrompt: string,\n  organize: boolean\n): GeminiPrompt {\n  context ??= [];\n  const instruction = `${preamble(extraPlannerPrompt)}`;\n\n  const contents = [...context, prependInstruction(instruction, objective)];\n  return new GeminiPrompt({\n    body: {\n      contents,\n      safetySettings: defaultSafetySettings(),\n      generationConfig: {\n        responseSchema: planSchema(organize),\n        responseMimeType: \"application/json\",\n      },\n    },\n  });\n}\n",
          "language": "typescript"
        },
        "description": "Contains the planner prompt.",
        "runnable": false
      }
    },
    "conversational-planner-prompt": {
      "code": "/**\n * @fileoverview Contains the planner prompt for the conversational think as I go.\n */\nimport { defaultSafetySettings } from \"./a2/gemini\";\nimport { GeminiPrompt } from \"./a2/gemini-prompt\";\nimport { llm, err } from \"./a2/utils\";\nimport {} from \"./types\";\nexport { plannerPrompt, thinkingPlannerPrompt, getPlan };\nfunction preamble(userObjective) {\n    return llm `You are an adaptive AI agent controller.\n\nOverall Objective: Your primary goal is to guide an agent to achieve the following objective:\n<AGENT_INSTRUCTIONS>\n${userObjective}\n</AGENT_INSTRUCTIONS>\n`.asContent();\n}\nfunction planSchema(organize) {\n    const organizeFlag = (organize\n        ? {\n            summarizeResults: {\n                type: \"boolean\",\n                description: \"Set to true if and only if the objective calls for summarizing results at the end. Set to false otherwise.\",\n            },\n        }\n        : {});\n    const required = [\"thinking\", \"todo\"];\n    if (organize) {\n        required.push(...Object.keys(organizeFlag));\n    }\n    return {\n        type: \"object\",\n        properties: {\n            thinking: {\n                type: \"string\",\n                description: \"Brief reasoning on why these steps are the right steps to fulfill the objective, or why termination is necessary.\",\n            },\n            todo: {\n                type: \"array\",\n                description: \"The list of tasks to perform. If terminating, this is empty.\", // Updated description\n                items: {\n                    type: \"object\",\n                    properties: {\n                        task: {\n                            type: \"string\",\n                            description: \"The task description. Use action-oriented language, starting with a verb that fits the task.\",\n                        },\n                        label: {\n                            description: \"Short, precise label for that describes the task.\",\n                            type: \"string\",\n                        },\n                    },\n                    required: [\"task\", \"label\"],\n                },\n            },\n            ...organizeFlag,\n        },\n        required,\n    };\n}\nfunction getPlan(content) {\n    const planPart = content.parts.at(0);\n    if (!planPart || !(\"json\" in planPart)) {\n        // TODO: Error recovery.\n        return err(`Gemini generated invalid plan`);\n    }\n    console.log(\"PLAN\", planPart.json);\n    return planPart.json;\n}\nfunction prependInstruction(text, plan) {\n    return {\n        ...plan,\n        parts: [...plan.parts, { text }],\n    };\n}\nfunction thinkingPlannerPrompt(context, objective, plan, steps, extraPlannerPrompt) {\n    const instruction = llm `\n${preamble(JSON.stringify(objective))}\n\nInitial Plan Outline (Guideline Only):\nThis was the originally intended sequence of actions. Use it as a reference, but do not follow it rigidly if the situation changes.\n\n${JSON.stringify(plan)}\n\nCurrent Situation:\n\nHistory: (A summary of steps already completed and their outcomes)\n\n\\`\\`\\`json\n${JSON.stringify(context)}\n\\`\\`\\`\n\nYour Task:\n\nBased on the Overall Objective, the Initial Plan Outline, the Available Tools, the History, and crucially, the Last Outcome, determine the single best next step for the agent to take right now. Your absolute priority is to make progress towards the objective *while acknowledging the user's input*.\n\nCritical Instructions:\n\n1.  **Evaluate Last Outcome:** Carefully analyze the \\`Last Outcome\\`.\n\n2.  **Expected Outcome:** If the outcome was expected (e.g., successful tool execution providing needed info, user response directly answering the previous question) and aligns with progressing the Initial Plan Outline, determine the next logical step from that outline or towards the objective.\n\n3.  **Unexpected/User-Driven Outcome:** If the \\`Last Outcome\\` is unexpected, problematic, reveals new user needs, is off-topic, emotionally charged, evasive, indicates a failure, or otherwise deviates from the expected path:\n    * **PRIORITY 1: Address the User Input:**\n        * **Acknowledge & Validate:** *Explicitly acknowledge* the user's statement, question, or expressed feeling. Use empathetic phrasing. Examples: \"I understand you're asking about X...\", \"That's an interesting point about Y...\", \"It sounds like you're concerned about Z...\", \"Thanks for sharing that perspective on W...\" Avoid language that dismisses or ignores their contribution.\n        * **Assess Relevance & Safety:** Quickly determine if the user's input relates to the objective, introduces a new constraint, signals discomfort, raises ethical flags, or indicates a desire to stop.\n        * **Connect (If Possible):** If the user's point can be briefly and naturally linked back to the overall objective, do so. Example: \"...and thinking about Y might actually help us clarify [part of the objective]...\"\n        * **Empathetically Redirect:** Gently guide the conversation back towards the objective or the next logical step needed to achieve it. Frame it collaboratively. Examples: \"...to make sure we achieve [objective], perhaps we could first look at...?\", \"...I want to make sure I help you with [objective], so could we focus back on...?\", \"Given our goal of [objective], the next step I think would be helpful is...\"\n        * **Adapt the Plan:** Based on the user's input, *fundamentally reassess* the next step. Do NOT blindly follow the Initial Plan Outline. Your next action *must* account for what the user just said.\n    * **PRIORITY 2: Determine Adaptive Next Step:** Based on the above interaction, decide the *actual* next step. This might be:\n        * Asking the user a clarifying question that incorporates their last point.\n        * Rephrasing your previous question or request.\n        * Using a different tool or strategy better suited to the new situation.\n        * Acknowledging an error and proposing a correction.\n        * Pausing the plan to address a user concern directly.\n        * Deciding *not* to proceed if the user signals discomfort or the topic becomes inappropriate.\n        * Ending the interaction politely if the objective is blocked, achieved, or the user wishes to stop.\n\n4.  **Justify (Mandatory for Deviations):** Briefly explain *why* you chose this next step, *especially* if deviating from the original plan due to an unexpected outcome or user input. Explicitly mention how your chosen step acknowledges the user's input and aims to get back on track empathetically.\n\n5.  **Specify Action:** Clearly define the single next action for the agent.\n    * **User Interaction:** Provide the *exact text* the agent should say to the user. Otherwise we will confuse the user.\n    * **Tool Use:** State the tool name and the precise inputs required, ensuring inputs are updated based on the latest context and user feedback.\n    * **Internal Step:** Describe the internal calculation, data processing, or analysis the agent needs to perform.\n\n6. Note when the agent finishes executing, we'll automatically show the results to the user, so there's no need for an explicit 'present' or 'show' step.\n\nOutput:\nProvide a clear description of the next step the agent should execute now, and a list of good steps to follow after that (generate them assuming the single next step was sucessful).\n\nFor example,\n\\`\\`\\`json\n{\n    \"summarizeResults\": false,\n    \"thinking\": \"The user responded with \\\"hot dog please\\\" after the introduction. This is nonsensical and off-topic. I should acknowledge their input and try to redirect the conversation back to the original goal.\",\n    \"todo\": [\n        {\n            \"label\": \"Gather Information\",\n            \"task\": \"Tell the user that while a hotdog sure sounds tasty, I can't offer the user a hotdog. Then ask the user what brand or business they would like to highlight in the post.\"\n        },\n        {\n            \"label\": \"Generate Post\",\n            \"task\": \"Use the brand name and description to generate an instagram post.\"\n        }\n    ]\n}\n\\`\\`\\`\n\nExtra instructions:\n${extraPlannerPrompt}\n`.asContent();\n    const contents = [instruction];\n    let prompt = new GeminiPrompt({\n        body: {\n            contents,\n            safetySettings: defaultSafetySettings(),\n            generationConfig: {\n                responseSchema: planSchema(true),\n                responseMimeType: \"application/json\",\n            },\n        },\n    });\n    console.log(\"thinkingPlannerPrompt: \", prompt);\n    return prompt;\n}\nfunction plannerPrompt(context, objective, extraPlannerPrompt, organize) {\n    context ??= [];\n    const instruction = preamble(JSON.stringify(objective));\n    const epilogue = llm `\nExtra instructions:\n${extraPlannerPrompt}\n`.asContent();\n    const contents = [...context, instruction, epilogue];\n    let prompt = new GeminiPrompt({\n        body: {\n            contents,\n            safetySettings: defaultSafetySettings(),\n            generationConfig: {\n                responseSchema: planSchema(organize),\n                responseMimeType: \"application/json\",\n            },\n        },\n    });\n    console.log(\"plannerPrompt: \", prompt);\n    return prompt;\n}\n",
      "metadata": {
        "title": "conversational-planner-prompt",
        "source": {
          "code": "/**\n * @fileoverview Contains the planner prompt for the conversational think as I go.\n */\n\nimport { type GeminiSchema, defaultSafetySettings } from \"./a2/gemini\";\nimport { GeminiPrompt } from \"./a2/gemini-prompt\";\nimport { llm, err } from \"./a2/utils\";\nimport { type Plan } from \"./types\";\n\nexport { plannerPrompt, thinkingPlannerPrompt, getPlan };\n\nfunction preamble(userObjective: string) {\n  return llm`You are an adaptive AI agent controller.\n\nOverall Objective: Your primary goal is to guide an agent to achieve the following objective:\n<AGENT_INSTRUCTIONS>\n${userObjective}\n</AGENT_INSTRUCTIONS>\n`.asContent();\n}\n\nfunction planSchema(organize?: boolean): GeminiSchema {\n  const organizeFlag = (\n    organize\n      ? {\n          summarizeResults: {\n            type: \"boolean\",\n            description:\n              \"Set to true if and only if the objective calls for summarizing results at the end. Set to false otherwise.\",\n          },\n        }\n      : {}\n  ) as Record<string, GeminiSchema>;\n  const required = [\"thinking\", \"todo\"];\n  if (organize) {\n    required.push(...Object.keys(organizeFlag));\n  }\n  return {\n    type: \"object\",\n    properties: {\n      thinking: {\n        type: \"string\",\n        description:\n          \"Brief reasoning on why these steps are the right steps to fulfill the objective, or why termination is necessary.\",\n      },\n      todo: {\n        type: \"array\",\n        description:\n          \"The list of tasks to perform. If terminating, this is empty.\", // Updated description\n        items: {\n          type: \"object\",\n          properties: {\n            task: {\n              type: \"string\",\n              description:\n                \"The task description. Use action-oriented language, starting with a verb that fits the task.\",\n            },\n            label: {\n              description: \"Short, precise label for that describes the task.\",\n              type: \"string\",\n            },\n          },\n          required: [\"task\", \"label\"],\n        },\n      },\n      ...organizeFlag,\n    },\n    required,\n  };\n}\n\nfunction getPlan(content: LLMContent): Outcome<Plan> {\n  const planPart = content.parts.at(0);\n  if (!planPart || !(\"json\" in planPart)) {\n    // TODO: Error recovery.\n    return err(`Gemini generated invalid plan`);\n  }\n  console.log(\"PLAN\", planPart.json);\n  return planPart.json as Plan;\n}\n\nfunction prependInstruction(text: string, plan: LLMContent): LLMContent {\n  return {\n    ...plan,\n    parts: [...plan.parts, { text }],\n  };\n}\n\nfunction thinkingPlannerPrompt(\n  context: LLMContent[],\n  objective: LLMContent,\n  plan: Plan,\n  steps: string[],\n  extraPlannerPrompt: string\n): GeminiPrompt {\n  const instruction = llm`\n${preamble(JSON.stringify(objective))}\n\nInitial Plan Outline (Guideline Only):\nThis was the originally intended sequence of actions. Use it as a reference, but do not follow it rigidly if the situation changes.\n\n${JSON.stringify(plan)}\n\nCurrent Situation:\n\nHistory: (A summary of steps already completed and their outcomes)\n\n\\`\\`\\`json\n${JSON.stringify(context)}\n\\`\\`\\`\n\nYour Task:\n\nBased on the Overall Objective, the Initial Plan Outline, the Available Tools, the History, and crucially, the Last Outcome, determine the single best next step for the agent to take right now. Your absolute priority is to make progress towards the objective *while acknowledging the user's input*.\n\nCritical Instructions:\n\n1.  **Evaluate Last Outcome:** Carefully analyze the \\`Last Outcome\\`.\n\n2.  **Expected Outcome:** If the outcome was expected (e.g., successful tool execution providing needed info, user response directly answering the previous question) and aligns with progressing the Initial Plan Outline, determine the next logical step from that outline or towards the objective.\n\n3.  **Unexpected/User-Driven Outcome:** If the \\`Last Outcome\\` is unexpected, problematic, reveals new user needs, is off-topic, emotionally charged, evasive, indicates a failure, or otherwise deviates from the expected path:\n    * **PRIORITY 1: Address the User Input:**\n        * **Acknowledge & Validate:** *Explicitly acknowledge* the user's statement, question, or expressed feeling. Use empathetic phrasing. Examples: \"I understand you're asking about X...\", \"That's an interesting point about Y...\", \"It sounds like you're concerned about Z...\", \"Thanks for sharing that perspective on W...\" Avoid language that dismisses or ignores their contribution.\n        * **Assess Relevance & Safety:** Quickly determine if the user's input relates to the objective, introduces a new constraint, signals discomfort, raises ethical flags, or indicates a desire to stop.\n        * **Connect (If Possible):** If the user's point can be briefly and naturally linked back to the overall objective, do so. Example: \"...and thinking about Y might actually help us clarify [part of the objective]...\"\n        * **Empathetically Redirect:** Gently guide the conversation back towards the objective or the next logical step needed to achieve it. Frame it collaboratively. Examples: \"...to make sure we achieve [objective], perhaps we could first look at...?\", \"...I want to make sure I help you with [objective], so could we focus back on...?\", \"Given our goal of [objective], the next step I think would be helpful is...\"\n        * **Adapt the Plan:** Based on the user's input, *fundamentally reassess* the next step. Do NOT blindly follow the Initial Plan Outline. Your next action *must* account for what the user just said.\n    * **PRIORITY 2: Determine Adaptive Next Step:** Based on the above interaction, decide the *actual* next step. This might be:\n        * Asking the user a clarifying question that incorporates their last point.\n        * Rephrasing your previous question or request.\n        * Using a different tool or strategy better suited to the new situation.\n        * Acknowledging an error and proposing a correction.\n        * Pausing the plan to address a user concern directly.\n        * Deciding *not* to proceed if the user signals discomfort or the topic becomes inappropriate.\n        * Ending the interaction politely if the objective is blocked, achieved, or the user wishes to stop.\n\n4.  **Justify (Mandatory for Deviations):** Briefly explain *why* you chose this next step, *especially* if deviating from the original plan due to an unexpected outcome or user input. Explicitly mention how your chosen step acknowledges the user's input and aims to get back on track empathetically.\n\n5.  **Specify Action:** Clearly define the single next action for the agent.\n    * **User Interaction:** Provide the *exact text* the agent should say to the user. Otherwise we will confuse the user.\n    * **Tool Use:** State the tool name and the precise inputs required, ensuring inputs are updated based on the latest context and user feedback.\n    * **Internal Step:** Describe the internal calculation, data processing, or analysis the agent needs to perform.\n\n6. Note when the agent finishes executing, we'll automatically show the results to the user, so there's no need for an explicit 'present' or 'show' step.\n\nOutput:\nProvide a clear description of the next step the agent should execute now, and a list of good steps to follow after that (generate them assuming the single next step was sucessful).\n\nFor example,\n\\`\\`\\`json\n{\n    \"summarizeResults\": false,\n    \"thinking\": \"The user responded with \\\"hot dog please\\\" after the introduction. This is nonsensical and off-topic. I should acknowledge their input and try to redirect the conversation back to the original goal.\",\n    \"todo\": [\n        {\n            \"label\": \"Gather Information\",\n            \"task\": \"Tell the user that while a hotdog sure sounds tasty, I can't offer the user a hotdog. Then ask the user what brand or business they would like to highlight in the post.\"\n        },\n        {\n            \"label\": \"Generate Post\",\n            \"task\": \"Use the brand name and description to generate an instagram post.\"\n        }\n    ]\n}\n\\`\\`\\`\n\nExtra instructions:\n${extraPlannerPrompt}\n`.asContent();\n\n  const contents = [instruction];\n  let prompt = new GeminiPrompt({\n    body: {\n      contents,\n      safetySettings: defaultSafetySettings(),\n      generationConfig: {\n        responseSchema: planSchema(true),\n        responseMimeType: \"application/json\",\n      },\n    },\n  });\n\n  console.log(\"thinkingPlannerPrompt: \", prompt);\n  return prompt;\n}\n\nfunction plannerPrompt(\n  context: LLMContent[] | undefined,\n  objective: LLMContent,\n  extraPlannerPrompt: string,\n  organize: boolean\n): GeminiPrompt {\n  context ??= [];\n  const instruction = preamble(JSON.stringify(objective));\n  const epilogue = llm`\nExtra instructions:\n${extraPlannerPrompt}\n`.asContent();\n  const contents = [...context, instruction, epilogue];\n  let prompt = new GeminiPrompt({\n    body: {\n      contents,\n      safetySettings: defaultSafetySettings(),\n      generationConfig: {\n        responseSchema: planSchema(organize),\n        responseMimeType: \"application/json\",\n      },\n    },\n  });\n  console.log(\"plannerPrompt: \", prompt);\n  return prompt;\n}\n",
          "language": "typescript"
        },
        "description": "Contains the planner prompt for the conversational think as I go.",
        "runnable": false
      }
    },
    "types": {
      "code": "/**\n * @fileoverview Common types.\n */\n",
      "metadata": {
        "title": "types",
        "source": {
          "code": "/**\n * @fileoverview Common types.\n */\n\nexport type Task = {\n  label: string;\n  task: string;\n};\n\nexport type Plan = {\n  thinking?: string;\n  todo: Task[];\n  summarizeResults: boolean;\n};\n\nexport type Strategy = \"Parallel\" | \"Sequence\";\n\nexport type ExecuteStepFunction = (\n  item: Task\n) => Promise<LLMContent | undefined>;\n\nexport type Strategist = {\n  name: string;\n  execute(\n    singleStepExecutor: ExecuteStepFunction,\n    mutableContext: LLMContent[],\n    objective: LLMContent,\n    makeList?: boolean\n  ): Promise<Outcome<LLMContent[]>>;\n};\n\nexport type Invokable<T> = {\n  invoke(): T;\n};\n",
          "language": "typescript"
        },
        "description": "Common types.",
        "runnable": false
      }
    },
    "runtime": {
      "code": "/**\n * @fileoverview The runtime that powers going over the list.\n */\nimport { ToolManager } from \"./a2/tool-manager\";\nimport { defaultSystemInstruction } from \"./system-instruction\";\nimport { ok, toLLMContent } from \"./a2/utils\";\nimport { GeminiPrompt } from \"./a2/gemini-prompt\";\nimport {} from \"./types\";\nexport { Runtime, generateId };\nfunction generateId() {\n    return Math.random().toString(36).substring(2, 5);\n}\nclass Runtime {\n    toolManager;\n    makeList;\n    context;\n    errors = [];\n    execute;\n    constructor(context, toolManager, makeList) {\n        this.toolManager = toolManager;\n        this.makeList = makeList;\n        this.context = context ? [...context] : [];\n        this.execute = this.#execute.bind(this);\n    }\n    async executeStrategy(objective, strategist) {\n        return strategist.execute(this.execute, this.context, objective, this.makeList);\n    }\n    async #execute(item) {\n        const { toolManager, context, errors } = this;\n        const prompt = toLLMContent(item.task);\n        let contents;\n        let toolConfig = {};\n        if (!toolManager.hasTools()) {\n            contents = [...context, prompt];\n        }\n        else {\n            toolConfig = {\n                toolConfig: {\n                    functionCallingConfig: {\n                        mode: \"ANY\",\n                    },\n                },\n            };\n            contents = [...context, toLLMContent(item.task)];\n        }\n        const geminiPrompt = new GeminiPrompt({\n            body: {\n                contents,\n                tools: toolManager.list(),\n                ...toolConfig,\n            },\n            systemInstruction: defaultSystemInstruction(),\n        }, {\n            toolManager,\n            allowToolErrors: true,\n        });\n        const executing = await geminiPrompt.invoke();\n        if (!ok(executing)) {\n            errors.push(executing.$error);\n            return;\n        }\n        // gross hack. TODO: Instead, teach GeminiPrompt to do compositional\n        // function calling.\n        if (geminiPrompt.calledTools) {\n            return grossHackTransformFunctionResponses(executing.last);\n        }\n        return executing.last;\n    }\n}\nfunction grossHackTransformFunctionResponses(responses) {\n    const parts = responses.parts.map((part) => {\n        if (\"functionResponse\" in part) {\n            return {\n                text: JSON.stringify(part.functionResponse.response),\n            };\n        }\n        return part;\n    });\n    return { parts, role: responses.role || \"user\" };\n}\n",
      "metadata": {
        "title": "runtime",
        "source": {
          "code": "/**\n * @fileoverview The runtime that powers going over the list.\n */\n\nimport { ToolManager } from \"./a2/tool-manager\";\nimport { defaultSystemInstruction } from \"./system-instruction\";\nimport { ok, toLLMContent } from \"./a2/utils\";\nimport { GeminiPrompt } from \"./a2/gemini-prompt\";\nimport {\n  type ExecuteStepFunction,\n  type Task,\n  type Strategist,\n} from \"./types\";\n\nexport { Runtime, generateId };\n\nfunction generateId() {\n  return Math.random().toString(36).substring(2, 5);\n}\n\nclass Runtime {\n  readonly context: LLMContent[];\n  readonly errors: string[] = [];\n  readonly execute: ExecuteStepFunction;\n\n  constructor(\n    context: LLMContent[] | undefined,\n    public readonly toolManager: ToolManager,\n    public readonly makeList: boolean\n  ) {\n    this.context = context ? [...context] : [];\n    this.execute = this.#execute.bind(this);\n  }\n\n  async executeStrategy(\n    objective: LLMContent,\n    strategist: Strategist\n  ): Promise<Outcome<LLMContent[]>> {\n    return strategist.execute(\n      this.execute,\n      this.context,\n      objective,\n      this.makeList\n    );\n  }\n\n  async #execute(item: Task): Promise<LLMContent | undefined> {\n    const { toolManager, context, errors } = this;\n    const prompt = toLLMContent(item.task);\n    let contents;\n    let toolConfig = {};\n    if (!toolManager.hasTools()) {\n      contents = [ ... context, prompt];\n    } else {\n      toolConfig = {\n        toolConfig: {\n          functionCallingConfig: {\n            mode: \"ANY\",\n          },\n        },\n      };\n      contents = [...context, toLLMContent(item.task)];\n    }\n    const geminiPrompt = new GeminiPrompt(\n      {\n        body: {\n          contents,\n          tools: toolManager.list(),\n          ...toolConfig,\n        },\n        systemInstruction: defaultSystemInstruction(),\n      },\n      {\n        toolManager,\n        allowToolErrors: true,\n      }\n    );\n    const executing = await geminiPrompt.invoke();\n    if (!ok(executing)) {\n      errors.push(executing.$error);\n      return;\n    }\n    // gross hack. TODO: Instead, teach GeminiPrompt to do compositional\n    // function calling.\n    if (geminiPrompt.calledTools) {\n      return grossHackTransformFunctionResponses(executing.last);\n    }\n    return executing.last;\n  }\n}\n\nfunction grossHackTransformFunctionResponses(responses: LLMContent) {\n  const parts = responses.parts.map<DataPart>((part) => {\n    if (\"functionResponse\" in part) {\n      return {\n        text: JSON.stringify(part.functionResponse.response),\n      } as TextCapabilityPart;\n    }\n    return part;\n  });\n  return { parts, role: responses.role || \"user\" };\n}\n",
          "language": "typescript"
        },
        "description": "The runtime that powers going over the list.",
        "runnable": false
      }
    },
    "parallel-strategist": {
      "code": "/**\n * @fileoverview Executes a parallel strategy.\n */\nimport { report } from \"./a2/output\";\nimport { ok } from \"./a2/utils\";\nimport { plannerPrompt, getPlan } from \"./planner-prompt\";\nimport {} from \"./types\";\nexport { ParallelStrategist };\nclass ParallelStrategist {\n    name = \"All at once\";\n    extraPlannerPrompt = `\nAll tasks in the plan will be executed in any order or all at once, so make sure that the tasks don't depend on each other.\nThink carefully: for every task in the list, does any task depend on another task? If so, rethink your list\nuntil all tasks are indepedent`;\n    async execute(execute, mutableContext, objective) {\n        const planning = await plannerPrompt(mutableContext, objective, this.extraPlannerPrompt, false).invoke();\n        if (!ok(planning))\n            return planning;\n        const plan = getPlan(planning.last);\n        if (!ok(plan))\n            return plan;\n        await report({\n            actor: \"Planner\",\n            category: `Creating a plan`,\n            name: \"Here's my list\",\n            icon: \"laps\",\n            details: `\n${plan.todo.map((item) => `- ${item.label}`).join(\"\\n\")}\n\nI will now work on all items at the same time.`,\n        });\n        return (await Promise.all(plan.todo.map(execute))).filter((item) => !!item);\n    }\n}\n",
      "metadata": {
        "title": "parallel-strategist",
        "source": {
          "code": "/**\n * @fileoverview Executes a parallel strategy.\n */\n\nimport { report } from \"./a2/output\";\nimport { ok } from \"./a2/utils\";\nimport { plannerPrompt, getPlan } from \"./planner-prompt\";\nimport { type Strategist, type Plan, type ExecuteStepFunction } from \"./types\";\n\nexport { ParallelStrategist };\n\nclass ParallelStrategist implements Strategist {\n  readonly name = \"All at once\";\n  readonly extraPlannerPrompt = `\nAll tasks in the plan will be executed in any order or all at once, so make sure that the tasks don't depend on each other.\nThink carefully: for every task in the list, does any task depend on another task? If so, rethink your list\nuntil all tasks are indepedent`;\n\n  async execute(\n    execute: ExecuteStepFunction,\n    mutableContext: LLMContent[],\n    objective: LLMContent\n  ): Promise<Outcome<LLMContent[]>> {\n    const planning = await plannerPrompt(\n      mutableContext,\n      objective,\n      this.extraPlannerPrompt,\n      false\n    ).invoke();\n    if (!ok(planning)) return planning;\n    const plan = getPlan(planning.last);\n    if (!ok(plan)) return plan;\n\n    await report({\n      actor: \"Planner\",\n      category: `Creating a plan`,\n      name: \"Here's my list\",\n      icon: \"laps\",\n      details: `\n${plan.todo.map((item) => `- ${item.label}`).join(\"\\n\")}\n\nI will now work on all items at the same time.`,\n    });\n    return (await Promise.all(plan.todo.map(execute))).filter((item) => !!item);\n  }\n}\n",
          "language": "typescript"
        },
        "description": "Executes a parallel strategy.",
        "runnable": false
      }
    },
    "sequential-strategist": {
      "code": "/**\n * @fileoverview Executes sequential strategy.\n */\nimport {} from \"./types\";\nimport { plannerPrompt, getPlan } from \"./planner-prompt\";\nimport { toLLMContent, ok } from \"./a2/utils\";\nimport { report } from \"./a2/output\";\nexport { SequentialStrategist };\nclass SequentialStrategist {\n    name = \"Go in order\";\n    extraPlannerPrompt = `\nAll tasks in the plan will be executed in sequence, building on each other.`;\n    async execute(execute, mutableContext, objective) {\n        const planning = await plannerPrompt(mutableContext, objective, this.extraPlannerPrompt, false).invoke();\n        if (!ok(planning))\n            return planning;\n        const plan = getPlan(planning.last);\n        if (!ok(plan))\n            return plan;\n        await report({\n            actor: \"Planner\",\n            category: `Creating a list`,\n            name: \"Here's my list\",\n            icon: \"laps\",\n            details: `\n${plan.todo.map((item) => `1. ${item.label}`).join(\"\\n\")}\n\nI will now go over the list in order.`,\n        });\n        const results = [];\n        for (const task of plan.todo) {\n            await report({\n                actor: \"Worker\",\n                category: \"Working on a list item\",\n                name: \"Item\",\n                icon: \"laps\",\n                details: `Currently working on:\n  \n  ${task.task}\n  `,\n            });\n            const result = await execute(task);\n            if (result) {\n                mutableContext.push(toLLMContent(task.task), result);\n                results.push(result);\n            }\n        }\n        return results;\n    }\n}\n",
      "metadata": {
        "title": "sequential-strategist",
        "source": {
          "code": "/**\n * @fileoverview Executes sequential strategy.\n */\n\nimport { type Strategist, type Plan, type ExecuteStepFunction } from \"./types\";\nimport { plannerPrompt, getPlan } from \"./planner-prompt\";\nimport { toLLMContent, ok } from \"./a2/utils\";\nimport { report } from \"./a2/output\";\n\nexport { SequentialStrategist };\n\nclass SequentialStrategist implements Strategist {\n  readonly name = \"Go in order\";\n  readonly extraPlannerPrompt = `\nAll tasks in the plan will be executed in sequence, building on each other.`;\n\n  async execute(\n    execute: ExecuteStepFunction,\n    mutableContext: LLMContent[],\n    objective: LLMContent\n  ): Promise<Outcome<LLMContent[]>> {\n    const planning = await plannerPrompt(\n      mutableContext,\n      objective,\n      this.extraPlannerPrompt,\n      false\n    ).invoke();\n    if (!ok(planning)) return planning;\n    const plan = getPlan(planning.last);\n    if (!ok(plan)) return plan;\n\n    await report({\n      actor: \"Planner\",\n      category: `Creating a list`,\n      name: \"Here's my list\",\n      icon: \"laps\",\n      details: `\n${plan.todo.map((item) => `1. ${item.label}`).join(\"\\n\")}\n\nI will now go over the list in order.`,\n    });\n\n    const results: LLMContent[] = [];\n    for (const task of plan.todo) {\n      await report({\n        actor: \"Worker\",\n        category: \"Working on a list item\",\n        name: \"Item\",\n        icon: \"laps\",\n        details: `Currently working on:\n  \n  ${task.task}\n  `,\n      });\n      const result = await execute(task);\n      if (result) {\n        mutableContext.push(toLLMContent(task.task), result);\n        results.push(result);\n      }\n    }\n    return results;\n  }\n}\n",
          "language": "typescript"
        },
        "description": "Executes sequential strategy.",
        "runnable": false
      }
    },
    "think-strategist": {
      "code": "/**\n * @fileoverview Executes think-as-i strategy.\n */\nimport {} from \"./types\";\nimport { ok, toLLMContent } from \"./a2/utils\";\nimport { report } from \"./a2/output\";\nimport { organizerPrompt } from \"./organizer-prompt\";\nimport { plannerPrompt, thinkingPlannerPrompt, getPlan, } from \"./planner-prompt\";\nexport { ThinkStrategist };\n/**\n * How many extra steps beyond original plan do we allow\n * before concluding we are in the weeds and giving up.\n */\nconst OVERRUN_BUFFER = 10;\nclass ThinkStrategist {\n    name = \"Think as I go\";\n    tasks = [];\n    extraPlannerPrompt = `\nIf the objective calls to organize or summarize results at the end, do not add that as a step.\nInstead, set the \"organizeResults\" property to \"true\". This will let the organizing agent know\nto kick off the organizing task after you're done.\n\nWhen the objective does not explicitly contain the request to organize or summarize results,\nmake sure to set the \"organizeProperty\" to \"false\". Do not invent new work.\n\nNow think real hard: do you need to organize or summarize results?\n`;\n    async execute(execute, mutableContext, objective, makeList) {\n        const planning = await plannerPrompt(mutableContext, objective, this.extraPlannerPrompt, true).invoke();\n        if (!ok(planning))\n            return planning;\n        let plan = getPlan(planning.last);\n        if (!ok(plan))\n            return plan;\n        const results = [];\n        let max = plan.todo.length + OVERRUN_BUFFER;\n        let organizeResults = false;\n        let planDescription = \"Here is my starting plan\";\n        while (--max) {\n            const task = plan.todo.at(0);\n            if (plan.summarizeResults) {\n                organizeResults = true;\n            }\n            if (!task)\n                break;\n            await report({\n                actor: \"Planner\",\n                category: \"Progress update\",\n                name: \"Thinking\",\n                icon: \"laps\",\n                details: `\n\nHere's my thinking:\n\n${plan.thinking}\n        \n${planDescription}:\n\n${plan.todo.map((item) => `1. ${item.label}`).join(\"\\n\")}\n\nI will now go over the plan in order, thinking after each step\nand adjusting the plan if necessary.`,\n            });\n            const result = await execute(task);\n            if (result) {\n                mutableContext.push(toLLMContent(task.task), result);\n                results.push(result);\n            }\n            this.tasks.push(task.task);\n            const thinking = await thinkingPlannerPrompt(mutableContext, objective, plan, this.tasks, this.extraPlannerPrompt).invoke();\n            if (!ok(thinking))\n                return thinking;\n            const newPlan = getPlan(thinking.last);\n            if (!ok(newPlan))\n                return newPlan;\n            plan = newPlan;\n            planDescription = \"Here are the remaining steps in the plan\";\n        }\n        if (organizeResults) {\n            await report({\n                actor: \"Planner\",\n                category: \"Organizing work into a report\",\n                name: \"Organizing work report\",\n                icon: \"laps\",\n                details: `I will now organize all of my work into a report.`,\n            });\n            const organizing = await organizerPrompt(results, objective, makeList).invoke();\n            if (!ok(organizing))\n                return organizing;\n            return [organizing.last];\n        }\n        return results;\n    }\n}\n",
      "metadata": {
        "title": "think-strategist",
        "source": {
          "code": "/**\n * @fileoverview Executes think-as-i strategy.\n */\n\nimport { type Strategist, type Plan, type ExecuteStepFunction } from \"./types\";\nimport { ok, toLLMContent } from \"./a2/utils\";\nimport { report } from \"./a2/output\";\nimport { organizerPrompt } from \"./organizer-prompt\";\n\nimport {\n  plannerPrompt,\n  thinkingPlannerPrompt,\n  getPlan,\n} from \"./planner-prompt\";\n\nexport { ThinkStrategist };\n\n/**\n * How many extra steps beyond original plan do we allow\n * before concluding we are in the weeds and giving up.\n */\nconst OVERRUN_BUFFER = 10;\n\nclass ThinkStrategist implements Strategist {\n  readonly name = \"Think as I go\";\n  readonly tasks: string[] = [];\n  readonly extraPlannerPrompt = `\nIf the objective calls to organize or summarize results at the end, do not add that as a step.\nInstead, set the \"organizeResults\" property to \"true\". This will let the organizing agent know\nto kick off the organizing task after you're done.\n\nWhen the objective does not explicitly contain the request to organize or summarize results,\nmake sure to set the \"organizeProperty\" to \"false\". Do not invent new work.\n\nNow think real hard: do you need to organize or summarize results?\n`;\n\n  async execute(\n    execute: ExecuteStepFunction,\n    mutableContext: LLMContent[],\n    objective: LLMContent,\n    makeList: boolean\n  ): Promise<Outcome<LLMContent[]>> {\n    const planning = await plannerPrompt(\n      mutableContext,\n      objective,\n      this.extraPlannerPrompt,\n      true\n    ).invoke();\n    if (!ok(planning)) return planning;\n    let plan = getPlan(planning.last);\n    if (!ok(plan)) return plan;\n\n    const results: LLMContent[] = [];\n    let max = plan.todo.length + OVERRUN_BUFFER;\n    let organizeResults = false;\n\n    let planDescription = \"Here is my starting plan\";\n\n    while (--max) {\n      const task = plan.todo.at(0);\n      if (plan.summarizeResults) {\n        organizeResults = true;\n      }\n      if (!task) break;\n      await report({\n        actor: \"Planner\",\n        category: \"Progress update\",\n        name: \"Thinking\",\n        icon: \"laps\",\n        details: `\n\nHere's my thinking:\n\n${plan.thinking}\n        \n${planDescription}:\n\n${plan.todo.map((item) => `1. ${item.label}`).join(\"\\n\")}\n\nI will now go over the plan in order, thinking after each step\nand adjusting the plan if necessary.`,\n      });\n      const result = await execute(task);\n      if (result) {\n        mutableContext.push(toLLMContent(task.task), result);\n        results.push(result);\n      }\n      this.tasks.push(task.task);\n      const thinking = await thinkingPlannerPrompt(\n        mutableContext,\n        objective,\n        plan,\n        this.tasks,\n        this.extraPlannerPrompt\n      ).invoke();\n      if (!ok(thinking)) return thinking;\n      const newPlan = getPlan(thinking.last);\n      if (!ok(newPlan)) return newPlan;\n      plan = newPlan;\n      planDescription = \"Here are the remaining steps in the plan\";\n    }\n    if (organizeResults) {\n      await report({\n        actor: \"Planner\",\n        category: \"Organizing work into a report\",\n        name: \"Organizing work report\",\n        icon: \"laps\",\n        details: `I will now organize all of my work into a report.`,\n      });\n\n      const organizing = await organizerPrompt(\n        results,\n        objective,\n        makeList\n      ).invoke();\n      if (!ok(organizing)) return organizing;\n\n      return [organizing.last];\n    }\n    return results;\n  }\n}\n",
          "language": "typescript"
        },
        "description": "Executes think-as-i strategy.",
        "runnable": false
      }
    },
    "conversational-think-strategist": {
      "code": "/**\n * @fileoverview Executes think-as-i strategy, but gently redirects off topic user input back on topic.\n */\nimport {} from \"./types\";\nimport { ok, toLLMContent } from \"./a2/utils\";\nimport { report } from \"./a2/output\";\nimport { organizerPrompt } from \"./organizer-prompt\";\nimport { plannerPrompt, thinkingPlannerPrompt, getPlan, } from \"./conversational-planner-prompt\";\nexport { ConversationalThinkStrategist };\n/**\n * How many extra steps beyond original plan do we allow\n * before concluding we are in the weeds and giving up.\n */\nconst OVERRUN_BUFFER = 10;\nclass ConversationalThinkStrategist {\n    name = \"[Alpha] Conversational Think as I go\";\n    tasks = [];\n    extraPlannerPrompt = `\nIf the objective calls to organize or summarize results at the end, do not add that as a step.\nInstead, set the \"organizeResults\" property to \"true\". This will let the organizing agent know\nto kick off the organizing task after you're done.\n\nWhen the objective does not explicitly contain the request to organize or summarize results,\nmake sure to set the \"organizeProperty\" to \"false\". Do not invent new work.\n\nNow think real hard: do you need to organize or summarize results?\n`;\n    async execute(execute, mutableContext, objective, makeList) {\n        const planning = await plannerPrompt(mutableContext, objective, this.extraPlannerPrompt, true).invoke();\n        if (!ok(planning))\n            return planning;\n        let plan = getPlan(planning.last);\n        if (!ok(plan))\n            return plan;\n        const results = [];\n        let max = plan.todo.length + OVERRUN_BUFFER;\n        let organizeResults = false;\n        let planDescription = \"Here is my starting plan\";\n        while (--max) {\n            const task = plan.todo.at(0);\n            if (plan.summarizeResults) {\n                organizeResults = true;\n            }\n            if (!task)\n                break;\n            await report({\n                actor: \"Planner\",\n                category: \"Progress update\",\n                name: \"Thinking\",\n                icon: \"laps\",\n                details: `\n\nHere's my thinking:\n\n${plan.thinking}\n        \n${planDescription}:\n\n${plan.todo.map((item) => `1. ${item.label}`).join(\"\\n\")}\n\nI will now go over the plan in order, thinking after each step\nand adjusting the plan if necessary.`,\n            });\n            const result = await execute(task);\n            if (result) {\n                // Add an 'Action' label so it's easier for the planner to tell what's an agent action vs response.\n                // Unfortunately every item in the context except for function calls has 'role: user'.\n                mutableContext.push(toLLMContent(\"Action: \" + task.task), result);\n                console.log(\"think-strategist\", mutableContext);\n                results.push(result);\n            }\n            this.tasks.push(task.task);\n            const thinking = await thinkingPlannerPrompt(mutableContext, objective, plan, this.tasks, this.extraPlannerPrompt).invoke();\n            if (!ok(thinking))\n                return thinking;\n            const newPlan = getPlan(thinking.last);\n            if (!ok(newPlan))\n                return newPlan;\n            plan = newPlan;\n            planDescription = \"Here are the remaining steps in the plan\";\n        }\n        if (organizeResults) {\n            await report({\n                actor: \"Planner\",\n                category: \"Organizing work into a report\",\n                name: \"Organizing work report\",\n                icon: \"laps\",\n                details: `I will now organize all of my work into a report.`,\n            });\n            const organizing = await organizerPrompt(results, objective, makeList).invoke();\n            if (!ok(organizing))\n                return organizing;\n            return [organizing.last];\n        }\n        return results;\n    }\n}\n",
      "metadata": {
        "title": "conversational-think-strategist",
        "source": {
          "code": "/**\n * @fileoverview Executes think-as-i strategy, but gently redirects off topic user input back on topic.\n */\n\nimport { type Strategist, type Plan, type ExecuteStepFunction } from \"./types\";\nimport { ok, toLLMContent } from \"./a2/utils\";\nimport { report } from \"./a2/output\";\nimport { organizerPrompt } from \"./organizer-prompt\";\n\nimport {\n  plannerPrompt,\n  thinkingPlannerPrompt,\n  getPlan,\n} from \"./conversational-planner-prompt\";\n\nexport { ConversationalThinkStrategist };\n\n/**\n * How many extra steps beyond original plan do we allow\n * before concluding we are in the weeds and giving up.\n */\nconst OVERRUN_BUFFER = 10;\n\nclass ConversationalThinkStrategist implements Strategist {\n  readonly name = \"[Alpha] Conversational Think as I go\";\n  readonly tasks: string[] = [];\n  readonly extraPlannerPrompt = `\nIf the objective calls to organize or summarize results at the end, do not add that as a step.\nInstead, set the \"organizeResults\" property to \"true\". This will let the organizing agent know\nto kick off the organizing task after you're done.\n\nWhen the objective does not explicitly contain the request to organize or summarize results,\nmake sure to set the \"organizeProperty\" to \"false\". Do not invent new work.\n\nNow think real hard: do you need to organize or summarize results?\n`;\n\n  async execute(\n    execute: ExecuteStepFunction,\n    mutableContext: LLMContent[],\n    objective: LLMContent,\n    makeList: boolean\n  ): Promise<Outcome<LLMContent[]>> {\n    const planning = await plannerPrompt(\n      mutableContext,\n      objective,\n      this.extraPlannerPrompt,\n      true\n    ).invoke();\n    if (!ok(planning)) return planning;\n    let plan = getPlan(planning.last);\n    if (!ok(plan)) return plan;\n\n    const results: LLMContent[] = [];\n    let max = plan.todo.length + OVERRUN_BUFFER;\n    let organizeResults = false;\n\n    let planDescription = \"Here is my starting plan\";\n\n    while (--max) {\n      const task = plan.todo.at(0);\n      if (plan.summarizeResults) {\n        organizeResults = true;\n      }\n      if (!task) break;\n      await report({\n        actor: \"Planner\",\n        category: \"Progress update\",\n        name: \"Thinking\",\n        icon: \"laps\",\n        details: `\n\nHere's my thinking:\n\n${plan.thinking}\n        \n${planDescription}:\n\n${plan.todo.map((item) => `1. ${item.label}`).join(\"\\n\")}\n\nI will now go over the plan in order, thinking after each step\nand adjusting the plan if necessary.`,\n      });\n      const result = await execute(task);\n      if (result) {\n        // Add an 'Action' label so it's easier for the planner to tell what's an agent action vs response.\n        // Unfortunately every item in the context except for function calls has 'role: user'.\n        mutableContext.push(toLLMContent(\"Action: \" + task.task), result);\n        console.log(\"think-strategist\", mutableContext);\n        results.push(result);\n      }\n      this.tasks.push(task.task);\n      const thinking = await thinkingPlannerPrompt(\n        mutableContext,\n        objective,\n        plan,\n        this.tasks,\n        this.extraPlannerPrompt\n      ).invoke();\n      if (!ok(thinking)) return thinking;\n      const newPlan = getPlan(thinking.last);\n      if (!ok(newPlan)) return newPlan;\n      plan = newPlan;\n      planDescription = \"Here are the remaining steps in the plan\";\n    }\n    if (organizeResults) {\n      await report({\n        actor: \"Planner\",\n        category: \"Organizing work into a report\",\n        name: \"Organizing work report\",\n        icon: \"laps\",\n        details: `I will now organize all of my work into a report.`,\n      });\n\n      const organizing = await organizerPrompt(\n        results,\n        objective,\n        makeList\n      ).invoke();\n      if (!ok(organizing)) return organizing;\n\n      return [organizing.last];\n    }\n    return results;\n  }\n}\n",
          "language": "typescript"
        },
        "description": "Executes think-as-i strategy, but gently redirects off topic user input back on topic.",
        "runnable": false
      }
    },
    "organizer-prompt": {
      "code": "/**\n * @fileoverview Plumbing that handles organizing/summarizing content at the end.\n */\nimport { defaultSafetySettings } from \"./a2/gemini\";\nimport { llm, ok } from \"./a2/utils\";\nimport { GeminiPrompt } from \"./a2/gemini-prompt\";\nimport {} from \"./types\";\nimport { listPrompt, listSchema, toList } from \"./a2/lists\";\nimport { defaultSystemInstruction } from \"./system-instruction\";\nexport { organizerPrompt };\nfunction organizerPrompt(results, objective, makeList) {\n    const research = {\n        parts: results.flatMap((item) => item.parts),\n    };\n    const extra = makeList\n        ? `\nYour job is to examine in detail and organize the provided raw material into\na thorough, detailed list of write-ups, so that the final list of write-ups\nis a perfect response to the objective. The number of write-ups in the list must be the same as\nasked in the objective.\n\nMake sure each write-up is complete as its own document, because these write-ups\nwill be used separately from each other.\n\n\n`\n        : `\nYour job is to examine in detail and organize the provided raw material into\na thorough, detailed write-up that captures all of it in one place, so that\nthe final product is a perfect response to the objective.\n\nThe final must product must contain references to the sources (always cite your sources).`;\n    const prompt = llm `\nYou are an expert organizer of raw material. This raw material was produced by \nan AI agent that was tasked with satisfying the the provided objective.\n\n${extra}\n\n## Objective\n\n${objective}\n\n## Raw Research\n\n\\`\\`\\`\n${research}\n\n\\`\\`\\`\n`.asContent();\n    if (makeList) {\n        const geminiPrompt = new GeminiPrompt({\n            body: {\n                contents: [listPrompt(prompt)],\n                safetySettings: defaultSafetySettings(),\n                generationConfig: {\n                    responseSchema: listSchema(),\n                    responseMimeType: \"application/json\",\n                },\n            },\n        });\n        return {\n            invoke: async () => {\n                const invoking = await geminiPrompt.invoke();\n                if (!ok(invoking))\n                    return invoking;\n                const last = toList(invoking.last);\n                if (!ok(last))\n                    return last;\n                return { ...invoking, last };\n            },\n        };\n    }\n    else {\n        const geminiPrompt = new GeminiPrompt({\n            body: {\n                systemInstruction: defaultSystemInstruction(),\n                contents: [prompt],\n                safetySettings: defaultSafetySettings(),\n            },\n        });\n        return {\n            invoke: async () => {\n                const invoking = await geminiPrompt.invoke();\n                if (!ok(invoking))\n                    return invoking;\n                const response = invoking.last;\n                return {\n                    ...invoking,\n                    last: response,\n                };\n            },\n        };\n    }\n}\n",
      "metadata": {
        "title": "organizer-prompt",
        "source": {
          "code": "/**\n * @fileoverview Plumbing that handles organizing/summarizing content at the end.\n */\n\nimport { defaultSafetySettings } from \"./a2/gemini\";\nimport { llm, ok } from \"./a2/utils\";\nimport { GeminiPrompt } from \"./a2/gemini-prompt\";\nimport { type Invokable } from \"./types\";\nimport { listPrompt, listSchema, toList } from \"./a2/lists\";\nimport { defaultSystemInstruction } from \"./system-instruction\";\n\nexport { organizerPrompt };\n\ntype InvokeReturnType = ReturnType<GeminiPrompt[\"invoke\"]>;\n\nfunction organizerPrompt(\n  results: LLMContent[],\n  objective: LLMContent,\n  makeList: boolean\n): Invokable<InvokeReturnType> {\n  const research = {\n    parts: results.flatMap((item) => item.parts),\n  };\n  const extra = makeList\n    ? `\nYour job is to examine in detail and organize the provided raw material into\na thorough, detailed list of write-ups, so that the final list of write-ups\nis a perfect response to the objective. The number of write-ups in the list must be the same as\nasked in the objective.\n\nMake sure each write-up is complete as its own document, because these write-ups\nwill be used separately from each other.\n\n\n`\n    : `\nYour job is to examine in detail and organize the provided raw material into\na thorough, detailed write-up that captures all of it in one place, so that\nthe final product is a perfect response to the objective.\n\nThe final must product must contain references to the sources (always cite your sources).`;\n\n  const prompt = llm`\nYou are an expert organizer of raw material. This raw material was produced by \nan AI agent that was tasked with satisfying the the provided objective.\n\n${extra}\n\n## Objective\n\n${objective}\n\n## Raw Research\n\n\\`\\`\\`\n${research}\n\n\\`\\`\\`\n`.asContent();\n\n  if (makeList) {\n    const geminiPrompt = new GeminiPrompt({\n      body: {\n        contents: [listPrompt(prompt)],\n        safetySettings: defaultSafetySettings(),\n        generationConfig: {\n          responseSchema: listSchema(),\n          responseMimeType: \"application/json\",\n        },\n      },\n    });\n    return {\n      invoke: async () => {\n        const invoking = await geminiPrompt.invoke();\n        if (!ok(invoking)) return invoking;\n        const last = toList(invoking.last);\n        if (!ok(last)) return last;\n        return { ...invoking, last };\n      },\n    };\n  } else {\n    const geminiPrompt = new GeminiPrompt({\n      body: {\n        systemInstruction: defaultSystemInstruction(),\n        contents: [prompt],\n        safetySettings: defaultSafetySettings(),\n      },\n    });\n\n    return {\n      invoke: async () => {\n        const invoking = await geminiPrompt.invoke();\n        if (!ok(invoking)) return invoking;\n        const response = invoking.last;\n        return {\n          ...invoking,\n          last: response,\n        };\n      },\n    };\n  }\n}\n",
          "language": "typescript"
        },
        "description": "Plumbing that handles organizing/summarizing content at the end.",
        "runnable": false
      }
    },
    "system-instruction": {
      "code": "/**\n * @fileoverview Default system instruction for the various prompts.\n */\nimport { llm } from \"./a2/utils\";\nexport { defaultSystemInstruction };\nfunction defaultSystemInstruction() {\n    return llm `You are working as part of an AI system, so no chit-chat and no explaining what you're doing and why.\nDO NOT start with \"Okay\", or \"Alright\" or any preambles. Just the output, please.`.asContent();\n}\n",
      "metadata": {
        "title": "system-instruction",
        "source": {
          "code": "/**\n * @fileoverview Default system instruction for the various prompts.\n */\n\nimport { llm } from \"./a2/utils\";\n\nexport { defaultSystemInstruction };\n\nfunction defaultSystemInstruction(): LLMContent {\n  return llm`You are working as part of an AI system, so no chit-chat and no explaining what you're doing and why.\nDO NOT start with \"Okay\", or \"Alright\" or any preambles. Just the output, please.`.asContent();\n}\n",
          "language": "typescript"
        },
        "description": "Default system instruction for the various prompts.",
        "runnable": false
      }
    }
  },
  "imports": {
    "a2": {
      "url": "./a2.bgl.json"
    }
  },
  "exports": [
    "#module:main"
  ]
}