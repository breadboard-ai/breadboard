{
  "title": "A2 Deep Research",
  "description": "",
  "version": "0.0.1",
  "nodes": [],
  "edges": [],
  "metadata": {
    "comments": [
      {
        "id": "comment-cc94afe8",
        "text": "Intentionally Left Blank",
        "metadata": {
          "title": "Comment",
          "visual": {
            "x": 531,
            "y": 374,
            "collapsed": "expanded",
            "outputHeight": 0
          }
        }
      }
    ],
    "visual": {
      "presentation": {
        "themes": {
          "5f3ca599-8fee-46fb-951f-0d47b16a6d56": {
            "themeColors": {
              "primaryColor": "#246db5",
              "secondaryColor": "#5cadff",
              "backgroundColor": "#ffffff",
              "textColor": "#1a1a1a",
              "primaryTextColor": "#ffffff"
            },
            "template": "basic",
            "splashScreen": {
              "storedData": {
                "handle": "/images/app/generic-flow.jpg",
                "mimeType": "image/jpeg"
              }
            }
          }
        },
        "theme": "5f3ca599-8fee-46fb-951f-0d47b16a6d56"
      }
    },
    "tags": [
      "published",
      "tool",
      "component"
    ]
  },
  "modules": {
    "main": {
      "code": "/**\n * @fileoverview Recursively search the web for in-depth answers to your query.\n */\nimport { ToolManager } from \"./a2/tool-manager\";\nimport { Template } from \"./a2/template\";\nimport invokeGraph from \"@invoke\";\nimport invokeGemini, { defaultSafetySettings, } from \"./a2/gemini\";\nimport { ok, err, llm, toLLMContent, toText, addUserTurn } from \"./a2/utils\";\nimport { report } from \"./a2/output\";\nimport {} from \"./a2/common\";\nimport { ArgumentNameGenerator } from \"./a2/introducer\";\nexport { invoke as default, describe };\nconst RESEARCH_TOOLS = [\n    {\n        url: \"embed://a2/tools.bgl.json#module:search-web\",\n        title: \"Search Web\",\n    },\n    {\n        url: \"embed://a2/tools.bgl.json#module:get-webpage\",\n        title: \"Get Webpage\",\n    },\n    {\n        url: \"embed://a2/tools.bgl.json#module:search-maps\",\n        title: \"Search Maps\",\n    },\n];\nconst RESEARCH_MODEL = \"gemini-2.5-flash-preview-04-17\";\nconst MAX_ITERATIONS = 7;\nfunction systemInstruction(first) {\n    const which = first ? \"first\" : \"next\";\n    return `You are a researcher.\n\nYour job is to use the provided query to produce raw research that will be later turned into a detailed research report.\nYou are tasked with finding as much of relevant information as possible.\n\nYou examine the conversation context so far and come up with the ${which} step to produce the report, \nusing the conversation context as the the guide of steps taken so far and the outcomes recorded.\n\nYou do not ask user for feedback. You do not try to have a conversation with the user. \nYou know that the user will only ask you to proceed to next step.\n\nLooking back at all that you've researched and the query/research plan, do you have enough to produce the detailed report? If so, you are done.\n\nNow, provide a response. Your response must contain two parts:\nThought: a brief plain text reasoning why this is the right ${which} step and a description of what you will do in plain English.\nAction: invoking the tools are your disposal, more than one if necessary. If you're done, do not invoke any tools.`;\n}\nfunction researcherPrompt(contents, query, tools, first) {\n    return {\n        model: RESEARCH_MODEL,\n        body: {\n            contents: addUserTurn(llm `\nDo the research according about this topic:\n\n---\n\n${query}\n\n---\n`.asContent(), contents),\n            tools,\n            systemInstruction: toLLMContent(systemInstruction(first)),\n            safetySettings: defaultSafetySettings(),\n        },\n    };\n}\nfunction reportWriterInstruction() {\n    return `You are a research report writer.\nYour teammates produced a wealth of raw research about the supplied query.\n\nYour task is to take the raw research and write a thorough, detailed research report that answers the provided query. Use markdown.\n\nA report must additionally contain references to the source (always cite your sources).`;\n}\nfunction reportWriterPrompt(query, research) {\n    return {\n        model: RESEARCH_MODEL,\n        body: {\n            contents: [toLLMContent(research.join(\"\\n\\n\"))],\n            systemInstruction: toLLMContent(reportWriterInstruction()),\n            safetySettings: defaultSafetySettings(),\n        },\n    };\n}\nasync function thought(response, iteration) {\n    const first = response.parts?.at(0);\n    if (!first || !(\"text\" in first)) {\n        return;\n    }\n    await report({\n        actor: \"Researcher\",\n        category: `Progress report, iteration ${iteration + 1}`,\n        name: \"Thought\",\n        icon: \"generative\",\n        details: first.text\n            .replace(/^Thought: ?/gm, \"\")\n            .replace(/^Action:.*$/gm, \"\")\n            .trim(),\n    });\n}\nasync function invoke({ context, query, summarize, ...params }) {\n    const tools = RESEARCH_TOOLS.map((descriptor) => descriptor.url);\n    const toolManager = new ToolManager(new ArgumentNameGenerator());\n    let content = context || [toLLMContent(\"Start the research\")];\n    const template = new Template(query);\n    const substituting = await template.substitute(params, async ({ path: url, instance }) => toolManager.addTool(url, instance));\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    if (!toolManager.hasTools()) {\n        // If no tools supplied (legacy case, actually), initialize\n        // with a set of default tools.\n        const initializing = await toolManager.initialize(tools);\n        if (!initializing) {\n            return err(\"Unable to initialize tools\");\n        }\n    }\n    query = substituting;\n    const research = [];\n    for (let i = 0; i <= MAX_ITERATIONS; i++) {\n        const askingGemini = await invokeGemini(researcherPrompt(content, query, toolManager.list(), i === 0));\n        if (!ok(askingGemini)) {\n            return askingGemini;\n        }\n        if (\"context\" in askingGemini) {\n            return err(`Unexpected \"context\" response`);\n        }\n        const response = askingGemini.candidates.at(0)?.content;\n        if (!response) {\n            return err(\"No actionable response\");\n        }\n        await thought(response, i);\n        const toolResponses = [];\n        await toolManager.processResponse(response, async ($board, args) => {\n            toolResponses.push(JSON.stringify(await invokeGraph({ $board, ...args })));\n        });\n        if (toolResponses.length === 0) {\n            break;\n        }\n        research.push(...toolResponses);\n        content = [...content, response, toLLMContent(toolResponses.join(\"\\n\\n\"))];\n    }\n    if (research.length === 0) {\n        await report({\n            actor: \"Researcher\",\n            category: \"Error\",\n            name: \"Error\",\n            details: \"I was unable to obtain any research results\",\n        });\n        return { context };\n    }\n    if (summarize) {\n        const producingReport = await invokeGemini(reportWriterPrompt(query, research));\n        if (!ok(producingReport)) {\n            return producingReport;\n        }\n        if (\"context\" in producingReport) {\n            return err(`Unexpected \"context\" response`);\n        }\n        const response = producingReport.candidates.at(0)?.content;\n        if (!response) {\n            return err(\"No actionable response\");\n        }\n        return { context: [...(context || []), response] };\n    }\n    return { context: [...(context || []), toLLMContent(research.join(\"\\n\\n\"))] };\n}\nfunction toOxfordList(items) {\n    if (items.length === 0)\n        return \"\";\n    if (items.length === 1)\n        return items[0];\n    if (items.length === 2)\n        return items.join(\" and \");\n    const lastItem = items.pop();\n    return `${items.join(\", \")}, and ${lastItem}`;\n}\nfunction researchExample() {\n    const type = \"tool\";\n    const tools = RESEARCH_TOOLS.map(({ url: path, title }) => Template.part({ title, path, type }));\n    return [\n        JSON.stringify({\n            query: toLLMContent(`Research the topic provided using ${toOxfordList(tools)} tools`),\n        }),\n    ];\n}\nasync function describe({ inputs: { query } }) {\n    const template = new Template(query);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                query: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"Research Query\",\n                    description: \"Provide a brief description of what to research, what areas to cover, etc.\",\n                },\n                summarize: {\n                    type: \"boolean\",\n                    behavior: [\"config\", \"hint-preview\", \"hint-advanced\"],\n                    icon: \"summarize\",\n                    title: \"Summarize research\",\n                    description: \"If checked, the Researcher will summarize the results of the research and only pass the research summary along.\",\n                },\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n            additionalProperties: false,\n            examples: researchExample(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"main-port\", \"hint-text\"],\n                },\n            },\n            additionalProperties: false,\n        },\n        title: \"Do deep research\",\n        description: \"Do deep research on the provided query\",\n        metadata: {\n            icon: \"generative-search\",\n            tags: [\"quick-access\", \"generative\"],\n            order: 101,\n        },\n    };\n}\n",
      "metadata": {
        "title": "main",
        "source": {
          "code": "/**\n * @fileoverview Recursively search the web for in-depth answers to your query.\n */\nimport { ToolManager, type ToolDescriptor } from \"./a2/tool-manager\";\nimport { Template } from \"./a2/template\";\nimport invokeGraph from \"@invoke\";\nimport invokeGemini, {\n  type GeminiInputs,\n  type Tool,\n  defaultSafetySettings,\n} from \"./a2/gemini\";\nimport { ok, err, llm, toLLMContent, toText, addUserTurn } from \"./a2/utils\";\nimport { report } from \"./a2/output\";\nimport { type Params } from \"./a2/common\";\nimport { ArgumentNameGenerator } from \"./a2/introducer\";\n\nexport { invoke as default, describe };\n\nexport type ResearcherInputs = {\n  context?: LLMContent[];\n  query: LLMContent;\n  summarize: boolean;\n} & Params;\n\nexport type DefaultToolDescriptor = {\n  url: string;\n  title: string;\n};\n\nconst RESEARCH_TOOLS: DefaultToolDescriptor[] = [\n  {\n    url: \"embed://a2/tools.bgl.json#module:search-web\",\n    title: \"Search Web\",\n  },\n  {\n    url: \"embed://a2/tools.bgl.json#module:get-webpage\",\n    title: \"Get Webpage\",\n  },\n  {\n    url: \"embed://a2/tools.bgl.json#module:search-maps\",\n    title: \"Search Maps\",\n  },\n];\n\nconst RESEARCH_MODEL = \"gemini-2.5-flash-preview-04-17\";\n\nconst MAX_ITERATIONS = 7;\n\nfunction systemInstruction(first: boolean): string {\n  const which = first ? \"first\" : \"next\";\n  return `You are a researcher.\n\nYour job is to use the provided query to produce raw research that will be later turned into a detailed research report.\nYou are tasked with finding as much of relevant information as possible.\n\nYou examine the conversation context so far and come up with the ${which} step to produce the report, \nusing the conversation context as the the guide of steps taken so far and the outcomes recorded.\n\nYou do not ask user for feedback. You do not try to have a conversation with the user. \nYou know that the user will only ask you to proceed to next step.\n\nLooking back at all that you've researched and the query/research plan, do you have enough to produce the detailed report? If so, you are done.\n\nNow, provide a response. Your response must contain two parts:\nThought: a brief plain text reasoning why this is the right ${which} step and a description of what you will do in plain English.\nAction: invoking the tools are your disposal, more than one if necessary. If you're done, do not invoke any tools.`;\n}\n\nfunction researcherPrompt(\n  contents: LLMContent[],\n  query: LLMContent,\n  tools: Tool[],\n  first: boolean\n): GeminiInputs {\n  return {\n    model: RESEARCH_MODEL,\n    body: {\n      contents: addUserTurn(\n        llm`\nDo the research according about this topic:\n\n---\n\n${query}\n\n---\n`.asContent(),\n        contents\n      ),\n      tools,\n      systemInstruction: toLLMContent(systemInstruction(first)),\n      safetySettings: defaultSafetySettings(),\n    },\n  };\n}\n\nfunction reportWriterInstruction() {\n  return `You are a research report writer.\nYour teammates produced a wealth of raw research about the supplied query.\n\nYour task is to take the raw research and write a thorough, detailed research report that answers the provided query. Use markdown.\n\nA report must additionally contain references to the source (always cite your sources).`;\n}\n\nfunction reportWriterPrompt(\n  query: LLMContent,\n  research: string[]\n): GeminiInputs {\n  return {\n    model: RESEARCH_MODEL,\n    body: {\n      contents: [toLLMContent(research.join(\"\\n\\n\"))],\n      systemInstruction: toLLMContent(reportWriterInstruction()),\n      safetySettings: defaultSafetySettings(),\n    },\n  };\n}\n\nasync function thought(response: LLMContent, iteration: number) {\n  const first = response.parts?.at(0);\n  if (!first || !(\"text\" in first)) {\n    return;\n  }\n  await report({\n    actor: \"Researcher\",\n    category: `Progress report, iteration ${iteration + 1}`,\n    name: \"Thought\",\n    icon: \"generative\",\n    details: first.text\n      .replace(/^Thought: ?/gm, \"\")\n      .replace(/^Action:.*$/gm, \"\")\n      .trim(),\n  });\n}\n\nasync function invoke({\n  context,\n  query,\n  summarize,\n  ...params\n}: ResearcherInputs) {\n  const tools = RESEARCH_TOOLS.map((descriptor) => descriptor.url);\n  const toolManager = new ToolManager(new ArgumentNameGenerator());\n  let content = context || [toLLMContent(\"Start the research\")];\n\n  const template = new Template(query);\n  const substituting = await template.substitute(\n    params,\n    async ({ path: url, instance }) => toolManager.addTool(url, instance)\n  );\n  if (!ok(substituting)) {\n    return substituting;\n  }\n  if (!toolManager.hasTools()) {\n    // If no tools supplied (legacy case, actually), initialize\n    // with a set of default tools.\n    const initializing = await toolManager.initialize(tools);\n    if (!initializing) {\n      return err(\"Unable to initialize tools\");\n    }\n  }\n  query = substituting;\n\n  const research: string[] = [];\n  for (let i = 0; i <= MAX_ITERATIONS; i++) {\n    const askingGemini = await invokeGemini(\n      researcherPrompt(content, query, toolManager.list(), i === 0)\n    );\n\n    if (!ok(askingGemini)) {\n      return askingGemini;\n    }\n    if (\"context\" in askingGemini) {\n      return err(`Unexpected \"context\" response`);\n    }\n    const response = askingGemini.candidates.at(0)?.content;\n    if (!response) {\n      return err(\"No actionable response\");\n    }\n    await thought(response, i);\n\n    const toolResponses: string[] = [];\n    await toolManager.processResponse(response, async ($board, args) => {\n      toolResponses.push(\n        JSON.stringify(await invokeGraph({ $board, ...args }))\n      );\n    });\n    if (toolResponses.length === 0) {\n      break;\n    }\n    research.push(...toolResponses);\n    content = [...content, response, toLLMContent(toolResponses.join(\"\\n\\n\"))];\n  }\n  if (research.length === 0) {\n    await report({\n      actor: \"Researcher\",\n      category: \"Error\",\n      name: \"Error\",\n      details: \"I was unable to obtain any research results\",\n    });\n    return { context };\n  }\n  if (summarize) {\n    const producingReport = await invokeGemini(\n      reportWriterPrompt(query, research)\n    );\n    if (!ok(producingReport)) {\n      return producingReport;\n    }\n    if (\"context\" in producingReport) {\n      return err(`Unexpected \"context\" response`);\n    }\n    const response = producingReport.candidates.at(0)?.content;\n    if (!response) {\n      return err(\"No actionable response\");\n    }\n    return { context: [...(context || []), response] };\n  }\n  return { context: [...(context || []), toLLMContent(research.join(\"\\n\\n\"))] };\n}\n\ntype DescribeInputs = {\n  inputs: {\n    query: LLMContent;\n  };\n};\n\nfunction toOxfordList(items: string[]): string {\n  if (items.length === 0) return \"\";\n  if (items.length === 1) return items[0];\n  if (items.length === 2) return items.join(\" and \");\n  const lastItem = items.pop();\n  return `${items.join(\", \")}, and ${lastItem}`;\n}\n\nfunction researchExample(): string[] {\n  const type = \"tool\";\n  const tools = RESEARCH_TOOLS.map(({ url: path, title }) =>\n    Template.part({ title, path, type })\n  );\n  return [\n    JSON.stringify({\n      query: toLLMContent(\n        `Research the topic provided using ${toOxfordList(tools)} tools`\n      ),\n    }),\n  ];\n}\n\nasync function describe({ inputs: { query } }: DescribeInputs) {\n  const template = new Template(query);\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n        query: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n          title: \"Research Query\",\n          description:\n            \"Provide a brief description of what to research, what areas to cover, etc.\",\n        },\n        summarize: {\n          type: \"boolean\",\n          behavior: [\"config\", \"hint-preview\", \"hint-advanced\"],\n          icon: \"summarize\",\n          title: \"Summarize research\",\n          description:\n            \"If checked, the Researcher will summarize the results of the research and only pass the research summary along.\",\n        },\n        ...template.schemas(),\n      },\n      behavior: [\"at-wireable\"],\n      ...template.requireds(),\n      additionalProperties: false,\n      examples: researchExample(),\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n          behavior: [\"main-port\", \"hint-text\"],\n        },\n      },\n      additionalProperties: false,\n    } satisfies Schema,\n    title: \"Do deep research\",\n    description: \"Do deep research on the provided query\",\n    metadata: {\n      icon: \"generative-search\",\n      tags: [\"quick-access\", \"generative\"],\n      order: 101,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Recursively search the web for in-depth answers to your query.",
        "runnable": true
      }
    }
  },
  "imports": {
    "a2": {
      "url": "./a2.bgl.json"
    }
  },
  "exports": [
    "#module:main"
  ]
}