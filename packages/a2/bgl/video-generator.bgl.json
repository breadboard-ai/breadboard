{
  "title": "A2 Video Generation",
  "description": "",
  "version": "0.0.1",
  "nodes": [],
  "edges": [],
  "metadata": {
    "comments": [
      {
        "id": "comment-cc94afe8",
        "text": "Intentionally Left Blank",
        "metadata": {
          "title": "Comment",
          "visual": {
            "x": 531,
            "y": 374,
            "collapsed": "expanded",
            "outputHeight": 0
          }
        }
      }
    ],
    "visual": {
      "presentation": {
        "themes": {
          "5f3ca599-8fee-46fb-951f-0d47b16a6d56": {
            "themeColors": {
              "primaryColor": "#246db5",
              "secondaryColor": "#5cadff",
              "backgroundColor": "#ffffff",
              "textColor": "#1a1a1a",
              "primaryTextColor": "#ffffff"
            },
            "template": "basic",
            "splashScreen": {
              "storedData": {
                "handle": "/images/app/generic-flow.jpg",
                "mimeType": "image/jpeg"
              }
            }
          }
        },
        "theme": "5f3ca599-8fee-46fb-951f-0d47b16a6d56"
      }
    },
    "tags": [
      "published",
      "tool",
      "component"
    ]
  },
  "modules": {
    "main": {
      "code": "/**\n * @fileoverview Generates video output using supplied context.\n */\nimport gemini, { defaultSafetySettings, } from \"./a2/gemini\";\nimport { err, ok, llm, isStoredData, toTextConcat, joinContent, toLLMContent, toInlineReference, toLLMContentInline, toLLMContentStored, toText, toInlineData, extractMediaData, extractTextData, defaultLLMContent, } from \"./a2/utils\";\nimport { Template } from \"./a2/template\";\nimport { ToolManager } from \"./a2/tool-manager\";\nimport {} from \"./a2/common\";\nimport {} from \"./a2/common\";\nimport { ArgumentNameGenerator } from \"./a2/introducer\";\nimport { ListExpander } from \"./a2/lists\";\nimport { executeStep, } from \"./a2/step-executor\";\nconst ASPECT_RATIOS = [\"9:16\", \"16:9\"];\nconst OUTPUT_NAME = \"generated_video\";\nexport { invoke as default, describe };\nasync function callVideoGen(prompt, imageContent, disablePromptRewrite, aspectRatio) {\n    const executionInputs = {};\n    const encodedPrompt = btoa(unescape(encodeURIComponent(prompt)));\n    executionInputs[\"text_instruction\"] = {\n        chunks: [\n            {\n                mimetype: \"text/plain\",\n                data: encodedPrompt,\n            },\n        ],\n    };\n    executionInputs[\"aspect_ratio_key\"] = {\n        chunks: [\n            {\n                mimetype: \"text/plain\",\n                data: btoa(aspectRatio),\n            },\n        ],\n    };\n    const inputParameters = [\"text_instruction\"];\n    if (imageContent) {\n        console.log(\"Image found, using i2v\");\n        let imageChunk;\n        if (isStoredData(imageContent)) {\n            imageChunk = toInlineReference(imageContent);\n        }\n        else {\n            imageChunk = toInlineData(imageContent);\n        }\n        if (!imageChunk || typeof imageChunk == \"string\") {\n            return toLLMContent(\"Image content did not have expected format\");\n        }\n        executionInputs[\"reference_image\"] = {\n            chunks: [\n                {\n                    mimetype: imageChunk.mimeType,\n                    data: imageChunk.data,\n                },\n            ],\n        };\n        inputParameters.push(\"reference_image\");\n    }\n    else {\n        console.log(\"No image found, using t2v\");\n    }\n    const body = {\n        planStep: {\n            stepName: \"GenerateVideo\",\n            modelApi: \"generate_video\",\n            inputParameters: inputParameters,\n            systemPrompt: \"\",\n            output: OUTPUT_NAME,\n            options: {\n                disablePromptRewrite,\n            },\n        },\n        execution_inputs: executionInputs,\n    };\n    // TODO(askerryryan): Remove when stable.\n    console.log(\"REQUEST:\");\n    console.log(body);\n    const response = await executeStep(body);\n    if (!ok(response)) {\n        return toLLMContent(\"Video generation failed: \" + response.$error);\n    }\n    if (!response.executionOutputs) {\n        return toLLMContent(\"Video generation failed to generate video\");\n    }\n    let returnVal;\n    for (let value of Object.values(response.executionOutputs)) {\n        const mimetype = value.chunks[0].mimetype;\n        if (mimetype.startsWith(\"video\")) {\n            if (mimetype.endsWith(\"/storedData\")) {\n                returnVal = toLLMContentStored(mimetype.replace(\"/storedData\", \"\"), value.chunks[0].data);\n            }\n            else {\n                returnVal = toLLMContentInline(mimetype, value.chunks[0].data);\n            }\n        }\n    }\n    if (!returnVal) {\n        return toLLMContent(\"Error: No video returned from backend\");\n    }\n    return returnVal;\n}\nasync function invoke({ context, instruction, \"p-disable-prompt-rewrite\": disablePromptRewrite, \"p-aspect-ratio\": aspectRatio, ...params }) {\n    context ??= [];\n    let instructionText = \"\";\n    if (instruction) {\n        instructionText = toText(instruction).trim();\n    }\n    if (!aspectRatio) {\n        aspectRatio = \"16:9\";\n    }\n    // 2) Substitute variables and magic image reference.\n    // Note: it is important that images are not subsituted in here as they will\n    // not be handled properly. At this point, only text variables should be left.\n    const template = new Template(toLLMContent(instructionText));\n    const toolManager = new ToolManager(new ArgumentNameGenerator());\n    const substituting = await template.substitute(params, async ({ path: url, instance }) => toolManager.addTool(url, instance));\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    console.log(\"context\");\n    console.log(context);\n    console.log(\"instruction\");\n    console.log(instruction);\n    console.log(\"substituting\");\n    console.log(substituting);\n    const results = await new ListExpander(substituting, context).map(async (itemInstruction, itemContext) => {\n        // 1) Extract any image and text data from context (with history).\n        let imageContext = extractMediaData(itemContext);\n        const textContext = extractTextData(itemContext);\n        // 3) Extract image and text data from (non-history) references.\n        const refImages = extractMediaData([itemInstruction]);\n        const refText = extractTextData([itemInstruction]);\n        // 4) Combine with whatever data was extracted from context.\n        // Validate that we did not find any images, given this isn't supported yet.\n        imageContext = imageContext.concat(refImages);\n        if (imageContext.length > 1) {\n            return toLLMContent(`Video generation expects either a single text description, or text plus a single image. Got ${imageContext.length} images.`);\n        }\n        const combinedInstruction = toTextConcat(joinContent(toTextConcat(refText), textContext, false));\n        if (!combinedInstruction) {\n            return toLLMContent(\"A video instruction must be provided.\");\n        }\n        console.log(\"PROMPT: \", combinedInstruction);\n        // 2) Call backend to generate video.\n        const content = await callVideoGen(combinedInstruction, imageContext.at(0), disablePromptRewrite, aspectRatio);\n        return content;\n    });\n    if (!ok(results))\n        return results;\n    return { context: results };\n}\nasync function describe({ inputs: { instruction } }) {\n    const template = new Template(instruction);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                instruction: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"Instruction\",\n                    description: \"Instructions for how to render the video. Use @ to reference upstream steps.\",\n                    default: defaultLLMContent(),\n                },\n                \"p-disable-prompt-rewrite\": {\n                    type: \"boolean\",\n                    title: \"Disable prompt expansion\",\n                    behavior: [\"config\", \"hint-preview\"],\n                    description: \"By default, inputs and instructions can be automatically expanded into a higher quality video prompt. Check to disable this re-writing behavior.\",\n                },\n                \"p-aspect-ratio\": {\n                    type: \"string\",\n                    behavior: [\"hint-text\", \"config\"],\n                    title: \"Aspect Ratio\",\n                    enum: ASPECT_RATIOS,\n                    description: \"The aspect ratio of the generated video\",\n                    default: \"1:1\",\n                },\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"hint-multimodal\", \"main-port\"],\n                },\n            },\n            additionalProperties: false,\n        },\n        title: \"Make Video\",\n        metadata: {\n            icon: \"generative-video\",\n            tags: [\"quick-access\", \"generative\"],\n            order: 3,\n        },\n    };\n}\n",
      "metadata": {
        "title": "main",
        "source": {
          "code": "/**\n * @fileoverview Generates video output using supplied context.\n */\n\nimport gemini, {\n  defaultSafetySettings,\n  type GeminiOutputs,\n  type GeminiInputs,\n} from \"./a2/gemini\";\nimport {\n  err,\n  ok,\n  llm,\n  isStoredData,\n  toTextConcat,\n  joinContent,\n  toLLMContent,\n  toInlineReference,\n  toLLMContentInline,\n  toLLMContentStored,\n  toText,\n  toInlineData,\n  extractMediaData,\n  extractTextData,\n  defaultLLMContent,\n} from \"./a2/utils\";\nimport { Template } from \"./a2/template\";\nimport { ToolManager } from \"./a2/tool-manager\";\nimport { type Params } from \"./a2/common\";\nimport { type DescriberResult } from \"./a2/common\";\nimport { ArgumentNameGenerator } from \"./a2/introducer\";\nimport { ListExpander } from \"./a2/lists\";\n\nimport {\n  type ContentMap,\n  type ExecuteStepRequest,\n  executeStep,\n} from \"./a2/step-executor\";\n\nconst ASPECT_RATIOS = [\"9:16\", \"16:9\"];\nconst OUTPUT_NAME = \"generated_video\";\n\ntype VideoGeneratorInputs = {\n  context: LLMContent[];\n  instruction?: LLMContent;\n  \"p-disable-prompt-rewrite\": boolean;\n  \"p-aspect-ratio\": string;\n};\n\ntype VideoGeneratorOutputs = {\n  context: LLMContent[] | DescriberResult;\n};\n\nexport { invoke as default, describe };\n\nasync function callVideoGen(\n  prompt: string,\n  imageContent: LLMContent | undefined,\n  disablePromptRewrite: boolean,\n  aspectRatio: string\n): Promise<LLMContent> {\n  const executionInputs: ContentMap = {};\n  const encodedPrompt = btoa(unescape(encodeURIComponent(prompt)));\n  executionInputs[\"text_instruction\"] = {\n    chunks: [\n      {\n        mimetype: \"text/plain\",\n        data: encodedPrompt,\n      },\n    ],\n  };\n  executionInputs[\"aspect_ratio_key\"] = {\n    chunks: [\n      {\n        mimetype: \"text/plain\",\n        data: btoa(aspectRatio),\n      },\n    ],\n  };\n  const inputParameters: string[] = [\"text_instruction\"];\n  if (imageContent) {\n    console.log(\"Image found, using i2v\");\n    let imageChunk;\n    if (isStoredData(imageContent)) {\n      imageChunk = toInlineReference(imageContent);\n    } else {\n      imageChunk = toInlineData(imageContent);\n    }\n    if (!imageChunk || typeof imageChunk == \"string\") {\n      return toLLMContent(\"Image content did not have expected format\");\n    }\n    executionInputs[\"reference_image\"] = {\n      chunks: [\n        {\n          mimetype: imageChunk.mimeType,\n          data: imageChunk.data,\n        },\n      ],\n    };\n    inputParameters.push(\"reference_image\");\n  } else {\n    console.log(\"No image found, using t2v\");\n  }\n  const body = {\n    planStep: {\n      stepName: \"GenerateVideo\",\n      modelApi: \"generate_video\",\n      inputParameters: inputParameters,\n      systemPrompt: \"\",\n      output: OUTPUT_NAME,\n      options: {\n        disablePromptRewrite,\n      },\n    },\n    execution_inputs: executionInputs,\n  } satisfies ExecuteStepRequest;\n  // TODO(askerryryan): Remove when stable.\n  console.log(\"REQUEST:\");\n  console.log(body);\n  const response = await executeStep(body);\n  if (!ok(response)) {\n    return toLLMContent(\"Video generation failed: \" + response.$error);\n  }\n  if (!response.executionOutputs) {\n    return toLLMContent(\"Video generation failed to generate video\");\n  }\n\n  let returnVal;\n  for (let value of Object.values(response.executionOutputs)) {\n    const mimetype = value.chunks[0].mimetype;\n    if (mimetype.startsWith(\"video\")) {\n      if (mimetype.endsWith(\"/storedData\")) {\n        returnVal = toLLMContentStored(\n          mimetype.replace(\"/storedData\", \"\"),\n          value.chunks[0].data\n        );\n      } else {\n        returnVal = toLLMContentInline(mimetype, value.chunks[0].data);\n      }\n    }\n  }\n  if (!returnVal) {\n    return toLLMContent(\"Error: No video returned from backend\");\n  }\n  return returnVal;\n}\n\nasync function invoke({\n  context,\n  instruction,\n  \"p-disable-prompt-rewrite\": disablePromptRewrite,\n  \"p-aspect-ratio\": aspectRatio,\n  ...params\n}: VideoGeneratorInputs): Promise<Outcome<VideoGeneratorOutputs>> {\n  context ??= [];\n  let instructionText = \"\";\n  if (instruction) {\n    instructionText = toText(instruction).trim();\n  }\n  if (!aspectRatio) {\n    aspectRatio = \"16:9\";\n  }\n  // 2) Substitute variables and magic image reference.\n  // Note: it is important that images are not subsituted in here as they will\n  // not be handled properly. At this point, only text variables should be left.\n  const template = new Template(toLLMContent(instructionText));\n  const toolManager = new ToolManager(new ArgumentNameGenerator());\n  const substituting = await template.substitute(\n    params,\n    async ({ path: url, instance }) => toolManager.addTool(url, instance)\n  );\n  if (!ok(substituting)) {\n    return substituting;\n  }\n  console.log(\"context\");\n  console.log(context);\n  console.log(\"instruction\");\n  console.log(instruction);\n  console.log(\"substituting\");\n  console.log(substituting);\n\n  const results = await new ListExpander(substituting, context).map(\n    async (itemInstruction, itemContext) => {\n      // 1) Extract any image and text data from context (with history).\n      let imageContext = extractMediaData(itemContext);\n      const textContext = extractTextData(itemContext);\n\n      // 3) Extract image and text data from (non-history) references.\n      const refImages = extractMediaData([itemInstruction]);\n      const refText = extractTextData([itemInstruction]);\n\n      // 4) Combine with whatever data was extracted from context.\n      // Validate that we did not find any images, given this isn't supported yet.\n      imageContext = imageContext.concat(refImages);\n      if (imageContext.length > 1) {\n        return toLLMContent(\n          `Video generation expects either a single text description, or text plus a single image. Got ${imageContext.length} images.`\n        );\n      }\n      const combinedInstruction = toTextConcat(\n        joinContent(toTextConcat(refText), textContext, false)\n      );\n      if (!combinedInstruction) {\n        return toLLMContent(\"A video instruction must be provided.\");\n      }\n\n      console.log(\"PROMPT: \", combinedInstruction);\n\n      // 2) Call backend to generate video.\n      const content = await callVideoGen(\n        combinedInstruction,\n        imageContext.at(0),\n        disablePromptRewrite,\n        aspectRatio\n      );\n      return content;\n    }\n  );\n  if (!ok(results)) return results;\n  return { context: results };\n}\n\ntype DescribeInputs = {\n  inputs: {\n    instruction?: LLMContent;\n  };\n};\n\nasync function describe({ inputs: { instruction } }: DescribeInputs) {\n  const template = new Template(instruction);\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n        instruction: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n          title: \"Instruction\",\n          description:\n            \"Instructions for how to render the video. Use @ to reference upstream steps.\",\n          default: defaultLLMContent(),\n        },\n        \"p-disable-prompt-rewrite\": {\n          type: \"boolean\",\n          title: \"Disable prompt expansion\",\n          behavior: [\"config\", \"hint-preview\"],\n          description:\n            \"By default, inputs and instructions can be automatically expanded into a higher quality video prompt. Check to disable this re-writing behavior.\",\n        },\n        \"p-aspect-ratio\": {\n          type: \"string\",\n          behavior: [\"hint-text\", \"config\"],\n          title: \"Aspect Ratio\",\n          enum: ASPECT_RATIOS,\n          description: \"The aspect ratio of the generated video\",\n          default: \"1:1\",\n        },\n        ...template.schemas(),\n      },\n      behavior: [\"at-wireable\"],\n      ...template.requireds(),\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n          behavior: [\"hint-multimodal\", \"main-port\"],\n        },\n      },\n      additionalProperties: false,\n    } satisfies Schema,\n    title: \"Make Video\",\n    metadata: {\n      icon: \"generative-video\",\n      tags: [\"quick-access\", \"generative\"],\n      order: 3,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Generates video output using supplied context.",
        "runnable": true
      }
    }
  },
  "imports": {
    "a2": {
      "url": "./a2.bgl.json"
    }
  },
  "exports": [
    "#module:main"
  ]
}