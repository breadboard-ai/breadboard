{
  "title": "Gemini",
  "description": "Calls Gemini model `generateContent` API. ",
  "version": "1.1.0",
  "main": "main",
  "modules": {
    "main": {
      "code": "/**\n * @fileoverview Gemini Model Family.\n */\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\nimport output from \"@output\";\nimport { ok, err, isLLMContentArray } from \"./utils\";\nconst defaultSafetySettings = () => [\n    {\n        category: \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        threshold: \"BLOCK_NONE\",\n    },\n    {\n        category: \"HARM_CATEGORY_HARASSMENT\",\n        threshold: \"BLOCK_NONE\",\n    },\n    {\n        category: \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        threshold: \"BLOCK_NONE\",\n    },\n];\nasync function endpointURL(model) {\n    const $metadata = {\n        title: \"Get GEMINI_KEY\",\n        description: \"Getting GEMINI_KEY from secrets\",\n    };\n    const key = await secrets({ $metadata, keys: [\"GEMINI_KEY\"] });\n    return `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${key.GEMINI_KEY}`;\n}\nexport { invoke as default, describe, defaultSafetySettings };\nconst VALID_MODALITIES = [\"Text\", \"Text and Image\", \"Audio\"];\nconst MODELS = [\n    \"gemini-1.5-flash-latest\",\n    \"gemini-1.5-pro-latest\",\n    \"gemini-2.0-flash-exp\",\n    \"gemini-2.0-flash-thinking-exp\",\n    \"gemini-exp-1206\",\n    \"gemini-exp-1121\",\n    \"learnlm-1.5-pro-experimental\",\n    \"gemini-1.5-pro-001\",\n    \"gemini-1.5-pro-002\",\n    \"gemini-1.5-pro-exp-0801\",\n    \"gemini-1.5-pro-exp-0827\",\n    \"gemini-1.5-flash-001\",\n    \"gemini-1.5-flash-002\",\n    \"gemini-1.5-flash-8b-exp-0924\",\n    \"gemini-1.5-flash-8b-exp-0827\",\n    \"gemini-1.5-flash-exp-0827\",\n];\nconst NO_RETRY_CODES = [400, 429, 404];\nasync function callAPI(retries, model, body, $metadata) {\n    let $error = \"Unknown error\";\n    while (retries) {\n        const result = await fetch({\n            $metadata,\n            url: await endpointURL(model),\n            method: \"POST\",\n            body,\n        });\n        if (!ok(result)) {\n            // Fetch is a bit weird, because it returns various props\n            // along with the `$error`. Let's handle that here.\n            const { status, $error: errObject } = result;\n            if (!status) {\n                // This is not an error response, presume fatal error.\n                return { $error };\n            }\n            $error = maybeExtractError(errObject);\n            if (NO_RETRY_CODES.includes(status)) {\n                return { $error };\n            }\n            retries--;\n        }\n        else {\n            return result.response;\n        }\n    }\n    return { $error };\n}\nfunction maybeExtractError(e) {\n    try {\n        const parsed = JSON.parse(e);\n        return parsed.error.message;\n    }\n    catch (error) {\n        return e;\n    }\n}\nfunction isEmptyLLMContent(content) {\n    if (!content || !content.parts || content.parts.length === 0)\n        return true;\n    return content.parts.every((part) => {\n        if (\"text\" in part) {\n            return !part.text?.trim();\n        }\n        return true;\n    });\n}\nfunction addModality(body, modality) {\n    if (!modality)\n        return;\n    switch (modality) {\n        case \"Text\":\n            // No change, defaults.\n            break;\n        case \"Text and Image\":\n            body.generationConfig ??= {};\n            body.generationConfig.responseModalities = [\"TEXT\", \"IMAGE\"];\n            break;\n        case \"Audio\":\n            body.generationConfig ??= {};\n            body.generationConfig.responseModalities = [\"AUDIO\"];\n            break;\n    }\n}\nfunction constructBody(context = [], systemInstruction, prompt, modality) {\n    const contents = [...context];\n    if (!isEmptyLLMContent(prompt)) {\n        contents.push(prompt);\n    }\n    const body = {\n        contents,\n        safetySettings: defaultSafetySettings(),\n    };\n    const canHaveSystemInstruction = modality === \"Text\";\n    if (!isEmptyLLMContent(systemInstruction) && canHaveSystemInstruction) {\n        body.systemInstruction = systemInstruction;\n    }\n    addModality(body, modality);\n    return body;\n}\nfunction augmentBody(body, systemInstruction, prompt, modality) {\n    if (!body.systemInstruction || !isEmptyLLMContent(systemInstruction)) {\n        body.systemInstruction = systemInstruction;\n    }\n    if (!isEmptyLLMContent(prompt)) {\n        body.contents = [...body.contents, prompt];\n    }\n    addModality(body, modality);\n    return body;\n}\nfunction validateInputs(inputs) {\n    if (\"body\" in inputs) {\n        return;\n    }\n    if (inputs.context) {\n        const { context } = inputs;\n        if (!Array.isArray(context)) {\n            return err(\"Incoming context must be an array.\");\n        }\n        if (!isLLMContentArray(context)) {\n            return err(\"Malformed incoming context\");\n        }\n        return;\n    }\n    return err(\"Either body or context is required\");\n}\nasync function invoke(inputs) {\n    const validatingInputs = validateInputs(inputs);\n    if (!ok(validatingInputs)) {\n        return validatingInputs;\n    }\n    let { model } = inputs;\n    if (!model) {\n        model = MODELS[0];\n    }\n    const { context, systemInstruction, prompt, modality, body, $metadata } = inputs;\n    // TODO: Make this configurable.\n    const retries = 5;\n    if (!(\"body\" in inputs)) {\n        // Public API is being used.\n        // Behave as if we're wired in.\n        const result = await callAPI(retries, model, constructBody(context, systemInstruction, prompt, modality));\n        if (!ok(result)) {\n            return result;\n        }\n        const content = result.candidates.at(0)?.content;\n        if (!content) {\n            return err(\"Unable to get a good response from Gemini\");\n        }\n        return { context: [...context, content] };\n    }\n    else {\n        // Private API is being used.\n        // Behave as if we're being invoked.\n        return callAPI(retries, model, augmentBody(body, systemInstruction, prompt, modality), $metadata);\n    }\n}\nasync function describe({ inputs }) {\n    const canHaveModalities = inputs.model === \"gemini-2.0-flash-exp\";\n    const canHaveSystemInstruction = !canHaveModalities || (canHaveModalities && inputs.modality == \"Text\");\n    const maybeAddSystemInstruction = canHaveSystemInstruction\n        ? {\n            systemInstruction: {\n                type: \"object\",\n                behavior: [\"llm-content\", \"config\"],\n                title: \"System Instruction\",\n                default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n                description: \"(Optional) Give the model additional context on what to do,\" +\n                    \"like specific rules/guidelines to adhere to or specify behavior\" +\n                    \"separate from the provided context\",\n            },\n        }\n        : {};\n    const maybeAddModalities = canHaveModalities\n        ? {\n            modality: {\n                type: \"string\",\n                enum: [...VALID_MODALITIES],\n                title: \"Output Modality\",\n                behavior: [\"config\"],\n                description: \"(Optional) Tell the model what kind of output you're looking for.\",\n            },\n        }\n        : {};\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                model: {\n                    type: \"string\",\n                    behavior: [\"config\"],\n                    title: \"Model Name\",\n                    enum: MODELS,\n                    default: MODELS[0],\n                },\n                prompt: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\"],\n                    title: \"Prompt\",\n                    default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n                    description: \"(Optional) A prompt. Will be added to the end of the the conversation context.\",\n                },\n                ...maybeAddSystemInstruction,\n                ...maybeAddModalities,\n                context: {\n                    type: \"array\",\n                    items: {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                    },\n                    title: \"Context in\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                    },\n                    title: \"Context out\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "description": "Gemini Model Family.",
        "url": "main.js",
        "source": {
          "code": "/**\n * @fileoverview Gemini Model Family.\n */\n\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\nimport output from \"@output\";\n\nimport { ok, err, isLLMContentArray } from \"./utils\";\n\nconst defaultSafetySettings = (): SafetySetting[] => [\n  {\n    category: \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n    threshold: \"BLOCK_NONE\",\n  },\n  {\n    category: \"HARM_CATEGORY_HARASSMENT\",\n    threshold: \"BLOCK_NONE\",\n  },\n  {\n    category: \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n    threshold: \"BLOCK_NONE\",\n  },\n];\n\nasync function endpointURL(model: string): Promise<string> {\n  const $metadata = {\n    title: \"Get GEMINI_KEY\",\n    description: \"Getting GEMINI_KEY from secrets\",\n  };\n  const key = await secrets({ $metadata, keys: [\"GEMINI_KEY\"] });\n  return `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${key.GEMINI_KEY}`;\n}\n\nexport { invoke as default, describe, defaultSafetySettings };\n\nconst VALID_MODALITIES = [\"Text\", \"Text and Image\", \"Audio\"] as const;\ntype ValidModalities = (typeof VALID_MODALITIES)[number];\n\nexport type HarmBlockThreshold =\n  // Content with NEGLIGIBLE will be allowed.\n  | \"BLOCK_LOW_AND_ABOVE\"\n  // Content with NEGLIGIBLE and LOW will be allowed.\n  | \"BLOCK_MEDIUM_AND_ABOVE\"\n  // Content with NEGLIGIBLE, LOW, and MEDIUM will be allowed.\n  | \"BLOCK_ONLY_HIGH\"\n  // All content will be allowed.\n  | \"BLOCK_NONE\"\n  // Turn off the safety filter.\n  | \"OFF\";\n\nexport type HarmCategory =\n  // Gemini - Harassment content\n  | \"HARM_CATEGORY_HARASSMENT\"\n  //\tGemini - Hate speech and content.\n  | \"HARM_CATEGORY_HATE_SPEECH\"\n  // Gemini - Sexually explicit content.\n  | \"HARM_CATEGORY_SEXUALLY_EXPLICIT\"\n  // \tGemini - Dangerous content.\n  | \"HARM_CATEGORY_DANGEROUS_CONTENT\"\n  // Gemini - Content that may be used to harm civic integrity.\n  | \"HARM_CATEGORY_CIVIC_INTEGRITY\";\n\nexport type GeminiSchema = {\n  type: \"string\" | \"number\" | \"integer\" | \"boolean\" | \"object\" | \"array\";\n  format?: string;\n  description?: string;\n  nullable?: boolean;\n  enum?: string[];\n  maxItems?: string;\n  minItems?: string;\n  properties?: Record<string, GeminiSchema>;\n  required?: string[];\n  items?: GeminiSchema;\n};\n\nexport type Modality = \"TEXT\" | \"IMAGE\" | \"AUDIO\";\n\nexport type GenerationConfig = {\n  responseMimeType?: \"text/plain\" | \"application/json\" | \"text/x.enum\";\n  responseSchema?: GeminiSchema;\n  responseModalities?: Modality[];\n};\n\nexport type SafetySetting = {\n  category: HarmCategory;\n  threshold: HarmBlockThreshold;\n};\n\nexport type Metadata = {\n  title?: string;\n  description?: string;\n};\n\nexport type GeminiBody = {\n  contents: LLMContent[];\n  tools?: Tool[];\n  toolConfig?: ToolConfig;\n  systemInstruction?: LLMContent;\n  safetySettings?: SafetySetting[];\n  generationConfig?: GenerationConfig;\n};\n\nexport type GeminiInputs = {\n  // The wireable/configurable properties.\n  model?: string;\n  context?: LLMContent[];\n  systemInstruction?: LLMContent;\n  prompt?: LLMContent;\n  modality?: ValidModalities;\n  // The \"private API\" properties\n  $metadata?: Metadata;\n  body: GeminiBody;\n};\n\nexport type Tool = {\n  functionDeclarations?: FunctionDeclaration[];\n  googleSearchRetrieval?: GoogleSearchRetrieval[];\n  codeExecution?: CodeExecution[];\n};\n\nexport type ToolConfig = {\n  functionCallingConfig?: FunctionCallingConfig;\n};\n\nexport type FunctionCallingConfig = {\n  mode?: \"MODE_UNSPECIFIED\" | \"AUTO\" | \"ANY\" | \"NONE\";\n  allowedFunctionNames?: string[];\n};\n\nexport type FunctionDeclaration = {\n  name: string;\n  description: string;\n  parameters: GeminiSchema;\n};\n\nexport type GoogleSearchRetrieval = {\n  dynamicRetrievalConfig: {\n    mode: \"MODE_UNSPECIFIED\" | \"MODE_DYNAMIC\";\n    dynamicThreshold: number;\n  };\n};\n\nexport type CodeExecution = {\n  // Type contains no fields.\n};\n\nexport type Candidate = {\n  content: LLMContent;\n};\n\nexport type GeminiAPIOutputs = {\n  candidates: Candidate[];\n};\n\nexport type GeminiOutputs =\n  | GeminiAPIOutputs\n  | {\n      context: LLMContent[];\n    };\n\nconst MODELS: readonly string[] = [\n  \"gemini-1.5-flash-latest\",\n  \"gemini-1.5-pro-latest\",\n  \"gemini-2.0-flash-exp\",\n  \"gemini-2.0-flash-thinking-exp\",\n  \"gemini-exp-1206\",\n  \"gemini-exp-1121\",\n  \"learnlm-1.5-pro-experimental\",\n  \"gemini-1.5-pro-001\",\n  \"gemini-1.5-pro-002\",\n  \"gemini-1.5-pro-exp-0801\",\n  \"gemini-1.5-pro-exp-0827\",\n  \"gemini-1.5-flash-001\",\n  \"gemini-1.5-flash-002\",\n  \"gemini-1.5-flash-8b-exp-0924\",\n  \"gemini-1.5-flash-8b-exp-0827\",\n  \"gemini-1.5-flash-exp-0827\",\n];\n\nconst NO_RETRY_CODES: readonly number[] = [400, 429, 404];\n\ntype FetchErrorResponse = {\n  $error: string;\n  status: number;\n  statusText: string;\n  contentType: string;\n  responseHeaders: Record<string, string>;\n};\n\n/**\n * Using\n * `{\"error\":{\"code\":400,\"message\":\"Invalid JSON paylo…'contents[0].parts[0]': Cannot find field.\"}]}]}\n * as template for this type.\n */\ntype GeminiError = {\n  error: {\n    code: number;\n    details: {\n      type: string;\n      fieldViolations: {\n        description: string;\n        field: string;\n      }[];\n    }[];\n    message: string;\n    status: string;\n  };\n};\n\nasync function callAPI(\n  retries: number,\n  model: string,\n  body: GeminiBody,\n  $metadata?: Metadata\n): Promise<Outcome<GeminiAPIOutputs>> {\n  let $error: string = \"Unknown error\";\n  while (retries) {\n    const result = await fetch({\n      $metadata,\n      url: await endpointURL(model),\n      method: \"POST\",\n      body,\n    });\n    if (!ok(result)) {\n      // Fetch is a bit weird, because it returns various props\n      // along with the `$error`. Let's handle that here.\n      const { status, $error: errObject } = result as FetchErrorResponse;\n      if (!status) {\n        // This is not an error response, presume fatal error.\n        return { $error };\n      }\n      $error = maybeExtractError(errObject);\n      if (NO_RETRY_CODES.includes(status)) {\n        return { $error };\n      }\n      retries--;\n    } else {\n      return result.response as GeminiAPIOutputs;\n    }\n  }\n  return { $error };\n}\n\nfunction maybeExtractError(e: string): string {\n  try {\n    const parsed = JSON.parse(e) as GeminiError;\n    return parsed.error.message;\n  } catch (error) {\n    return e;\n  }\n}\n\nfunction isEmptyLLMContent(content?: LLMContent): content is undefined {\n  if (!content || !content.parts || content.parts.length === 0) return true;\n  return content.parts.every((part) => {\n    if (\"text\" in part) {\n      return !part.text?.trim();\n    }\n    return true;\n  });\n}\n\nfunction addModality(body: GeminiBody, modality?: ValidModalities) {\n  if (!modality) return;\n  switch (modality) {\n    case \"Text\":\n      // No change, defaults.\n      break;\n    case \"Text and Image\":\n      body.generationConfig ??= {};\n      body.generationConfig.responseModalities = [\"TEXT\", \"IMAGE\"];\n      break;\n    case \"Audio\":\n      body.generationConfig ??= {};\n      body.generationConfig.responseModalities = [\"AUDIO\"];\n      break;\n  }\n}\n\nfunction constructBody(\n  context: LLMContent[] = [],\n  systemInstruction?: LLMContent,\n  prompt?: LLMContent,\n  modality?: ValidModalities\n): GeminiBody {\n  const contents = [...context];\n  if (!isEmptyLLMContent(prompt)) {\n    contents.push(prompt);\n  }\n  const body: GeminiBody = {\n    contents,\n    safetySettings: defaultSafetySettings(),\n  };\n  const canHaveSystemInstruction = modality === \"Text\";\n  if (!isEmptyLLMContent(systemInstruction) && canHaveSystemInstruction) {\n    body.systemInstruction = systemInstruction;\n  }\n  addModality(body, modality);\n  return body;\n}\n\nfunction augmentBody(\n  body: GeminiBody,\n  systemInstruction?: LLMContent,\n  prompt?: LLMContent,\n  modality?: ValidModalities\n): GeminiBody {\n  if (!body.systemInstruction || !isEmptyLLMContent(systemInstruction)) {\n    body.systemInstruction = systemInstruction;\n  }\n  if (!isEmptyLLMContent(prompt)) {\n    body.contents = [...body.contents, prompt];\n  }\n  addModality(body, modality);\n  return body;\n}\n\nfunction validateInputs(inputs: GeminiInputs): Outcome<void> {\n  if (\"body\" in (inputs as object)) {\n    return;\n  }\n  if (inputs.context) {\n    const { context } = inputs;\n    if (!Array.isArray(context)) {\n      return err(\"Incoming context must be an array.\");\n    }\n    if (!isLLMContentArray(context)) {\n      return err(\"Malformed incoming context\");\n    }\n    return;\n  }\n  return err(\"Either body or context is required\");\n}\n\nasync function invoke(inputs: GeminiInputs): Promise<Outcome<GeminiOutputs>> {\n  const validatingInputs = validateInputs(inputs);\n  if (!ok(validatingInputs)) {\n    return validatingInputs;\n  }\n  let { model } = inputs;\n  if (!model) {\n    model = MODELS[0];\n  }\n  const { context, systemInstruction, prompt, modality, body, $metadata } =\n    inputs;\n  // TODO: Make this configurable.\n  const retries = 5;\n  if (!(\"body\" in inputs)) {\n    // Public API is being used.\n    // Behave as if we're wired in.\n    const result = await callAPI(\n      retries,\n      model,\n      constructBody(context, systemInstruction, prompt, modality)\n    );\n    if (!ok(result)) {\n      return result;\n    }\n    const content = result.candidates.at(0)?.content;\n    if (!content) {\n      return err(\"Unable to get a good response from Gemini\");\n    }\n    return { context: [...context!, content] };\n  } else {\n    // Private API is being used.\n    // Behave as if we're being invoked.\n    return callAPI(\n      retries,\n      model,\n      augmentBody(body, systemInstruction, prompt, modality),\n      $metadata\n    );\n  }\n}\n\ntype DescribeInputs = {\n  inputs: {\n    modality?: ValidModalities;\n    model: string;\n  };\n};\n\nasync function describe({ inputs }: DescribeInputs) {\n  const canHaveModalities = inputs.model === \"gemini-2.0-flash-exp\";\n  const canHaveSystemInstruction =\n    !canHaveModalities || (canHaveModalities && inputs.modality == \"Text\");\n  const maybeAddSystemInstruction: Schema[\"properties\"] =\n    canHaveSystemInstruction\n      ? {\n          systemInstruction: {\n            type: \"object\",\n            behavior: [\"llm-content\", \"config\"],\n            title: \"System Instruction\",\n            default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n            description:\n              \"(Optional) Give the model additional context on what to do,\" +\n              \"like specific rules/guidelines to adhere to or specify behavior\" +\n              \"separate from the provided context\",\n          },\n        }\n      : {};\n  const maybeAddModalities: Schema[\"properties\"] = canHaveModalities\n    ? {\n        modality: {\n          type: \"string\",\n          enum: [...VALID_MODALITIES],\n          title: \"Output Modality\",\n          behavior: [\"config\"],\n          description:\n            \"(Optional) Tell the model what kind of output you're looking for.\",\n        },\n      }\n    : {};\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        model: {\n          type: \"string\",\n          behavior: [\"config\"],\n          title: \"Model Name\",\n          enum: MODELS as string[],\n          default: MODELS[0],\n        },\n        prompt: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\"],\n          title: \"Prompt\",\n          default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n          description:\n            \"(Optional) A prompt. Will be added to the end of the the conversation context.\",\n        },\n        ...maybeAddSystemInstruction,\n        ...maybeAddModalities,\n        context: {\n          type: \"array\",\n          items: {\n            type: \"object\",\n            behavior: [\"llm-content\"],\n          },\n          title: \"Context in\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: {\n            type: \"object\",\n            behavior: [\"llm-content\"],\n          },\n          title: \"Context out\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "runnable": true
      }
    },
    "utils": {
      "code": "export { ok, err };\nfunction ok(o) {\n    return !(o && typeof o === \"object\" && \"$error\" in o);\n}\nfunction err($error) {\n    return { $error };\n}\nexport function isLLMContent(nodeValue) {\n    if (typeof nodeValue !== \"object\" || !nodeValue)\n        return false;\n    if (nodeValue === null || nodeValue === undefined)\n        return false;\n    if (\"parts\" in nodeValue &&\n        Array.isArray(nodeValue.parts) &&\n        nodeValue.parts.length > 0) {\n        return nodeValue.parts.every((part) => {\n            return part && typeof part === \"object\";\n        });\n    }\n    return false;\n}\nexport function isLLMContentArray(nodeValue) {\n    if (typeof nodeValue !== \"object\" || !nodeValue)\n        return false;\n    if (!Array.isArray(nodeValue))\n        return false;\n    if (nodeValue === null || nodeValue === undefined)\n        return false;\n    return (Array.isArray(nodeValue) && nodeValue.every((entry) => isLLMContent(entry)));\n}\n",
      "metadata": {
        "title": "utils",
        "source": {
          "code": "export { ok, err };\n\nfunction ok<T>(o: Outcome<T>): o is T {\n  return !(o && typeof o === \"object\" && \"$error\" in o);\n}\n\nfunction err($error: string) {\n  return { $error };\n}\n\nexport function isLLMContent(nodeValue: unknown): nodeValue is LLMContent {\n  if (typeof nodeValue !== \"object\" || !nodeValue) return false;\n  if (nodeValue === null || nodeValue === undefined) return false;\n\n  if (\n    \"parts\" in nodeValue &&\n    Array.isArray(nodeValue.parts) &&\n    nodeValue.parts.length > 0\n  ) {\n    return nodeValue.parts.every((part) => {\n      return part && typeof part === \"object\";\n    });\n  }\n  return false;\n}\n\nexport function isLLMContentArray(\n  nodeValue: unknown\n): nodeValue is LLMContent[] {\n  if (typeof nodeValue !== \"object\" || !nodeValue) return false;\n  if (!Array.isArray(nodeValue)) return false;\n  if (nodeValue === null || nodeValue === undefined) return false;\n\n  return (\n    Array.isArray(nodeValue) && nodeValue.every((entry) => isLLMContent(entry))\n  );\n}\n",
          "language": "typescript"
        },
        "description": "",
        "runnable": false
      }
    }
  },
  "nodes": [
    {
      "id": "input",
      "type": "input",
      "metadata": {
        "title": "Input"
      }
    },
    {
      "id": "run-module",
      "type": "runModule",
      "configuration": {
        "$module": "main"
      },
      "metadata": {
        "title": "Run \"Gemini\" module"
      }
    },
    {
      "id": "output",
      "type": "output",
      "metadata": {
        "title": "Output"
      }
    }
  ],
  "edges": [
    {
      "from": "input",
      "to": "run-module",
      "out": "*",
      "in": ""
    },
    {
      "from": "run-module",
      "to": "output",
      "out": "*",
      "in": ""
    }
  ],
  "metadata": {
    "tags": [
      "published",
      "component",
      "tool"
    ]
  }
}