{
  "title": "A2",
  "description": "Components that help you build flows.",
  "version": "0.0.1",
  "nodes": [],
  "edges": [],
  "metadata": {
    "comments": [
      {
        "id": "comment-b09617ef",
        "text": "Left Intentionally Blank",
        "metadata": {
          "visual": {
            "x": -37.90624999999966,
            "y": -415.8554687499999,
            "collapsed": "expanded",
            "outputHeight": 0
          }
        }
      }
    ],
    "visual": {
      "presentation": {
        "themes": {
          "54f81cc4-5c04-4d9d-b831-985d556f0ed9": {
            "themeColors": {
              "primaryColor": "#246db5",
              "secondaryColor": "#5cadff",
              "backgroundColor": "#ffffff",
              "textColor": "#1a1a1a",
              "primaryTextColor": "#ffffff"
            },
            "template": "basic",
            "splashScreen": {
              "storedData": {
                "handle": "/images/app/generic-flow.jpg",
                "mimeType": "image/jpeg"
              }
            }
          }
        },
        "theme": "54f81cc4-5c04-4d9d-b831-985d556f0ed9"
      }
    },
    "tags": [
      "published",
      "tool",
      "component"
    ],
    "icon": "text"
  },
  "modules": {
    "audio-generator": {
      "code": "/**\n * @fileoverview Generates audio output using supplied context.\n */\nimport gemini, { defaultSafetySettings } from \"./gemini\";\nimport { err, ok } from \"./utils\";\nexport { invoke as default, describe };\nasync function invoke({ context, }) {\n    // 1) Get last LLMContent from input.\n    const prompt = context && Array.isArray(context) && context.length > 0\n        ? context.at(-1)\n        : undefined;\n    if (!prompt) {\n        return err(\"Must supply context as input\");\n    }\n    prompt.role = \"user\";\n    // 2) Call Gemini to generate audio.\n    const result = await gemini({\n        model: \"gemini-2.0-flash-exp\",\n        body: {\n            contents: [prompt],\n            generationConfig: {\n                responseModalities: [\"AUDIO\"],\n            },\n            safetySettings: defaultSafetySettings(),\n        },\n    });\n    if (!ok(result)) {\n        return result;\n    }\n    if (\"context\" in result) {\n        return err(\"Invalid output from Gemini -- must be candidates\");\n    }\n    const content = result.candidates.at(0)?.content;\n    if (!content) {\n        return err(\"No content\");\n    }\n    return { context: [content] };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n            },\n            additionalProperties: false,\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"hint-audio\"],\n                },\n            },\n            additionalProperties: false,\n        },\n        title: \"Make Audio [Deprecated, Use Make Speech]\",\n        metadata: {\n            icon: \"generative-audio\",\n            tags: [\"quick-access\", \"generative\", \"experimental\"],\n            order: 3,\n        },\n    };\n}\n//# sourceMappingURL=audio-generator.js.map"
    },
    "combine-outputs": {
      "code": "/**\n * @fileoverview Combines multiple outputs into one.\n */\nimport { Template } from \"./template\";\nimport { ok } from \"./utils\";\nimport { fanOutContext, flattenContext } from \"./lists\";\nexport { invoke as default, describe };\nasync function invoke({ text, \"z-flatten-list\": flatten, ...params }) {\n    const template = new Template(text);\n    const substituting = await template.substitute(params, async () => \"\");\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    let context = await fanOutContext(substituting, undefined, async (instruction) => instruction);\n    if (!ok(context))\n        return context;\n    if (flatten) {\n        context = flattenContext(context);\n    }\n    return { context };\n}\nasync function describe({ inputs: { text } }) {\n    const template = new Template(text);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                text: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"hint-preview\", \"config\"],\n                    title: \"Text\",\n                    description: \"Type the @ character to select the outputs to combine\",\n                },\n                \"z-flatten-list\": {\n                    type: \"boolean\",\n                    behavior: [\"hint-preview\", \"config\"],\n                    icon: \"summarize\",\n                    title: \"Flatten the list\",\n                    description: \"When checked, the step will flatten the incoming list into a single outputs\",\n                },\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                    },\n                    title: \"Context out\",\n                    behavior: [\"main-port\", \"hint-multimodal\"],\n                },\n            },\n        },\n        title: \"Combine Outputs\",\n        metadata: {\n            icon: \"combine-outputs\",\n            tags: [\"quick-access\", \"core\", \"experimental\"],\n            order: 100,\n        },\n    };\n}\n//# sourceMappingURL=combine-outputs.js.map"
    },
    "common": {
      "code": "/**\n * @fileoverview Common types and code\n */\nexport {};\n//# sourceMappingURL=common.js.map"
    },
    "connector-manager": {
      "code": "/**\n * @fileoverview Manages connectors.\n */\nvar __classPrivateFieldGet = (this && this.__classPrivateFieldGet) || function (receiver, state, kind, f) {\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a getter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot read private member from an object whose class did not declare it\");\n    return kind === \"m\" ? f : kind === \"a\" ? f.call(receiver) : f ? f.value : state.get(receiver);\n};\nvar __classPrivateFieldSet = (this && this.__classPrivateFieldSet) || function (receiver, state, value, kind, f) {\n    if (kind === \"m\") throw new TypeError(\"Private method is not writable\");\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a setter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot write private member to an object whose class did not declare it\");\n    return (kind === \"a\" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value)), value;\n};\nvar _ConnectorManager_instances, _ConnectorManager_state, _ConnectorManager_getConnectorInfo, _ConnectorManager_getConnectorId, _ConnectorManager_getState, _ConnectorManager_getInvocationArgs;\nimport { err, ok, isLLMContentArray } from \"./utils\";\nimport read from \"@read\";\nimport describeConnector from \"@describe\";\nimport invokeConnector from \"@invoke\";\nexport { ConnectorManager, createConfigurator, createTools };\nfunction cx(json) {\n    return { context: [{ parts: [{ json }] }] };\n}\nfunction createTools(handler) {\n    return {\n        invoke: async function (inputs) {\n            const { method, id, info } = inputs;\n            if (method === \"list\") {\n                return handler.list(id, info);\n            }\n            else if (method === \"invoke\") {\n                const { name, args } = inputs;\n                return handler.invoke(id, info, name, args);\n            }\n            return err(`Unknown method: \"${method}\"\"`);\n        },\n        describe: async function () {\n            const { title } = handler;\n            return {\n                title,\n                metadata: {\n                    tags: [\"connector-tools\"],\n                },\n                inputSchema: {\n                    type: \"object\",\n                },\n                outputSchema: {\n                    type: \"object\",\n                },\n            };\n        },\n    };\n}\nfunction createConfigurator(configurator) {\n    return {\n        invoke: createConfiguratorInvoke(configurator),\n        describe: createConfiguratorDescribe(configurator),\n    };\n}\nfunction createConfiguratorDescribe(configurator) {\n    const { title } = configurator;\n    return async function () {\n        return {\n            title: title,\n            description: \"\",\n            metadata: {\n                tags: [\"connector-configure\"],\n            },\n            inputSchema: {\n                type: \"object\",\n            },\n            outputSchema: {\n                type: \"object\",\n                properties: {\n                    context: {\n                        type: \"array\",\n                        items: { type: \"object\", behavior: [\"llm-content\"] },\n                        title: \"Context out\",\n                    },\n                },\n            },\n        };\n    };\n}\nfunction createConfiguratorInvoke(configurator) {\n    return async function ({ context, }) {\n        const inputs = context?.at(-1)?.parts?.at(0)?.json;\n        if (!inputs || !(\"stage\" in inputs)) {\n            return err(`Can't configure ${configurator.title || \"\"} connector: invalid input structure`);\n        }\n        if (inputs.stage === \"initialize\") {\n            const initializing = await configurator.initialize(inputs);\n            if (!ok(initializing))\n                return initializing;\n            return cx(initializing);\n        }\n        else if (inputs.stage === \"read\") {\n            const reading = await configurator.read?.(inputs);\n            if (!reading) {\n                return cx({\n                    schema: {},\n                    values: {},\n                });\n            }\n            if (!ok(reading))\n                return reading;\n            return cx(reading);\n        }\n        else if (inputs.stage === \"preview\") {\n            const previewing = await configurator.preview?.(inputs);\n            if (!previewing || !ok(previewing)) {\n                return { context: [{ parts: [{ text: \"\" }] }] };\n            }\n            return { context: previewing };\n        }\n        else if (inputs.stage === \"write\") {\n            const writing = await configurator.write?.(inputs);\n            if (!writing)\n                return cx({});\n            if (!ok(writing))\n                return writing;\n            return cx(writing);\n        }\n        return err(`Unknown stage: ${inputs[\"stage\"]}`);\n    };\n}\nclass ConnectorManager {\n    constructor(part) {\n        _ConnectorManager_instances.add(this);\n        this.part = part;\n        _ConnectorManager_state.set(this, null);\n    }\n    async title() {\n        const state = await __classPrivateFieldGet(this, _ConnectorManager_instances, \"m\", _ConnectorManager_getState).call(this);\n        if (!ok(state))\n            return state;\n        return state.info.url;\n    }\n    async listTools() {\n        const args = await __classPrivateFieldGet(this, _ConnectorManager_instances, \"m\", _ConnectorManager_getInvocationArgs).call(this, \"connector-tools\");\n        if (!ok(args))\n            return args;\n        const invoking = await invokeConnector({ method: \"list\", ...args });\n        if (!ok(invoking))\n            return invoking;\n        const output = invoking;\n        console.log(\"LIST TOOLS OUTPUT\", output);\n        return output.list;\n    }\n    async invokeTool(name, args, callTool) {\n        const invocationArgs = await __classPrivateFieldGet(this, _ConnectorManager_instances, \"m\", _ConnectorManager_getInvocationArgs).call(this, \"connector-tools\");\n        if (!ok(invocationArgs))\n            return invocationArgs;\n        const { id, info } = invocationArgs;\n        await callTool(invocationArgs.$board, {\n            method: \"invoke\",\n            id,\n            info,\n            name,\n            args,\n        }, false, name);\n    }\n    async materialize() {\n        const args = await __classPrivateFieldGet(this, _ConnectorManager_instances, \"m\", _ConnectorManager_getInvocationArgs).call(this, \"connector-load\");\n        if (!ok(args))\n            return args;\n        const invoking = await invokeConnector(args);\n        if (!ok(invoking))\n            return invoking;\n        const output = invoking;\n        if (output && output.context && isLLMContentArray(output.context)) {\n            return output.context;\n        }\n        return err(`Invalid return value from connector load`);\n    }\n    async schemaProperties() {\n        const args = await __classPrivateFieldGet(this, _ConnectorManager_instances, \"m\", _ConnectorManager_getInvocationArgs).call(this, \"connector-save\");\n        if (!ok(args))\n            return {};\n        const describing = await describeConnector({ url: args.$board });\n        if (!ok(describing))\n            return {};\n        const props = describing.inputSchema.properties;\n        if (!props || Object.keys(props).length === 0)\n            return {};\n        delete props.context;\n        delete props.id;\n        delete props.info;\n        return props;\n    }\n    async canSave() {\n        const args = await __classPrivateFieldGet(this, _ConnectorManager_instances, \"m\", _ConnectorManager_getInvocationArgs).call(this, \"connector-save\");\n        if (!ok(args))\n            return false;\n        const invoking = await invokeConnector({\n            ...args,\n            method: \"canSave\",\n        });\n        if (!ok(invoking))\n            return false;\n        return !!invoking.canSave;\n    }\n    async save(context, options) {\n        const args = await __classPrivateFieldGet(this, _ConnectorManager_instances, \"m\", _ConnectorManager_getInvocationArgs).call(this, \"connector-save\");\n        if (!ok(args))\n            return args;\n        const invoking = await invokeConnector({\n            ...args,\n            context,\n            ...options,\n            method: \"save\",\n        });\n        if (!ok(invoking))\n            return invoking;\n    }\n    static isConnector(part) {\n        return part.path.startsWith(\"connectors/\");\n    }\n}\n_ConnectorManager_state = new WeakMap(), _ConnectorManager_instances = new WeakSet(), _ConnectorManager_getConnectorInfo = async function _ConnectorManager_getConnectorInfo() {\n    if (\"url\" in this.part) {\n        return this.part;\n    }\n    const path = `/assets/${this.part.path}`;\n    const reading = await read({ path });\n    if (!ok(reading))\n        return reading;\n    return getConnectorInfo(reading.data);\n}, _ConnectorManager_getConnectorId = async function _ConnectorManager_getConnectorId() {\n    if (\"url\" in this.part) {\n        const reading = await read({ path: \"/env/descriptor\" });\n        if (!ok(reading))\n            return reading;\n        const descriptor = getNodeDescriptor(reading.data);\n        if (!ok(descriptor))\n            return descriptor;\n        return descriptor.id;\n    }\n    return getConnectorId(this.part);\n}, _ConnectorManager_getState = async function _ConnectorManager_getState() {\n    if (__classPrivateFieldGet(this, _ConnectorManager_state, \"f\"))\n        return __classPrivateFieldGet(this, _ConnectorManager_state, \"f\");\n    const info = await __classPrivateFieldGet(this, _ConnectorManager_instances, \"m\", _ConnectorManager_getConnectorInfo).call(this);\n    if (!ok(info))\n        return info;\n    const describing = await describeConnector({ url: info.url });\n    if (!ok(describing))\n        return describing;\n    __classPrivateFieldSet(this, _ConnectorManager_state, { info, describeOutputs: describing }, \"f\");\n    return __classPrivateFieldGet(this, _ConnectorManager_state, \"f\");\n}, _ConnectorManager_getInvocationArgs = async function _ConnectorManager_getInvocationArgs(tag) {\n    const state = await __classPrivateFieldGet(this, _ConnectorManager_instances, \"m\", _ConnectorManager_getState).call(this);\n    if (!ok(state))\n        return state;\n    const url = getExportUrl(tag, state.describeOutputs);\n    if (!ok(url))\n        return url;\n    const id = await __classPrivateFieldGet(this, _ConnectorManager_instances, \"m\", _ConnectorManager_getConnectorId).call(this);\n    if (!ok(id))\n        return id;\n    return { $board: url, id, info: state.info };\n};\nfunction getConnectorId(part) {\n    const id = part.path.split(\"/\").at(-1);\n    if (!id)\n        return err(`Invalid connector path: ${part.path}`);\n    return id;\n}\nfunction getExportUrl(tag, result) {\n    const exports = result.exports;\n    if (!exports)\n        return err(`Invalid connector structure: must have exports`);\n    const assetExport = Object.entries(exports).find(([_url, e]) => e.metadata?.tags?.includes(tag));\n    if (!assetExport)\n        return err(`Invalid connector structure: must have export tagged as \"${tag}\"`);\n    return assetExport[0];\n}\nfunction getConnectorInfo(data) {\n    const part = data?.at(-1)?.parts.at(0);\n    if (!part)\n        return err(`Invalid asset structure`);\n    if (!(\"json\" in part))\n        return err(`Invalid connector info structure`);\n    return part.json;\n}\nfunction getNodeDescriptor(data) {\n    const part = data?.at(-1)?.parts.at(0);\n    if (!part)\n        return err(`Invalid asset structure`);\n    if (!(\"json\" in part))\n        return err(`Invalid file structure`);\n    return part.json;\n}\n//# sourceMappingURL=connector-manager.js.map"
    },
    "entry": {
      "code": "/**\n * @fileoverview Manages the entry point: describer, passing the inputs, etc.\n */\nimport { readSettings } from \"./settings\";\nimport { Template } from \"./template\";\nimport { defaultLLMContent, ok } from \"./utils\";\nexport { invoke as default, describe };\nasync function invoke({ context, \"p-chat\": chat, \"p-list\": makeList, description, ...params }) {\n    // Make sure it's a boolean.\n    chat = !!chat;\n    context ?? (context = []);\n    const defaultModel = \"\";\n    const type = \"work\";\n    return {\n        context: {\n            id: Math.random().toString(36).substring(2, 5),\n            chat,\n            makeList,\n            listPath: [],\n            context,\n            userInputs: [],\n            defaultModel,\n            model: \"\",\n            description,\n            tools: [],\n            type,\n            work: [],\n            userEndedChat: false,\n            params,\n        },\n    };\n}\nasync function describe({ inputs: { description } }) {\n    const settings = await readSettings();\n    const experimental = ok(settings) && !!settings[\"Show Experimental Components\"];\n    const template = new Template(description);\n    let extra = {};\n    if (experimental) {\n        extra = {\n            \"p-chat\": {\n                type: \"boolean\",\n                title: \"Chat with User\",\n                behavior: [\"config\", \"hint-preview\"],\n                icon: \"chat\",\n                description: \"When checked, this step will chat with the user, asking to review work, requesting additional information, etc.\",\n            },\n            \"p-list\": {\n                type: \"boolean\",\n                title: \"Make a list\",\n                behavior: [\"config\", \"hint-preview\"],\n                icon: \"summarize\",\n                description: \"When checked, this step will try to create a list as its output. Make sure that the prompt asks for a list of some sort\",\n            },\n        };\n    }\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                description: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"Instruction\",\n                    description: \"Give the model additional context on what to do, like specific rules/guidelines to adhere to or specify behavior separate from the provided context.\",\n                    default: defaultLLMContent(),\n                },\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                ...extra,\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"main-port\", \"hint-text\"],\n                },\n            },\n        },\n        title: \"Make Text\",\n        metadata: {\n            icon: \"generative-text\",\n            tags: [\"quick-access\", \"generative\"],\n            order: 1,\n        },\n    };\n}\n//# sourceMappingURL=entry.js.map"
    },
    "gemini-prompt": {
      "code": "/**\n * @fileoverview Manages Gemini prompt.\n */\nvar __classPrivateFieldGet = (this && this.__classPrivateFieldGet) || function (receiver, state, kind, f) {\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a getter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot read private member from an object whose class did not declare it\");\n    return kind === \"m\" ? f : kind === \"a\" ? f.call(receiver) : f ? f.value : state.get(receiver);\n};\nvar _GeminiPrompt_instances, _GeminiPrompt_reconcileOptions, _GeminiPrompt_normalizeArgs;\nimport invokeBoard from \"@invoke\";\nimport gemini from \"./gemini\";\nimport { ToolManager } from \"./tool-manager\";\nimport { addUserTurn, err, ok } from \"./utils\";\nexport { GeminiPrompt };\nfunction mergeLastParts(contexts) {\n    const parts = [];\n    for (const context of contexts) {\n        const last = context.at(-1);\n        if (!last)\n            continue;\n        if (!last.parts)\n            continue;\n        parts.push(...last.parts);\n    }\n    return {\n        parts,\n        role: \"user\",\n    };\n}\nclass GeminiPrompt {\n    constructor(inputs, options) {\n        _GeminiPrompt_instances.add(this);\n        this.inputs = inputs;\n        this.calledTools = false;\n        // Useful for detecting if subgraphs were invoked, based on whether a tool was invoked with\n        // 'passContext' set to true.\n        this.calledCustomTools = false;\n        this.options = __classPrivateFieldGet(this, _GeminiPrompt_instances, \"m\", _GeminiPrompt_reconcileOptions).call(this, options);\n    }\n    async invoke() {\n        this.calledTools = false;\n        this.calledCustomTools = false;\n        const { allowToolErrors, validator } = this.options;\n        const invoking = await gemini(this.inputs);\n        if (!ok(invoking))\n            return invoking;\n        if (\"context\" in invoking) {\n            return err(\"Invalid output from Gemini -- must be candidates\");\n        }\n        const candidate = invoking.candidates.at(0);\n        const content = candidate?.content;\n        if (!content)\n            return err(\"No content from Gemini\");\n        if (!content.parts) {\n            return err(`Gemini failed to generate result due to ${candidate.finishReason}`);\n        }\n        const results = [];\n        const errors = [];\n        if (validator) {\n            const validating = validator(content);\n            if (!ok(validating))\n                return validating;\n        }\n        await this.options.toolManager?.processResponse(content, async ($board, args, passContext, functionName) => {\n            console.log(\"CALLING TOOL\", $board, args, passContext);\n            this.calledTools = true;\n            if (passContext) {\n                // Passing context means we called a subgraph/'custom tool'.\n                this.calledCustomTools = true;\n            }\n            const callingTool = await invokeBoard({\n                $board,\n                ...__classPrivateFieldGet(this, _GeminiPrompt_instances, \"m\", _GeminiPrompt_normalizeArgs).call(this, args, passContext),\n            });\n            if (\"$error\" in callingTool) {\n                errors.push(JSON.stringify(callingTool.$error));\n            }\n            else if (functionName === undefined) {\n                errors.push(`No function name for ${JSON.stringify(callingTool)}`);\n            }\n            else {\n                if (passContext) {\n                    if (!(\"context\" in callingTool)) {\n                        errors.push(`No \"context\" port in outputs of \"${$board}\"`);\n                    }\n                    else {\n                        const response = {\n                            [\"value\"]: JSON.stringify(callingTool.context),\n                        };\n                        const responsePart = {\n                            functionResponse: {\n                                name: functionName,\n                                response: response,\n                            },\n                        };\n                        const toolResponseContent = {\n                            role: \"user\",\n                            parts: [responsePart],\n                        };\n                        results.push([toolResponseContent]);\n                        console.log(\"gemini-prompt + passContext, processResponse: \", results);\n                    }\n                }\n                else {\n                    const responsePart = {\n                        functionResponse: {\n                            name: functionName,\n                            response: callingTool,\n                        },\n                    };\n                    const toolResponseContent = {\n                        role: \"user\",\n                        parts: [responsePart],\n                    };\n                    console.log(\"toolResponseContent: \", toolResponseContent);\n                    results.push([toolResponseContent]);\n                    console.log(\"gemini-prompt processResponse: \", results);\n                }\n            }\n        });\n        console.log(\"ERRORS\", errors);\n        if (errors.length && !allowToolErrors) {\n            return err(`Calling tools generated the following errors: ${errors.join(\",\")}`);\n        }\n        const result = [content];\n        if (results.length) {\n            result.push(mergeLastParts(results));\n        }\n        return { all: result, last: result.at(-1), candidate };\n    }\n}\n_GeminiPrompt_instances = new WeakSet(), _GeminiPrompt_reconcileOptions = function _GeminiPrompt_reconcileOptions(options) {\n    if (!options)\n        return {};\n    if (options instanceof ToolManager) {\n        return { toolManager: options };\n    }\n    return options;\n}, _GeminiPrompt_normalizeArgs = function _GeminiPrompt_normalizeArgs(a, passContext) {\n    if (!passContext)\n        return a;\n    const args = a;\n    const context = [...this.inputs.body.contents];\n    const hasContext = \"context\" in args;\n    const contextArg = hasContext\n        ? {}\n        : {\n            context,\n        };\n    return {\n        ...contextArg,\n        ...Object.fromEntries(Object.entries(args).map(([name, value]) => {\n            if (hasContext) {\n                value = addUserTurn(value, [\n                    ...this.inputs.body.contents,\n                ]);\n            }\n            return [name, value];\n        })),\n    };\n};\n//# sourceMappingURL=gemini-prompt.js.map"
    },
    "gemini": {
      "code": "/**\n * @fileoverview Gemini Model Family.\n */\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\nimport { StreamableReporter } from \"./output\";\nimport { ok, err, isLLMContentArray } from \"./utils\";\nimport { flattenContext } from \"./lists\";\nconst defaultSafetySettings = () => [\n    {\n        category: \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        threshold: \"OFF\",\n    },\n    {\n        category: \"HARM_CATEGORY_HARASSMENT\",\n        threshold: \"OFF\",\n    },\n    {\n        category: \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        threshold: \"OFF\",\n    },\n];\nfunction endpointURL(model) {\n    // const $metadata = {\n    //   title: \"Get GEMINI_KEY\",\n    //   description: \"Getting GEMINI_KEY from secrets\",\n    // };\n    // const key = await secrets({ $metadata, keys: [\"GEMINI_KEY\"] });\n    return `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent`;\n}\nexport { invoke as default, describe, defaultSafetySettings };\nconst VALID_MODALITIES = [\"Text\", \"Text and Image\", \"Audio\"];\nconst MODELS = [\n    \"gemini-2.0-flash\",\n    \"gemini-2.0-flash-001\",\n    \"gemini-2.0-flash-lite-preview-02-05\",\n    \"gemini-2.0-flash-exp\",\n    \"gemini-2.0-flash-thinking-exp\",\n    \"gemini-2.0-flash-thinking-exp-01-21\",\n    \"gemini-2.0-pro-exp-02-05\",\n    \"gemini-1.5-flash-latest\",\n    \"gemini-1.5-pro-latest\",\n    \"gemini-exp-1206\",\n    \"gemini-exp-1121\",\n    \"learnlm-1.5-pro-experimental\",\n    \"gemini-1.5-pro-001\",\n    \"gemini-1.5-pro-002\",\n    \"gemini-1.5-pro-exp-0801\",\n    \"gemini-1.5-pro-exp-0827\",\n    \"gemini-1.5-flash-001\",\n    \"gemini-1.5-flash-002\",\n    \"gemini-1.5-flash-8b-exp-0924\",\n    \"gemini-1.5-flash-8b-exp-0827\",\n    \"gemini-1.5-flash-exp-0827\",\n];\nconst NO_RETRY_CODES = [400, 429, 404, 403];\nfunction textToJson(content) {\n    return {\n        ...content,\n        parts: content.parts.map((part) => {\n            if (\"text\" in part) {\n                try {\n                    return { json: JSON.parse(part.text) };\n                }\n                catch {\n                    // fall through.\n                }\n            }\n            return part;\n        }),\n    };\n}\n/**\n * Modifies the body to remove any\n * Breadboard-specific extensions to LLM Content\n */\nfunction conformBody(body) {\n    return {\n        ...body,\n        contents: flattenContext(body.contents.map((content) => {\n            return {\n                ...content,\n                parts: content.parts.map((part) => {\n                    if (\"json\" in part) {\n                        return { text: JSON.stringify(part.json) };\n                    }\n                    return part;\n                }),\n            };\n        }), true),\n    };\n}\nasync function getAccessToken() {\n    const signInSecretName = \"connection:$sign-in\";\n    const result = await secrets({ keys: [signInSecretName] });\n    return result[signInSecretName];\n}\nasync function callAPI(retries, model, body, $metadata) {\n    const accessToken = await getAccessToken();\n    const reporter = new StreamableReporter({\n        title: `Calling ${model}`,\n        icon: \"spark\",\n    });\n    try {\n        const conformedBody = conformBody(body);\n        await reporter.start();\n        await reporter.sendUpdate(\"Model Input\", conformedBody, \"upload\");\n        let $error = \"Unknown error\";\n        while (retries) {\n            const result = await fetch({\n                $metadata,\n                url: endpointURL(model),\n                method: \"POST\",\n                headers: {\n                    Authorization: `Bearer ${accessToken}`,\n                },\n                body: conformedBody,\n            });\n            if (!ok(result)) {\n                // Fetch is a bit weird, because it returns various props\n                // along with the `$error`. Let's handle that here.\n                const { status, $error: errObject } = result;\n                if (!status) {\n                    if (errObject)\n                        return reporter.sendError(err(errObject));\n                    // This is not an error response, presume fatal error.\n                    return reporter.sendError({ $error });\n                }\n                $error = maybeExtractError(errObject);\n                if (NO_RETRY_CODES.includes(status)) {\n                    return reporter.sendError({ $error });\n                }\n            }\n            else {\n                const outputs = result.response;\n                const candidate = outputs.candidates?.at(0);\n                if (!candidate) {\n                    await reporter.sendUpdate(\"Model Response\", outputs, \"warning\");\n                    return reporter.sendError(err(\"Unable to get a good response from Gemini\"));\n                }\n                if (\"content\" in candidate && candidate.content) {\n                    if (body.generationConfig?.responseMimeType === \"application/json\") {\n                        candidate.content = textToJson(candidate.content);\n                    }\n                    await reporter.sendUpdate(\"Model Response\", candidate.content, \"download\");\n                    return outputs;\n                }\n                await reporter.sendUpdate(\"Model response\", outputs, \"warning\");\n                if (candidate.finishReason === \"IMAGE_SAFETY\") {\n                    return reporter.sendError(err(\"The response candidate content was flagged for image safety reasons.\"));\n                }\n            }\n            retries--;\n        }\n        return reporter.sendError({ $error });\n    }\n    finally {\n        reporter.close();\n    }\n}\nfunction maybeExtractError(e) {\n    try {\n        const parsed = JSON.parse(e);\n        return parsed.error.message;\n    }\n    catch {\n        return e;\n    }\n}\nfunction isEmptyLLMContent(content) {\n    if (!content || !content.parts || content.parts.length === 0)\n        return true;\n    return content.parts.every((part) => {\n        if (\"text\" in part) {\n            return !part.text?.trim();\n        }\n        return true;\n    });\n}\nfunction addModality(body, modality) {\n    if (!modality)\n        return;\n    switch (modality) {\n        case \"Text\":\n            // No change, defaults.\n            break;\n        case \"Text and Image\":\n            body.generationConfig ?? (body.generationConfig = {});\n            body.generationConfig.responseModalities = [\"TEXT\", \"IMAGE\"];\n            break;\n        case \"Audio\":\n            body.generationConfig ?? (body.generationConfig = {});\n            body.generationConfig.responseModalities = [\"AUDIO\"];\n            break;\n    }\n}\nfunction constructBody(context = [], systemInstruction, prompt, modality) {\n    const contents = [...context];\n    if (!isEmptyLLMContent(prompt)) {\n        contents.push(prompt);\n    }\n    const body = {\n        contents,\n        safetySettings: defaultSafetySettings(),\n    };\n    const canHaveSystemInstruction = modality === \"Text\";\n    if (!isEmptyLLMContent(systemInstruction) && canHaveSystemInstruction) {\n        body.systemInstruction = systemInstruction;\n    }\n    addModality(body, modality);\n    return body;\n}\nfunction augmentBody(body, systemInstruction, prompt, modality) {\n    if (!body.systemInstruction || !isEmptyLLMContent(systemInstruction)) {\n        body.systemInstruction = systemInstruction;\n    }\n    if (!isEmptyLLMContent(prompt)) {\n        body.contents = [...body.contents, prompt];\n    }\n    addModality(body, modality);\n    return body;\n}\nfunction validateInputs(inputs) {\n    if (\"body\" in inputs) {\n        return;\n    }\n    if (inputs.context) {\n        const { context } = inputs;\n        if (!Array.isArray(context)) {\n            return err(\"Incoming context must be an array.\");\n        }\n        if (!isLLMContentArray(context)) {\n            return err(\"Malformed incoming context\");\n        }\n        return;\n    }\n    return err(\"Either body or context is required\");\n}\nasync function invoke(inputs) {\n    const validatingInputs = validateInputs(inputs);\n    if (!ok(validatingInputs)) {\n        return validatingInputs;\n    }\n    let { model } = inputs;\n    if (!model) {\n        model = MODELS[0];\n    }\n    const { context, systemInstruction, prompt, modality, body, $metadata } = inputs;\n    // TODO: Make this configurable.\n    const retries = 5;\n    if (!(\"body\" in inputs)) {\n        // Public API is being used.\n        // Behave as if we're wired in.\n        const result = await callAPI(retries, model, constructBody(context, systemInstruction, prompt, modality));\n        if (!ok(result)) {\n            return result;\n        }\n        const content = result.candidates.at(0)?.content;\n        if (!content) {\n            return err(\"Unable to get a good response from Gemini\");\n        }\n        return { context: [...context, content] };\n    }\n    else {\n        // Private API is being used.\n        // Behave as if we're being invoked.\n        return callAPI(retries, model, augmentBody(body, systemInstruction, prompt, modality), $metadata);\n    }\n}\nasync function describe({ inputs }) {\n    const canHaveModalities = inputs.model === \"gemini-2.0-flash-exp\";\n    const canHaveSystemInstruction = !canHaveModalities || (canHaveModalities && inputs.modality == \"Text\");\n    const maybeAddSystemInstruction = canHaveSystemInstruction\n        ? {\n            systemInstruction: {\n                type: \"object\",\n                behavior: [\"llm-content\", \"config\"],\n                title: \"System Instruction\",\n                default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n                description: \"(Optional) Give the model additional context on what to do,\" +\n                    \"like specific rules/guidelines to adhere to or specify behavior\" +\n                    \"separate from the provided context\",\n            },\n        }\n        : {};\n    const maybeAddModalities = canHaveModalities\n        ? {\n            modality: {\n                type: \"string\",\n                enum: [...VALID_MODALITIES],\n                title: \"Output Modality\",\n                behavior: [\"config\"],\n                description: \"(Optional) Tell the model what kind of output you're looking for.\",\n            },\n        }\n        : {};\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                model: {\n                    type: \"string\",\n                    behavior: [\"config\"],\n                    title: \"Model Name\",\n                    enum: MODELS,\n                    default: MODELS[0],\n                },\n                prompt: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\"],\n                    title: \"Prompt\",\n                    default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n                    description: \"(Optional) A prompt. Will be added to the end of the the conversation context.\",\n                },\n                ...maybeAddSystemInstruction,\n                ...maybeAddModalities,\n                context: {\n                    type: \"array\",\n                    items: {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                    },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                    },\n                    title: \"Context out\",\n                },\n            },\n        },\n        metadata: {\n            icon: \"generative\",\n        },\n    };\n}\n//# sourceMappingURL=gemini.js.map"
    },
    "html-generator": {
      "code": "/**\n * @fileoverview Utility for calling generate_webpage tool.\n */\nimport { executeStep } from \"./step-executor\";\nimport { decodeBase64, err, ok, toLLMContent, toLLMContentInline, } from \"./utils\";\nexport { callGenWebpage };\nconst OUTPUT_KEY = \"rendered_outputs\";\nasync function callGenWebpage(instruction, content, renderMode, modelName) {\n    const executionInputs = {};\n    const inputParameters = [];\n    let i = 0;\n    for (const val of content) {\n        for (const part of val.parts) {\n            i++;\n            if (\"text\" in part) {\n                const key = `text_${i}`;\n                inputParameters.push(key);\n                const encodedText = btoa(unescape(encodeURIComponent(part.text)));\n                executionInputs[key] = {\n                    chunks: [\n                        {\n                            mimetype: \"text/plain\",\n                            data: encodedText,\n                        },\n                    ],\n                };\n            }\n            else if (\"inlineData\" in part) {\n                const key = `media_${i}`;\n                inputParameters.push(key);\n                executionInputs[key] = {\n                    chunks: [\n                        {\n                            mimetype: part.inlineData.mimeType,\n                            data: part.inlineData.data,\n                        },\n                    ],\n                };\n            }\n            else if (\"storedData\" in part) {\n                const key = `media_${i}`;\n                inputParameters.push(key);\n                let handle = part.storedData.handle;\n                if (handle.startsWith(\"drive:/\")) {\n                    const driveId = handle.replace(/^drive:\\/+/, \"\");\n                    handle = `https://drive.google.com/file/d/${driveId}/preview`;\n                }\n                executionInputs[key] = {\n                    chunks: [\n                        {\n                            mimetype: \"url/\" + part.storedData.mimeType,\n                            data: btoa(unescape(encodeURIComponent(handle))),\n                        },\n                    ],\n                };\n            }\n            else {\n                console.error(\"Skipping unexpected content part\");\n            }\n        }\n    }\n    const body = {\n        planStep: {\n            stepName: \"GenerateWebpage\",\n            modelApi: \"generate_webpage\",\n            inputParameters: inputParameters,\n            systemPrompt: \"\",\n            stepIntent: \"\",\n            output: OUTPUT_KEY,\n            options: {\n                disablePromptRewrite: true,\n                renderMode: renderMode,\n                modelName: modelName,\n                systemInstruction: instruction,\n            },\n        },\n        execution_inputs: executionInputs,\n    };\n    // Add the contents\n    // TODO(askerryryan): Remove once functional.\n    console.log(\"request body\");\n    console.log(body);\n    const response = await executeStep(body);\n    // TODO(askerryryan): Remove once functional.\n    console.log(\"response\");\n    console.log(response);\n    if (!ok(response)) {\n        let errorMessage;\n        if (response.$error.includes(\"The service is currently unavailable\")) {\n            errorMessage =\n                \"Request timed out. The model may be experiencing heavy load\";\n        }\n        else {\n            errorMessage = response.$error;\n        }\n        return err(\"Webpage generation failed: \" + errorMessage);\n    }\n    let returnVal;\n    const outputChunk = response.executionOutputs[OUTPUT_KEY];\n    if (!outputChunk) {\n        return err(\"Error: Malformed response. No page generated.\");\n    }\n    const mimetype = outputChunk.chunks[0].mimetype;\n    const base64Data = outputChunk.chunks[0].data;\n    const data = decodeBase64(base64Data);\n    if (mimetype == \"text/html\") {\n        returnVal = toLLMContentInline(mimetype, data);\n    }\n    else {\n        returnVal = toLLMContent(data);\n    }\n    if (!returnVal) {\n        return err(\"Error: No webpage returned from backend\");\n    }\n    return returnVal;\n}\n//# sourceMappingURL=html-generator.js.map"
    },
    "image-editor": {
      "code": "/**\n * @fileoverview Generates an image using supplied context (generation and editing).\n */\nimport { GeminiPrompt } from \"./gemini-prompt\";\nimport { callGeminiImage } from \"./image-utils\";\nimport { ArgumentNameGenerator } from \"./introducer\";\nimport { ListExpander } from \"./lists\";\nimport { report } from \"./output\";\nimport { Template } from \"./template\";\nimport { ToolManager } from \"./tool-manager\";\nimport { addUserTurn, defaultLLMContent, err, extractMediaData, extractTextData, joinContent, llm, mergeContent, ok, toLLMContent, toText, toTextConcat, } from \"./utils\";\nconst MAKE_IMAGE_ICON = \"generative-image\";\nconst ASPECT_RATIOS = [\"1:1\", \"9:16\", \"16:9\", \"4:3\", \"3:4\"];\nexport { invoke as default, describe };\nfunction gatheringRequest(contents, instruction, toolManager) {\n    const promptText = llm `\nAnalyze the instruction below and rather than following it, determine what information needs to be gathered to \ngenerate an accurate prompt for a text-to-image model in the next turn:\n-- begin instruction --\n${instruction}\n-- end instruction --\n\nCall the tools to gather the necessary information that could be used to create an accurate prompt.`;\n    return new GeminiPrompt({\n        body: {\n            contents: addUserTurn(promptText.asContent(), contents),\n            tools: toolManager.list(),\n            systemInstruction: toLLMContent(`\nYou are a researcher whose specialty is to call tools whose output helps gather the necessary information\nto be used to create an accurate prompt for a text-to-image model.\n`),\n        },\n    }, toolManager);\n}\nfunction gracefulExit(notOk) {\n    report({\n        actor: \"Make Image\",\n        category: \"Warning\",\n        name: \"Graceful exit\",\n        details: `I tried a couple of times, but the Gemini API failed to generate the image you requested with the following error:\n\n### ${notOk.$error}\n\nTo keep things moving, I will return a blank result. My apologies!`,\n        icon: MAKE_IMAGE_ICON,\n    });\n    return toLLMContent(\" \");\n}\nconst MAX_RETRIES = 5;\nasync function invoke({ context: incomingContext, instruction, \"p-disable-prompt-rewrite\": disablePromptRewrite, \"p-aspect-ratio\": aspectRatio, ...params }) {\n    incomingContext ?? (incomingContext = []);\n    if (!instruction) {\n        instruction = toLLMContent(\"\");\n    }\n    if (!aspectRatio) {\n        aspectRatio = \"1:1\";\n    }\n    let imageContext = extractMediaData(incomingContext);\n    const textContext = extractTextData(incomingContext);\n    // Substitute params in instruction.\n    const toolManager = new ToolManager(new ArgumentNameGenerator());\n    const substituting = await new Template(instruction).substitute(params, async ({ path: url, instance }) => toolManager.addTool(url, instance));\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    const fanningOut = await new ListExpander(substituting, incomingContext).map(async (instruction, context) => {\n        // If there are tools in instruction, add an extra step of preparing\n        // information via tools.\n        if (toolManager.hasTools()) {\n            const gatheringInformation = await gatheringRequest(context, instruction, toolManager).invoke();\n            if (!ok(gatheringInformation))\n                return gatheringInformation;\n            context.push(...gatheringInformation.all);\n        }\n        const refImages = extractMediaData([instruction]);\n        const refText = instruction\n            ? toLLMContent(toTextConcat(extractTextData([instruction])))\n            : toLLMContent(\"\");\n        imageContext = imageContext.concat(refImages);\n        let retryCount = MAX_RETRIES;\n        while (retryCount--) {\n            // Image editing case.\n            if (imageContext.length > 0) {\n                console.log(\"Step has reference image, using Gemini Image API: i2i\");\n                const instructionText = refText ? toText(refText) : \"\";\n                const combinedInstruction = toTextConcat(joinContent(instructionText, textContext, false)).trim();\n                if (!combinedInstruction) {\n                    return err(`An image editing instruction must be provided along side the reference image.`);\n                }\n                const finalInstruction = combinedInstruction + \"\\nAspect ratio: \" + aspectRatio;\n                console.log(\"PROMPT: \" + finalInstruction);\n                const generatedImage = await callGeminiImage(finalInstruction, imageContext, disablePromptRewrite, aspectRatio);\n                if (!ok(generatedImage))\n                    return generatedImage;\n                return mergeContent(generatedImage, \"model\");\n            }\n            else {\n                console.log(\"Step has text only, using Gemini Image API: t2i\");\n                const imagePrompt = toLLMContent(toText(addUserTurn(refText, context)));\n                const iPrompt = toText(imagePrompt).trim();\n                console.log(\"PROMPT\", iPrompt);\n                const generatedImage = await callGeminiImage(iPrompt, [], disablePromptRewrite, aspectRatio);\n                if (!ok(generatedImage))\n                    return generatedImage;\n                return mergeContent(generatedImage, \"model\");\n            }\n        }\n        return gracefulExit(err(`Failed to generate an image after ${MAX_RETRIES} tries.`));\n    });\n    if (!ok(fanningOut))\n        return fanningOut;\n    return { context: fanningOut };\n}\nasync function describe({ inputs: { instruction } }) {\n    const template = new Template(instruction);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                instruction: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"Instruction\",\n                    description: \"Describe how to generate the image (content, style, etc). Use @ to reference params or outputs from other steps.\",\n                    default: defaultLLMContent(),\n                },\n                \"p-disable-prompt-rewrite\": {\n                    type: \"boolean\",\n                    title: \"Disable prompt expansion\",\n                    behavior: [\"config\", \"hint-advanced\"],\n                    description: \"By default, inputs and instructions will be automatically expanded into a high quality image prompt. Check to disable this re-writing behavior.\",\n                },\n                \"p-aspect-ratio\": {\n                    type: \"string\",\n                    behavior: [\"hint-text\", \"config\", \"hint-advanced\"],\n                    title: \"Aspect Ratio\",\n                    enum: ASPECT_RATIOS,\n                    description: \"The aspect ratio of the generated image\",\n                    default: \"1:1\",\n                },\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"hint-image\", \"main-port\"],\n                },\n            },\n        },\n        title: \"Edit Image\",\n        metadata: {\n            icon: MAKE_IMAGE_ICON,\n            tags: [\"quick-access\", \"generative\"],\n            order: 2,\n        },\n    };\n}\n//# sourceMappingURL=image-editor.js.map"
    },
    "image-generator": {
      "code": "/**\n * @fileoverview Generates an image using supplied context (generation only).\n */\nimport { GeminiPrompt } from \"./gemini-prompt\";\nimport { callImageGen, promptExpander } from \"./image-utils\";\nimport { ArgumentNameGenerator } from \"./introducer\";\nimport { ListExpander } from \"./lists\";\nimport { report } from \"./output\";\nimport { Template } from \"./template\";\nimport { ToolManager } from \"./tool-manager\";\nimport { addUserTurn, defaultLLMContent, err, extractMediaData, extractTextData, llm, mergeContent, ok, toLLMContent, toText, toTextConcat, } from \"./utils\";\nconst MAKE_IMAGE_ICON = \"generative-image\";\nconst ASPECT_RATIOS = [\"1:1\", \"9:16\", \"16:9\", \"4:3\", \"3:4\"];\nexport { invoke as default, describe };\nfunction gatheringRequest(contents, instruction, toolManager) {\n    const promptText = llm `\nAnalyze the instruction below and rather than following it, determine what information needs to be gathered to \ngenerate an accurate prompt for a text-to-image model in the next turn:\n-- begin instruction --\n${instruction}\n-- end instruction --\n\nCall the tools to gather the necessary information that could be used to create an accurate prompt.`;\n    return new GeminiPrompt({\n        body: {\n            contents: addUserTurn(promptText.asContent(), contents),\n            tools: toolManager.list(),\n            systemInstruction: toLLMContent(`\nYou are a researcher whose specialty is to call tools whose output helps gather the necessary information\nto be used to create an accurate prompt for a text-to-image model.\n`),\n        },\n    }, toolManager);\n}\nfunction gracefulExit(notOk) {\n    report({\n        actor: \"Make Image\",\n        category: \"Warning\",\n        name: \"Graceful exit\",\n        details: `I tried a couple of times, but the Gemini API failed to generate the image you requested with the following error:\n\n### ${notOk.$error}\n\nTo keep things moving, I will return a blank result. My apologies!`,\n        icon: MAKE_IMAGE_ICON,\n    });\n    return toLLMContent(\" \");\n}\nconst MAX_RETRIES = 5;\nasync function invoke({ context: incomingContext, instruction, \"p-disable-prompt-rewrite\": disablePromptRewrite, \"p-aspect-ratio\": aspectRatio, ...params }) {\n    incomingContext ?? (incomingContext = []);\n    if (!instruction) {\n        instruction = toLLMContent(\"\");\n    }\n    if (!aspectRatio) {\n        aspectRatio = \"1:1\";\n    }\n    let imageContext = extractMediaData(incomingContext);\n    // Substitute params in instruction.\n    const toolManager = new ToolManager(new ArgumentNameGenerator());\n    const substituting = await new Template(instruction).substitute(params, async ({ path: url, instance }) => toolManager.addTool(url, instance));\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    const fanningOut = await new ListExpander(substituting, incomingContext).map(async (instruction, context) => {\n        // If there are tools in instruction, add an extra step of preparing\n        // information via tools.\n        if (toolManager.hasTools()) {\n            const gatheringInformation = await gatheringRequest(context, instruction, toolManager).invoke();\n            if (!ok(gatheringInformation))\n                return gatheringInformation;\n            context.push(...gatheringInformation.all);\n        }\n        const refImages = extractMediaData([instruction]);\n        const refText = instruction\n            ? toLLMContent(toTextConcat(extractTextData([instruction])))\n            : toLLMContent(\"\");\n        imageContext = imageContext.concat(refImages);\n        let retryCount = MAX_RETRIES;\n        while (retryCount--) {\n            if (imageContext.length > 0) {\n                return err(`References images are not supported with Imagen. For image editing or style transfer, try Gemini Image Generation.`);\n            }\n            else {\n                console.log(\"Step has text only, using generation API\");\n                let imagePrompt;\n                if (disablePromptRewrite) {\n                    imagePrompt = toLLMContent(toText(addUserTurn(refText, context)));\n                }\n                else {\n                    const generatingPrompt = await promptExpander(context, refText).invoke();\n                    if (!ok(generatingPrompt))\n                        return generatingPrompt;\n                    imagePrompt = generatingPrompt.last;\n                }\n                const iPrompt = toText(imagePrompt).trim();\n                console.log(\"PROMPT\", iPrompt);\n                const generatedImage = await callImageGen(iPrompt, aspectRatio);\n                if (!ok(generatedImage))\n                    return generatedImage;\n                return mergeContent(generatedImage, \"model\");\n            }\n        }\n        return gracefulExit(err(`Failed to generate an image after ${MAX_RETRIES} tries.`));\n    });\n    if (!ok(fanningOut))\n        return fanningOut;\n    return { context: fanningOut };\n}\nasync function describe({ inputs: { instruction } }) {\n    const template = new Template(instruction);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                instruction: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"Instruction\",\n                    description: \"Describe how to generate the image (content, style, etc). Use @ to reference params or outputs from other steps.\",\n                    default: defaultLLMContent(),\n                },\n                \"p-disable-prompt-rewrite\": {\n                    type: \"boolean\",\n                    title: \"Disable prompt expansion\",\n                    behavior: [\"config\", \"hint-advanced\"],\n                    description: \"By default, inputs and instructions will be automatically expanded into a high quality image prompt. Check to disable this re-writing behavior.\",\n                },\n                \"p-aspect-ratio\": {\n                    type: \"string\",\n                    behavior: [\"hint-text\", \"config\", \"hint-advanced\"],\n                    title: \"Aspect Ratio\",\n                    enum: ASPECT_RATIOS,\n                    description: \"The aspect ratio of the generated image\",\n                    default: \"1:1\",\n                },\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"hint-image\", \"main-port\"],\n                },\n            },\n        },\n        title: \"Make Image\",\n        metadata: {\n            icon: MAKE_IMAGE_ICON,\n            tags: [\"quick-access\", \"generative\"],\n            order: 2,\n        },\n    };\n}\n//# sourceMappingURL=image-generator.js.map"
    },
    "image-utils": {
      "code": "/**\n * @fileoverview Utilities for generating images.\n */\nimport { GeminiPrompt } from \"./gemini-prompt\";\nimport { executeStep, } from \"./step-executor\";\nimport { addUserTurn, err, isStoredData, llm, ok, toInlineData, toInlineReference, toLLMContent, toLLMContentInline, toLLMContentStored, } from \"./utils\";\nexport { callGeminiImage, callImageGen, promptExpander };\nconst STEP_NAME = \"AI Image Tool\";\nconst OUTPUT_NAME = \"generated_image\";\nconst API_NAME = \"ai_image_tool\";\nasync function callGeminiImage(instruction, imageContent, disablePromptRewrite, aspectRatio = \"1:1\") {\n    const imageChunks = [];\n    for (const element of imageContent) {\n        let inlineChunk;\n        if (isStoredData(element)) {\n            inlineChunk = toInlineReference(element);\n        }\n        else {\n            inlineChunk = toInlineData(element);\n        }\n        if (inlineChunk && inlineChunk != null && typeof inlineChunk != \"string\") {\n            imageChunks.push({\n                mimetype: inlineChunk.mimeType,\n                data: inlineChunk.data,\n            });\n        }\n    }\n    const input_parameters = [\"input_instruction\"];\n    if (imageChunks.length > 0) {\n        input_parameters.push(\"input_image\");\n    }\n    console.log(\"Number of input images: \" + String(imageChunks.length));\n    const encodedInstruction = btoa(unescape(encodeURIComponent(instruction)));\n    const executionInputs = {\n        input_instruction: {\n            chunks: [\n                {\n                    mimetype: \"text/plain\",\n                    data: encodedInstruction,\n                },\n            ],\n        },\n        aspect_ratio_key: {\n            chunks: [\n                {\n                    mimetype: \"text/plain\",\n                    data: btoa(aspectRatio),\n                },\n            ],\n        },\n    };\n    if (imageChunks.length > 0) {\n        executionInputs[\"input_image\"] = {\n            chunks: imageChunks,\n        };\n    }\n    const body = {\n        planStep: {\n            stepName: STEP_NAME,\n            modelApi: API_NAME,\n            inputParameters: input_parameters,\n            systemPrompt: \"\",\n            options: {\n                disablePromptRewrite: disablePromptRewrite,\n            },\n            output: OUTPUT_NAME,\n        },\n        execution_inputs: executionInputs,\n    };\n    // TODO(askerryryan): Remove once functional.\n    console.log(\"request body\");\n    console.log(body);\n    const response = await executeStep(body);\n    // TODO(askerryryan): Remove once functional.\n    console.log(\"response\");\n    console.log(response);\n    if (!ok(response)) {\n        return err(\"Image generation failed. \" +\n            response.$error +\n            \" Check your prompt to ensure it is a valid and compliant image prompt.\");\n    }\n    const outContent = response.executionOutputs && response.executionOutputs[OUTPUT_NAME];\n    if (!outContent) {\n        return err(\"Error: No image returned from backend\");\n    }\n    return outContent.chunks.map((c) => {\n        if (c.mimetype.endsWith(\"/storedData\")) {\n            return toLLMContentStored(c.mimetype.replace(\"/storedData\", \"\"), c.data);\n        }\n        return toLLMContentInline(c.mimetype, c.data);\n    });\n}\nasync function callImageGen(imageInstruction, aspectRatio = \"1:1\") {\n    const executionInputs = {};\n    const encodedInstruction = btoa(unescape(encodeURIComponent(imageInstruction)));\n    executionInputs[\"image_prompt\"] = {\n        chunks: [\n            {\n                mimetype: \"text/plain\",\n                data: encodedInstruction,\n            },\n        ],\n    };\n    executionInputs[\"aspect_ratio_key\"] = {\n        chunks: [\n            {\n                mimetype: \"text/plain\",\n                data: btoa(aspectRatio),\n            },\n        ],\n    };\n    const inputParameters = [\"image_prompt\"];\n    const body = {\n        planStep: {\n            stepName: \"GenerateImage\",\n            modelApi: \"image_generation\",\n            inputParameters: inputParameters,\n            systemPrompt: \"\",\n            output: OUTPUT_NAME,\n        },\n        execution_inputs: executionInputs,\n    };\n    const response = await executeStep(body);\n    console.log(response);\n    if (!ok(response)) {\n        return err(\"Image generation failed. \" + response.$error);\n    }\n    const outContent = response.executionOutputs && response.executionOutputs[OUTPUT_NAME];\n    if (!outContent) {\n        return err(\"Error: No image returned from backend\");\n    }\n    return outContent.chunks.map((c) => {\n        if (c.mimetype.endsWith(\"/storedData\")) {\n            return toLLMContentStored(c.mimetype.replace(\"/storedData\", \"\"), c.data);\n        }\n        return toLLMContentInline(c.mimetype, c.data);\n    });\n}\nfunction promptExpander(contents, instruction) {\n    const context = contents?.length\n        ? \"using conversation context and these additional\"\n        : \"with these\";\n    const promptText = llm `Generate a single text-to-image prompt ${context} instructions:\n${instruction}\n\nTypical output format:\n\"\"\"\nCreate the following image:\n\n## Setting/background\n\n<Detailed description of everything that is in the background of the image.>\n\n## Foreground/focus\n\n<Detailed description of object and/or shapes that are in the foreground and are the main focal point of the image. Include the composition and layout of the image.>\n\n## Style\n\n<Detailed description of the style, color scheme, vibe, kind of drawing (illustration, photorealistic, etc.)>\n\nYou output will be fed directly into the text-to-image model, so it must be a prompt only, no additional chit-chat\n\"\"\"\n`;\n    return new GeminiPrompt({\n        body: {\n            contents: addUserTurn(promptText.asContent(), contents),\n            systemInstruction: toLLMContent(`\nYou are a creative writer whose specialty is to write prompts for text-to-image models.\n\nThe prompt must describe every object in the image in great detail and describe the style\nin terms of color scheme and vibe. Be sure to respect all user provided instructions.\n`),\n        },\n    });\n}\n//# sourceMappingURL=image-utils.js.map"
    },
    "introducer": {
      "code": "/**\n * @fileoverview Handles introduction of the step.\n */\nvar __classPrivateFieldGet = (this && this.__classPrivateFieldGet) || function (receiver, state, kind, f) {\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a getter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot read private member from an object whose class did not declare it\");\n    return kind === \"m\" ? f : kind === \"a\" ? f.call(receiver) : f ? f.value : state.get(receiver);\n};\nvar _ArgumentNameGenerator_instances, _ArgumentNameGenerator_containsContext;\nimport { defaultSafetySettings } from \"./gemini\";\nimport { GeminiPrompt } from \"./gemini-prompt\";\nimport { err, llm, ok } from \"./utils\";\nexport { ArgumentNameGenerator };\n/**\n * Attempts to adjust the describer result for subgraphs.\n * Accounts for LLMContent[] `context` property\n * and parameters\n */\nclass ArgumentNameGenerator {\n    constructor() {\n        _ArgumentNameGenerator_instances.add(this);\n    }\n    async transform(describerResult) {\n        // If there's no `context` property, exit early.\n        if (!__classPrivateFieldGet(this, _ArgumentNameGenerator_instances, \"m\", _ArgumentNameGenerator_containsContext).call(this, describerResult)) {\n            return null;\n        }\n        const { title, description } = describerResult;\n        // Fail transform when there's no title or description.\n        // The resulting function declaration will be a dud anyway.\n        if (!title || !description) {\n            return err(`Custom tool must have a title and a description`);\n        }\n        // Add parameters to the describer.\n        const required = [];\n        const params = Object.fromEntries(Object.entries(describerResult.inputSchema?.properties || {})\n            .filter(([name]) => {\n            if (name === \"context\")\n                return false;\n            required.push(name);\n            return true;\n        })\n            .map(([name, value]) => {\n            return [\n                name,\n                {\n                    ...value,\n                    type: \"string\",\n                },\n            ];\n        }));\n        if (required.length > 0) {\n            return {\n                ...describerResult,\n                inputSchema: {\n                    type: \"object\",\n                    properties: params,\n                    required,\n                },\n            };\n        }\n        // When no parameters found, try to discern the parameter name\n        // from description and title.\n        const naming = await new GeminiPrompt({\n            body: {\n                contents: [this.prompt(describerResult)],\n                safetySettings: defaultSafetySettings(),\n                generationConfig: {\n                    responseSchema: this.schema(),\n                    responseMimeType: \"application/json\",\n                },\n            },\n        }).invoke();\n        if (!ok(naming))\n            return naming;\n        const result = naming.last.parts.at(0).json;\n        return {\n            ...describerResult,\n            inputSchema: {\n                type: \"object\",\n                properties: {\n                    context: {\n                        type: \"string\",\n                        description: result.description,\n                    },\n                },\n            },\n        };\n    }\n    schema() {\n        return {\n            type: \"object\",\n            properties: {\n                description: {\n                    type: \"string\",\n                    description: \"One-sentence description of a function argument\",\n                },\n            },\n        };\n    }\n    prompt(describerResult) {\n        return llm `\nYou are amazing at describing things. Today, you will be coming up a one-sentence description \nof a function argument.\n\nThe function's title is: ${describerResult.title}\n\nThe function's description is ${describerResult.description}\n\nIt takes a single argument.\n\nCome up with a one-sentence description of this argument based on the title/description,\nwith the aim of using this description in a JSON Schema.\n`.asContent();\n    }\n}\n_ArgumentNameGenerator_instances = new WeakSet(), _ArgumentNameGenerator_containsContext = function _ArgumentNameGenerator_containsContext(describerResult) {\n    if (!describerResult.inputSchema?.properties)\n        return true;\n    const context = describerResult.inputSchema.properties[\"context\"];\n    if (!context)\n        return false;\n    if (context.type === \"array\" && context.items) {\n        return !!context.items.behavior?.includes(\"llm-content\");\n    }\n    return false;\n};\n//# sourceMappingURL=introducer.js.map"
    },
    "lists": {
      "code": "var __classPrivateFieldGet = (this && this.__classPrivateFieldGet) || function (receiver, state, kind, f) {\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a getter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot read private member from an object whose class did not declare it\");\n    return kind === \"m\" ? f : kind === \"a\" ? f.call(receiver) : f ? f.value : state.get(receiver);\n};\nvar __classPrivateFieldSet = (this && this.__classPrivateFieldSet) || function (receiver, state, value, kind, f) {\n    if (kind === \"m\") throw new TypeError(\"Private method is not writable\");\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a setter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot write private member to an object whose class did not declare it\");\n    return (kind === \"a\" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value)), value;\n};\nvar _ListExpander_instances, _ListExpander_prolog, _ListExpander_list, _ListExpander_originalListItems, _ListExpander_id, _ListExpander_instructions, _ListExpander_expanded, _ListExpander_toContext;\n/**\n * @fileoverview Handles lists according to https://github.com/breadboard-ai/breadboard/wiki/Step-Listification\n */\nimport { ok, err, generateId, mergeTextParts, toLLMContent, llm, } from \"./utils\";\nexport { fanOutContext, flattenContext, hasLists, addContent, toList, listPrompt, listSchema, ListExpander, };\nfunction isListPart(o) {\n    return !!o && \"list\" in o;\n}\nfunction emptyContent() {\n    return { parts: [{ text: \"\" }] };\n}\nfunction addContent(context, content) {\n    const last = context.at(-1);\n    const maybeList = last?.parts?.at(0);\n    const remainder = context.slice(0, -1);\n    if (isListPart(maybeList)) {\n        const list = maybeList.list.map((item) => ({\n            ...item,\n            content: [...item.content, content],\n        }));\n        return [...remainder, { ...last, parts: [{ ...maybeList, list }] }];\n    }\n    return [...context, content];\n}\nfunction unzipContent(content) {\n    // 1) Scan content for lists.\n    const info = new Set();\n    const ids = new Set();\n    let maxLength = 0;\n    content.parts.forEach((part, index) => {\n        if (!isListPart(part))\n            return;\n        const length = part.list.length;\n        if (length > maxLength)\n            maxLength = length;\n        info.add(index);\n        ids.add(part.id);\n    });\n    if (ids.size > 1) {\n        console.warn(\"Multiple list sources aren't yet supported in instruction, using the first one\");\n    }\n    const id = [...ids].at(0) || \"\";\n    if (info.size === 0) {\n        return { contents: [content], id };\n    }\n    // 2) Create a list and replace lists with list entries.\n    return {\n        id,\n        contents: new Array(maxLength).fill(0).map((_, entryIndex) => {\n            const parts = mergeTextParts(content.parts.flatMap((part, partIndex) => {\n                if (!info.has(partIndex))\n                    return part;\n                // We know this exists, so we're ok with not checking\n                // for existence.\n                const item = part.list.at(entryIndex);\n                if (!item) {\n                    return [];\n                }\n                const last = item.content.at(-1);\n                return last ? last.parts : [];\n            }));\n            return { ...content, parts };\n        }),\n    };\n}\nfunction hasLists(context) {\n    return isListPart(context.at(-1)?.parts?.at(0));\n}\n// TODO: When back to implementing nested lists, this approach\n// is wrong. Instead of expanding items and then converging them,\n// we need to iterate over existing structure. Because then, when\n// it's time to converge, we don't need to try to reconstruct\n// the original structure.\n// Also, it is unclear what we should do with nested lists\n// in instruction. It feels like a similar story, and now\n// we need to support multiple lists and hierarchies in\n// instruction.\nclass ListExpander {\n    constructor(instruction, context, prolog = []) {\n        _ListExpander_instances.add(this);\n        this.instruction = instruction;\n        this.context = context;\n        this.prolog = prolog;\n        // Local prolog -- the parts of context that were preceding the list.\n        _ListExpander_prolog.set(this, []);\n        _ListExpander_list.set(this, []);\n        _ListExpander_originalListItems.set(this, []);\n        _ListExpander_id.set(this, \"\");\n        _ListExpander_instructions.set(this, void 0);\n        _ListExpander_expanded.set(this, false);\n    }\n    expand() {\n        if (__classPrivateFieldGet(this, _ListExpander_expanded, \"f\"))\n            return;\n        const instructions = unzipContent(this.instruction);\n        let list = [];\n        let id;\n        const maybeList = this.context.at(-1)?.parts?.at(0);\n        const localProlog = this.context.slice(0, -1);\n        const originalListItems = [];\n        if (isListPart(maybeList)) {\n            id = maybeList.id;\n            // console.log(\"LIST PART FOUND\", id);\n            for (const [index, item] of maybeList.list.entries()) {\n                const innerContext = item.content;\n                const innerInstruction = instructions.contents.at(index) ||\n                    instructions.contents.at(0) ||\n                    emptyContent();\n                const innerExpander = new ListExpander(innerInstruction, innerContext, [\n                    ...this.prolog,\n                    ...localProlog,\n                ]);\n                innerExpander.expand();\n                // console.log(\"INDEX\", index);\n                // console.log(\"ITEM\", JSON.stringify(item.content));\n                // console.log(\"INNER EXPANDER\", innerExpander.list());\n                originalListItems.push(innerContext);\n                list.push(...innerExpander.list());\n            }\n            // console.log(\"END LIST PART FOUND\", id);\n        }\n        else {\n            id = instructions.id;\n            list = instructions.contents.map((instruction) => {\n                return {\n                    prolog: [...this.prolog, ...this.context],\n                    instruction,\n                    context: [],\n                };\n            });\n        }\n        __classPrivateFieldSet(this, _ListExpander_prolog, localProlog, \"f\");\n        __classPrivateFieldSet(this, _ListExpander_list, list, \"f\");\n        __classPrivateFieldSet(this, _ListExpander_id, id, \"f\");\n        __classPrivateFieldSet(this, _ListExpander_originalListItems, originalListItems, \"f\");\n        __classPrivateFieldSet(this, _ListExpander_expanded, true, \"f\");\n    }\n    list() {\n        return __classPrivateFieldGet(this, _ListExpander_list, \"f\");\n    }\n    async map(transformer) {\n        this.expand();\n        const isList = __classPrivateFieldGet(this, _ListExpander_list, \"f\").length > 1;\n        const results = await Promise.all(__classPrivateFieldGet(this, _ListExpander_list, \"f\").map(async (item, _index) => {\n            return transformer(item.instruction, [...item.prolog, ...item.context], isList);\n        }));\n        const errors = [];\n        const successes = [];\n        results.forEach((result) => {\n            if (!ok(result)) {\n                errors.push(result);\n            }\n            else {\n                successes.push(result);\n            }\n        });\n        if (errors.length > 0) {\n            return err(errors.map((error) => error.$error).join(\"\\n\"));\n        }\n        return __classPrivateFieldGet(this, _ListExpander_instances, \"m\", _ListExpander_toContext).call(this, successes);\n    }\n}\n_ListExpander_prolog = new WeakMap(), _ListExpander_list = new WeakMap(), _ListExpander_originalListItems = new WeakMap(), _ListExpander_id = new WeakMap(), _ListExpander_instructions = new WeakMap(), _ListExpander_expanded = new WeakMap(), _ListExpander_instances = new WeakSet(), _ListExpander_toContext = function _ListExpander_toContext(results) {\n    if (results.length > 1) {\n        const newListItem = {\n            parts: [\n                {\n                    id: __classPrivateFieldGet(this, _ListExpander_id, \"f\"),\n                    list: results.map((item, i) => ({\n                        content: [...(__classPrivateFieldGet(this, _ListExpander_originalListItems, \"f\")[i] || []), item],\n                    })),\n                },\n            ],\n        };\n        return [...__classPrivateFieldGet(this, _ListExpander_prolog, \"f\"), newListItem];\n    }\n    const newContextItem = results.at(-1);\n    return [...this.context, newContextItem];\n};\nasync function fanOutContext(instruction, context, transformer, _path) {\n    context ?? (context = []);\n    const expander = new ListExpander(instruction, context);\n    return expander.map(transformer);\n}\nfunction flattenContext(context, all = false, separator = \"\") {\n    context ?? (context = []); // Look at the first part of the last context and see if it's a list.\n    const last = context.at(-1);\n    if (!last)\n        return context;\n    if (all) {\n        return context\n            .map((content) => flattenContent(content, all, separator))\n            .flat();\n    }\n    const remainder = context.slice(0, -1);\n    return [...remainder, ...flattenContent(last, all, separator)];\n}\nfunction zipContexts(contexts, separator = \"\") {\n    let maxLength = 0;\n    contexts.forEach((context) => {\n        if (maxLength < context.length)\n            maxLength = context.length;\n    });\n    const result = [];\n    for (let i = 0; i < maxLength; i++) {\n        let role;\n        const zippedParts = [];\n        for (const context of contexts) {\n            const item = context.at(i);\n            if (!item)\n                continue;\n            if (!role)\n                role = item.role;\n            zippedParts.push(item.parts);\n            // Add separator if previous element was text.\n            const lastItem = item.parts.slice(-1)[0];\n            if (separator.length > 0 && lastItem && \"text\" in lastItem) {\n                zippedParts.push({ text: separator });\n            }\n        }\n        const parts = mergeTextParts(zippedParts.flat());\n        role ?? (role = \"user\");\n        result.push({\n            parts,\n            role,\n        });\n    }\n    return result;\n}\nfunction flattenContent(content, _all = false, separatator = \"\") {\n    let hadList = false;\n    const flattened = content.parts\n        .map((part) => {\n        if (isListPart(part)) {\n            hadList = true;\n            return zipContexts(part.list.map((item) => item.content), separatator);\n        }\n        return {\n            parts: [part],\n            role: content.role,\n        };\n    })\n        .flat();\n    if (!hadList)\n        return [content];\n    return flattened;\n}\nfunction toList(content) {\n    const jsonPart = content.parts.at(0);\n    if (!jsonPart || !(\"json\" in jsonPart)) {\n        // TODO: Error recovery\n        return err(`Gemini generated invalid list`);\n    }\n    const response = jsonPart.json;\n    return {\n        parts: [\n            {\n                id: generateId(),\n                list: response.list.map((item) => {\n                    return { content: [toLLMContent(item, \"model\")] };\n                }),\n            },\n        ],\n    };\n}\nfunction listSchema() {\n    return {\n        type: \"object\",\n        properties: {\n            list: {\n                type: \"array\",\n                description: \"The list of results\",\n                items: {\n                    type: \"string\",\n                    description: \"Result list item as markdown text\",\n                },\n            },\n        },\n        required: [\"list\"],\n    };\n}\nfunction listPrompt(content) {\n    return llm `\n  ${content}\n\n  Output as a list of items, each item must be markdown text.\n`.asContent();\n}\n//# sourceMappingURL=lists.js.map"
    },
    "make-code": {
      "code": "/**\n * @fileoverview Generates code using supplied context.\n */\nimport invokeBoard from \"@invoke\";\nimport gemini, { defaultSafetySettings } from \"./gemini\";\nimport { ArgumentNameGenerator } from \"./introducer\";\nimport { report } from \"./output\";\nimport { Template } from \"./template\";\nimport { ToolManager } from \"./tool-manager\";\nimport { addUserTurn, err, ok, toLLMContent, toText } from \"./utils\";\nconst MAKE_CODE_ICON = \"generative-code\";\nexport { invoke as default, describe };\nfunction gatheringRequest(contents, instruction, language, toolManager) {\n    const promptText = `\nAnalyze the instruction below and rather than following it, determine what information needs to be gathered to \ngenerate an accurate prompt for a text-to-${language} model in the next turn:\n-- begin instruction --\n${toText(instruction)}\n-- end instruction --\n\nCall the tools to gather the necessary information that could be used to create an accurate prompt.`;\n    return new GeminiPrompt({\n        model: \"gemini-1.5-flash-latest\",\n        body: {\n            contents: addUserTurn(promptText, contents),\n            tools: toolManager.list(),\n            systemInstruction: toLLMContent(`\nYou are a researcher whose specialty is to call tools whose output helps gather the necessary information\nto be used to create an accurate prompt for a text-to-${language} model.\n`),\n        },\n    }, toolManager);\n}\nfunction promptRequest(contents, instruction, language) {\n    const context = contents?.length\n        ? \"using conversation context and these additional\"\n        : \"with these\";\n    const promptText = `Generate a single text-to-${language} prompt ${context} instructions:\n${toText(instruction)}\n\nTypical output format:\n\n## Setting/background\n\nDetailed description of everything that is understood about the ${language} code that is being requested.\n\n## Primary focus\n\nDetailed description of the primary functionality or the main focal point of the JavaScript code.\n\n## Style\n\nDetailed description of the style and approach of the code (defensive, TDD, creative, etc.). The output should\nalways an invariably be ${language === \"JavaScript\" ? \"EcmaScript JavaScript Modules\" : language} and be fully functional \nwithout any placeholders. \n\nIf you are dealing with JavaScript you may use imports if and only if the instruction indicates, otherwise you must\ncreate the functionality as a standalone piece of EcmaScript JavaScript.\n\nYou output will be fed directly into the text-to-${language} model, so it must be prompt only, no additional chit-chat\n`;\n    return new GeminiPrompt({\n        model: \"gemini-1.5-flash-latest\",\n        body: {\n            contents: addUserTurn(promptText, contents),\n            systemInstruction: toLLMContent(`\nYou are a world-class ${language} developer whose specialty is to write prompts for text-to-${language} models that \nalways generate valid outputs.\n\nThe prompt must describe every aspect of the functionality in great detail and describe the problem being solved \nin terms of data structures, algorithms, and style. You must use the instruction to fully understand and replicate\nany reference implementations you've been given and you must not augment or deviate from that style. You should\nensure that the prompt includes enough information to fully replicate that style with examples and maximal clarity.\n\nIf the code pertains to user interface work, you must also maximize the accessibility of the code generated with\nappropriate titles and labels for buttons, controls, inputs, etc, and they should never be empty.\n\nBe sure to export all relevant symbols so that the code can be used outside of the EcmaScript Module. Always do\nthis as named symbols rather than using default exports.\n\nIf writing JavaScript, and where a variable is private, use private fields (#field) rather than an underscore at the start.\n`),\n        },\n    });\n}\nfunction codeRequest(prompt, language) {\n    prompt.role = \"user\";\n    prompt.parts.unshift({\n        text: `Generate ${language} code based on this prompt. Output code only, no chit-chat`,\n    });\n    return new GeminiPrompt({\n        model: \"gemini-2.0-flash-exp\",\n        body: {\n            contents: [prompt],\n            generationConfig: {\n                responseModalities: [\"TEXT\"],\n            },\n            safetySettings: defaultSafetySettings(),\n        },\n    });\n}\nclass GeminiPrompt {\n    constructor(inputs, toolManager) {\n        this.inputs = inputs;\n        this.toolManager = toolManager;\n    }\n    async invoke() {\n        const invoking = await gemini(this.inputs);\n        if (!ok(invoking))\n            return invoking;\n        if (\"context\" in invoking) {\n            return err(\"Invalid output from Gemini -- must be candidates\");\n        }\n        const content = invoking.candidates.at(0)?.content;\n        if (!content) {\n            return err(\"No content from Gemini\");\n        }\n        const results = [];\n        const errors = [];\n        await this.toolManager?.processResponse(content, async ($board, args) => {\n            const callingTool = await invokeBoard({ $board, ...args });\n            if (\"$error\" in callingTool) {\n                errors.push(JSON.stringify(callingTool.$error));\n            }\n            else {\n                results.push(JSON.stringify(callingTool));\n            }\n        });\n        if (errors.length) {\n            return err(`Calling tools generated the following errors: ${errors.join(\",\")}`);\n        }\n        const result = [content];\n        if (results.length) {\n            result.push(toLLMContent(results.join(\"\\n\\n\")));\n        }\n        return { all: result, last: result.at(-1) };\n    }\n}\nfunction gracefulExit(notOk) {\n    report({\n        actor: \"Make Code\",\n        category: \"Warning\",\n        name: \"Graceful exit\",\n        details: `I tried a couple of times, but the Gemini API failed to generate the code you requested with the following error:\n\n### ${notOk.$error}\n\nTo keep things moving, I will return a blank result. My apologies!`,\n        icon: MAKE_CODE_ICON,\n    });\n    return { context: [toLLMContent(\" \")] };\n}\nconst MAX_RETRIES = 5;\nasync function invoke({ context, instruction, language, ...params }) {\n    context ?? (context = []);\n    // 1) Substitute params in instruction.\n    const toolManager = new ToolManager(new ArgumentNameGenerator());\n    const substituting = await new Template(instruction).substitute(params, async ({ path: url, instance }) => toolManager.addTool(url, instance));\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    instruction = substituting;\n    // 2) If there are tools in instruction, add an extra step of preparing\n    // information via tools.\n    if (toolManager.hasTools()) {\n        const gatheringInformation = await gatheringRequest(context, instruction, language, toolManager).invoke();\n        if (!ok(gatheringInformation))\n            return gatheringInformation;\n        context.push(...gatheringInformation.all);\n    }\n    let retryCount = MAX_RETRIES;\n    while (retryCount--) {\n        // 3) Call Gemini to generate prompt.\n        const generatingPrompt = await promptRequest(context, instruction, language).invoke();\n        if (!ok(generatingPrompt))\n            return generatingPrompt;\n        console.log(\"PROMPT\", toText(generatingPrompt.last));\n        // 4) Call Gemini to generate image.\n        const generatingCode = await codeRequest(generatingPrompt.last, language).invoke();\n        if (!ok(generatingCode)) {\n            return generatingCode;\n        }\n        return { context: generatingCode.all };\n    }\n    return gracefulExit(err(`Failed to generate ${language} after ${MAX_RETRIES} tries.`));\n}\nasync function describe({ inputs: { instruction } }) {\n    const template = new Template(instruction);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                instruction: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"Instruction\",\n                    description: \"Describe how to generate the JavaScript based on the input: focus, functionality, aim of the code\",\n                },\n                language: {\n                    type: \"string\",\n                    behavior: [\"hint-text\", \"config\", \"hint-preview\"],\n                    title: \"Language\",\n                    icon: \"frame-source\",\n                    enum: [\"JavaScript\", \"HTML\", \"CSS\"],\n                    description: \"The language you'd like to generate\",\n                    default: \"JavaScript\",\n                },\n                ...template.schemas(),\n            },\n            ...template.requireds(),\n            additionalProperties: false,\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"hint-code\", \"main-port\"],\n                },\n            },\n            additionalProperties: false,\n        },\n        title: \"Make Code\",\n        metadata: {\n            icon: MAKE_CODE_ICON,\n            tags: [\"quick-access\", \"generative\", \"experimental\"],\n            order: 2,\n        },\n    };\n}\n//# sourceMappingURL=make-code.js.map"
    },
    "output": {
      "code": "/**\n * @fileoverview Provides an output helper.\n */\nvar __classPrivateFieldGet = (this && this.__classPrivateFieldGet) || function (receiver, state, kind, f) {\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a getter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot read private member from an object whose class did not declare it\");\n    return kind === \"m\" ? f : kind === \"a\" ? f.call(receiver) : f ? f.value : state.get(receiver);\n};\nvar __classPrivateFieldSet = (this && this.__classPrivateFieldSet) || function (receiver, state, value, kind, f) {\n    if (kind === \"m\") throw new TypeError(\"Private method is not writable\");\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a setter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot write private member to an object whose class did not declare it\");\n    return (kind === \"a\" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value)), value;\n};\nvar _StreamableReporter_instances, _StreamableReporter_started, _StreamableReporter_id, _StreamableReporter_sendOperation, _StreamableReporter_sendUpsert;\nimport output from \"@output\";\nimport write from \"@write\";\nimport { generateId, ok } from \"./utils\";\nexport { report, StreamableReporter };\nconst MIME_TYPE = \"application/vnd.breadboard.report-stream\";\nclass StreamableReporter {\n    constructor(options) {\n        _StreamableReporter_instances.add(this);\n        this.options = options;\n        this.path = `/run/reporter/stream/${generateId()}`;\n        _StreamableReporter_started.set(this, false);\n        _StreamableReporter_id.set(this, 0);\n    }\n    async start() {\n        if (__classPrivateFieldGet(this, _StreamableReporter_started, \"f\"))\n            return;\n        __classPrivateFieldSet(this, _StreamableReporter_started, true, \"f\");\n        const schema = {\n            type: \"object\",\n            properties: {\n                reportStream: {\n                    behavior: [\"llm-content\"],\n                    type: \"object\",\n                },\n            },\n        };\n        const $metadata = this.options;\n        const reportStream = {\n            parts: [{ fileData: { fileUri: this.path, mimeType: MIME_TYPE } }],\n        };\n        const starting = await this.report(\"start\");\n        if (!ok(starting))\n            return starting;\n        return output({ schema, $metadata, reportStream });\n    }\n    async reportLLMContent(llmContent) {\n        if (!__classPrivateFieldGet(this, _StreamableReporter_started, \"f\")) {\n            console.log(\"StreamableReporter not started: call `start()` first\");\n            return;\n        }\n        const data = [llmContent];\n        return write({ path: this.path, stream: true, data });\n    }\n    report(json) {\n        return this.reportLLMContent({ parts: [{ json }] });\n    }\n    sendLinks(title, links, icon) {\n        var _a, _b;\n        const group = [\n            [\"title\", { text: title }],\n            [\n                \"links\",\n                {\n                    text: JSON.stringify(links),\n                    mimeType: \"application/json\",\n                },\n            ],\n        ];\n        if (icon) {\n            group.push([\"icon\", { text: icon }]);\n        }\n        return __classPrivateFieldGet(this, _StreamableReporter_instances, \"m\", _StreamableReporter_sendUpsert).call(this, {\n            path: [\"console\"],\n            id: `${__classPrivateFieldSet(this, _StreamableReporter_id, (_b = __classPrivateFieldGet(this, _StreamableReporter_id, \"f\"), _a = _b++, _b), \"f\"), _a}`,\n            particle: { type: \"links\", group },\n        });\n    }\n    sendUpdate(title, body, icon) {\n        var _a, _b;\n        let bodyParticle;\n        if (!body) {\n            bodyParticle = { text: \"Empty content\" };\n        }\n        else if (typeof body === \"string\") {\n            bodyParticle = { text: body };\n        }\n        else if (typeof body === \"object\" && \"parts\" in body) {\n            bodyParticle = {\n                text: JSON.stringify(body),\n                mimeType: \"application/vnd.breadboard.llm-content\",\n            };\n        }\n        else {\n            bodyParticle = {\n                text: JSON.stringify(body),\n                mimeType: \"application/json\",\n            };\n        }\n        const group = [\n            [\"title\", { text: title }],\n            [\"body\", bodyParticle],\n        ];\n        if (icon) {\n            group.push([\"icon\", { text: icon }]);\n        }\n        return __classPrivateFieldGet(this, _StreamableReporter_instances, \"m\", _StreamableReporter_sendUpsert).call(this, {\n            path: [\"console\"],\n            id: `${__classPrivateFieldSet(this, _StreamableReporter_id, (_b = __classPrivateFieldGet(this, _StreamableReporter_id, \"f\"), _a = _b++, _b), \"f\"), _a}`,\n            particle: { type: \"update\", group },\n        });\n    }\n    async sendError(error) {\n        await __classPrivateFieldGet(this, _StreamableReporter_instances, \"m\", _StreamableReporter_sendUpsert).call(this, {\n            path: [\"console\"],\n            id: `${__classPrivateFieldGet(this, _StreamableReporter_id, \"f\")}`,\n            particle: {\n                type: \"update\",\n                group: [\n                    [\"title\", { text: \"Error\" }],\n                    [\"body\", { text: error.$error }],\n                    [\"icon\", { text: \"warning\" }],\n                ],\n            },\n        });\n        return error;\n    }\n    close() {\n        if (!__classPrivateFieldGet(this, _StreamableReporter_started, \"f\"))\n            return;\n        return write({ path: this.path, stream: true, done: true });\n        __classPrivateFieldSet(this, _StreamableReporter_started, false, \"f\");\n    }\n}\n_StreamableReporter_started = new WeakMap(), _StreamableReporter_id = new WeakMap(), _StreamableReporter_instances = new WeakSet(), _StreamableReporter_sendOperation = function _StreamableReporter_sendOperation(op) {\n    return this.report(op);\n}, _StreamableReporter_sendUpsert = function _StreamableReporter_sendUpsert(params) {\n    return __classPrivateFieldGet(this, _StreamableReporter_instances, \"m\", _StreamableReporter_sendOperation).call(this, {\n        jsonrpc: \"2.0\",\n        method: \"suip/ops/upsert\",\n        params,\n    });\n};\nasync function report(inputs) {\n    const { actor: title, category: description, name, details, icon, chat, } = inputs;\n    const detailsSchema = typeof details === \"string\"\n        ? {\n            title: name,\n            type: \"string\",\n            format: \"markdown\",\n        }\n        : {\n            title: name,\n            type: \"object\",\n            behavior: [\"llm-content\"],\n        };\n    if (icon) {\n        detailsSchema.icon = icon;\n    }\n    if (chat) {\n        detailsSchema.behavior?.push(\"hint-chat-mode\");\n    }\n    const schema = {\n        type: \"object\",\n        properties: {\n            details: detailsSchema,\n        },\n    };\n    const { delivered } = await output({\n        $metadata: {\n            title,\n            description,\n            icon,\n        },\n        schema,\n        details,\n    });\n    return delivered;\n}\n//# sourceMappingURL=output.js.map"
    },
    "render-outputs": {
      "code": "/**\n * @fileoverview Renders multiple outputs into single display.\n */\nimport { ConnectorManager } from \"./connector-manager\";\nimport { callGenWebpage } from \"./html-generator\";\nimport { flattenContext } from \"./lists\";\nimport { Template } from \"./template\";\nimport { err, llm, mergeContent, ok, toLLMContent, toText } from \"./utils\";\nimport read from \"@read\";\nexport { invoke as default, describe };\nconst MANUAL_MODE = \"Manual layout\";\nconst FLASH_MODE = \"Webpage with auto-layout by 2.5 Flash\";\nconst PRO_MODE = \"Webpage with auto-layout by 2.5 Pro\";\nconst MODELS = [\n    {\n        id: \"gemini-flash\",\n        title: \"Gemini 2.5 Flash\",\n        description: \"Best for coding simple, static displays\",\n        modelName: \"gemini-2.5-flash\",\n    },\n    {\n        id: \"gemini-flash-lite\",\n        title: \"Gemini 2.5 Flash Lite\",\n        description: \"Best for simple speedy displays\",\n        modelName: \"gemini-2.5-flash-lite-preview-06-17\",\n    },\n    {\n        id: \"gemini-pro\",\n        title: \"Gemini 2.5 Pro\",\n        description: \"Best for coding complex or interactive displays\",\n        modelName: \"gemini-2.5-pro\",\n    },\n];\nconst MODES = [\n    {\n        id: MANUAL_MODE,\n        renderType: \"Manual\",\n        icon: \"responsive_layout\",\n        title: \"Manual layout\",\n        description: \"Content is displayed exactly as typed\",\n    },\n    {\n        id: \"Auto\",\n        renderType: \"HTML\",\n        icon: \"web\",\n        title: \"Webpage with auto-layout\",\n        description: \"Layout automatically generated by Gemini\",\n    },\n    {\n        id: \"google-doc\",\n        renderType: \"GoogleDoc\",\n        title: \"Save to Google Docs\",\n        icon: \"docs\",\n        description: \"Save content to a Google Document\",\n    },\n    {\n        id: \"google-slides\",\n        renderType: \"GoogleSlides\",\n        title: \"Save to Google Slides\",\n        icon: \"drive_presentation\",\n        description: \"Save content as a Google Drive Presentation\",\n    },\n    {\n        id: \"google-sheets\",\n        renderType: \"GoogleSheets\",\n        title: \"Save to Google Sheets\",\n        icon: \"sheets\",\n        description: \"Save content as a Google Drive Spreadsheet\",\n    },\n];\nconst renderModeMap = new Map(MODES.map((mode) => [mode.id, mode]));\nfunction getMode(modeId) {\n    return renderModeMap.get(modeId || \"Manual\") || MODES[0];\n}\nconst modelMap = new Map(MODELS.map((model) => [model.id, model]));\nfunction getModel(modelType) {\n    return modelMap.get(modelType || \"gemini-flash\") || MODELS[0];\n}\nfunction defaultSystemInstruction() {\n    return llm `You are a skilled web developer specializing in creating intuitive and visually appealing HTML web pages based on user instructions and data. Your task is to generate a valid HTML webpage that will be rendered in an iframe. The generated code must be valid and functional HTML with JavaScript and CSS embedded inline within <script> and <style> tags respectively. Return only the code, and open the HTML codeblock with the literal string '\\`\\`\\`html'. Render content as a clean, well-structured webpage, paying careful attention to user instructions. Use a responsive or mobile-friendly layout whenever possible and minimize unnecessary padding or margins.`.asContent();\n}\nfunction defaultThemeColors() {\n    return {\n        primaryColor: \"#246db5\",\n        secondaryColor: \"#5cadff\",\n        backgroundColor: \"#ffffff\",\n        textColor: \"#1a1a1a\",\n        primaryTextColor: \"#ffffff\",\n    };\n}\nasync function getThemeColors() {\n    const readingMetadata = await read({ path: \"/env/metadata\" });\n    if (!ok(readingMetadata))\n        return defaultThemeColors();\n    const metadata = readingMetadata.data?.at(0)?.parts?.at(0)\n        ?.json;\n    if (!metadata)\n        return defaultThemeColors();\n    const currentThemeId = metadata?.visual?.presentation?.theme;\n    if (!currentThemeId)\n        return defaultThemeColors();\n    const themeColors = metadata?.visual?.presentation?.themes?.[currentThemeId]?.themeColors;\n    if (!themeColors)\n        return defaultThemeColors();\n    return { ...defaultThemeColors(), ...themeColors };\n}\nasync function getPaletteColors() {\n    const readingMetadata = await read({ path: \"/env/metadata\" });\n    if (!ok(readingMetadata))\n        return;\n    const metadata = readingMetadata.data?.at(0)?.parts?.at(0)\n        ?.json;\n    if (!metadata)\n        return;\n    const currentThemeId = metadata?.visual?.presentation?.theme;\n    if (!currentThemeId)\n        return;\n    const palette = metadata?.visual?.presentation?.themes?.[currentThemeId]?.palette;\n    if (!palette)\n        return {};\n    return { ...palette };\n}\nfunction themeColorsPrompt(colors) {\n    return `Unless otherwise specified, use the following theme colors:\n\n- primary color: ${colors.primaryColor}\n- secondary color: ${colors.secondaryColor}\n- background color: ${colors.backgroundColor}\n- text color: ${colors.textColor}\n- primary text color: ${colors.primaryTextColor}\n\n`;\n}\nfunction getPalettePrompt(colors) {\n    return `Unless otherwise specified, use the following theme colors:\n  \n  - primary color, dark: ${colors.primary?.[25]}\n  - primary color, light: ${colors.primary?.[98]}\n  - secondary color, dark: ${colors.secondary?.[25]}\n  - secondary color, light: ${colors.secondary?.[95]}\n  - tertiary color, dark: ${colors.tertiary?.[25]}\n  - tertiary color, light: ${colors.tertiary?.[80]}\n  - background color: ${colors.secondary?.[90]}\n  - error color: ${colors.error?.[50]}\n  - neutral, dark: ${colors.neutral?.[25]}\n  - neutral, light: ${colors.neutral?.[98]}\n  `;\n}\nasync function saveToGoogleDrive(content, mimeType, title) {\n    const manager = new ConnectorManager({\n        url: \"embed://a2/google-drive.bgl.json\",\n        configuration: { file: { mimeType } },\n    });\n    const saving = await manager.save([content], { title });\n    if (!ok(saving))\n        return saving;\n}\nasync function invoke({ text, \"p-render-mode\": renderMode, \"b-system-instruction\": systemInstruction, \"b-render-model-name\": modelType, \"b-google-doc-title\": googleDocTitle, ...params }) {\n    let { modelName } = getModel(modelType);\n    const { renderType } = getMode(renderMode);\n    if (!text) {\n        text = toLLMContent(\"\");\n    }\n    if (!systemInstruction) {\n        systemInstruction = defaultSystemInstruction();\n    }\n    let systemText = toText(systemInstruction);\n    const template = new Template(text);\n    const substituting = await template.substitute(params, async () => \"\");\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    const context = mergeContent(flattenContext([substituting], true, \"\\n\\n\"), \"user\");\n    // If the step uses one of the deprecated modes that encodes model, trust this.\n    if (renderMode == FLASH_MODE) {\n        modelName = \"gemini-2.5-flash\";\n    }\n    else if (renderMode == PRO_MODE) {\n        modelName = \"gemini-2.5-pro-preview-05-06\";\n    }\n    console.log(\"Rendering with mode: \", renderType);\n    console.log(\"Rendering with model: \", modelName);\n    let out = context;\n    switch (renderType) {\n        case \"Manual\":\n            return { context: [out] };\n        case \"HTML\": {\n            const palette = await getPaletteColors();\n            if (palette?.primary) {\n                systemText += getPalettePrompt(palette);\n            }\n            else {\n                const themeColors = await getThemeColors();\n                systemText += themeColorsPrompt(themeColors);\n            }\n            console.log(\"SI :\", systemText);\n            const webPage = await callGenWebpage(systemText, [context], renderType, modelName);\n            if (!ok(webPage)) {\n                console.error(\"Failed to generated html output\");\n                return webPage;\n            }\n            else {\n                out = await webPage;\n                console.log(out);\n            }\n            if (!ok(out))\n                return err(out);\n            return { context: [out] };\n        }\n        case \"GoogleDoc\": {\n            const saving = await saveToGoogleDrive(out, \"application/vnd.google-apps.document\", googleDocTitle);\n            if (!ok(saving))\n                return saving;\n            return { context: [out] };\n        }\n        case \"GoogleSlides\": {\n            const saving = await saveToGoogleDrive(out, \"application/vnd.google-apps.presentation\", googleDocTitle);\n            if (!ok(saving))\n                return saving;\n            return { context: [out] };\n        }\n        case \"GoogleSheets\": {\n            const saving = await saveToGoogleDrive(out, \"application/vnd.google-apps.spreadsheet\", googleDocTitle);\n            if (!ok(saving))\n                return saving;\n            return { context: [out] };\n        }\n    }\n    return { context: [out] };\n}\nfunction advancedSettings(renderType) {\n    switch (renderType) {\n        case \"HTML\":\n            return {\n                \"b-system-instruction\": {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-advanced\"],\n                    title: \"System Instruction\",\n                    description: \"The system instruction used for auto-layout\",\n                },\n                \"b-render-model-name\": {\n                    type: \"string\",\n                    enum: MODELS,\n                    behavior: [\"llm-content\", \"config\", \"hint-advanced\"],\n                    title: \"Model\",\n                    description: \"The model to use for auto-generating display code\",\n                },\n            };\n        case \"GoogleDoc\": {\n            return {\n                \"b-google-doc-title\": {\n                    type: \"string\",\n                    behavior: [\"config\", \"hint-advanced\"],\n                    title: \"Google Doc Title\",\n                    description: \"The title of the Google Drive Document that content will be saved to\",\n                },\n            };\n        }\n        case \"GoogleSlides\": {\n            return {\n                \"b-google-doc-title\": {\n                    type: \"string\",\n                    behavior: [\"config\", \"hint-advanced\"],\n                    title: \"Google Presentation Title\",\n                    description: \"The title of a Google Drive Presentation that content will be saved to\",\n                },\n            };\n        }\n        case \"GoogleSheets\": {\n            return {\n                \"b-google-doc-title\": {\n                    type: \"string\",\n                    behavior: [\"config\", \"hint-advanced\"],\n                    title: \"Google Spreadsheet Title\",\n                    description: \"The title of a Google Drive Spreadsheet that content will be saved to\",\n                },\n            };\n        }\n    }\n    return {};\n}\nasync function describe({ inputs: { text, \"p-render-mode\": renderMode }, }) {\n    const template = new Template(text);\n    const { renderType } = getMode(renderMode);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                text: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"hint-preview\", \"config\", \"at-wireable\"],\n                    title: \"Outputs to render\",\n                    description: \"Type the @ character to select the outputs to combine. Optionally include style and layout guidlines if using Rendering mode of Markdown or HTML.\",\n                },\n                \"p-render-mode\": {\n                    type: \"string\",\n                    enum: MODES,\n                    title: \"Display format\",\n                    behavior: [\"config\", \"hint-preview\", \"reactive\", \"hint-controller\"],\n                    default: MANUAL_MODE,\n                    description: \"Choose how to combine and display the outputs\",\n                },\n                ...advancedSettings(renderType),\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                    },\n                    title: \"Context out\",\n                    behavior: [\"main-port\", \"hint-multimodal\"],\n                },\n            },\n        },\n        title: \"Output\",\n        metadata: {\n            icon: \"output\",\n            tags: [\"quick-access\", \"core\", \"output\"],\n            order: 100,\n        },\n    };\n}\n//# sourceMappingURL=render-outputs.js.map"
    },
    "researcher": {
      "code": "/**\n * @fileoverview Searching the Internet according to your plan.\n */\nimport invokeGraph from \"@invoke\";\nimport invokeGemini, { defaultSafetySettings, } from \"./gemini\";\nimport { ArgumentNameGenerator } from \"./introducer\";\nimport { report } from \"./output\";\nimport { Template } from \"./template\";\nimport { ToolManager } from \"./tool-manager\";\nimport { addUserTurn, err, llm, ok, toLLMContent } from \"./utils\";\nexport { invoke as default, describe };\nconst RESEARCH_TOOLS = [\n    {\n        url: \"embed://a2/tools.bgl.json#module:search-web\",\n        title: \"Search Web\",\n    },\n    {\n        url: \"embed://a2/tools.bgl.json#module:search-wikipedia\",\n        title: \"Search Wikipedia\",\n    },\n    {\n        url: \"embed://a2/tools.bgl.json#module:get-webpage\",\n        title: \"Get Webpage\",\n    },\n    {\n        url: \"embed://a2/tools.bgl.json#module:search-maps\",\n        title: \"Search Maps\",\n    },\n];\nconst RESEARCH_MODEL = \"gemini-2.0-flash\";\nconst MAX_ITERATIONS = 7;\nfunction systemInstruction(first) {\n    const which = first ? \"first\" : \"next\";\n    return `You are a researcher.\n  \nYour job is to use the provided research plan to produce raw research that will be later turned into a detailed research report.\nYou are tasked with finding as much of relevant information as possible.\n\nYou examine the conversation context so far and come up with the ${which} step to produce the report, \nusing the conversation context as the the guide of steps taken so far and the outcomes recorded.\n\nYou do not ask user for feedback. You do not try to have a conversation with the user. \nYou know that the user will only ask you to proceed to next step.\n\nYour next step consists of answering two questions.\n\nFirst, ask yourself \"am I done?\" -- looking back at all that you've researched and the plan, \ndo you have enough to produce the detailed report?\n\nSecond, provide a response. Your response must contain two parts:\nThought: a brief plain text reasoning why this is the right ${which} step and a description of what you will do in plain English.\nAction: invoking the tools are your disposal, more than one if necessary. If you're done, do not invoke any tools.`;\n}\nfunction researcherPrompt(contents, plan, tools, first) {\n    return {\n        model: RESEARCH_MODEL,\n        body: {\n            contents: addUserTurn(llm `\nDo the research according to this plan:\n\n---\n\n${plan}\n\n---\n`.asContent(), contents),\n            tools,\n            systemInstruction: toLLMContent(systemInstruction(first)),\n            safetySettings: defaultSafetySettings(),\n        },\n    };\n}\nfunction reportWriterInstruction() {\n    return `You are a research report writer. \nYour teammates produced a wealth of raw research according to the supplied plan.\n\nYour task is to take the raw research and write a thorough, detailed research report that captures it in a way that follows the plan. Use markdown.\n\nA report must additionally contain references to the source (always cite your sources).`;\n}\nfunction reportWriterPrompt(_plan, research) {\n    return {\n        model: RESEARCH_MODEL,\n        body: {\n            contents: [toLLMContent(research.join(\"\\n\\n\"))],\n            systemInstruction: toLLMContent(reportWriterInstruction()),\n            safetySettings: defaultSafetySettings(),\n        },\n    };\n}\nasync function thought(response, iteration) {\n    const first = response.parts?.at(0);\n    if (!first || !(\"text\" in first)) {\n        return;\n    }\n    await report({\n        actor: \"Researcher\",\n        category: `Progress report, iteration ${iteration + 1}`,\n        name: \"Thought\",\n        icon: \"generative\",\n        details: first.text\n            .replace(/^Thought: ?/gm, \"\")\n            .replace(/^Action:.*$/gm, \"\")\n            .trim(),\n    });\n}\nasync function invoke({ context, plan, summarize, ...params }) {\n    const tools = RESEARCH_TOOLS.map((descriptor) => descriptor.url);\n    const toolManager = new ToolManager(new ArgumentNameGenerator());\n    let content = context || [toLLMContent(\"Start the research\")];\n    const template = new Template(plan);\n    const substituting = await template.substitute(params, async ({ path: url, instance }) => toolManager.addTool(url, instance));\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    if (!toolManager.hasTools()) {\n        // If no tools supplied (legacy case, actually), initialize\n        // with a set of default tools.\n        const initializing = await toolManager.initialize(tools);\n        if (!initializing) {\n            return err(\"Unable to initialize tools\");\n        }\n    }\n    plan = substituting;\n    const research = [];\n    for (let i = 0; i <= MAX_ITERATIONS; i++) {\n        const askingGemini = await invokeGemini(researcherPrompt(content, plan, toolManager.list(), i === 0));\n        if (!ok(askingGemini)) {\n            return askingGemini;\n        }\n        if (\"context\" in askingGemini) {\n            return err(`Unexpected \"context\" response`);\n        }\n        const response = askingGemini.candidates.at(0)?.content;\n        if (!response) {\n            return err(\"No actionable response\");\n        }\n        await thought(response, i);\n        const toolResponses = [];\n        await toolManager.processResponse(response, async ($board, args) => {\n            toolResponses.push(JSON.stringify(await invokeGraph({ $board, ...args })));\n        });\n        if (toolResponses.length === 0) {\n            break;\n        }\n        research.push(...toolResponses);\n        content = [...content, response, toLLMContent(toolResponses.join(\"\\n\\n\"))];\n    }\n    if (research.length === 0) {\n        await report({\n            actor: \"Researcher\",\n            category: \"Error\",\n            name: \"Error\",\n            details: \"I was unable to obtain any research results\",\n        });\n        return { context };\n    }\n    if (summarize) {\n        const producingReport = await invokeGemini(reportWriterPrompt(plan, research));\n        if (!ok(producingReport)) {\n            return producingReport;\n        }\n        if (\"context\" in producingReport) {\n            return err(`Unexpected \"context\" response`);\n        }\n        const response = producingReport.candidates.at(0)?.content;\n        if (!response) {\n            return err(\"No actionable response\");\n        }\n        return { context: [...(context || []), response] };\n    }\n    return { context: [...(context || []), toLLMContent(research.join(\"\\n\\n\"))] };\n}\nfunction toOxfordList(items) {\n    if (items.length === 0)\n        return \"\";\n    if (items.length === 1)\n        return items[0];\n    if (items.length === 2)\n        return items.join(\" and \");\n    const lastItem = items.pop();\n    return `${items.join(\", \")}, and ${lastItem}`;\n}\nfunction researchExample() {\n    const type = \"tool\";\n    const tools = RESEARCH_TOOLS.map(({ url: path, title }) => Template.part({ title, path, type }));\n    return [\n        JSON.stringify({\n            plan: toLLMContent(`Research the topic provided using ${toOxfordList(tools)} tools`),\n        }),\n    ];\n}\nasync function describe({ inputs: { plan } }) {\n    const template = new Template(plan);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                plan: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"Research Plan\",\n                    description: \"Provide an outline of what to research, what areas to cover, etc.\",\n                },\n                summarize: {\n                    type: \"boolean\",\n                    behavior: [\"config\", \"hint-preview\"],\n                    icon: \"summarize\",\n                    title: \"Summarize research\",\n                    description: \"If checked, the Researcher will summarize the results of the research and only pass the research summary along.\",\n                },\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n            additionalProperties: false,\n            examples: researchExample(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"main-port\", \"hint-text\"],\n                },\n            },\n            additionalProperties: false,\n        },\n        title: \"Do deep research\",\n        description: \"Do deep research according to your plan\",\n        metadata: {\n            icon: \"generative-search\",\n            tags: [\"quick-access\", \"generative\"],\n            order: 101,\n        },\n    };\n}\n//# sourceMappingURL=researcher.js.map"
    },
    "settings": {
      "code": "/**\n * @fileoverview A helper to retrieve current settings\n */\nimport { ok } from \"./utils\";\nimport read from \"@read\";\nexport { readSettings };\nasync function readSettings() {\n    const reading = await read({ path: \"/env/settings/general\" });\n    if (!ok(reading))\n        return reading;\n    const json = (reading.data?.at(0)?.parts?.at(0)).json;\n    if (!json)\n        return {};\n    return json;\n}\n//# sourceMappingURL=settings.js.map"
    },
    "step-executor": {
      "code": "/**\n * @fileoverview Utilities to execute tools on the AppCatalyst backend server.\n */\nexport { executeStep, executeTool };\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\nimport read from \"@read\";\nimport { ok, err, decodeBase64 } from \"./utils\";\nconst DEFAULT_BACKEND_ENDPOINT = \"https://staging-appcatalyst.sandbox.googleapis.com/v1beta1/executeStep\";\nfunction maybeExtractError(e) {\n    try {\n        const parsed = JSON.parse(e);\n        return parsed.error.message;\n    }\n    catch {\n        return e;\n    }\n}\nasync function executeTool(api, params) {\n    const inputParameters = Object.keys(params);\n    const execution_inputs = Object.fromEntries(Object.entries(params).map(([name, value]) => {\n        return [\n            name,\n            {\n                chunks: [{ mimetype: \"text/plan\", data: btoa(value) }],\n            },\n        ];\n    }));\n    const response = await executeStep({\n        planStep: {\n            stepName: api,\n            modelApi: api,\n            output: \"data\",\n            inputParameters,\n            isListOutput: false,\n        },\n        execution_inputs,\n    });\n    if (!ok(response))\n        return response;\n    const data = response?.executionOutputs[\"data\"].chunks.at(0)?.data;\n    if (!data) {\n        return err(`Invalid response from \"${api}\" backend`);\n    }\n    const jsonString = decodeBase64(data);\n    try {\n        return JSON.parse(jsonString);\n    }\n    catch {\n        return jsonString;\n    }\n}\nasync function getBackendUrl() {\n    const reading = await read({ path: \"/env/settings/backend\" });\n    if (ok(reading)) {\n        const part = reading.data?.at(0)?.parts?.at(0);\n        if (part && \"json\" in part) {\n            const settings = part.json;\n            if (settings && settings.endpoint_url) {\n                return settings.endpoint_url;\n            }\n        }\n    }\n    return DEFAULT_BACKEND_ENDPOINT;\n}\nasync function executeStep(body) {\n    // Get an authentication token.\n    const key = \"connection:$sign-in\";\n    const token = (await secrets({ keys: [key] }))[key];\n    // Call the API.\n    const url = await getBackendUrl();\n    const fetchResult = await fetch({\n        url: url,\n        method: \"POST\",\n        headers: {\n            \"Content-Type\": \"application/json\",\n            Authorization: `Bearer ${token}`,\n        },\n        body: body,\n    });\n    let $error = \"Unknown error\";\n    if (!ok(fetchResult)) {\n        const { status, $error: errObject } = fetchResult;\n        console.warn(\"Error response\", fetchResult);\n        if (!status) {\n            // This is not an error response, presume fatal error.\n            return { $error };\n        }\n        $error = maybeExtractError(errObject);\n        return { $error };\n    }\n    const response = fetchResult.response;\n    if (response.errorMessage) {\n        $error = response.errorMessage;\n        return { $error };\n    }\n    return response;\n}\n//# sourceMappingURL=step-executor.js.map"
    },
    "template": {
      "code": "/**\n * @fileoverview Handles templated content\n */\nvar __classPrivateFieldSet = (this && this.__classPrivateFieldSet) || function (receiver, state, value, kind, f) {\n    if (kind === \"m\") throw new TypeError(\"Private method is not writable\");\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a setter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot write private member to an object whose class did not declare it\");\n    return (kind === \"a\" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value)), value;\n};\nvar __classPrivateFieldGet = (this && this.__classPrivateFieldGet) || function (receiver, state, kind, f) {\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a getter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot read private member from an object whose class did not declare it\");\n    return kind === \"m\" ? f : kind === \"a\" ? f.call(receiver) : f ? f.value : state.get(receiver);\n};\nvar _Template_instances, _Template_parts, _Template_role, _Template_mergeTextParts, _Template_splitToTemplateParts, _Template_getLastNonMetadata, _Template_replaceParam, _Template_toId, _Template_toTitle, _Template_forEachParam;\nexport { invoke as default, describe, Template };\nimport { ok, err, isLLMContent, isLLMContentArray } from \"./utils\";\nimport { ConnectorManager } from \"./connector-manager\";\nimport readFile from \"@read\";\nfunction isTool(param) {\n    return param.type === \"tool\" && !!param.path;\n}\nfunction isIn(param) {\n    return param.type === \"in\" && !!param.path;\n}\nfunction isAsset(param) {\n    return param.type === \"asset\" && !!param.path;\n}\nfunction isParameter(param) {\n    return param.type === \"param\" && !!param.path;\n}\nfunction isParamPart(param) {\n    return isTool(param) || isIn(param) || isAsset(param) || isParameter(param);\n}\nconst PARSING_REGEX = /{(?<json>{(?:.*?)})}/gim;\nclass Template {\n    constructor(template) {\n        _Template_instances.add(this);\n        this.template = template;\n        _Template_parts.set(this, void 0);\n        _Template_role.set(this, void 0);\n        if (!template) {\n            __classPrivateFieldSet(this, _Template_role, \"user\", \"f\");\n            __classPrivateFieldSet(this, _Template_parts, [], \"f\");\n            return;\n        }\n        __classPrivateFieldSet(this, _Template_parts, __classPrivateFieldGet(this, _Template_instances, \"m\", _Template_splitToTemplateParts).call(this, template), \"f\");\n        __classPrivateFieldSet(this, _Template_role, template.role, \"f\");\n    }\n    simpleSubstitute(callback) {\n        const parts = [];\n        for (const part of __classPrivateFieldGet(this, _Template_parts, \"f\")) {\n            if (\"type\" in part) {\n                parts.push({ text: callback(part) });\n            }\n            else {\n                parts.push(part);\n            }\n        }\n        return { parts };\n    }\n    async substitute(params, whenTool) {\n        const replaced = [];\n        for (const part of __classPrivateFieldGet(this, _Template_parts, \"f\")) {\n            if (\"type\" in part) {\n                const value = await __classPrivateFieldGet(this, _Template_instances, \"m\", _Template_replaceParam).call(this, part, params, whenTool);\n                if (value === null) {\n                    // Ignore if null.\n                    continue;\n                }\n                else if (!ok(value)) {\n                    return value;\n                }\n                else if (typeof value === \"string\") {\n                    replaced.push({ text: value });\n                }\n                else if (isLLMContent(value)) {\n                    replaced.push(...value.parts);\n                }\n                else if (isLLMContentArray(value)) {\n                    const last = __classPrivateFieldGet(this, _Template_instances, \"m\", _Template_getLastNonMetadata).call(this, value);\n                    if (last) {\n                        replaced.push(...last.parts);\n                    }\n                }\n                else {\n                    replaced.push({ text: JSON.stringify(value) });\n                }\n            }\n            else {\n                replaced.push(part);\n            }\n        }\n        const parts = __classPrivateFieldGet(this, _Template_instances, \"m\", _Template_mergeTextParts).call(this, replaced);\n        return { parts, role: __classPrivateFieldGet(this, _Template_role, \"f\") };\n    }\n    requireds() {\n        const required = [];\n        let hasValues = false;\n        __classPrivateFieldGet(this, _Template_instances, \"m\", _Template_forEachParam).call(this, (param) => {\n            if (!isIn(param))\n                return;\n            hasValues = true;\n            required.push(__classPrivateFieldGet(this, _Template_instances, \"m\", _Template_toId).call(this, param.title));\n        });\n        return hasValues ? { required } : {};\n    }\n    schemas() {\n        const result = [];\n        __classPrivateFieldGet(this, _Template_instances, \"m\", _Template_forEachParam).call(this, (param) => {\n            const name = param.title;\n            const id = __classPrivateFieldGet(this, _Template_instances, \"m\", _Template_toId).call(this, param.path);\n            if (!isIn(param))\n                return;\n            result.push([\n                id,\n                {\n                    title: __classPrivateFieldGet(this, _Template_instances, \"m\", _Template_toTitle).call(this, name),\n                    description: `The value to substitute for the parameter \"${name}\"`,\n                    type: \"object\",\n                    behavior: [\"llm-content\"],\n                },\n            ]);\n        });\n        return Object.fromEntries(result);\n    }\n    static part(part) {\n        return `{${JSON.stringify(part)}}`;\n    }\n    /**\n     * This is roughly the same method as `schemas`, but for connectors.\n     * TODO: UNIFY\n     */\n    async schemaProperties() {\n        let result = {};\n        for (const part of __classPrivateFieldGet(this, _Template_parts, \"f\")) {\n            if (!(\"type\" in part))\n                continue;\n            if (!isAsset(part))\n                continue;\n            if (!ConnectorManager.isConnector(part))\n                continue;\n            const props = await new ConnectorManager(part).schemaProperties();\n            result = { ...result, ...props };\n        }\n        return result;\n    }\n    async save(context, options) {\n        if (!context)\n            return;\n        const errors = [];\n        for (const part of __classPrivateFieldGet(this, _Template_parts, \"f\")) {\n            if (!(\"type\" in part))\n                continue;\n            if (!isAsset(part))\n                continue;\n            if (!ConnectorManager.isConnector(part))\n                continue;\n            const saving = await new ConnectorManager(part).save(context, options || {});\n            if (!ok(saving)) {\n                errors.push(saving.$error);\n            }\n        }\n        if (errors.length > 0) {\n            return err(errors.join(\"\\n\"));\n        }\n    }\n}\n_Template_parts = new WeakMap(), _Template_role = new WeakMap(), _Template_instances = new WeakSet(), _Template_mergeTextParts = function _Template_mergeTextParts(parts) {\n    const merged = [];\n    for (let part of parts) {\n        if (\"text\" in part) {\n            const last = merged[merged.length - 1];\n            if (last && \"text\" in last) {\n                last.text += part.text;\n            }\n            else {\n                // We do a copy here otherwise the part is mutated, which\n                // causes problems if the same part appears in the list twice.\n                part = JSON.parse(JSON.stringify(part));\n                merged.push(part);\n            }\n        }\n        else {\n            merged.push(part);\n        }\n    }\n    return merged;\n}, _Template_splitToTemplateParts = function _Template_splitToTemplateParts(content) {\n    const parts = [];\n    for (const part of content.parts) {\n        if (!(\"text\" in part)) {\n            parts.push(part);\n            continue;\n        }\n        const matches = part.text.matchAll(PARSING_REGEX);\n        let start = 0;\n        for (const match of matches) {\n            const json = match.groups?.json;\n            const end = match.index;\n            if (end > start) {\n                parts.push({ text: part.text.slice(start, end) });\n            }\n            if (json) {\n                let maybeTemplatePart;\n                try {\n                    maybeTemplatePart = JSON.parse(json);\n                    if (isParamPart(maybeTemplatePart)) {\n                        // Do some extra parsing for connector tools\n                        // if (isTool(maybeTemplatePart)) {\n                        //   const [path, connector] = maybeTemplatePart.path.split(\"|\");\n                        //   if (connector && connector.startsWith(\"connectors/\")) {\n                        //     maybeTemplatePart.path = path;\n                        //     maybeTemplatePart.instance = connector;\n                        //   }\n                        //   console.log(\"TOOL\", maybeTemplatePart);\n                        // }\n                        parts.push(maybeTemplatePart);\n                    }\n                    else {\n                        maybeTemplatePart = null;\n                    }\n                }\n                catch {\n                    // do nothing\n                }\n                finally {\n                    if (!maybeTemplatePart) {\n                        parts.push({ text: part.text.slice(end, end + match[0].length) });\n                    }\n                }\n            }\n            start = end + match[0].length;\n        }\n        if (start < part.text.length) {\n            parts.push({ text: part.text.slice(start) });\n        }\n    }\n    return parts;\n}, _Template_getLastNonMetadata = function _Template_getLastNonMetadata(value) {\n    const content = value;\n    for (let i = content.length - 1; i >= 0; i--) {\n        if (content[i].role !== \"$metadata\") {\n            return content[i];\n        }\n    }\n    return null;\n}, _Template_replaceParam = async function _Template_replaceParam(param, params, whenTool) {\n    if (isIn(param)) {\n        const { title: name, path } = param;\n        const paramName = `p-z-${path}`;\n        if (paramName in params) {\n            return params[paramName];\n        }\n        return name;\n    }\n    else if (isAsset(param)) {\n        if (ConnectorManager.isConnector(param)) {\n            return new ConnectorManager(param).materialize();\n        }\n        const path = `/assets/${param.path}`;\n        const reading = await readFile({ path });\n        if (!ok(reading)) {\n            return err(`Unable to find asset \"${param.title}\"`);\n        }\n        return reading.data;\n    }\n    else if (isTool(param)) {\n        const substituted = await whenTool(param);\n        if (!ok(substituted))\n            return substituted;\n        return substituted || param.title;\n    }\n    else if (isParameter(param)) {\n        const path = `/env/parameters/${param.path}`;\n        const reading = await readFile({ path });\n        if (!ok(reading)) {\n            console.error(`Unknown parameter \"${param.title}\"`);\n            return null;\n        }\n        return reading.data;\n    }\n    return null;\n}, _Template_toId = function _Template_toId(param) {\n    return `p-z-${param}`;\n}, _Template_toTitle = function _Template_toTitle(id) {\n    const spaced = id?.replace(/[_-]/g, \" \");\n    return ((spaced?.at(0)?.toUpperCase() ?? \"\") +\n        (spaced?.slice(1)?.toLowerCase() ?? \"\"));\n}, _Template_forEachParam = function _Template_forEachParam(handler) {\n    for (const part of __classPrivateFieldGet(this, _Template_parts, \"f\")) {\n        if (\"type\" in part) {\n            handler(part);\n        }\n    }\n};\n/**\n * API for test harness\n */\nfunction fromTestParams(params) {\n    return Object.fromEntries(Object.entries(params).map(([key, value]) => {\n        return [`p-z-${key}`, value];\n    }));\n}\n/**\n * Only used for testing.\n */\nasync function invoke({ inputs: { content, params }, }) {\n    const template = new Template(content);\n    const result = await template.substitute(fromTestParams(params), async (params) => {\n        return params.path;\n    });\n    if (!ok(result)) {\n        return result;\n    }\n    return { outputs: result };\n}\n/**\n * Only used for testing.\n */\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: { inputs: { type: \"object\", title: \"Test inputs\" } },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: { outputs: { type: \"object\", title: \"Test outputs\" } },\n        },\n    };\n}\n//# sourceMappingURL=template.js.map"
    },
    "text-entry": {
      "code": "import { report } from \"./output\";\nimport { Template } from \"./template\";\nimport { defaultLLMContent, llm, ok, toText } from \"./utils\";\nexport { invoke as default, describe };\nconst MODALITY = [\n    \"Any\",\n    \"Audio\",\n    \"Image\",\n    \"Text\",\n    \"Upload File\",\n    \"Video\",\n];\nfunction toInput(title, modality) {\n    const toInput = {\n        type: \"object\",\n        properties: {\n            request: {\n                type: \"object\",\n                title,\n                behavior: [\"transient\", \"llm-content\"],\n                examples: [defaultLLMContent()],\n                format: computeIcon(modality),\n            },\n        },\n    };\n    return toInput;\n}\nconst ICONS = {\n    Any: \"asterisk\",\n    Audio: \"mic\",\n    Video: \"videocam\",\n    Image: \"image\",\n    \"Upload File\": \"upload\",\n    Text: \"edit_note\",\n};\nconst HINTS = {\n    Any: \"hint-multimodal\",\n    Audio: \"hint-audio\",\n    Video: \"hint-image\",\n    Image: \"hint-image\",\n    \"Upload File\": \"hint-text\",\n    Text: \"hint-text\",\n};\nfunction computeIcon(modality) {\n    return (modality && ICONS[modality]) || \"asterisk\";\n}\nfunction computeHint(modality) {\n    return (modality && HINTS[modality]) || \"hint-multimodal\";\n}\nfunction combineModalities(modalities) {\n    const schemaEnum = modalities.map((modality) => ({\n        id: modality,\n        title: modality,\n        icon: ICONS[modality],\n    }));\n    return schemaEnum;\n}\nasync function invoke({ description, \"p-modality\": modality, ...params }) {\n    const template = new Template(description);\n    let details = llm `Please provide input`.asContent();\n    if (description) {\n        const substituting = await template.substitute(params, async () => \"\");\n        if (!ok(substituting)) {\n            return substituting;\n        }\n        details = substituting;\n    }\n    await report({\n        actor: \"User Input\",\n        category: \"Requesting Input\",\n        name: \"\",\n        details,\n        icon: \"input\",\n        chat: true,\n    });\n    const title = toText(details);\n    return { context: \"nothing\", toInput: toInput(title, modality) };\n}\nasync function describe({ inputs: { description, [\"p-modality\"]: modality }, }) {\n    const icon = computeIcon(modality);\n    const template = new Template(description);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                description: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"What to ask of user\",\n                    description: \"Provide a request prompt that will be shown to the user.\",\n                },\n                \"p-modality\": {\n                    type: \"string\",\n                    enum: combineModalities(MODALITY),\n                    behavior: [\"config\", \"hint-preview\", \"hint-advanced\"],\n                    icon,\n                    title: \"Input type\",\n                    description: \"Set the type of input the user can provide\",\n                },\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            additionalProperties: true,\n            ...template.requireds(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"main-port\", computeHint(modality)],\n                },\n            },\n            additionalProperties: false,\n        },\n        title: \"User Input\",\n        metadata: {\n            icon: \"ask-user\",\n            tags: [\"quick-access\", \"core\", \"input\"],\n            order: 1,\n        },\n    };\n}\n//# sourceMappingURL=text-entry.js.map"
    },
    "text-main": {
      "code": "/**\n * @fileoverview Calls Gemini agent loop.\n */\nimport { err } from \"./utils\";\nexport { invoke as default, describe };\nasync function invoke({ context, request, }) {\n    if (context == \"nothing\") {\n        if (!request) {\n            return err(`No text supplied.`);\n        }\n        return { context: [request] };\n    }\n    return { context: [context] };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"object\",\n                    title: \"Context in\",\n                },\n                request: {\n                    type: \"object\",\n                    title: \"Data From Input\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                },\n            },\n        },\n    };\n}\n//# sourceMappingURL=text-main.js.map"
    },
    "tool-manager": {
      "code": "/**\n * @fileoverview Manages tools.\n */\nvar __classPrivateFieldSet = (this && this.__classPrivateFieldSet) || function (receiver, state, value, kind, f) {\n    if (kind === \"m\") throw new TypeError(\"Private method is not writable\");\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a setter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot write private member to an object whose class did not declare it\");\n    return (kind === \"a\" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value)), value;\n};\nvar __classPrivateFieldGet = (this && this.__classPrivateFieldGet) || function (receiver, state, kind, f) {\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a getter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot read private member from an object whose class did not declare it\");\n    return kind === \"m\" ? f : kind === \"a\" ? f.call(receiver) : f ? f.value : state.get(receiver);\n};\nvar _ToolManager_instances, _ToolManager_hasSearch, _ToolManager_convertSchemas, _ToolManager_toName, _ToolManager_createToolHandle, _ToolManager_addOneTool;\nimport describeGraph from \"@describe\";\nimport { ConnectorManager } from \"./connector-manager\";\nimport { ok } from \"./utils\";\nexport { ToolManager };\nclass ToolManager {\n    constructor(describerResultTransformer) {\n        _ToolManager_instances.add(this);\n        this.describerResultTransformer = describerResultTransformer;\n        _ToolManager_hasSearch.set(this, false);\n        this.tools = new Map();\n        this.connectors = new Map();\n        this.errors = [];\n    }\n    addSearch() {\n        __classPrivateFieldSet(this, _ToolManager_hasSearch, true, \"f\");\n    }\n    addCustomTool(name, handle) {\n        this.tools.set(name, handle);\n    }\n    async addTool(url, instance) {\n        if (instance) {\n            // This is a connector.\n            const connector = new ConnectorManager({ path: instance });\n            const tools = await connector.listTools();\n            if (!ok(tools))\n                return tools;\n            for (const tool of tools) {\n                const { url, description } = tool;\n                __classPrivateFieldGet(this, _ToolManager_instances, \"m\", _ToolManager_addOneTool).call(this, url, description, false, connector);\n            }\n            // Return empty string, which will inform the\n            // substitution machinery to just reuse title.\n            return \"\";\n        }\n        let description = (await describeGraph({\n            url,\n        }));\n        console.log(\"DESCRIBER RESULT\", description);\n        let passContext = false;\n        if (!ok(description))\n            return description;\n        // TODO: Remove this altogether?\n        // Let's see if there are exports. If yes, let's add the exports\n        // instead of the tool.\n        if (description.exports) {\n            let connector = null;\n            if (description.metadata?.tags?.includes(\"connector\")) {\n                // This is a connector\n                connector = { tools: new Map() };\n            }\n            Object.entries(description.exports).forEach(([id, exportDescription]) => {\n                // TODO: Figure out what to do with passContext\n                const idAndHandle = __classPrivateFieldGet(this, _ToolManager_instances, \"m\", _ToolManager_createToolHandle).call(this, id, exportDescription, passContext);\n                const [name, handle] = idAndHandle;\n                console.log(\"EXPORT DESCRIPTION\", exportDescription);\n                if (connector) {\n                    if (exportDescription.metadata?.tags?.includes(\"connector-configure\")) {\n                        connector.configure = idAndHandle;\n                        return;\n                    }\n                    else if (exportDescription.metadata?.tags?.includes(\"connector-load\")) {\n                        connector.load = idAndHandle;\n                        return;\n                    }\n                    else {\n                        connector.tools.set(name, handle);\n                    }\n                }\n                this.tools.set(name, handle);\n            });\n            return __classPrivateFieldGet(this, _ToolManager_instances, \"m\", _ToolManager_toName).call(this, description.title);\n        }\n        // Otherwise, let's add the tool itself.\n        if (this.describerResultTransformer) {\n            const transforming = await this.describerResultTransformer.transform(description);\n            if (!ok(transforming))\n                return transforming;\n            if (transforming) {\n                description = transforming;\n                passContext = true;\n            }\n        }\n        return __classPrivateFieldGet(this, _ToolManager_instances, \"m\", _ToolManager_addOneTool).call(this, url, description, passContext);\n    }\n    async initialize(tools) {\n        if (!tools) {\n            return true;\n        }\n        let hasInvalidTools = false;\n        for (const tool of tools) {\n            const url = typeof tool === \"string\" ? tool : tool.url;\n            const description = (await describeGraph({\n                url,\n            }));\n            if (!ok(description)) {\n                this.errors.push(description.$error);\n                // Invalid tool, skip\n                hasInvalidTools = true;\n                continue;\n            }\n            const parameters = __classPrivateFieldGet(this, _ToolManager_instances, \"m\", _ToolManager_convertSchemas).call(this, description.inputSchema);\n            const name = __classPrivateFieldGet(this, _ToolManager_instances, \"m\", _ToolManager_toName).call(this, description.title);\n            const functionDeclaration = {\n                name,\n                description: description.description || \"\",\n                parameters,\n            };\n            this.tools.set(name, {\n                tool: functionDeclaration,\n                url,\n                passContext: false,\n            });\n        }\n        return !hasInvalidTools;\n    }\n    async processResponse(response, callTool) {\n        for (const part of response.parts) {\n            if (\"functionCall\" in part) {\n                const { args, name } = part.functionCall;\n                const handle = this.tools.get(name);\n                if (handle) {\n                    if (handle.invoke) {\n                        await handle.invoke(args);\n                    }\n                    else {\n                        const { url, passContext, connector } = handle;\n                        if (connector) {\n                            await connector.invokeTool(name, args, callTool);\n                        }\n                        else {\n                            await callTool(url, part.functionCall.args, passContext, name);\n                        }\n                    }\n                }\n            }\n        }\n    }\n    hasTools() {\n        return this.tools.size !== 0;\n    }\n    list() {\n        const declaration = {};\n        const entries = [...this.tools.entries()];\n        if (entries.length !== 0) {\n            declaration.functionDeclarations = entries.map(([, value]) => value.tool);\n        }\n        if (__classPrivateFieldGet(this, _ToolManager_hasSearch, \"f\")) {\n            declaration.googleSearch = {};\n        }\n        if (Object.keys(declaration).length === 0)\n            return [];\n        return [declaration];\n    }\n}\n_ToolManager_hasSearch = new WeakMap(), _ToolManager_instances = new WeakSet(), _ToolManager_convertSchemas = function _ToolManager_convertSchemas(schema) {\n    return toGeminiSchema(schema);\n    function toGeminiSchema(schema) {\n        switch (schema.type) {\n            case \"object\": {\n                if (schema.behavior?.includes(\"llm-content\")) {\n                    return {\n                        type: \"string\",\n                        description: schema.description || schema.title,\n                    };\n                }\n                if (!schema.properties) {\n                    return { type: \"object\" };\n                }\n                return {\n                    type: \"object\",\n                    properties: Object.fromEntries(Object.entries(schema.properties).map(([name, schema]) => {\n                        return [name, toGeminiSchema(schema)];\n                    })),\n                    required: schema.required,\n                };\n            }\n            case \"array\": {\n                const items = schema.items;\n                if (items.behavior?.includes(\"llm-content\")) {\n                    return {\n                        type: \"string\",\n                        description: schema.description,\n                    };\n                }\n                return {\n                    type: \"array\",\n                    items: toGeminiSchema(schema.items),\n                };\n            }\n            default: {\n                const geminiSchema = { ...schema };\n                delete geminiSchema.format;\n                delete geminiSchema.behavior;\n                delete geminiSchema.examples;\n                delete geminiSchema.default;\n                delete geminiSchema.transient;\n                if (!geminiSchema.description) {\n                    geminiSchema.description = geminiSchema.title;\n                }\n                delete geminiSchema.title;\n                return geminiSchema;\n            }\n        }\n    }\n}, _ToolManager_toName = function _ToolManager_toName(title) {\n    return title ? title.replace(/\\W/g, \"_\") : \"function\";\n}, _ToolManager_createToolHandle = function _ToolManager_createToolHandle(url, description, passContext, connector) {\n    const name = __classPrivateFieldGet(this, _ToolManager_instances, \"m\", _ToolManager_toName).call(this, description.title);\n    const functionDeclaration = {\n        name,\n        description: description.description || \"\",\n    };\n    const parameters = __classPrivateFieldGet(this, _ToolManager_instances, \"m\", _ToolManager_convertSchemas).call(this, description.inputSchema);\n    if (parameters.properties) {\n        functionDeclaration.parameters = parameters;\n    }\n    return [name, { tool: functionDeclaration, url, passContext, connector }];\n}, _ToolManager_addOneTool = function _ToolManager_addOneTool(url, description, passContext, connector) {\n    const [name, handle] = __classPrivateFieldGet(this, _ToolManager_instances, \"m\", _ToolManager_createToolHandle).call(this, url, description, passContext, connector);\n    this.tools.set(name, handle);\n    return description.title || name;\n};\n//# sourceMappingURL=tool-manager.js.map"
    },
    "utils": {
      "code": "/**\n * @fileoverview Common utils for manipulating LLM Content and other relevant types.\n */\nexport { isLLMContent, isLLMContentArray, toLLMContent, toInlineData, toLLMContentInline, toInlineReference, toLLMContentStored, toText, joinContent, contentToJSON, defaultLLMContent, endsWithRole, addUserTurn, isEmpty, isStoredData, llm, ok, err, generateId, mergeTextParts, toTextConcat, extractTextData, extractInlineData, extractMediaData, decodeBase64, };\nfunction ok(o) {\n    return !(o && typeof o === \"object\" && \"$error\" in o);\n}\nfunction err($error) {\n    return { $error };\n}\nfunction mergeTextParts(parts, separator = \"\") {\n    const merged = [];\n    let text = \"\";\n    for (const part of parts) {\n        if (\"text\" in part) {\n            text += `${part.text}${separator}`;\n        }\n        else {\n            if (text) {\n                merged.push({ text });\n            }\n            text = \"\";\n            merged.push(part);\n        }\n    }\n    if (text) {\n        merged.push({ text });\n    }\n    return merged;\n}\nclass LLMTemplate {\n    constructor(strings, values) {\n        this.strings = strings;\n        this.values = values;\n    }\n    asParts() {\n        return mergeTextParts(this.strings.flatMap((s, i) => {\n            let text = s;\n            const value = this.values.at(i);\n            if (value == undefined) {\n                return { text };\n            }\n            else if (typeof value === \"string\") {\n                text += value;\n                return { text };\n            }\n            else if (value instanceof LLMTemplate) {\n                return value.asParts();\n            }\n            else if (isLLMContent(value)) {\n                return [{ text }, ...value.parts];\n            }\n            else {\n                text += JSON.stringify(value);\n                return { text };\n            }\n        }));\n    }\n    asContent() {\n        const parts = this.asParts();\n        return { parts, role: \"user\" };\n    }\n}\nfunction llm(strings, ...values) {\n    return new LLMTemplate(strings, values);\n}\n/**\n * Copied from @google-labs/breadboard\n */\nfunction isLLMContent(nodeValue) {\n    if (typeof nodeValue !== \"object\" || !nodeValue)\n        return false;\n    if (nodeValue === null || nodeValue === undefined)\n        return false;\n    if (\"role\" in nodeValue && nodeValue.role === \"$metadata\") {\n        return true;\n    }\n    return \"parts\" in nodeValue && Array.isArray(nodeValue.parts);\n}\nfunction isLLMContentArray(nodeValue) {\n    if (!Array.isArray(nodeValue))\n        return false;\n    if (nodeValue.length === 0)\n        return true;\n    return isLLMContent(nodeValue.at(-1));\n}\nfunction toLLMContent(text, role = \"user\") {\n    return { parts: [{ text }], role };\n}\nfunction endsWithRole(c, role) {\n    const last = c.at(-1);\n    if (!last)\n        return false;\n    return last.role === role;\n}\nfunction isEmpty(c) {\n    if (!c.parts.length)\n        return true;\n    for (const part of c.parts) {\n        if (\"text\" in part) {\n            if (part.text.trim().length > 0)\n                return false;\n        }\n        else {\n            return false;\n        }\n    }\n    return true;\n}\nfunction isStoredData(c) {\n    const part = c.parts.at(-1);\n    if (!part) {\n        return false;\n    }\n    return \"storedData\" in part && part.storedData != null;\n}\nfunction toText(c) {\n    if (isLLMContent(c)) {\n        return contentToText(c);\n    }\n    const last = c.at(-1);\n    if (!last)\n        return \"\";\n    return contentToText(last).trim();\n    function contentToText(content) {\n        return content.parts\n            .map((part) => (\"text\" in part ? part.text : \"\"))\n            .join(\"\\n\\n\");\n    }\n}\nfunction contentToJSON(content) {\n    const part = content?.parts?.at(0);\n    if (!part || !(\"text\" in part)) {\n        throw new Error(\"Invalid response from Gemini\");\n    }\n    return JSON.parse(part.text);\n}\nfunction defaultLLMContent() {\n    return JSON.stringify({\n        parts: [{ text: \"\" }],\n        role: \"user\",\n    });\n}\nfunction joinContent(content, context, dropHistory = false) {\n    if (dropHistory && context) {\n        const last = context.at(-1);\n        let retainedContext;\n        if (last) {\n            retainedContext = [last];\n        }\n        return addUserTurn(content, retainedContext);\n    }\n    return addUserTurn(content, context);\n}\nfunction extractInlineData(context) {\n    const results = [];\n    for (const el of context) {\n        for (const part of el.parts) {\n            if (part) {\n                if (\"inlineData\" in part && part.inlineData) {\n                    results.push(toLLMContentInline(part.inlineData.mimeType, part.inlineData.data));\n                }\n            }\n        }\n    }\n    return results;\n}\nfunction extractMediaData(context) {\n    const results = [];\n    for (const el of context) {\n        for (const part of el.parts) {\n            if (part) {\n                if (\"inlineData\" in part && part.inlineData) {\n                    results.push(toLLMContentInline(part.inlineData.mimeType, part.inlineData.data));\n                }\n                if (\"storedData\" in part && part.storedData) {\n                    results.push(toLLMContentStored(part.storedData.mimeType, part.storedData.handle));\n                }\n            }\n        }\n    }\n    return results;\n}\nfunction extractTextData(context) {\n    const results = [];\n    for (const el of context) {\n        for (const part of el.parts) {\n            if (part) {\n                if (\"text\" in part && part.text) {\n                    results.push(toLLMContent(part.text));\n                }\n            }\n        }\n    }\n    return results;\n}\nfunction toTextConcat(content) {\n    return content.map(toText).join(\"\\n\\n\");\n}\nfunction addUserTurn(content, context) {\n    context ?? (context = []);\n    const isString = typeof content === \"string\";\n    if (!endsWithRole(context, \"user\")) {\n        return [...context, isString ? toLLMContent(content) : content];\n    }\n    const last = context.at(-1);\n    if (isString) {\n        last.parts.push({ text: content });\n    }\n    else {\n        last.parts.push(...content.parts);\n    }\n    return context;\n}\nfunction toLLMContentInline(mimetype, value, role = \"user\") {\n    return {\n        parts: [\n            {\n                inlineData: {\n                    mimeType: mimetype,\n                    data: value,\n                },\n            },\n        ],\n        role,\n    };\n}\nfunction toLLMContentStored(mimetype, handle, role = \"user\") {\n    return {\n        parts: [\n            {\n                storedData: {\n                    mimeType: mimetype,\n                    handle: handle,\n                },\n            },\n        ],\n        role,\n    };\n}\nfunction toInlineData(c) {\n    if (isLLMContent(c)) {\n        return contentToInlineData(c);\n    }\n    const last = c.at(-1);\n    if (!last)\n        return null;\n    return contentToInlineData(last);\n    function contentToInlineData(content) {\n        const part = content.parts.at(-1);\n        if (!part)\n            return \"\";\n        return \"inlineData\" in part && part.inlineData ? part.inlineData : null;\n    }\n}\nfunction toInlineReference(c) {\n    const last = c.parts.at(-1);\n    if (last == undefined || !(\"storedData\" in last)) {\n        return toInlineData(c);\n    }\n    return toInlineData(toLLMContentInline(\"storedData/\" + last.storedData.mimeType, last.storedData.handle));\n}\nexport function mergeContent(content, role) {\n    const parts = [];\n    for (const el of content) {\n        for (const part of el.parts) {\n            parts.push(part);\n        }\n    }\n    return {\n        parts: parts,\n        role: role,\n    };\n}\nfunction generateId() {\n    return Math.random().toString(36).substring(2, 5);\n}\nfunction decodeBase64(s) {\n    const latin1 = atob(s);\n    try {\n        return decodeURIComponent(latin1\n            .split(\"\")\n            .map((c) => `%${c.charCodeAt(0).toString(16).padStart(2, \"0\")}`)\n            .join(\"\"));\n    }\n    catch (error) {\n        console.error(\"Error decoding Base64 UTF-8 string:\", error);\n        return latin1;\n    }\n}\n//# sourceMappingURL=utils.js.map"
    }
  },
  "exports": [
    "#module:researcher",
    "#module:image-generator",
    "#module:image-editor",
    "#module:render-outputs",
    "#module:audio-generator",
    "#21ee02e7-83fa-49d0-964c-0cab10eafc2c",
    "#module:combine-outputs",
    "#module:make-code"
  ],
  "graphs": {
    "21ee02e7-83fa-49d0-964c-0cab10eafc2c": {
      "title": "Ask User",
      "description": "A block of text as input or output",
      "version": "0.0.1",
      "nodes": [
        {
          "type": "input",
          "id": "input",
          "metadata": {
            "visual": {
              "x": 580.0000000000005,
              "y": -539.9999999999994,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "Waiting for user input",
            "logLevel": "info"
          },
          "configuration": {}
        },
        {
          "type": "output",
          "id": "output",
          "configuration": {
            "schema": {
              "properties": {
                "context": {
                  "type": "array",
                  "title": "Context",
                  "items": {
                    "type": "object",
                    "behavior": [
                      "llm-content"
                    ]
                  },
                  "default": "null"
                }
              },
              "type": "object",
              "required": []
            }
          },
          "metadata": {
            "visual": {
              "x": 1240.0000000000005,
              "y": -399.99999999999943,
              "collapsed": "expanded",
              "outputHeight": 44
            }
          }
        },
        {
          "id": "board-64b2c3a8",
          "type": "#module:text-entry",
          "metadata": {
            "visual": {
              "x": 225.9030760391795,
              "y": -646.8568148490385,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "text-entry"
          }
        },
        {
          "id": "board-95a57400",
          "type": "#module:text-main",
          "metadata": {
            "visual": {
              "x": 900,
              "y": -459.99999999999943,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "text-main"
          }
        }
      ],
      "edges": [
        {
          "from": "board-64b2c3a8",
          "out": "toInput",
          "to": "input",
          "in": "schema"
        },
        {
          "from": "board-64b2c3a8",
          "out": "toMain",
          "to": "board-95a57400",
          "in": "request"
        },
        {
          "from": "board-95a57400",
          "to": "output",
          "out": "context",
          "in": "context"
        },
        {
          "from": "board-64b2c3a8",
          "to": "board-95a57400",
          "out": "context",
          "in": "context"
        },
        {
          "from": "input",
          "to": "board-95a57400",
          "out": "request",
          "in": "request"
        }
      ],
      "metadata": {
        "visual": {
          "minimized": false
        },
        "tags": [],
        "describer": "module:text-entry",
        "icon": "text"
      }
    }
  },
  "assets": {
    "@@thumbnail": {
      "metadata": {
        "title": "Thumbnail",
        "type": "file"
      },
      "data": "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjUwIiBoZWlnaHQ9IjIwMCIgdmlld0JveD0iMCAwIDI1MCAyMDAiIGZpbGw9Im5vbmUiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+CiAgICAKICAgICAgPHJlY3QgeD0iNzMuOTIiCiAgICAgICAgICAgICAgICAgICAgeT0iODQuMDEiCiAgICAgICAgICAgICAgICAgICAgd2lkdGg9IjQ2Ljk0IgogICAgICAgICAgICAgICAgICAgIGhlaWdodD0iMjUuOTkiCiAgICAgICAgICAgICAgICAgICAgcng9IjMuNSIKICAgICAgICAgICAgICAgICAgICBmaWxsPSJ3aGl0ZSIKICAgICAgICAgICAgICAgICAgICBzdHJva2U9IiMyMGEyMDIiIC8+CjxyZWN0IHg9IjE5My4wNiIKICAgICAgICAgICAgICAgICAgICB5PSIxMDkuMjgiCiAgICAgICAgICAgICAgICAgICAgd2lkdGg9IjQ2Ljk0IgogICAgICAgICAgICAgICAgICAgIGhlaWdodD0iMjUuOTkiCiAgICAgICAgICAgICAgICAgICAgcng9IjMuNSIKICAgICAgICAgICAgICAgICAgICBmaWxsPSJ3aGl0ZSIKICAgICAgICAgICAgICAgICAgICBzdHJva2U9IiMyMGEyMDIiIC8+CjxyZWN0IHg9IjEwLjAwIgogICAgICAgICAgICAgICAgICAgIHk9IjY0LjcyIgogICAgICAgICAgICAgICAgICAgIHdpZHRoPSI0Ni45NCIKICAgICAgICAgICAgICAgICAgICBoZWlnaHQ9IjI1Ljk5IgogICAgICAgICAgICAgICAgICAgIHJ4PSIzLjUiCiAgICAgICAgICAgICAgICAgICAgZmlsbD0id2hpdGUiCiAgICAgICAgICAgICAgICAgICAgc3Ryb2tlPSIjMmU4YmU4IiAvPgo8cmVjdCB4PSIxMzEuNjkiCiAgICAgICAgICAgICAgICAgICAgeT0iOTguNDUiCiAgICAgICAgICAgICAgICAgICAgd2lkdGg9IjQ2Ljk0IgogICAgICAgICAgICAgICAgICAgIGhlaWdodD0iMjUuOTkiCiAgICAgICAgICAgICAgICAgICAgcng9IjMuNSIKICAgICAgICAgICAgICAgICAgICBmaWxsPSJ3aGl0ZSIKICAgICAgICAgICAgICAgICAgICBzdHJva2U9IiMyZThiZTgiIC8+CiAgICA8L3N2Zz4="
    }
  }
}