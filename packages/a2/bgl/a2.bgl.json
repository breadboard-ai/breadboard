{
  "title": "A2",
  "description": "Calls an LLM and so much more. Insert real description here.",
  "version": "0.0.1",
  "nodes": [],
  "edges": [],
  "metadata": {
    "comments": [
      {
        "id": "comment-b09617ef",
        "text": "Left Intentionally Blank",
        "metadata": {
          "visual": {
            "x": -37.90625,
            "y": -415.85546875,
            "collapsed": "expanded"
          }
        }
      }
    ],
    "visual": {},
    "tags": [
      "published",
      "tool",
      "component"
    ]
  },
  "modules": {
    "common": {
      "code": "/**\n * @fileoverview Common types and code\n */\n",
      "metadata": {
        "title": "common",
        "source": {
          "code": "/**\n * @fileoverview Common types and code\n */\n\nexport type UserInput = unknown;\n\nexport type AgentInputs = {\n  /**\n   * Whether (true) or not (false) the agent is allowed to chat with user.\n   */\n  chat: boolean;\n  /**\n   * The incoming conversation context.\n   */\n  context: LLMContent[];\n  /**\n   * Accumulated work context. This is the internal conversation, a result\n   * of talking with the user, for instance.\n   * This context is discarded at the end of interacting with the agent.\n   */\n  work: LLMContent[];\n  /**\n   * Agent's job description.\n   */\n  description?: LLMContent;\n  /**\n   * Type of the task.\n   */\n  type: \"introduction\" | \"work\";\n  /**\n   * The board URL of the model\n   */\n  model: string;\n  /**\n   * The default model that is passed along by the manager\n   */\n  defaultModel: string;\n  /**\n   * The tools that the worker can use\n   */\n  tools?: string[];\n};\n\nexport type AgentContext = AgentInputs & {\n  /**\n   * A unique identifier for the session.\n   * Currently used to have a persistent part separator across conversation context\n   */\n  id: string;\n  /**\n   * Accumulating list of user inputs\n   */\n  userInputs: UserInput[];\n};\n",
          "language": "typescript"
        },
        "description": "Common types and code",
        "runnable": false
      }
    },
    "utils": {
      "code": "/**\n * @fileoverview Common utils for manipulating LLM Content and other relevant types.\n */\nexport { isLLMContent, isLLMContentArray, toLLMContent, toText, contentToJSON, defaultLLMContent, };\nexport { ok, err };\nfunction ok(o) {\n    return !(o && typeof o === \"object\" && \"$error\" in o);\n}\nfunction err($error) {\n    return { $error };\n}\n/**\n * Copied from @google-labs/breadboard\n */\nfunction isLLMContent(nodeValue) {\n    if (typeof nodeValue !== \"object\" || !nodeValue)\n        return false;\n    if (nodeValue === null || nodeValue === undefined)\n        return false;\n    if (\"role\" in nodeValue && nodeValue.role === \"$metadata\") {\n        return true;\n    }\n    return \"parts\" in nodeValue && Array.isArray(nodeValue.parts);\n}\nfunction isLLMContentArray(nodeValue) {\n    if (!Array.isArray(nodeValue))\n        return false;\n    if (nodeValue.length === 0)\n        return true;\n    return isLLMContent(nodeValue.at(-1));\n}\nfunction toLLMContent(text, role = \"user\") {\n    return { parts: [{ text }], role };\n}\nfunction toText(c) {\n    if (isLLMContent(c)) {\n        return contentToText(c);\n    }\n    const last = c.at(-1);\n    if (!last)\n        return \"\";\n    return contentToText(last);\n    function contentToText(content) {\n        return content.parts\n            .map((part) => (\"text\" in part ? part.text : \"\"))\n            .join(\"\\n\\n\");\n    }\n}\nfunction contentToJSON(content) {\n    const part = content?.parts?.at(0);\n    if (!part || !(\"text\" in part)) {\n        throw new Error(\"Invalid response from Gemini\");\n    }\n    return JSON.parse(part.text);\n}\nfunction defaultLLMContent() {\n    return JSON.stringify({\n        parts: [{ text: \"\" }],\n        role: \"user\",\n    });\n}\n",
      "metadata": {
        "title": "utils",
        "source": {
          "code": "/**\n * @fileoverview Common utils for manipulating LLM Content and other relevant types.\n */\nexport {\n  isLLMContent,\n  isLLMContentArray,\n  toLLMContent,\n  toText,\n  contentToJSON,\n  defaultLLMContent,\n};\n\nexport { ok, err };\n\nfunction ok<T>(o: Outcome<T>): o is T {\n  return !(o && typeof o === \"object\" && \"$error\" in o);\n}\n\nfunction err($error: string) {\n  return { $error };\n}\n\n/**\n * Copied from @google-labs/breadboard\n */\nfunction isLLMContent(nodeValue: unknown): nodeValue is LLMContent {\n  if (typeof nodeValue !== \"object\" || !nodeValue) return false;\n  if (nodeValue === null || nodeValue === undefined) return false;\n\n  if (\"role\" in nodeValue && nodeValue.role === \"$metadata\") {\n    return true;\n  }\n\n  return \"parts\" in nodeValue && Array.isArray(nodeValue.parts);\n}\n\nfunction isLLMContentArray(nodeValue: unknown): nodeValue is LLMContent[] {\n  if (!Array.isArray(nodeValue)) return false;\n  if (nodeValue.length === 0) return true;\n  return isLLMContent(nodeValue.at(-1));\n}\n\nfunction toLLMContent(\n  text: string,\n  role: LLMContent[\"role\"] = \"user\"\n): LLMContent {\n  return { parts: [{ text }], role };\n}\n\nfunction toText(c: LLMContent | LLMContent[]): string {\n  if (isLLMContent(c)) {\n    return contentToText(c);\n  }\n  const last = c.at(-1);\n  if (!last) return \"\";\n  return contentToText(last);\n\n  function contentToText(content: LLMContent) {\n    return content.parts\n      .map((part) => (\"text\" in part ? part.text : \"\"))\n      .join(\"\\n\\n\");\n  }\n}\n\nfunction contentToJSON<T>(content?: LLMContent): T {\n  const part = content?.parts?.at(0);\n  if (!part || !(\"text\" in part)) {\n    throw new Error(\"Invalid response from Gemini\");\n  }\n  return JSON.parse(part.text) as T;\n}\n\nfunction defaultLLMContent(): string {\n  return JSON.stringify({\n    parts: [{ text: \"\" }],\n    role: \"user\",\n  } satisfies LLMContent);\n}\n",
          "language": "typescript"
        },
        "description": "Common utils for manipulating LLM Content and other relevant types.",
        "runnable": false
      }
    },
    "gemini": {
      "code": "/**\n * @fileoverview Gemini Model Family.\n */\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\nimport output from \"@output\";\nimport { ok, err, isLLMContentArray } from \"./utils\";\nconst defaultSafetySettings = () => [\n    {\n        category: \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        threshold: \"BLOCK_NONE\",\n    },\n    {\n        category: \"HARM_CATEGORY_HARASSMENT\",\n        threshold: \"BLOCK_NONE\",\n    },\n    {\n        category: \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        threshold: \"BLOCK_NONE\",\n    },\n];\nasync function endpointURL(model) {\n    const $metadata = {\n        title: \"Get GEMINI_KEY\",\n        description: \"Getting GEMINI_KEY from secrets\",\n    };\n    const key = await secrets({ $metadata, keys: [\"GEMINI_KEY\"] });\n    return `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${key.GEMINI_KEY}`;\n}\nexport { invoke as default, describe, defaultSafetySettings };\nconst VALID_MODALITIES = [\"Text\", \"Text and Image\", \"Audio\"];\nconst MODELS = [\n    \"gemini-1.5-flash-latest\",\n    \"gemini-1.5-pro-latest\",\n    \"gemini-2.0-flash-exp\",\n    \"gemini-2.0-flash-thinking-exp\",\n    \"gemini-exp-1206\",\n    \"gemini-exp-1121\",\n    \"learnlm-1.5-pro-experimental\",\n    \"gemini-1.5-pro-001\",\n    \"gemini-1.5-pro-002\",\n    \"gemini-1.5-pro-exp-0801\",\n    \"gemini-1.5-pro-exp-0827\",\n    \"gemini-1.5-flash-001\",\n    \"gemini-1.5-flash-002\",\n    \"gemini-1.5-flash-8b-exp-0924\",\n    \"gemini-1.5-flash-8b-exp-0827\",\n    \"gemini-1.5-flash-exp-0827\",\n];\nconst NO_RETRY_CODES = [400, 429, 404];\nasync function callAPI(retries, model, body, $metadata) {\n    let $error = \"Unknown error\";\n    while (retries) {\n        const result = await fetch({\n            $metadata,\n            url: await endpointURL(model),\n            method: \"POST\",\n            body,\n        });\n        if (!ok(result)) {\n            // Fetch is a bit weird, because it returns various props\n            // along with the `$error`. Let's handle that here.\n            const { status, $error: errObject } = result;\n            if (!status) {\n                // This is not an error response, presume fatal error.\n                return { $error };\n            }\n            $error = maybeExtractError(errObject);\n            if (NO_RETRY_CODES.includes(status)) {\n                return { $error };\n            }\n        }\n        else {\n            const outputs = result.response;\n            const candidate = outputs.candidates.at(0);\n            if (!candidate) {\n                return err(\"Unable to get a good response from Gemini\");\n            }\n            if (\"content\" in candidate) {\n                return outputs;\n            }\n        }\n        retries--;\n    }\n    return { $error };\n}\nfunction maybeExtractError(e) {\n    try {\n        const parsed = JSON.parse(e);\n        return parsed.error.message;\n    }\n    catch (error) {\n        return e;\n    }\n}\nfunction isEmptyLLMContent(content) {\n    if (!content || !content.parts || content.parts.length === 0)\n        return true;\n    return content.parts.every((part) => {\n        if (\"text\" in part) {\n            return !part.text?.trim();\n        }\n        return true;\n    });\n}\nfunction addModality(body, modality) {\n    if (!modality)\n        return;\n    switch (modality) {\n        case \"Text\":\n            // No change, defaults.\n            break;\n        case \"Text and Image\":\n            body.generationConfig ??= {};\n            body.generationConfig.responseModalities = [\"TEXT\", \"IMAGE\"];\n            break;\n        case \"Audio\":\n            body.generationConfig ??= {};\n            body.generationConfig.responseModalities = [\"AUDIO\"];\n            break;\n    }\n}\nfunction constructBody(context = [], systemInstruction, prompt, modality) {\n    const contents = [...context];\n    if (!isEmptyLLMContent(prompt)) {\n        contents.push(prompt);\n    }\n    const body = {\n        contents,\n        safetySettings: defaultSafetySettings(),\n    };\n    const canHaveSystemInstruction = modality === \"Text\";\n    if (!isEmptyLLMContent(systemInstruction) && canHaveSystemInstruction) {\n        body.systemInstruction = systemInstruction;\n    }\n    addModality(body, modality);\n    return body;\n}\nfunction augmentBody(body, systemInstruction, prompt, modality) {\n    if (!body.systemInstruction || !isEmptyLLMContent(systemInstruction)) {\n        body.systemInstruction = systemInstruction;\n    }\n    if (!isEmptyLLMContent(prompt)) {\n        body.contents = [...body.contents, prompt];\n    }\n    addModality(body, modality);\n    return body;\n}\nfunction validateInputs(inputs) {\n    if (\"body\" in inputs) {\n        return;\n    }\n    if (inputs.context) {\n        const { context } = inputs;\n        if (!Array.isArray(context)) {\n            return err(\"Incoming context must be an array.\");\n        }\n        if (!isLLMContentArray(context)) {\n            return err(\"Malformed incoming context\");\n        }\n        return;\n    }\n    return err(\"Either body or context is required\");\n}\nasync function invoke(inputs) {\n    const validatingInputs = validateInputs(inputs);\n    if (!ok(validatingInputs)) {\n        return validatingInputs;\n    }\n    let { model } = inputs;\n    if (!model) {\n        model = MODELS[0];\n    }\n    const { context, systemInstruction, prompt, modality, body, $metadata } = inputs;\n    // TODO: Make this configurable.\n    const retries = 5;\n    if (!(\"body\" in inputs)) {\n        // Public API is being used.\n        // Behave as if we're wired in.\n        const result = await callAPI(retries, model, constructBody(context, systemInstruction, prompt, modality));\n        if (!ok(result)) {\n            return result;\n        }\n        const content = result.candidates.at(0)?.content;\n        if (!content) {\n            return err(\"Unable to get a good response from Gemini\");\n        }\n        return { context: [...context, content] };\n    }\n    else {\n        // Private API is being used.\n        // Behave as if we're being invoked.\n        return callAPI(retries, model, augmentBody(body, systemInstruction, prompt, modality), $metadata);\n    }\n}\nasync function describe({ inputs }) {\n    const canHaveModalities = inputs.model === \"gemini-2.0-flash-exp\";\n    const canHaveSystemInstruction = !canHaveModalities || (canHaveModalities && inputs.modality == \"Text\");\n    const maybeAddSystemInstruction = canHaveSystemInstruction\n        ? {\n            systemInstruction: {\n                type: \"object\",\n                behavior: [\"llm-content\", \"config\"],\n                title: \"System Instruction\",\n                default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n                description: \"(Optional) Give the model additional context on what to do,\" +\n                    \"like specific rules/guidelines to adhere to or specify behavior\" +\n                    \"separate from the provided context\",\n            },\n        }\n        : {};\n    const maybeAddModalities = canHaveModalities\n        ? {\n            modality: {\n                type: \"string\",\n                enum: [...VALID_MODALITIES],\n                title: \"Output Modality\",\n                behavior: [\"config\"],\n                description: \"(Optional) Tell the model what kind of output you're looking for.\",\n            },\n        }\n        : {};\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                model: {\n                    type: \"string\",\n                    behavior: [\"config\"],\n                    title: \"Model Name\",\n                    enum: MODELS,\n                    default: MODELS[0],\n                },\n                prompt: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\"],\n                    title: \"Prompt\",\n                    default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n                    description: \"(Optional) A prompt. Will be added to the end of the the conversation context.\",\n                },\n                ...maybeAddSystemInstruction,\n                ...maybeAddModalities,\n                context: {\n                    type: \"array\",\n                    items: {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                    },\n                    title: \"Context in\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                    },\n                    title: \"Context out\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "Gemini",
        "source": {
          "code": "/**\n * @fileoverview Gemini Model Family.\n */\n\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\nimport output from \"@output\";\n\nimport { ok, err, isLLMContentArray } from \"./utils\";\n\nconst defaultSafetySettings = (): SafetySetting[] => [\n  {\n    category: \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n    threshold: \"BLOCK_NONE\",\n  },\n  {\n    category: \"HARM_CATEGORY_HARASSMENT\",\n    threshold: \"BLOCK_NONE\",\n  },\n  {\n    category: \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n    threshold: \"BLOCK_NONE\",\n  },\n];\n\nasync function endpointURL(model: string): Promise<string> {\n  const $metadata = {\n    title: \"Get GEMINI_KEY\",\n    description: \"Getting GEMINI_KEY from secrets\",\n  };\n  const key = await secrets({ $metadata, keys: [\"GEMINI_KEY\"] });\n  return `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${key.GEMINI_KEY}`;\n}\n\nexport { invoke as default, describe, defaultSafetySettings };\n\nconst VALID_MODALITIES = [\"Text\", \"Text and Image\", \"Audio\"] as const;\ntype ValidModalities = (typeof VALID_MODALITIES)[number];\n\nexport type HarmBlockThreshold =\n  // Content with NEGLIGIBLE will be allowed.\n  | \"BLOCK_LOW_AND_ABOVE\"\n  // Content with NEGLIGIBLE and LOW will be allowed.\n  | \"BLOCK_MEDIUM_AND_ABOVE\"\n  // Content with NEGLIGIBLE, LOW, and MEDIUM will be allowed.\n  | \"BLOCK_ONLY_HIGH\"\n  // All content will be allowed.\n  | \"BLOCK_NONE\"\n  // Turn off the safety filter.\n  | \"OFF\";\n\nexport type HarmCategory =\n  // Gemini - Harassment content\n  | \"HARM_CATEGORY_HARASSMENT\"\n  //\tGemini - Hate speech and content.\n  | \"HARM_CATEGORY_HATE_SPEECH\"\n  // Gemini - Sexually explicit content.\n  | \"HARM_CATEGORY_SEXUALLY_EXPLICIT\"\n  // \tGemini - Dangerous content.\n  | \"HARM_CATEGORY_DANGEROUS_CONTENT\"\n  // Gemini - Content that may be used to harm civic integrity.\n  | \"HARM_CATEGORY_CIVIC_INTEGRITY\";\n\nexport type GeminiSchema = {\n  type: \"string\" | \"number\" | \"integer\" | \"boolean\" | \"object\" | \"array\";\n  format?: string;\n  description?: string;\n  nullable?: boolean;\n  enum?: string[];\n  maxItems?: string;\n  minItems?: string;\n  properties?: Record<string, GeminiSchema>;\n  required?: string[];\n  items?: GeminiSchema;\n};\n\nexport type Modality = \"TEXT\" | \"IMAGE\" | \"AUDIO\";\n\nexport type GenerationConfig = {\n  responseMimeType?: \"text/plain\" | \"application/json\" | \"text/x.enum\";\n  responseSchema?: GeminiSchema;\n  responseModalities?: Modality[];\n};\n\nexport type SafetySetting = {\n  category: HarmCategory;\n  threshold: HarmBlockThreshold;\n};\n\nexport type Metadata = {\n  title?: string;\n  description?: string;\n};\n\nexport type GeminiBody = {\n  contents: LLMContent[];\n  tools?: Tool[];\n  toolConfig?: ToolConfig;\n  systemInstruction?: LLMContent;\n  safetySettings?: SafetySetting[];\n  generationConfig?: GenerationConfig;\n};\n\nexport type GeminiInputs = {\n  // The wireable/configurable properties.\n  model?: string;\n  context?: LLMContent[];\n  systemInstruction?: LLMContent;\n  prompt?: LLMContent;\n  modality?: ValidModalities;\n  // The \"private API\" properties\n  $metadata?: Metadata;\n  body: GeminiBody;\n};\n\nexport type Tool = {\n  functionDeclarations?: FunctionDeclaration[];\n  googleSearchRetrieval?: GoogleSearchRetrieval[];\n  codeExecution?: CodeExecution[];\n};\n\nexport type ToolConfig = {\n  functionCallingConfig?: FunctionCallingConfig;\n};\n\nexport type FunctionCallingConfig = {\n  mode?: \"MODE_UNSPECIFIED\" | \"AUTO\" | \"ANY\" | \"NONE\";\n  allowedFunctionNames?: string[];\n};\n\nexport type FunctionDeclaration = {\n  name: string;\n  description: string;\n  parameters: GeminiSchema;\n};\n\nexport type GoogleSearchRetrieval = {\n  dynamicRetrievalConfig: {\n    mode: \"MODE_UNSPECIFIED\" | \"MODE_DYNAMIC\";\n    dynamicThreshold: number;\n  };\n};\n\nexport type CodeExecution = {\n  // Type contains no fields.\n};\n\nexport type FinishReason =\n  // Natural stop point of the model or provided stop sequence.\n  | \"STOP\"\n  // The maximum number of tokens as specified in the request was reached.\n  | \"MAX_TOKENS\"\n  // The response candidate content was flagged for safety reasons.\n  | \"SAFETY\"\n  // The response candidate content was flagged for recitation reasons.\n  | \"RECITATION\"\n  // The response candidate content was flagged for using an unsupported language.\n  | \"LANGUAGE\"\n  // Unknown reason.\n  | \"OTHER\"\n  // Token generation stopped because the content contains forbidden terms.\n  | \"BLOCKLIST\"\n  // Token generation stopped for potentially containing prohibited content.\n  | \"PROHIBITED_CONTENT\"\n  // Token generation stopped because the content potentially contains Sensitive Personally Identifiable Information (SPII).\n  | \"SPII\"\n  // The function call generated by the model is invalid.\n  | \"MALFORMED_FUNCTION_CALL\";\n\nexport type GroundingMetadata = {\n  groundingChunks: {\n    web: {\n      uri: string;\n      title: string;\n    };\n  }[];\n  groundingSupports: {\n    groundingChunkIndices: number[];\n    confidenceScores: number[];\n    segment: {\n      partIndex: number;\n      startIndex: number;\n      endIndex: number;\n      text: string;\n    };\n  };\n  webSearchQueries: string[];\n  searchEntryPoint: {\n    renderedContent: string;\n    /**\n     * Base64 encoded JSON representing array of <search term, search url> tuple.\n     * A base64-encoded string.\n     */\n    sdkBlob: string;\n  };\n  retrievalMetadata: {\n    googleSearchDynamicRetrievalScore: number;\n  };\n};\n\nexport type Candidate = {\n  content?: LLMContent;\n  finishReason?: FinishReason;\n  safetyRatings?: SafetySetting[];\n  tokenOutput: number;\n  groundingMetadata: GroundingMetadata;\n};\n\nexport type GeminiAPIOutputs = {\n  candidates: Candidate[];\n};\n\nexport type GeminiOutputs =\n  | GeminiAPIOutputs\n  | {\n      context: LLMContent[];\n    };\n\nconst MODELS: readonly string[] = [\n  \"gemini-1.5-flash-latest\",\n  \"gemini-1.5-pro-latest\",\n  \"gemini-2.0-flash-exp\",\n  \"gemini-2.0-flash-thinking-exp\",\n  \"gemini-exp-1206\",\n  \"gemini-exp-1121\",\n  \"learnlm-1.5-pro-experimental\",\n  \"gemini-1.5-pro-001\",\n  \"gemini-1.5-pro-002\",\n  \"gemini-1.5-pro-exp-0801\",\n  \"gemini-1.5-pro-exp-0827\",\n  \"gemini-1.5-flash-001\",\n  \"gemini-1.5-flash-002\",\n  \"gemini-1.5-flash-8b-exp-0924\",\n  \"gemini-1.5-flash-8b-exp-0827\",\n  \"gemini-1.5-flash-exp-0827\",\n];\n\nconst NO_RETRY_CODES: readonly number[] = [400, 429, 404];\n\ntype FetchErrorResponse = {\n  $error: string;\n  status: number;\n  statusText: string;\n  contentType: string;\n  responseHeaders: Record<string, string>;\n};\n\n/**\n * Using\n * `{\"error\":{\"code\":400,\"message\":\"Invalid JSON payloâ€¦'contents[0].parts[0]': Cannot find field.\"}]}]}\n * as template for this type.\n */\ntype GeminiError = {\n  error: {\n    code: number;\n    details: {\n      type: string;\n      fieldViolations: {\n        description: string;\n        field: string;\n      }[];\n    }[];\n    message: string;\n    status: string;\n  };\n};\n\nasync function callAPI(\n  retries: number,\n  model: string,\n  body: GeminiBody,\n  $metadata?: Metadata\n): Promise<Outcome<GeminiAPIOutputs>> {\n  let $error: string = \"Unknown error\";\n  while (retries) {\n    const result = await fetch({\n      $metadata,\n      url: await endpointURL(model),\n      method: \"POST\",\n      body,\n    });\n    if (!ok(result)) {\n      // Fetch is a bit weird, because it returns various props\n      // along with the `$error`. Let's handle that here.\n      const { status, $error: errObject } = result as FetchErrorResponse;\n      if (!status) {\n        // This is not an error response, presume fatal error.\n        return { $error };\n      }\n      $error = maybeExtractError(errObject);\n      if (NO_RETRY_CODES.includes(status)) {\n        return { $error };\n      }\n    } else {\n      const outputs = result.response as GeminiAPIOutputs;\n      const candidate = outputs.candidates.at(0);\n      if (!candidate) {\n        return err(\"Unable to get a good response from Gemini\");\n      }\n      if (\"content\" in candidate) {\n        return outputs;\n      }\n    }\n    retries--;\n  }\n  return { $error };\n}\n\nfunction maybeExtractError(e: string): string {\n  try {\n    const parsed = JSON.parse(e) as GeminiError;\n    return parsed.error.message;\n  } catch (error) {\n    return e;\n  }\n}\n\nfunction isEmptyLLMContent(content?: LLMContent): content is undefined {\n  if (!content || !content.parts || content.parts.length === 0) return true;\n  return content.parts.every((part) => {\n    if (\"text\" in part) {\n      return !part.text?.trim();\n    }\n    return true;\n  });\n}\n\nfunction addModality(body: GeminiBody, modality?: ValidModalities) {\n  if (!modality) return;\n  switch (modality) {\n    case \"Text\":\n      // No change, defaults.\n      break;\n    case \"Text and Image\":\n      body.generationConfig ??= {};\n      body.generationConfig.responseModalities = [\"TEXT\", \"IMAGE\"];\n      break;\n    case \"Audio\":\n      body.generationConfig ??= {};\n      body.generationConfig.responseModalities = [\"AUDIO\"];\n      break;\n  }\n}\n\nfunction constructBody(\n  context: LLMContent[] = [],\n  systemInstruction?: LLMContent,\n  prompt?: LLMContent,\n  modality?: ValidModalities\n): GeminiBody {\n  const contents = [...context];\n  if (!isEmptyLLMContent(prompt)) {\n    contents.push(prompt);\n  }\n  const body: GeminiBody = {\n    contents,\n    safetySettings: defaultSafetySettings(),\n  };\n  const canHaveSystemInstruction = modality === \"Text\";\n  if (!isEmptyLLMContent(systemInstruction) && canHaveSystemInstruction) {\n    body.systemInstruction = systemInstruction;\n  }\n  addModality(body, modality);\n  return body;\n}\n\nfunction augmentBody(\n  body: GeminiBody,\n  systemInstruction?: LLMContent,\n  prompt?: LLMContent,\n  modality?: ValidModalities\n): GeminiBody {\n  if (!body.systemInstruction || !isEmptyLLMContent(systemInstruction)) {\n    body.systemInstruction = systemInstruction;\n  }\n  if (!isEmptyLLMContent(prompt)) {\n    body.contents = [...body.contents, prompt];\n  }\n  addModality(body, modality);\n  return body;\n}\n\nfunction validateInputs(inputs: GeminiInputs): Outcome<void> {\n  if (\"body\" in (inputs as object)) {\n    return;\n  }\n  if (inputs.context) {\n    const { context } = inputs;\n    if (!Array.isArray(context)) {\n      return err(\"Incoming context must be an array.\");\n    }\n    if (!isLLMContentArray(context)) {\n      return err(\"Malformed incoming context\");\n    }\n    return;\n  }\n  return err(\"Either body or context is required\");\n}\n\nasync function invoke(inputs: GeminiInputs): Promise<Outcome<GeminiOutputs>> {\n  const validatingInputs = validateInputs(inputs);\n  if (!ok(validatingInputs)) {\n    return validatingInputs;\n  }\n  let { model } = inputs;\n  if (!model) {\n    model = MODELS[0];\n  }\n  const { context, systemInstruction, prompt, modality, body, $metadata } =\n    inputs;\n  // TODO: Make this configurable.\n  const retries = 5;\n  if (!(\"body\" in inputs)) {\n    // Public API is being used.\n    // Behave as if we're wired in.\n    const result = await callAPI(\n      retries,\n      model,\n      constructBody(context, systemInstruction, prompt, modality)\n    );\n    if (!ok(result)) {\n      return result;\n    }\n    const content = result.candidates.at(0)?.content;\n    if (!content) {\n      return err(\"Unable to get a good response from Gemini\");\n    }\n    return { context: [...context!, content] };\n  } else {\n    // Private API is being used.\n    // Behave as if we're being invoked.\n    return callAPI(\n      retries,\n      model,\n      augmentBody(body, systemInstruction, prompt, modality),\n      $metadata\n    );\n  }\n}\n\ntype DescribeInputs = {\n  inputs: {\n    modality?: ValidModalities;\n    model: string;\n  };\n};\n\nasync function describe({ inputs }: DescribeInputs) {\n  const canHaveModalities = inputs.model === \"gemini-2.0-flash-exp\";\n  const canHaveSystemInstruction =\n    !canHaveModalities || (canHaveModalities && inputs.modality == \"Text\");\n  const maybeAddSystemInstruction: Schema[\"properties\"] =\n    canHaveSystemInstruction\n      ? {\n          systemInstruction: {\n            type: \"object\",\n            behavior: [\"llm-content\", \"config\"],\n            title: \"System Instruction\",\n            default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n            description:\n              \"(Optional) Give the model additional context on what to do,\" +\n              \"like specific rules/guidelines to adhere to or specify behavior\" +\n              \"separate from the provided context\",\n          },\n        }\n      : {};\n  const maybeAddModalities: Schema[\"properties\"] = canHaveModalities\n    ? {\n        modality: {\n          type: \"string\",\n          enum: [...VALID_MODALITIES],\n          title: \"Output Modality\",\n          behavior: [\"config\"],\n          description:\n            \"(Optional) Tell the model what kind of output you're looking for.\",\n        },\n      }\n    : {};\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        model: {\n          type: \"string\",\n          behavior: [\"config\"],\n          title: \"Model Name\",\n          enum: MODELS as string[],\n          default: MODELS[0],\n        },\n        prompt: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\"],\n          title: \"Prompt\",\n          default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n          description:\n            \"(Optional) A prompt. Will be added to the end of the the conversation context.\",\n        },\n        ...maybeAddSystemInstruction,\n        ...maybeAddModalities,\n        context: {\n          type: \"array\",\n          items: {\n            type: \"object\",\n            behavior: [\"llm-content\"],\n          },\n          title: \"Context in\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: {\n            type: \"object\",\n            behavior: [\"llm-content\"],\n          },\n          title: \"Context out\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Gemini Model Family.",
        "runnable": false
      }
    },
    "entry": {
      "code": "/**\n * @fileoverview Manages the entry point: describer, passing the inputs, etc.\n */\nimport {} from \"./common\";\nimport { toLLMContent, defaultLLMContent } from \"./utils\";\nexport { invoke as default, describe };\nasync function invoke({ context, chat, defaultModel, description, type = \"work\", }) {\n    // Make sure it's a boolean.\n    chat = !!chat;\n    context ??= [];\n    return {\n        context: {\n            id: Math.random().toString(36).substring(2, 5),\n            chat,\n            context,\n            userInputs: [],\n            defaultModel,\n            model: \"\",\n            description,\n            tools: [],\n            type,\n            work: [],\n        },\n    };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                description: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\"],\n                    title: \"Job Description\",\n                    description: \"A detailed list of skills and capabilities of this agent.\",\n                    default: defaultLLMContent(),\n                },\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                },\n                \"p-chat\": {\n                    type: \"boolean\",\n                    title: \"Chat with User\",\n                    behavior: [\"config\"],\n                    description: \"When checked, the agent will talk with the user, asking to review work, requesting additional information, etc.\",\n                },\n                \"p-critique\": {\n                    type: \"boolean\",\n                    title: \"Self-critique\",\n                    behavior: [\"config\"],\n                    description: \"When checked, the agent will critique itself to ensure the best quality output, in exchange for taking a little bit more time.\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "entry",
        "source": {
          "code": "/**\n * @fileoverview Manages the entry point: describer, passing the inputs, etc.\n */\n\nimport { type AgentContext, type AgentInputs } from \"./common\";\nimport { toLLMContent, defaultLLMContent } from \"./utils\";\n\nexport { invoke as default, describe };\n\ntype Outputs = {\n  context: AgentContext;\n};\n\nasync function invoke({\n  context,\n  chat,\n  defaultModel,\n  description,\n  type = \"work\",\n}: AgentInputs): Promise<Outputs> {\n  // Make sure it's a boolean.\n  chat = !!chat;\n  context ??= [];\n  return {\n    context: {\n      id: Math.random().toString(36).substring(2, 5),\n      chat,\n      context,\n      userInputs: [],\n      defaultModel,\n      model: \"\",\n      description,\n      tools: [],\n      type,\n      work: [],\n    },\n  };\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        description: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\"],\n          title: \"Job Description\",\n          description:\n            \"A detailed list of skills and capabilities of this agent.\",\n          default: defaultLLMContent(),\n        },\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n        },\n        \"p-chat\": {\n          type: \"boolean\",\n          title: \"Chat with User\",\n          behavior: [\"config\"],\n          description:\n            \"When checked, the agent will talk with the user, asking to review work, requesting additional information, etc.\",\n        },\n        \"p-critique\": {\n          type: \"boolean\",\n          title: \"Self-critique\",\n          behavior: [\"config\"],\n          description:\n            \"When checked, the agent will critique itself to ensure the best quality output, in exchange for taking a little bit more time.\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Manages the entry point: describer, passing the inputs, etc.",
        "runnable": true
      }
    },
    "join": {
      "code": "/**\n * @fileoverview Joins user input and Agent Context\n */\nimport {} from \"./common\";\nexport { invoke as default, describe };\nasync function invoke({ context, request }) {\n    context.userInputs.push(request);\n    context.work.push(request);\n    return { context };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    title: \"Agent Context\",\n                    type: \"object\",\n                },\n                request: {\n                    title: \"User Input\",\n                    type: \"object\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    title: \"Agent Context\",\n                    type: \"object\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "join",
        "source": {
          "code": "/**\n * @fileoverview Joins user input and Agent Context\n */\n\nimport { type AgentContext } from \"./common\";\n\nexport { invoke as default, describe };\n\ntype Inputs = {\n  context: AgentContext;\n  request: LLMContent;\n};\n\ntype Outputs = {\n  context: AgentContext;\n};\n\nasync function invoke({ context, request }: Inputs): Promise<Outputs> {\n  context.userInputs.push(request);\n  context.work.push(request);\n  return { context };\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          title: \"Agent Context\",\n          type: \"object\",\n        },\n        request: {\n          title: \"User Input\",\n          type: \"object\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          title: \"Agent Context\",\n          type: \"object\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Joins user input and Agent Context",
        "runnable": true
      }
    },
    "output": {
      "code": "/**\n * @fileoverview Provides an output helper.\n */\nimport output from \"@output\";\nexport { report };\nasync function report(inputs) {\n    const { actor: title, category: description, name, details } = inputs;\n    const detailsSchema = typeof details === \"string\"\n        ? {\n            title: name,\n            type: \"string\",\n            format: \"markdown\",\n        }\n        : {\n            title: name,\n            type: \"object\",\n            behavior: [\"llm-content\"],\n        };\n    const { delivered } = await output({\n        $metadata: {\n            title,\n            description,\n        },\n        schema: {\n            type: \"object\",\n            properties: {\n                details: detailsSchema,\n            },\n        },\n        details,\n    });\n    return delivered;\n}\n",
      "metadata": {
        "title": "output",
        "source": {
          "code": "/**\n * @fileoverview Provides an output helper.\n */\n\nimport output from \"@output\";\n\ntype ReportInputs = {\n  /**\n   * The name of the actor providing the report\n   */\n  actor: string;\n  /**\n   * The general category of the report\n   */\n  category: string;\n  /**\n   * The name of the report\n   */\n  name: string;\n  /**\n   * The details of the report\n   */\n  details: string | LLMContent;\n};\n\nexport { report };\n\nasync function report(inputs: ReportInputs): Promise<boolean> {\n  const { actor: title, category: description, name, details } = inputs;\n\n  const detailsSchema: Schema =\n    typeof details === \"string\"\n      ? {\n          title: name,\n          type: \"string\",\n          format: \"markdown\",\n        }\n      : {\n          title: name,\n          type: \"object\",\n          behavior: [\"llm-content\"],\n        };\n\n  const { delivered } = await output({\n    $metadata: {\n      title,\n      description,\n    },\n    schema: {\n      type: \"object\",\n      properties: {\n        details: detailsSchema,\n      },\n    } satisfies Schema,\n    details,\n  });\n  return delivered;\n}\n",
          "language": "typescript"
        },
        "description": "Provides an output helper.",
        "runnable": false
      }
    },
    "tool-manager": {
      "code": "/**\n * @fileoverview Add a description for your module here.\n */\nimport describeGraph from \"@describe\";\nimport {} from \"./gemini\";\nexport { ToolManager };\nclass ToolManager {\n    tools = new Map();\n    errors = [];\n    #convertSchemas(schema) {\n        return toGeminiSchema(schema);\n        function toGeminiSchema(schema) {\n            switch (schema.type) {\n                case \"object\": {\n                    if (!schema.properties) {\n                        return { type: \"object\" };\n                    }\n                    return {\n                        type: \"object\",\n                        properties: Object.fromEntries(Object.entries(schema.properties).map(([name, schema]) => {\n                            return [name, toGeminiSchema(schema)];\n                        })),\n                    };\n                }\n                case \"array\": {\n                    return {\n                        type: \"array\",\n                        items: toGeminiSchema(schema),\n                    };\n                }\n                default: {\n                    const geminiSchema = { ...schema };\n                    delete geminiSchema.format;\n                    delete geminiSchema.behavior;\n                    delete geminiSchema.examples;\n                    delete geminiSchema.default;\n                    delete geminiSchema.transient;\n                    if (!geminiSchema.description) {\n                        geminiSchema.description = geminiSchema.title;\n                    }\n                    delete geminiSchema.title;\n                    return geminiSchema;\n                }\n            }\n        }\n    }\n    #toName(title) {\n        return title ? title.replace(/\\W/g, \"_\") : \"function\";\n    }\n    async initialize(tools) {\n        if (!tools) {\n            return true;\n        }\n        let hasInvalidTools = false;\n        for (const tool of tools) {\n            const url = typeof tool === \"string\" ? tool : tool.url;\n            const description = await describeGraph({ url });\n            if (description.$error) {\n                this.errors.push(description.$error);\n                // Invalid tool, skip\n                hasInvalidTools = true;\n                continue;\n            }\n            const parameters = this.#convertSchemas(description.inputSchema);\n            const name = this.#toName(description.title);\n            const functionDeclaration = {\n                name,\n                description: description.description || \"\",\n                parameters,\n            };\n            this.tools.set(name, { tool: functionDeclaration, url });\n        }\n        return !hasInvalidTools;\n    }\n    async processResponse(response, callTool) {\n        for (const part of response.parts) {\n            if (\"functionCall\" in part) {\n                const { args, name } = part.functionCall;\n                const url = this.tools.get(name)?.url;\n                if (url) {\n                    await callTool(url, part.functionCall.args);\n                }\n            }\n        }\n    }\n    list() {\n        const entries = [...this.tools.entries()];\n        if (entries.length === 0)\n            return [];\n        return [\n            {\n                functionDeclarations: entries.map(([, value]) => value.tool),\n            },\n        ];\n    }\n}\n",
      "metadata": {
        "title": "tool-manager",
        "source": {
          "code": "/**\n * @fileoverview Add a description for your module here.\n */\n\nimport describeGraph from \"@describe\";\n\nimport {\n  type FunctionDeclaration,\n  type GeminiSchema,\n  type Tool,\n} from \"./gemini\";\n\nexport type CallToolCallback = (tool: string, args: object) => Promise<void>;\n\nexport type ToolHandle = {\n  tool: FunctionDeclaration;\n  url: string;\n};\n\nexport type ToolDescriptor =\n  | string\n  | {\n      kind: \"board\";\n      url: string;\n    };\n\nexport { ToolManager };\n\nclass ToolManager {\n  tools: Map<string, ToolHandle> = new Map();\n  errors: string[] = [];\n\n  #convertSchemas(schema: Schema): GeminiSchema {\n    return toGeminiSchema(schema);\n\n    function toGeminiSchema(schema: Schema): GeminiSchema {\n      switch (schema.type) {\n        case \"object\": {\n          if (!schema.properties) {\n            return { type: \"object\" };\n          }\n          return {\n            type: \"object\",\n            properties: Object.fromEntries(\n              Object.entries(schema.properties).map(([name, schema]) => {\n                return [name, toGeminiSchema(schema)];\n              })\n            ),\n          };\n        }\n        case \"array\": {\n          return {\n            type: \"array\",\n            items: toGeminiSchema(schema),\n          };\n        }\n        default: {\n          const geminiSchema = { ...schema };\n          delete geminiSchema.format;\n          delete geminiSchema.behavior;\n          delete geminiSchema.examples;\n          delete geminiSchema.default;\n          delete geminiSchema.transient;\n          if (!geminiSchema.description) {\n            geminiSchema.description = geminiSchema.title;\n          }\n          delete geminiSchema.title;\n          return geminiSchema as GeminiSchema;\n        }\n      }\n    }\n  }\n\n  #toName(title?: string) {\n    return title ? title.replace(/\\W/g, \"_\") : \"function\";\n  }\n\n  async initialize(tools?: ToolDescriptor[]): Promise<boolean> {\n    if (!tools) {\n      return true;\n    }\n    let hasInvalidTools = false;\n    for (const tool of tools) {\n      const url = typeof tool === \"string\" ? tool : tool.url;\n      const description = await describeGraph({ url });\n      if (description.$error) {\n        this.errors.push(description.$error);\n        // Invalid tool, skip\n        hasInvalidTools = true;\n        continue;\n      }\n      const parameters = this.#convertSchemas(description.inputSchema);\n      const name = this.#toName(description.title);\n      const functionDeclaration = {\n        name,\n        description: description.description || \"\",\n        parameters,\n      };\n      this.tools.set(name, { tool: functionDeclaration, url });\n    }\n    return !hasInvalidTools;\n  }\n\n  async processResponse(response: LLMContent, callTool: CallToolCallback) {\n    for (const part of response.parts) {\n      if (\"functionCall\" in part) {\n        const { args, name } = part.functionCall;\n        const url = this.tools.get(name)?.url;\n        if (url) {\n          await callTool(url, part.functionCall.args);\n        }\n      }\n    }\n  }\n\n  list(): Tool[] {\n    const entries = [...this.tools.entries()];\n    if (entries.length === 0) return [];\n    return [\n      {\n        functionDeclarations: entries.map(([, value]) => value.tool),\n      },\n    ];\n  }\n}\n",
          "language": "typescript"
        },
        "description": "Add a description for your module here.",
        "runnable": false
      }
    },
    "worker-introducer": {
      "code": "/**\n * @fileoverview Summarizes worker's abilities for the purpose of introduction.\n */\nimport { defaultSafetySettings, } from \"./gemini\";\nimport { toText, toLLMContent, err, ok } from \"./utils\";\nimport invokeBoard from \"@invoke\";\nfunction introductionSchema() {\n    return {\n        type: \"object\",\n        properties: {\n            title: {\n                type: \"string\",\n                description: \"The title of the agent\",\n            },\n            abilities: {\n                type: \"string\",\n                description: \"Verb-first, third-person summary of the agent's abilities\",\n            },\n        },\n    };\n}\nfunction introductionInstruction(description, tools) {\n    let toolInstruction = \"You have no access to tools of any kind.\";\n    if (tools.length > 0) {\n        toolInstruction = `You have access to the following tools:\n${tools.map((tool) => JSON.stringify(tool)).join(\"\\n\\n\")}\n\nMake sure to mention your ability to call these particular tools in your abilities.\n`;\n    }\n    return toLLMContent(`You are a project manager for team of AI agents.\nRight now, your job is to summarize what one AI agent does.\nGiven an agent prompt, you distill it to a brief summary of abilities.\nThis summary will be used to decide when to invoke this agent.\n\n# AI Agent Prompt\n\n\\`\\`\\`\n${toText(description)}\n\nYour output is in Markdown format, and you have no means to create \nanything other than text.\n\n${toolInstruction}\n\n\\`\\`\\`\n\nReply in JSON using the provided schema.\n`);\n}\nexport { invoke as default, describe };\nasync function invoke({ description, model, tools }) {\n    const response = (await invokeBoard({\n        $board: model,\n        $metadata: {\n            title: \"Make Introductions\",\n            description: \"Introducing the agent to the team\",\n        },\n        body: {\n            contents: [toLLMContent(\"Write a summary of abilities\")],\n            systemInstruction: introductionInstruction(description, tools),\n            safetySettings: defaultSafetySettings(),\n            generationConfig: {\n                responseSchema: introductionSchema(),\n                responseMimeType: \"application/json\",\n            },\n        },\n    }));\n    if (!ok(response)) {\n        console.error(\"ERROR FROM GEMINI\", response.$error);\n        return response;\n    }\n    if (\"context\" in response) {\n        return err(\"Context should be the output, something's gone wrong.\");\n    }\n    const introduction = response.candidates?.at(0)?.content || toLLMContent(\"No valid response\");\n    return {\n        introduction,\n    };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                description: {\n                    type: \"object\",\n                    behavior: [\"llm-content\"],\n                    title: \"Job Description\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                introduction: {\n                    type: \"object\",\n                    behavior: [\"llm-content\"],\n                    title: \"Introduction\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "worker-introducer",
        "source": {
          "code": "/**\n * @fileoverview Summarizes worker's abilities for the purpose of introduction.\n */\n\nimport {\n  type GeminiSchema,\n  type GeminiOutputs,\n  defaultSafetySettings,\n  type Tool,\n} from \"./gemini\";\nimport { toText, toLLMContent, err, ok } from \"./utils\";\nimport invokeBoard from \"@invoke\";\n\nfunction introductionSchema(): GeminiSchema {\n  return {\n    type: \"object\",\n    properties: {\n      title: {\n        type: \"string\",\n        description: \"The title of the agent\",\n      },\n      abilities: {\n        type: \"string\",\n        description:\n          \"Verb-first, third-person summary of the agent's abilities\",\n      },\n    },\n  };\n}\n\nfunction introductionInstruction(\n  description: LLMContent,\n  tools: Tool[]\n): LLMContent {\n  let toolInstruction = \"You have no access to tools of any kind.\";\n  if (tools.length > 0) {\n    toolInstruction = `You have access to the following tools:\n${tools.map((tool) => JSON.stringify(tool)).join(\"\\n\\n\")}\n\nMake sure to mention your ability to call these particular tools in your abilities.\n`;\n  }\n  return toLLMContent(`You are a project manager for team of AI agents.\nRight now, your job is to summarize what one AI agent does.\nGiven an agent prompt, you distill it to a brief summary of abilities.\nThis summary will be used to decide when to invoke this agent.\n\n# AI Agent Prompt\n\n\\`\\`\\`\n${toText(description)}\n\nYour output is in Markdown format, and you have no means to create \nanything other than text.\n\n${toolInstruction}\n\n\\`\\`\\`\n\nReply in JSON using the provided schema.\n`);\n}\n\nexport { invoke as default, describe };\n\ntype Inputs = {\n  description: LLMContent;\n  model: string;\n  tools: Tool[];\n};\n\ntype Outputs = {\n  $error?: string;\n  introduction?: LLMContent;\n};\n\nasync function invoke({ description, model, tools }: Inputs): Promise<Outputs> {\n  const response = (await invokeBoard({\n    $board: model,\n    $metadata: {\n      title: \"Make Introductions\",\n      description: \"Introducing the agent to the team\",\n    },\n    body: {\n      contents: [toLLMContent(\"Write a summary of abilities\")],\n      systemInstruction: introductionInstruction(description, tools),\n      safetySettings: defaultSafetySettings(),\n      generationConfig: {\n        responseSchema: introductionSchema(),\n        responseMimeType: \"application/json\",\n      },\n    },\n  })) as Outcome<GeminiOutputs>;\n  if (!ok(response)) {\n    console.error(\"ERROR FROM GEMINI\", response.$error);\n    return response;\n  }\n  if (\"context\" in response) {\n    return err(\"Context should be the output, something's gone wrong.\");\n  }\n  const introduction =\n    response.candidates?.at(0)?.content || toLLMContent(\"No valid response\");\n  return {\n    introduction,\n  };\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        description: {\n          type: \"object\",\n          behavior: [\"llm-content\"],\n          title: \"Job Description\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        introduction: {\n          type: \"object\",\n          behavior: [\"llm-content\"],\n          title: \"Introduction\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Summarizes worker's abilities for the purpose of introduction.",
        "runnable": false
      }
    },
    "worker-worker": {
      "code": "/**\n * @fileoverview Performs assigned task. Part of the worker.\n */\nimport { toText, toLLMContent, ok, err } from \"./utils\";\nimport { defaultSafetySettings, } from \"./gemini\";\nimport { callGemini } from \"./gemini-client\";\nimport invokeBoard from \"@invoke\";\nimport output from \"@output\";\nexport { invoke as default, describe };\nfunction computeWorkMode(tools, summarize) {\n    if (tools.length > 0) {\n        return \"call-tools\";\n    }\n    if (summarize) {\n        return \"summarize\";\n    }\n    return \"generate\";\n}\nclass StructuredResponse {\n    id;\n    prolog = \"\";\n    epilog = \"\";\n    body = \"\";\n    response = undefined;\n    constructor(id) {\n        this.id = id;\n    }\n    get separator() {\n        return `<sep-${this.id}>`;\n    }\n    parse(response) {\n        const r = response;\n        const part = r.candidates?.at(0)?.content?.parts?.at(0);\n        if (!part || !(\"text\" in part)) {\n            return { ok: false, error: \"No text in part\" };\n        }\n        this.response = r.candidates.at(0).content;\n        const structure = part.text.split(this.separator);\n        if (structure.length !== 3) {\n            return { ok: false, error: \"The output must contain 3 parts\" };\n        }\n        this.prolog = structure[0];\n        this.body = structure[1].trim();\n        this.epilog = structure[2].trim();\n        return { ok: true };\n    }\n}\nasync function callTools(inputs, model, tools, retries) {\n    inputs.body.tools = tools;\n    inputs.body.toolConfig = {\n        functionCallingConfig: {\n            mode: \"ANY\",\n        },\n    };\n    const response = await callGemini(inputs, model, (response) => {\n        const r = response;\n        if (r.candidates?.at(0)?.content)\n            return { ok: true };\n        return { ok: false, error: \"No content\" };\n    }, retries);\n    if (!ok(response)) {\n        return toLLMContent(\"TODO: Handle Gemini error response\");\n    }\n    const r = response;\n    return r.candidates?.at(0)?.content || toLLMContent(\"No valid response\");\n}\nasync function generate(inputs, model, responseManager, retries) {\n    const response = await callGemini(inputs, model, (response) => {\n        return responseManager.parse(response);\n    }, retries);\n    if (!ok(response)) {\n        return response;\n    }\n    else {\n        console.log(\"RESPONSE MANAGER\", responseManager);\n        return {\n            product: toLLMContent(responseManager.body, \"model\"),\n            response: responseManager.response,\n        };\n    }\n}\nasync function invoke({ id, work, description, model, tools, summarize, chat, }) {\n    // TODO: Make this a parameter.\n    const retries = 5;\n    const mode = computeWorkMode(tools, summarize);\n    const responseManager = new StructuredResponse(id);\n    const inputs = {\n        body: {\n            contents: work,\n            systemInstruction: systemInstruction(responseManager.separator, description, mode, chat),\n            safetySettings: defaultSafetySettings(),\n        },\n    };\n    if (mode === \"call-tools\") {\n        const product = await callTools(inputs, model, tools, retries);\n        return { product, response: undefined };\n    }\n    else {\n        const result = await generate(inputs, model, responseManager, retries);\n        console.log(\"RESULT?\", result);\n        if (\"$error\" in result) {\n            return result;\n        }\n        if (chat) {\n            await output({\n                schema: {\n                    type: \"object\",\n                    properties: {\n                        product: {\n                            type: \"object\",\n                            behavior: [\"llm-content\"],\n                            title: \"Draft\",\n                        },\n                        message: {\n                            type: \"string\",\n                            title: \"Requesting feedback\",\n                            format: \"markdown\",\n                        },\n                    },\n                },\n                message: responseManager.epilog,\n                product: result.product,\n            });\n        }\n        console.log(\"RESULT\", result);\n        return result;\n    }\n}\n/**\n * Returns the system instruction based on on the provided parameters.\n */\nfunction systemInstruction(separator, description, mode, chat) {\n    const preamble = `Here is your job description:\n${toText(description)}\n\n`;\n    const chatOrConclude = chat\n        ? `Finally, ask the user to provide feedback on your output as a friendly assistant might.`\n        : `Finally, you briefly summarize what the work product was and how it fulfills the task.`;\n    const postamble = `\n\nToday is ${new Date()}`;\n    const outputInstruction = `\nYour response must consist of three parts. The parts must be separated by the ${separator} tag.\n\nFirst, briefly introduce the work product (\"Okay, here is ... \") and why it fulfills the specified task, followed by a ${separator} tag to separate the parts.\nThen, you present the work product only, without any additional conversation or comments about your output, followed by a ${separator} tag to separate the parts.\n${chatOrConclude}\n`;\n    switch (mode) {\n        case \"summarize\":\n            return toLLMContent(` \n${preamble}\nSummarize the research results to fulfill the specified task.\n${outputInstruction}\n${postamble}`);\n        case \"call-tools\":\n            return toLLMContent(`\n${preamble}\nGenerate multiple function calls to fulfill the specified task.\n${postamble}`);\n        case \"generate\":\n            return toLLMContent(`\n${preamble}\nProvide the response that fulfills the specified task.\n${outputInstruction}\n${postamble}`);\n    }\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                work: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Work\",\n                },\n                description: {\n                    type: \"object\",\n                    behavior: [\"llm-content\"],\n                    title: \"Job Description\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"object\",\n                    behavior: [\"llm-content\"],\n                    title: \"Work Product\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "worker-worker",
        "source": {
          "code": "/**\n * @fileoverview Performs assigned task. Part of the worker.\n */\nimport { toText, toLLMContent, ok, err } from \"./utils\";\nimport {\n  type GeminiSchema,\n  type GeminiInputs,\n  type GeminiOutputs,\n  type Tool,\n  defaultSafetySettings,\n  type GeminiAPIOutputs,\n} from \"./gemini\";\n\nimport { type ValidatoResult, callGemini } from \"./gemini-client\";\nimport invokeBoard from \"@invoke\";\nimport output from \"@output\";\n\nexport { invoke as default, describe };\n\ntype Inputs = {\n  id: string;\n  work: LLMContent[];\n  description: LLMContent;\n  model: string;\n  tools: Tool[];\n  summarize: boolean;\n  chat: boolean;\n};\n\ntype Outputs = Outcome<{\n  product: LLMContent;\n  response?: LLMContent;\n}>;\n\ntype WorkMode = \"generate\" | \"call-tools\" | \"summarize\";\n\nfunction computeWorkMode(tools: Tool[], summarize: boolean): WorkMode {\n  if (tools.length > 0) {\n    return \"call-tools\";\n  }\n  if (summarize) {\n    return \"summarize\";\n  }\n  return \"generate\";\n}\n\nclass StructuredResponse {\n  public prolog: string = \"\";\n  public epilog: string = \"\";\n  public body: string = \"\";\n  response: LLMContent | undefined = undefined;\n\n  constructor(public readonly id: string) {}\n\n  get separator() {\n    return `<sep-${this.id}>`;\n  }\n\n  parse(response: GeminiOutputs): ValidatoResult {\n    const r = response as GeminiAPIOutputs;\n    const part = r.candidates?.at(0)?.content?.parts?.at(0);\n    if (!part || !(\"text\" in part)) {\n      return { ok: false, error: \"No text in part\" };\n    }\n    this.response = r.candidates.at(0)!.content!;\n    const structure = part.text.split(this.separator);\n    if (structure.length !== 3) {\n      return { ok: false, error: \"The output must contain 3 parts\" };\n    }\n    this.prolog = structure[0];\n    this.body = structure[1].trim();\n    this.epilog = structure[2].trim();\n    return { ok: true };\n  }\n}\n\nasync function callTools(\n  inputs: Omit<GeminiInputs, \"model\">,\n  model: string,\n  tools: Tool[],\n  retries: number\n): Promise<LLMContent> {\n  inputs.body.tools = tools;\n  inputs.body.toolConfig = {\n    functionCallingConfig: {\n      mode: \"ANY\",\n    },\n  };\n  const response = await callGemini(\n    inputs,\n    model,\n    (response) => {\n      const r = response as GeminiAPIOutputs;\n      if (r.candidates?.at(0)?.content) return { ok: true };\n      return { ok: false, error: \"No content\" };\n    },\n    retries\n  );\n  if (!ok(response)) {\n    return toLLMContent(\"TODO: Handle Gemini error response\");\n  }\n  const r = response as GeminiAPIOutputs;\n  return r.candidates?.at(0)?.content || toLLMContent(\"No valid response\");\n}\n\nasync function generate(\n  inputs: Omit<GeminiInputs, \"model\">,\n  model: string,\n  responseManager: StructuredResponse,\n  retries: number\n): Promise<Outputs> {\n  const response = await callGemini(\n    inputs,\n    model,\n    (response) => {\n      return responseManager.parse(response);\n    },\n    retries\n  );\n  if (!ok(response)) {\n    return response;\n  } else {\n    console.log(\"RESPONSE MANAGER\", responseManager);\n    return {\n      product: toLLMContent(responseManager.body, \"model\"),\n      response: responseManager.response!,\n    };\n  }\n}\n\nasync function invoke({\n  id,\n  work,\n  description,\n  model,\n  tools,\n  summarize,\n  chat,\n}: Inputs): Promise<Outputs> {\n  // TODO: Make this a parameter.\n  const retries = 5;\n  const mode = computeWorkMode(tools, summarize);\n  const responseManager = new StructuredResponse(id);\n  const inputs: Omit<GeminiInputs, \"model\"> = {\n    body: {\n      contents: work,\n      systemInstruction: systemInstruction(\n        responseManager.separator,\n        description,\n        mode,\n        chat\n      ),\n      safetySettings: defaultSafetySettings(),\n    },\n  };\n  if (mode === \"call-tools\") {\n    const product = await callTools(inputs, model, tools, retries);\n    return { product, response: undefined };\n  } else {\n    const result = await generate(inputs, model, responseManager, retries);\n    console.log(\"RESULT?\", result);\n    if (\"$error\" in result) {\n      return result;\n    }\n    if (chat) {\n      await output({\n        schema: {\n          type: \"object\",\n          properties: {\n            product: {\n              type: \"object\",\n              behavior: [\"llm-content\"],\n              title: \"Draft\",\n            },\n            message: {\n              type: \"string\",\n              title: \"Requesting feedback\",\n              format: \"markdown\",\n            },\n          },\n        },\n        message: responseManager.epilog,\n        product: result.product,\n      });\n    }\n    console.log(\"RESULT\", result);\n    return result;\n  }\n}\n\n/**\n * Returns the system instruction based on on the provided parameters.\n */\nfunction systemInstruction(\n  separator: string,\n  description: LLMContent,\n  mode: WorkMode,\n  chat: boolean\n): LLMContent {\n  const preamble = `Here is your job description:\n${toText(description)}\n\n`;\n  const chatOrConclude = chat\n    ? `Finally, ask the user to provide feedback on your output as a friendly assistant might.`\n    : `Finally, you briefly summarize what the work product was and how it fulfills the task.`;\n\n  const postamble = `\n\nToday is ${new Date()}`;\n  const outputInstruction = `\nYour response must consist of three parts. The parts must be separated by the ${separator} tag.\n\nFirst, briefly introduce the work product (\"Okay, here is ... \") and why it fulfills the specified task, followed by a ${separator} tag to separate the parts.\nThen, you present the work product only, without any additional conversation or comments about your output, followed by a ${separator} tag to separate the parts.\n${chatOrConclude}\n`;\n\n  switch (mode) {\n    case \"summarize\":\n      return toLLMContent(` \n${preamble}\nSummarize the research results to fulfill the specified task.\n${outputInstruction}\n${postamble}`);\n\n    case \"call-tools\":\n      return toLLMContent(`\n${preamble}\nGenerate multiple function calls to fulfill the specified task.\n${postamble}`);\n\n    case \"generate\":\n      return toLLMContent(`\n${preamble}\nProvide the response that fulfills the specified task.\n${outputInstruction}\n${postamble}`);\n  }\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        work: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Work\",\n        },\n        description: {\n          type: \"object\",\n          behavior: [\"llm-content\"],\n          title: \"Job Description\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"object\",\n          behavior: [\"llm-content\"],\n          title: \"Work Product\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Performs assigned task. Part of the worker.",
        "runnable": false
      }
    },
    "gemini-client": {
      "code": "import invokeGemini, {} from \"./gemini\";\nimport { ok, err } from \"./utils\";\nexport { callGemini };\nasync function callGemini(inputs, model, validator, retries) {\n    // TODO: Add more nuanced logic around retries\n    for (let i = 0; i < retries; ++i) {\n        const response = await invokeGemini(inputs);\n        if (!ok(response)) {\n            const nextStep = i == retries ? \"bailing\" : \"will retry\";\n            console.error(`Error from model, ${nextStep}`, response.$error);\n        }\n        else if (!validator(response)) {\n            console.error(\"Invalid response\");\n        }\n        else {\n            return response;\n        }\n    }\n    return err(`Failed to get valid response after ${retries} tries`);\n}\n",
      "metadata": {
        "title": "gemini-client",
        "source": {
          "code": "import invokeGemini, { type GeminiInputs, type GeminiOutputs } from \"./gemini\";\nimport { ok, err } from \"./utils\";\n\nexport type ValidatoResult = { ok: true } | { ok: false; error: string };\nexport type ValidatorFunction = (response: GeminiOutputs) => ValidatoResult;\n\nexport { callGemini };\n\nasync function callGemini(\n  inputs: Omit<GeminiInputs, \"model\">,\n  model: string,\n  validator: ValidatorFunction,\n  retries: number\n): Promise<Outcome<GeminiOutputs>> {\n  // TODO: Add more nuanced logic around retries\n  for (let i = 0; i < retries; ++i) {\n    const response = await invokeGemini(inputs);\n    if (!ok(response)) {\n      const nextStep = i == retries ? \"bailing\" : \"will retry\";\n      console.error(`Error from model, ${nextStep}`, response.$error);\n    } else if (!validator(response)) {\n      console.error(\"Invalid response\");\n    } else {\n      return response;\n    }\n  }\n  return err(`Failed to get valid response after ${retries} tries`);\n}\n",
          "language": "typescript"
        },
        "description": "",
        "runnable": false
      }
    },
    "agent-main": {
      "code": "/**\n * @fileoverview The main body of the agent\n */\nimport output from \"@output\";\nimport {} from \"./common\";\nimport { ToolManager } from \"./tool-manager\";\nimport workerIntroducer from \"./worker-introducer\";\nimport workerWorker from \"./worker-worker\";\nimport { report } from \"./output\";\nimport { defaultLLMContent } from \"./utils\";\nimport invokeGraph from \"@invoke\";\nexport { invoke as default, describe };\nfunction toLLMContent(text, role = \"user\") {\n    return { parts: [{ text }], role };\n}\nasync function invoke({ context }) {\n    let { id, description, type, context: initialContext, model, defaultModel, tools, chat, work: workContext, } = context;\n    if (!description) {\n        const $error = \"No Job description supplied\";\n        await report({\n            actor: \"Agent\",\n            name: $error,\n            category: \"Runtime error\",\n            details: `In order to run, Agent needs to have a job description. Please let it know what it's good at.`,\n        });\n        return { $error };\n    }\n    // For now, use the internal Gemini module to invoke\n    // TODO: Add back the capability to invoke model boards\n    //   if (!model) {\n    //     model = defaultModel;\n    //   }\n    //   if (!model) {\n    //     const $error = \"Model was not supplied\";\n    //     await report({\n    //       actor: \"Agent\",\n    //       name: $error,\n    //       category: \"Runtime error\",\n    //       details: `In order to run, Agent neeeds a model to be connected to it.\n    // Please drag one (\"Gemini\" should work) from the list of components over to the \"Model\"\n    // port`,\n    //     });\n    //     return { $error };\n    //   }\n    const toolManager = new ToolManager();\n    if (!(await toolManager.initialize(tools))) {\n        const $error = `Problem initializing tools. \nThe following errors were encountered: ${toolManager.errors.join(\",\")}`;\n        console.error(\"MAIN ERROR\", $error, toolManager.errors);\n        return { $error };\n    }\n    switch (type) {\n        case \"introduction\": {\n            const response = await workerIntroducer({\n                description,\n                model,\n                tools: toolManager.list(),\n            });\n            if (response.$error) {\n                console.error(\"INTRODUCER ERROR\", response.$error);\n                return {\n                    $error: response.$error,\n                };\n            }\n            return {\n                done: [response.introduction || toLLMContent(\"No valid response\")],\n            };\n        }\n        case \"work\": {\n            const work = [...initialContext, ...workContext];\n            const userInputs = context.userInputs;\n            const response = await workerWorker({\n                id,\n                description,\n                work,\n                model,\n                tools: toolManager.list(),\n                summarize: false,\n                chat,\n            });\n            if (\"$error\" in response) {\n                console.error(\"ERROR FROM WORKER\", response.$error);\n                return {\n                    $error: response.$error,\n                };\n            }\n            const workerResponse = response.product;\n            const toolResults = [];\n            await toolManager.processResponse(workerResponse, async ($board, args) => {\n                const result = await invokeGraph({\n                    $board,\n                    ...args,\n                });\n                toolResults.push(result);\n            });\n            if (toolResults.length > 0) {\n                const summary = await workerWorker({\n                    id,\n                    description,\n                    work: [\n                        ...toolResults.map((toolResult) => toLLMContent(JSON.stringify(toolResult))),\n                    ],\n                    model,\n                    tools: [],\n                    summarize: true,\n                    chat: false,\n                });\n                if (\"$error\" in summary) {\n                    console.error(\"ERROR FROM SUMMARY\", summary.$error);\n                    return {\n                        $error: summary.$error,\n                    };\n                }\n                const summaryResponse = summary.product;\n                return { done: [summaryResponse] };\n            }\n            else if (chat && context.userInputs.length == 0) {\n                const toInput = {\n                    type: \"object\",\n                    properties: {\n                        request: {\n                            type: \"object\",\n                            title: \"Please provide feedback\",\n                            behavior: [\"transient\", \"llm-content\"],\n                            examples: [defaultLLMContent()],\n                        },\n                    },\n                };\n                return {\n                    toInput,\n                    context: { ...context, work: [...workContext, response.response] },\n                };\n            }\n            return { done: [workerResponse] };\n        }\n        default:\n            return {\n                done: [\n                    { parts: [{ text: \"Unknown task type\" }] },\n                ],\n            };\n    }\n    const userInputs = context.userInputs;\n    if (!context.chat || userInputs.length > 2) {\n        return {\n            done: context.userInputs.map((item) => toLLMContent(item)),\n        };\n    }\n    console.log(\"INPUTS\", context);\n    await output({\n        schema: {\n            type: \"object\",\n            properties: {\n                message: {\n                    type: \"string\",\n                    format: \"markdown\",\n                },\n            },\n        },\n        message: \"**HELLO** THERE\",\n    });\n    const toInput = {\n        type: \"object\",\n        properties: {\n            request: {\n                type: \"string\",\n                title: \"Request\",\n                description: \"Answer me this\",\n                behavior: [\"transient\"],\n            },\n        },\n    };\n    return { toInput, context };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"object\",\n                    title: \"Agent Context\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                toInput: {\n                    type: \"object\",\n                    title: \"Input Schema\",\n                },\n                context: {\n                    type: \"object\",\n                    title: \"Agent Context\",\n                },\n                done: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Done\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "agent-main",
        "source": {
          "code": "/**\n * @fileoverview The main body of the agent\n */\nimport output from \"@output\";\nimport { type AgentContext } from \"./common\";\nimport { ToolManager } from \"./tool-manager\";\nimport workerIntroducer from \"./worker-introducer\";\nimport workerWorker from \"./worker-worker\";\nimport { report } from \"./output\";\nimport { defaultLLMContent } from \"./utils\";\nimport invokeGraph from \"@invoke\";\n\nexport { invoke as default, describe };\n\ntype Inputs = {\n  context: AgentContext;\n};\n\ntype Outputs = {\n  $error?: string;\n  context?: AgentContext;\n  toInput?: Schema;\n  done?: LLMContent[];\n};\n\nfunction toLLMContent(\n  text: string,\n  role: LLMContent[\"role\"] = \"user\"\n): LLMContent {\n  return { parts: [{ text }], role };\n}\n\nasync function invoke({ context }: Inputs): Promise<Outputs> {\n  let {\n    id,\n    description,\n    type,\n    context: initialContext,\n    model,\n    defaultModel,\n    tools,\n    chat,\n    work: workContext,\n  } = context;\n  if (!description) {\n    const $error = \"No Job description supplied\";\n    await report({\n      actor: \"Agent\",\n      name: $error,\n      category: \"Runtime error\",\n      details: `In order to run, Agent needs to have a job description. Please let it know what it's good at.`,\n    });\n    return { $error };\n  }\n  // For now, use the internal Gemini module to invoke\n  // TODO: Add back the capability to invoke model boards\n  //   if (!model) {\n  //     model = defaultModel;\n  //   }\n  //   if (!model) {\n  //     const $error = \"Model was not supplied\";\n\n  //     await report({\n  //       actor: \"Agent\",\n  //       name: $error,\n  //       category: \"Runtime error\",\n  //       details: `In order to run, Agent neeeds a model to be connected to it.\n  // Please drag one (\"Gemini\" should work) from the list of components over to the \"Model\"\n  // port`,\n  //     });\n  //     return { $error };\n  //   }\n  const toolManager = new ToolManager();\n  if (!(await toolManager.initialize(tools))) {\n    const $error = `Problem initializing tools. \nThe following errors were encountered: ${toolManager.errors.join(\",\")}`;\n    console.error(\"MAIN ERROR\", $error, toolManager.errors);\n    return { $error };\n  }\n\n  switch (type) {\n    case \"introduction\": {\n      const response = await workerIntroducer({\n        description,\n        model,\n        tools: toolManager.list(),\n      });\n      if (response.$error) {\n        console.error(\"INTRODUCER ERROR\", response.$error);\n        return {\n          $error: response.$error,\n        };\n      }\n      return {\n        done: [response.introduction || toLLMContent(\"No valid response\")],\n      };\n    }\n    case \"work\": {\n      const work = [...initialContext, ...workContext];\n      const userInputs = context.userInputs as LLMContent[];\n      const response = await workerWorker({\n        id,\n        description,\n        work,\n        model,\n        tools: toolManager.list(),\n        summarize: false,\n        chat,\n      });\n      if (\"$error\" in response) {\n        console.error(\"ERROR FROM WORKER\", response.$error);\n        return {\n          $error: response.$error,\n        };\n      }\n      const workerResponse = response.product;\n      const toolResults: object[] = [];\n      await toolManager.processResponse(\n        workerResponse,\n        async ($board, args) => {\n          const result = await invokeGraph({\n            $board,\n            ...args,\n          });\n          toolResults.push(result);\n        }\n      );\n      if (toolResults.length > 0) {\n        const summary = await workerWorker({\n          id,\n          description,\n          work: [\n            ...toolResults.map((toolResult) =>\n              toLLMContent(JSON.stringify(toolResult))\n            ),\n          ],\n          model,\n          tools: [],\n          summarize: true,\n          chat: false,\n        });\n        if (\"$error\" in summary) {\n          console.error(\"ERROR FROM SUMMARY\", summary.$error);\n          return {\n            $error: summary.$error,\n          };\n        }\n        const summaryResponse = summary.product;\n        return { done: [summaryResponse] };\n      } else if (chat && context.userInputs.length == 0) {\n        const toInput: Schema = {\n          type: \"object\",\n          properties: {\n            request: {\n              type: \"object\",\n              title: \"Please provide feedback\",\n              behavior: [\"transient\", \"llm-content\"],\n              examples: [defaultLLMContent()],\n            },\n          },\n        };\n        return {\n          toInput,\n          context: { ...context, work: [...workContext, response.response!] },\n        };\n      }\n      return { done: [workerResponse] };\n    }\n    default:\n      return {\n        done: [\n          { parts: [{ text: \"Unknown task type\" }] },\n        ] satisfies LLMContent[],\n      };\n  }\n\n  const userInputs = context.userInputs;\n  if (!context.chat || userInputs.length > 2) {\n    return {\n      done: context.userInputs.map((item) => toLLMContent(item as string)),\n    };\n  }\n  console.log(\"INPUTS\", context);\n  await output({\n    schema: {\n      type: \"object\",\n      properties: {\n        message: {\n          type: \"string\",\n          format: \"markdown\",\n        },\n      },\n    },\n    message: \"**HELLO** THERE\",\n  });\n  const toInput: Schema = {\n    type: \"object\",\n    properties: {\n      request: {\n        type: \"string\",\n        title: \"Request\",\n        description: \"Answer me this\",\n        behavior: [\"transient\"],\n      },\n    },\n  };\n  return { toInput, context };\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"object\",\n          title: \"Agent Context\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        toInput: {\n          type: \"object\",\n          title: \"Input Schema\",\n        },\n        context: {\n          type: \"object\",\n          title: \"Agent Context\",\n        },\n        done: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Done\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "The main body of the agent",
        "runnable": true
      }
    },
    "researcher": {
      "code": "/**\n * @fileoverview Add a description for your module here.\n */\nexport { invoke as default, describe };\nasync function invoke({ context }) {\n    return { context };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                },\n                plan: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\"],\n                    title: \"Research Plan\",\n                    description: \"Provide an outline of what to research, what areas to cover, etc.\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "Researcher",
        "source": {
          "code": "/**\n * @fileoverview Add a description for your module here.\n */\n\nexport { invoke as default, describe };\n\nasync function invoke({ context }: { context: LLMContent[] }) {\n  return { context };\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n        },\n        plan: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\"],\n          title: \"Research Plan\",\n          description:\n            \"Provide an outline of what to research, what areas to cover, etc.\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Add a description for your module here.",
        "runnable": true
      }
    },
    "image-generator": {
      "code": "/**\n * @fileoverview Generates an image using supplied context.\n */\nimport gemini, {} from \"./gemini\";\nimport { err, ok } from \"./utils\";\nexport { invoke as default, describe };\nasync function invoke({ context, }) {\n    const contents = context && Array.isArray(context) && context.length > 0\n        ? [context.at(-1)]\n        : undefined;\n    if (!contents) {\n        return err(\"Must supply context as input\");\n    }\n    const result = await gemini({\n        model: \"gemini-2.0-flash-exp\",\n        body: {\n            contents,\n            generationConfig: {\n                responseModalities: [\"TEXT\", \"IMAGE\"],\n            },\n        },\n    });\n    if (!ok(result)) {\n        return result;\n    }\n    if (\"context\" in result) {\n        return err(\"Invalid output from Gemini -- must be candidates\");\n    }\n    const content = result.candidates.at(0)?.content;\n    if (!content) {\n        return err(\"No content\");\n    }\n    return { context: [content] };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "Image Generator",
        "source": {
          "code": "/**\n * @fileoverview Generates an image using supplied context.\n */\n\nimport gemini, { type GeminiOutputs, type GeminiInputs } from \"./gemini\";\nimport { err, ok } from \"./utils\";\n\nexport { invoke as default, describe };\n\nasync function invoke({\n  context,\n}: {\n  context: LLMContent[];\n}): Promise<Outcome<{ context: LLMContent[] }>> {\n  const contents =\n    context && Array.isArray(context) && context.length > 0\n      ? [context.at(-1)!]\n      : undefined;\n  if (!contents) {\n    return err(\"Must supply context as input\");\n  }\n  const result = await gemini({\n    model: \"gemini-2.0-flash-exp\",\n    body: {\n      contents,\n      generationConfig: {\n        responseModalities: [\"TEXT\", \"IMAGE\"],\n      },\n    },\n  });\n  if (!ok(result)) {\n    return result;\n  }\n  if (\"context\" in result) {\n    return err(\"Invalid output from Gemini -- must be candidates\");\n  }\n\n  const content = result.candidates.at(0)?.content;\n  if (!content) {\n    return err(\"No content\");\n  }\n\n  return { context: [content] };\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Generates an image using supplied context.",
        "runnable": true
      }
    }
  },
  "exports": [
    "#module:gemini",
    "#daf082ca-c1aa-4aff-b2c8-abeb984ab66c",
    "#module:researcher",
    "#module:image-generator"
  ],
  "graphs": {
    "daf082ca-c1aa-4aff-b2c8-abeb984ab66c": {
      "title": "Text Generator",
      "description": "Calls an LLM and so much more. Insert real description here.",
      "version": "0.0.1",
      "describer": "module:entry",
      "nodes": [
        {
          "type": "output",
          "id": "output",
          "configuration": {
            "schema": {
              "properties": {
                "context": {
                  "type": "array",
                  "title": "Context",
                  "items": {
                    "type": "object",
                    "behavior": [
                      "llm-content"
                    ]
                  },
                  "default": "null"
                }
              },
              "type": "object",
              "required": []
            }
          },
          "metadata": {
            "visual": {
              "x": 708,
              "y": 44,
              "collapsed": "expanded"
            }
          }
        },
        {
          "id": "board-f138aa03",
          "type": "#module:entry",
          "metadata": {
            "visual": {
              "x": -47,
              "y": -72,
              "collapsed": "expanded"
            },
            "title": "entry"
          }
        },
        {
          "id": "board-d340ad8f",
          "type": "#module:agent-main",
          "metadata": {
            "visual": {
              "x": 332,
              "y": -6,
              "collapsed": "expanded"
            },
            "title": "agent-main"
          }
        },
        {
          "id": "board-1946064a",
          "type": "#module:join",
          "metadata": {
            "visual": {
              "x": 953,
              "y": -248,
              "collapsed": "expanded"
            },
            "title": "join"
          }
        },
        {
          "type": "input",
          "id": "input",
          "metadata": {
            "visual": {
              "x": 713,
              "y": 170,
              "collapsed": "advanced"
            },
            "title": "input"
          }
        }
      ],
      "edges": [
        {
          "from": "board-f138aa03",
          "to": "board-d340ad8f",
          "out": "context",
          "in": "context"
        },
        {
          "from": "board-d340ad8f",
          "to": "output",
          "out": "done",
          "in": "context"
        },
        {
          "from": "input",
          "to": "board-1946064a",
          "out": "request",
          "in": "request"
        },
        {
          "from": "board-d340ad8f",
          "to": "input",
          "out": "toInput",
          "in": "schema"
        },
        {
          "from": "board-d340ad8f",
          "to": "board-1946064a",
          "out": "context",
          "in": "context"
        },
        {
          "from": "board-1946064a",
          "to": "board-d340ad8f",
          "out": "context",
          "in": "context"
        }
      ],
      "metadata": {
        "visual": {
          "minimized": false
        },
        "describer": "module:entry",
        "tags": []
      }
    }
  }
}