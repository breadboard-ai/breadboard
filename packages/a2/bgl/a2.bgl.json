{
  "title": "A2",
  "description": "Components that help you build flows.",
  "version": "0.0.1",
  "nodes": [],
  "edges": [],
  "metadata": {
    "comments": [
      {
        "id": "comment-b09617ef",
        "text": "Left Intentionally Blank",
        "metadata": {
          "visual": {
            "x": -37.90624999999966,
            "y": -415.85546874999994,
            "collapsed": "expanded",
            "outputHeight": 0
          }
        }
      }
    ],
    "visual": {},
    "tags": [
      "published",
      "tool",
      "component"
    ]
  },
  "modules": {
    "common": {
      "code": "/**\n * @fileoverview Common types and code\n */\n",
      "metadata": {
        "title": "common",
        "source": {
          "code": "/**\n * @fileoverview Common types and code\n */\n\nexport type UserInput = LLMContent;\n\nexport type Params = {\n  [key: `p-z-${string}`]: unknown;\n};\n\nexport type AgentInputs = {\n  /**\n   * Whether (true) or not (false) the agent is allowed to chat with user.\n   */\n  chat: boolean;\n  /**\n   * The incoming conversation context.\n   */\n  context: LLMContent[];\n  /**\n   * Accumulated work context. This is the internal conversation, a result\n   * of talking with the user, for instance.\n   * This context is discarded at the end of interacting with the agent.\n   */\n  work: LLMContent[];\n  /**\n   * Agent's job description.\n   */\n  description?: LLMContent;\n  /**\n   * Last work product.\n   */\n  last?: LLMContent;\n  /**\n   * Type of the task.\n   */\n  type: \"introduction\" | \"work\";\n  /**\n   * The board URL of the model\n   */\n  model: string;\n  /**\n   * The default model that is passed along by the manager\n   */\n  defaultModel: string;\n  /**\n   * The tools that the worker can use\n   */\n  tools?: string[];\n  /**\n   * params\n   */\n  params: Params;\n};\n\nexport type AgentContext = AgentInputs & {\n  /**\n   * A unique identifier for the session.\n   * Currently used to have a persistent part separator across conversation context\n   */\n  id: string;\n  /**\n   * Accumulating list of user inputs\n   */\n  userInputs: UserInput[];\n  /**\n   * Indicator that the user ended chat.\n   */\n  userEndedChat: boolean;\n};\n\nexport type DescribeInputs = {\n  inputs: {\n    description: LLMContent | undefined;\n  };\n};\n",
          "language": "typescript"
        },
        "description": "Common types and code",
        "runnable": false
      }
    },
    "utils": {
      "code": "/**\n * @fileoverview Common utils for manipulating LLM Content and other relevant types.\n */\nexport { isLLMContent, isLLMContentArray, toLLMContent, toText, contentToJSON, defaultLLMContent, endsWithRole, };\nexport { ok, err };\nfunction ok(o) {\n    return !(o && typeof o === \"object\" && \"$error\" in o);\n}\nfunction err($error) {\n    return { $error };\n}\n/**\n * Copied from @google-labs/breadboard\n */\nfunction isLLMContent(nodeValue) {\n    if (typeof nodeValue !== \"object\" || !nodeValue)\n        return false;\n    if (nodeValue === null || nodeValue === undefined)\n        return false;\n    if (\"role\" in nodeValue && nodeValue.role === \"$metadata\") {\n        return true;\n    }\n    return \"parts\" in nodeValue && Array.isArray(nodeValue.parts);\n}\nfunction isLLMContentArray(nodeValue) {\n    if (!Array.isArray(nodeValue))\n        return false;\n    if (nodeValue.length === 0)\n        return true;\n    return isLLMContent(nodeValue.at(-1));\n}\nfunction toLLMContent(text, role = \"user\") {\n    return { parts: [{ text }], role };\n}\nfunction endsWithRole(c, role) {\n    const last = c.at(-1);\n    if (!last)\n        return false;\n    return last.role === role;\n}\nfunction toText(c) {\n    if (isLLMContent(c)) {\n        return contentToText(c);\n    }\n    const last = c.at(-1);\n    if (!last)\n        return \"\";\n    return contentToText(last).trim();\n    function contentToText(content) {\n        return content.parts\n            .map((part) => (\"text\" in part ? part.text : \"\"))\n            .join(\"\\n\\n\");\n    }\n}\nfunction contentToJSON(content) {\n    const part = content?.parts?.at(0);\n    if (!part || !(\"text\" in part)) {\n        throw new Error(\"Invalid response from Gemini\");\n    }\n    return JSON.parse(part.text);\n}\nfunction defaultLLMContent() {\n    return JSON.stringify({\n        parts: [{ text: \"\" }],\n        role: \"user\",\n    });\n}\n",
      "metadata": {
        "title": "utils",
        "source": {
          "code": "/**\n * @fileoverview Common utils for manipulating LLM Content and other relevant types.\n */\nexport {\n  isLLMContent,\n  isLLMContentArray,\n  toLLMContent,\n  toText,\n  contentToJSON,\n  defaultLLMContent,\n  endsWithRole,\n};\n\nexport { ok, err };\n\nfunction ok<T>(o: Outcome<T>): o is T {\n  return !(o && typeof o === \"object\" && \"$error\" in o);\n}\n\nfunction err($error: string) {\n  return { $error };\n}\n\n/**\n * Copied from @google-labs/breadboard\n */\nfunction isLLMContent(nodeValue: unknown): nodeValue is LLMContent {\n  if (typeof nodeValue !== \"object\" || !nodeValue) return false;\n  if (nodeValue === null || nodeValue === undefined) return false;\n\n  if (\"role\" in nodeValue && nodeValue.role === \"$metadata\") {\n    return true;\n  }\n\n  return \"parts\" in nodeValue && Array.isArray(nodeValue.parts);\n}\n\nfunction isLLMContentArray(nodeValue: unknown): nodeValue is LLMContent[] {\n  if (!Array.isArray(nodeValue)) return false;\n  if (nodeValue.length === 0) return true;\n  return isLLMContent(nodeValue.at(-1));\n}\n\nfunction toLLMContent(\n  text: string,\n  role: LLMContent[\"role\"] = \"user\"\n): LLMContent {\n  return { parts: [{ text }], role };\n}\n\nfunction endsWithRole(c: LLMContent[], role: \"user\" | \"model\"): boolean {\n  const last = c.at(-1);\n  if (!last) return false;\n  return last.role === role;\n}\n\nfunction toText(c: LLMContent | LLMContent[]): string {\n  if (isLLMContent(c)) {\n    return contentToText(c);\n  }\n  const last = c.at(-1);\n  if (!last) return \"\";\n  return contentToText(last).trim();\n\n  function contentToText(content: LLMContent) {\n    return content.parts\n      .map((part) => (\"text\" in part ? part.text : \"\"))\n      .join(\"\\n\\n\");\n  }\n}\n\nfunction contentToJSON<T>(content?: LLMContent): T {\n  const part = content?.parts?.at(0);\n  if (!part || !(\"text\" in part)) {\n    throw new Error(\"Invalid response from Gemini\");\n  }\n  return JSON.parse(part.text) as T;\n}\n\nfunction defaultLLMContent(): string {\n  return JSON.stringify({\n    parts: [{ text: \"\" }],\n    role: \"user\",\n  } satisfies LLMContent);\n}\n",
          "language": "typescript"
        },
        "description": "Common utils for manipulating LLM Content and other relevant types.",
        "runnable": false
      }
    },
    "gemini": {
      "code": "/**\n * @fileoverview Gemini Model Family.\n */\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\nimport output from \"@output\";\nimport { ok, err, isLLMContentArray } from \"./utils\";\nconst defaultSafetySettings = () => [\n    {\n        category: \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        threshold: \"BLOCK_NONE\",\n    },\n    {\n        category: \"HARM_CATEGORY_HARASSMENT\",\n        threshold: \"BLOCK_NONE\",\n    },\n    {\n        category: \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        threshold: \"BLOCK_NONE\",\n    },\n];\nasync function endpointURL(model) {\n    const $metadata = {\n        title: \"Get GEMINI_KEY\",\n        description: \"Getting GEMINI_KEY from secrets\",\n    };\n    const key = await secrets({ $metadata, keys: [\"GEMINI_KEY\"] });\n    return `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${key.GEMINI_KEY}`;\n}\nexport { invoke as default, describe, defaultSafetySettings };\nconst VALID_MODALITIES = [\"Text\", \"Text and Image\", \"Audio\"];\nconst MODELS = [\n    \"gemini-1.5-flash-latest\",\n    \"gemini-1.5-pro-latest\",\n    \"gemini-2.0-flash-exp\",\n    \"gemini-2.0-flash-thinking-exp\",\n    \"gemini-exp-1206\",\n    \"gemini-exp-1121\",\n    \"learnlm-1.5-pro-experimental\",\n    \"gemini-1.5-pro-001\",\n    \"gemini-1.5-pro-002\",\n    \"gemini-1.5-pro-exp-0801\",\n    \"gemini-1.5-pro-exp-0827\",\n    \"gemini-1.5-flash-001\",\n    \"gemini-1.5-flash-002\",\n    \"gemini-1.5-flash-8b-exp-0924\",\n    \"gemini-1.5-flash-8b-exp-0827\",\n    \"gemini-1.5-flash-exp-0827\",\n];\nconst NO_RETRY_CODES = [400, 429, 404];\nasync function callAPI(retries, model, body, $metadata) {\n    let $error = \"Unknown error\";\n    while (retries) {\n        const result = await fetch({\n            $metadata,\n            url: await endpointURL(model),\n            method: \"POST\",\n            body,\n        });\n        if (!ok(result)) {\n            // Fetch is a bit weird, because it returns various props\n            // along with the `$error`. Let's handle that here.\n            const { status, $error: errObject } = result;\n            if (!status) {\n                // This is not an error response, presume fatal error.\n                return { $error };\n            }\n            $error = maybeExtractError(errObject);\n            if (NO_RETRY_CODES.includes(status)) {\n                return { $error };\n            }\n        }\n        else {\n            const outputs = result.response;\n            const candidate = outputs.candidates?.at(0);\n            if (!candidate) {\n                return err(\"Unable to get a good response from Gemini\");\n            }\n            if (\"content\" in candidate) {\n                return outputs;\n            }\n        }\n        retries--;\n    }\n    return { $error };\n}\nfunction maybeExtractError(e) {\n    try {\n        const parsed = JSON.parse(e);\n        return parsed.error.message;\n    }\n    catch (error) {\n        return e;\n    }\n}\nfunction isEmptyLLMContent(content) {\n    if (!content || !content.parts || content.parts.length === 0)\n        return true;\n    return content.parts.every((part) => {\n        if (\"text\" in part) {\n            return !part.text?.trim();\n        }\n        return true;\n    });\n}\nfunction addModality(body, modality) {\n    if (!modality)\n        return;\n    switch (modality) {\n        case \"Text\":\n            // No change, defaults.\n            break;\n        case \"Text and Image\":\n            body.generationConfig ??= {};\n            body.generationConfig.responseModalities = [\"TEXT\", \"IMAGE\"];\n            break;\n        case \"Audio\":\n            body.generationConfig ??= {};\n            body.generationConfig.responseModalities = [\"AUDIO\"];\n            break;\n    }\n}\nfunction constructBody(context = [], systemInstruction, prompt, modality) {\n    const contents = [...context];\n    if (!isEmptyLLMContent(prompt)) {\n        contents.push(prompt);\n    }\n    const body = {\n        contents,\n        safetySettings: defaultSafetySettings(),\n    };\n    const canHaveSystemInstruction = modality === \"Text\";\n    if (!isEmptyLLMContent(systemInstruction) && canHaveSystemInstruction) {\n        body.systemInstruction = systemInstruction;\n    }\n    addModality(body, modality);\n    return body;\n}\nfunction augmentBody(body, systemInstruction, prompt, modality) {\n    if (!body.systemInstruction || !isEmptyLLMContent(systemInstruction)) {\n        body.systemInstruction = systemInstruction;\n    }\n    if (!isEmptyLLMContent(prompt)) {\n        body.contents = [...body.contents, prompt];\n    }\n    addModality(body, modality);\n    return body;\n}\nfunction validateInputs(inputs) {\n    if (\"body\" in inputs) {\n        return;\n    }\n    if (inputs.context) {\n        const { context } = inputs;\n        if (!Array.isArray(context)) {\n            return err(\"Incoming context must be an array.\");\n        }\n        if (!isLLMContentArray(context)) {\n            return err(\"Malformed incoming context\");\n        }\n        return;\n    }\n    return err(\"Either body or context is required\");\n}\nasync function invoke(inputs) {\n    const validatingInputs = validateInputs(inputs);\n    if (!ok(validatingInputs)) {\n        return validatingInputs;\n    }\n    let { model } = inputs;\n    if (!model) {\n        model = MODELS[0];\n    }\n    const { context, systemInstruction, prompt, modality, body, $metadata } = inputs;\n    // TODO: Make this configurable.\n    const retries = 5;\n    if (!(\"body\" in inputs)) {\n        // Public API is being used.\n        // Behave as if we're wired in.\n        const result = await callAPI(retries, model, constructBody(context, systemInstruction, prompt, modality));\n        if (!ok(result)) {\n            return result;\n        }\n        const content = result.candidates.at(0)?.content;\n        if (!content) {\n            return err(\"Unable to get a good response from Gemini\");\n        }\n        return { context: [...context, content] };\n    }\n    else {\n        // Private API is being used.\n        // Behave as if we're being invoked.\n        return callAPI(retries, model, augmentBody(body, systemInstruction, prompt, modality), $metadata);\n    }\n}\nasync function describe({ inputs }) {\n    const canHaveModalities = inputs.model === \"gemini-2.0-flash-exp\";\n    const canHaveSystemInstruction = !canHaveModalities || (canHaveModalities && inputs.modality == \"Text\");\n    const maybeAddSystemInstruction = canHaveSystemInstruction\n        ? {\n            systemInstruction: {\n                type: \"object\",\n                behavior: [\"llm-content\", \"config\"],\n                title: \"System Instruction\",\n                default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n                description: \"(Optional) Give the model additional context on what to do,\" +\n                    \"like specific rules/guidelines to adhere to or specify behavior\" +\n                    \"separate from the provided context\",\n            },\n        }\n        : {};\n    const maybeAddModalities = canHaveModalities\n        ? {\n            modality: {\n                type: \"string\",\n                enum: [...VALID_MODALITIES],\n                title: \"Output Modality\",\n                behavior: [\"config\"],\n                description: \"(Optional) Tell the model what kind of output you're looking for.\",\n            },\n        }\n        : {};\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                model: {\n                    type: \"string\",\n                    behavior: [\"config\"],\n                    title: \"Model Name\",\n                    enum: MODELS,\n                    default: MODELS[0],\n                },\n                prompt: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\"],\n                    title: \"Prompt\",\n                    default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n                    description: \"(Optional) A prompt. Will be added to the end of the the conversation context.\",\n                },\n                ...maybeAddSystemInstruction,\n                ...maybeAddModalities,\n                context: {\n                    type: \"array\",\n                    items: {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                    },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                    },\n                    title: \"Context out\",\n                },\n            },\n        },\n        metadata: {\n            icon: \"generative\",\n        },\n    };\n}\n",
      "metadata": {
        "title": "Gemini",
        "source": {
          "code": "/**\n * @fileoverview Gemini Model Family.\n */\n\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\nimport output from \"@output\";\n\nimport { ok, err, isLLMContentArray } from \"./utils\";\n\nconst defaultSafetySettings = (): SafetySetting[] => [\n  {\n    category: \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n    threshold: \"BLOCK_NONE\",\n  },\n  {\n    category: \"HARM_CATEGORY_HARASSMENT\",\n    threshold: \"BLOCK_NONE\",\n  },\n  {\n    category: \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n    threshold: \"BLOCK_NONE\",\n  },\n];\n\nasync function endpointURL(model: string): Promise<string> {\n  const $metadata = {\n    title: \"Get GEMINI_KEY\",\n    description: \"Getting GEMINI_KEY from secrets\",\n  };\n  const key = await secrets({ $metadata, keys: [\"GEMINI_KEY\"] });\n  return `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${key.GEMINI_KEY}`;\n}\n\nexport { invoke as default, describe, defaultSafetySettings };\n\nconst VALID_MODALITIES = [\"Text\", \"Text and Image\", \"Audio\"] as const;\ntype ValidModalities = (typeof VALID_MODALITIES)[number];\n\nexport type HarmBlockThreshold =\n  // Content with NEGLIGIBLE will be allowed.\n  | \"BLOCK_LOW_AND_ABOVE\"\n  // Content with NEGLIGIBLE and LOW will be allowed.\n  | \"BLOCK_MEDIUM_AND_ABOVE\"\n  // Content with NEGLIGIBLE, LOW, and MEDIUM will be allowed.\n  | \"BLOCK_ONLY_HIGH\"\n  // All content will be allowed.\n  | \"BLOCK_NONE\"\n  // Turn off the safety filter.\n  | \"OFF\";\n\nexport type HarmCategory =\n  // Gemini - Harassment content\n  | \"HARM_CATEGORY_HARASSMENT\"\n  //\tGemini - Hate speech and content.\n  | \"HARM_CATEGORY_HATE_SPEECH\"\n  // Gemini - Sexually explicit content.\n  | \"HARM_CATEGORY_SEXUALLY_EXPLICIT\"\n  // \tGemini - Dangerous content.\n  | \"HARM_CATEGORY_DANGEROUS_CONTENT\"\n  // Gemini - Content that may be used to harm civic integrity.\n  | \"HARM_CATEGORY_CIVIC_INTEGRITY\";\n\nexport type GeminiSchema = {\n  type: \"string\" | \"number\" | \"integer\" | \"boolean\" | \"object\" | \"array\";\n  format?: string;\n  description?: string;\n  nullable?: boolean;\n  enum?: string[];\n  maxItems?: string;\n  minItems?: string;\n  properties?: Record<string, GeminiSchema>;\n  required?: string[];\n  items?: GeminiSchema;\n};\n\nexport type Modality = \"TEXT\" | \"IMAGE\" | \"AUDIO\";\n\nexport type GenerationConfig = {\n  responseMimeType?: \"text/plain\" | \"application/json\" | \"text/x.enum\";\n  responseSchema?: GeminiSchema;\n  responseModalities?: Modality[];\n};\n\nexport type SafetySetting = {\n  category: HarmCategory;\n  threshold: HarmBlockThreshold;\n};\n\nexport type Metadata = {\n  title?: string;\n  description?: string;\n};\n\nexport type GeminiBody = {\n  contents: LLMContent[];\n  tools?: Tool[];\n  toolConfig?: ToolConfig;\n  systemInstruction?: LLMContent;\n  safetySettings?: SafetySetting[];\n  generationConfig?: GenerationConfig;\n};\n\nexport type GeminiInputs = {\n  // The wireable/configurable properties.\n  model?: string;\n  context?: LLMContent[];\n  systemInstruction?: LLMContent;\n  prompt?: LLMContent;\n  modality?: ValidModalities;\n  // The \"private API\" properties\n  $metadata?: Metadata;\n  body: GeminiBody;\n};\n\nexport type Tool = {\n  functionDeclarations?: FunctionDeclaration[];\n  googleSearchRetrieval?: GoogleSearchRetrieval[];\n  codeExecution?: CodeExecution[];\n};\n\nexport type ToolConfig = {\n  functionCallingConfig?: FunctionCallingConfig;\n};\n\nexport type FunctionCallingConfig = {\n  mode?: \"MODE_UNSPECIFIED\" | \"AUTO\" | \"ANY\" | \"NONE\";\n  allowedFunctionNames?: string[];\n};\n\nexport type FunctionDeclaration = {\n  name: string;\n  description: string;\n  parameters: GeminiSchema;\n};\n\nexport type GoogleSearchRetrieval = {\n  dynamicRetrievalConfig: {\n    mode: \"MODE_UNSPECIFIED\" | \"MODE_DYNAMIC\";\n    dynamicThreshold: number;\n  };\n};\n\nexport type CodeExecution = {\n  // Type contains no fields.\n};\n\nexport type FinishReason =\n  // Natural stop point of the model or provided stop sequence.\n  | \"STOP\"\n  // The maximum number of tokens as specified in the request was reached.\n  | \"MAX_TOKENS\"\n  // The response candidate content was flagged for safety reasons.\n  | \"SAFETY\"\n  // The response candidate content was flagged for recitation reasons.\n  | \"RECITATION\"\n  // The response candidate content was flagged for using an unsupported language.\n  | \"LANGUAGE\"\n  // Unknown reason.\n  | \"OTHER\"\n  // Token generation stopped because the content contains forbidden terms.\n  | \"BLOCKLIST\"\n  // Token generation stopped for potentially containing prohibited content.\n  | \"PROHIBITED_CONTENT\"\n  // Token generation stopped because the content potentially contains Sensitive Personally Identifiable Information (SPII).\n  | \"SPII\"\n  // The function call generated by the model is invalid.\n  | \"MALFORMED_FUNCTION_CALL\";\n\nexport type GroundingMetadata = {\n  groundingChunks: {\n    web: {\n      uri: string;\n      title: string;\n    };\n  }[];\n  groundingSupports: {\n    groundingChunkIndices: number[];\n    confidenceScores: number[];\n    segment: {\n      partIndex: number;\n      startIndex: number;\n      endIndex: number;\n      text: string;\n    };\n  };\n  webSearchQueries: string[];\n  searchEntryPoint: {\n    renderedContent: string;\n    /**\n     * Base64 encoded JSON representing array of <search term, search url> tuple.\n     * A base64-encoded string.\n     */\n    sdkBlob: string;\n  };\n  retrievalMetadata: {\n    googleSearchDynamicRetrievalScore: number;\n  };\n};\n\nexport type Candidate = {\n  content?: LLMContent;\n  finishReason?: FinishReason;\n  safetyRatings?: SafetySetting[];\n  tokenOutput: number;\n  groundingMetadata: GroundingMetadata;\n};\n\nexport type GeminiAPIOutputs = {\n  candidates: Candidate[];\n};\n\nexport type GeminiOutputs =\n  | GeminiAPIOutputs\n  | {\n      context: LLMContent[];\n    };\n\nconst MODELS: readonly string[] = [\n  \"gemini-1.5-flash-latest\",\n  \"gemini-1.5-pro-latest\",\n  \"gemini-2.0-flash-exp\",\n  \"gemini-2.0-flash-thinking-exp\",\n  \"gemini-exp-1206\",\n  \"gemini-exp-1121\",\n  \"learnlm-1.5-pro-experimental\",\n  \"gemini-1.5-pro-001\",\n  \"gemini-1.5-pro-002\",\n  \"gemini-1.5-pro-exp-0801\",\n  \"gemini-1.5-pro-exp-0827\",\n  \"gemini-1.5-flash-001\",\n  \"gemini-1.5-flash-002\",\n  \"gemini-1.5-flash-8b-exp-0924\",\n  \"gemini-1.5-flash-8b-exp-0827\",\n  \"gemini-1.5-flash-exp-0827\",\n];\n\nconst NO_RETRY_CODES: readonly number[] = [400, 429, 404];\n\ntype FetchErrorResponse = {\n  $error: string;\n  status: number;\n  statusText: string;\n  contentType: string;\n  responseHeaders: Record<string, string>;\n};\n\n/**\n * Using\n * `{\"error\":{\"code\":400,\"message\":\"Invalid JSON paylo…'contents[0].parts[0]': Cannot find field.\"}]}]}\n * as template for this type.\n */\ntype GeminiError = {\n  error: {\n    code: number;\n    details: {\n      type: string;\n      fieldViolations: {\n        description: string;\n        field: string;\n      }[];\n    }[];\n    message: string;\n    status: string;\n  };\n};\n\nasync function callAPI(\n  retries: number,\n  model: string,\n  body: GeminiBody,\n  $metadata?: Metadata\n): Promise<Outcome<GeminiAPIOutputs>> {\n  let $error: string = \"Unknown error\";\n  while (retries) {\n    const result = await fetch({\n      $metadata,\n      url: await endpointURL(model),\n      method: \"POST\",\n      body,\n    });\n    if (!ok(result)) {\n      // Fetch is a bit weird, because it returns various props\n      // along with the `$error`. Let's handle that here.\n      const { status, $error: errObject } = result as FetchErrorResponse;\n      if (!status) {\n        // This is not an error response, presume fatal error.\n        return { $error };\n      }\n      $error = maybeExtractError(errObject);\n      if (NO_RETRY_CODES.includes(status)) {\n        return { $error };\n      }\n    } else {\n      const outputs = result.response as GeminiAPIOutputs;\n      const candidate = outputs.candidates?.at(0);\n      if (!candidate) {\n        return err(\"Unable to get a good response from Gemini\");\n      }\n      if (\"content\" in candidate) {\n        return outputs;\n      }\n    }\n    retries--;\n  }\n  return { $error };\n}\n\nfunction maybeExtractError(e: string): string {\n  try {\n    const parsed = JSON.parse(e) as GeminiError;\n    return parsed.error.message;\n  } catch (error) {\n    return e;\n  }\n}\n\nfunction isEmptyLLMContent(content?: LLMContent): content is undefined {\n  if (!content || !content.parts || content.parts.length === 0) return true;\n  return content.parts.every((part) => {\n    if (\"text\" in part) {\n      return !part.text?.trim();\n    }\n    return true;\n  });\n}\n\nfunction addModality(body: GeminiBody, modality?: ValidModalities) {\n  if (!modality) return;\n  switch (modality) {\n    case \"Text\":\n      // No change, defaults.\n      break;\n    case \"Text and Image\":\n      body.generationConfig ??= {};\n      body.generationConfig.responseModalities = [\"TEXT\", \"IMAGE\"];\n      break;\n    case \"Audio\":\n      body.generationConfig ??= {};\n      body.generationConfig.responseModalities = [\"AUDIO\"];\n      break;\n  }\n}\n\nfunction constructBody(\n  context: LLMContent[] = [],\n  systemInstruction?: LLMContent,\n  prompt?: LLMContent,\n  modality?: ValidModalities\n): GeminiBody {\n  const contents = [...context];\n  if (!isEmptyLLMContent(prompt)) {\n    contents.push(prompt);\n  }\n  const body: GeminiBody = {\n    contents,\n    safetySettings: defaultSafetySettings(),\n  };\n  const canHaveSystemInstruction = modality === \"Text\";\n  if (!isEmptyLLMContent(systemInstruction) && canHaveSystemInstruction) {\n    body.systemInstruction = systemInstruction;\n  }\n  addModality(body, modality);\n  return body;\n}\n\nfunction augmentBody(\n  body: GeminiBody,\n  systemInstruction?: LLMContent,\n  prompt?: LLMContent,\n  modality?: ValidModalities\n): GeminiBody {\n  if (!body.systemInstruction || !isEmptyLLMContent(systemInstruction)) {\n    body.systemInstruction = systemInstruction;\n  }\n  if (!isEmptyLLMContent(prompt)) {\n    body.contents = [...body.contents, prompt];\n  }\n  addModality(body, modality);\n  return body;\n}\n\nfunction validateInputs(inputs: GeminiInputs): Outcome<void> {\n  if (\"body\" in (inputs as object)) {\n    return;\n  }\n  if (inputs.context) {\n    const { context } = inputs;\n    if (!Array.isArray(context)) {\n      return err(\"Incoming context must be an array.\");\n    }\n    if (!isLLMContentArray(context)) {\n      return err(\"Malformed incoming context\");\n    }\n    return;\n  }\n  return err(\"Either body or context is required\");\n}\n\nasync function invoke(inputs: GeminiInputs): Promise<Outcome<GeminiOutputs>> {\n  const validatingInputs = validateInputs(inputs);\n  if (!ok(validatingInputs)) {\n    return validatingInputs;\n  }\n  let { model } = inputs;\n  if (!model) {\n    model = MODELS[0];\n  }\n  const { context, systemInstruction, prompt, modality, body, $metadata } =\n    inputs;\n  // TODO: Make this configurable.\n  const retries = 5;\n  if (!(\"body\" in inputs)) {\n    // Public API is being used.\n    // Behave as if we're wired in.\n    const result = await callAPI(\n      retries,\n      model,\n      constructBody(context, systemInstruction, prompt, modality)\n    );\n    if (!ok(result)) {\n      return result;\n    }\n    const content = result.candidates.at(0)?.content;\n    if (!content) {\n      return err(\"Unable to get a good response from Gemini\");\n    }\n    return { context: [...context!, content] };\n  } else {\n    // Private API is being used.\n    // Behave as if we're being invoked.\n    return callAPI(\n      retries,\n      model,\n      augmentBody(body, systemInstruction, prompt, modality),\n      $metadata\n    );\n  }\n}\n\ntype DescribeInputs = {\n  inputs: {\n    modality?: ValidModalities;\n    model: string;\n  };\n};\n\nasync function describe({ inputs }: DescribeInputs) {\n  const canHaveModalities = inputs.model === \"gemini-2.0-flash-exp\";\n  const canHaveSystemInstruction =\n    !canHaveModalities || (canHaveModalities && inputs.modality == \"Text\");\n  const maybeAddSystemInstruction: Schema[\"properties\"] =\n    canHaveSystemInstruction\n      ? {\n          systemInstruction: {\n            type: \"object\",\n            behavior: [\"llm-content\", \"config\"],\n            title: \"System Instruction\",\n            default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n            description:\n              \"(Optional) Give the model additional context on what to do,\" +\n              \"like specific rules/guidelines to adhere to or specify behavior\" +\n              \"separate from the provided context\",\n          },\n        }\n      : {};\n  const maybeAddModalities: Schema[\"properties\"] = canHaveModalities\n    ? {\n        modality: {\n          type: \"string\",\n          enum: [...VALID_MODALITIES],\n          title: \"Output Modality\",\n          behavior: [\"config\"],\n          description:\n            \"(Optional) Tell the model what kind of output you're looking for.\",\n        },\n      }\n    : {};\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        model: {\n          type: \"string\",\n          behavior: [\"config\"],\n          title: \"Model Name\",\n          enum: MODELS as string[],\n          default: MODELS[0],\n        },\n        prompt: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\"],\n          title: \"Prompt\",\n          default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n          description:\n            \"(Optional) A prompt. Will be added to the end of the the conversation context.\",\n        },\n        ...maybeAddSystemInstruction,\n        ...maybeAddModalities,\n        context: {\n          type: \"array\",\n          items: {\n            type: \"object\",\n            behavior: [\"llm-content\"],\n          },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: {\n            type: \"object\",\n            behavior: [\"llm-content\"],\n          },\n          title: \"Context out\",\n        },\n      },\n    } satisfies Schema,\n    metadata: {\n      icon: \"generative\",\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Gemini Model Family.",
        "runnable": false
      }
    },
    "entry": {
      "code": "/**\n * @fileoverview Manages the entry point: describer, passing the inputs, etc.\n */\nimport {} from \"./common\";\nimport { toLLMContent, defaultLLMContent } from \"./utils\";\nimport { Template } from \"./template\";\nexport { invoke as default, describe };\nasync function invoke({ context, \"p-chat\": chat, \"p-critique\": critique, description, ...params }) {\n    // Make sure it's a boolean.\n    chat = !!chat;\n    context ??= [];\n    const defaultModel = \"\";\n    const type = \"work\";\n    return {\n        context: {\n            id: Math.random().toString(36).substring(2, 5),\n            chat,\n            context,\n            userInputs: [],\n            defaultModel,\n            model: \"\",\n            description,\n            tools: [],\n            type,\n            work: [],\n            userEndedChat: false,\n            params,\n        },\n    };\n}\nasync function describe({ inputs: { description } }) {\n    const template = new Template(description);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                description: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\"],\n                    title: \"Instruction\",\n                    description: \"Give the model additional context on what to do, like specific rules/guidelines to adhere to or specify behavior separate from the provided context.\",\n                    default: defaultLLMContent(),\n                },\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                },\n                \"p-chat\": {\n                    type: \"boolean\",\n                    title: \"Chat with User\",\n                    behavior: [\"config\"],\n                    description: \"When checked, the model will chat with the user, asking to review work, requesting additional information, etc.\",\n                },\n                \"p-critique\": {\n                    type: \"boolean\",\n                    title: \"Self-critique\",\n                    behavior: [\"config\"],\n                    description: \"When checked, the model will critique its output to to improve quality in exchange for taking a little bit more time.\",\n                },\n                ...template.schemas(),\n            },\n            ...template.requireds(),\n            additionalProperties: false,\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                },\n            },\n            additionalProperties: false,\n        },\n        metadata: {\n            icon: \"generative-text\",\n        },\n    };\n}\n",
      "metadata": {
        "title": "Text Generator",
        "source": {
          "code": "/**\n * @fileoverview Manages the entry point: describer, passing the inputs, etc.\n */\n\nimport {\n  type AgentContext,\n  type AgentInputs,\n  type DescribeInputs,\n} from \"./common\";\nimport { toLLMContent, defaultLLMContent } from \"./utils\";\nimport { Template } from \"./template\";\n\nexport { invoke as default, describe };\n\nexport type EntryInputs = {\n  context: LLMContent[];\n  description: LLMContent;\n  \"p-chat\": boolean;\n  \"p-critique\": boolean;\n  [key: `p-z-${string}`]: unknown;\n};\n\ntype Outputs = {\n  context: AgentContext;\n};\n\nasync function invoke({\n  context,\n  \"p-chat\": chat,\n  \"p-critique\": critique,\n  description,\n  ...params\n}: EntryInputs): Promise<Outputs> {\n  // Make sure it's a boolean.\n  chat = !!chat;\n  context ??= [];\n  const defaultModel = \"\";\n  const type = \"work\";\n  return {\n    context: {\n      id: Math.random().toString(36).substring(2, 5),\n      chat,\n      context,\n      userInputs: [],\n      defaultModel,\n      model: \"\",\n      description,\n      tools: [],\n      type,\n      work: [],\n      userEndedChat: false,\n      params,\n    },\n  };\n}\n\nasync function describe({ inputs: { description } }: DescribeInputs) {\n  const template = new Template(description);\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        description: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\"],\n          title: \"Instruction\",\n          description:\n            \"Give the model additional context on what to do, like specific rules/guidelines to adhere to or specify behavior separate from the provided context.\",\n          default: defaultLLMContent(),\n        },\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n        },\n        \"p-chat\": {\n          type: \"boolean\",\n          title: \"Chat with User\",\n          behavior: [\"config\"],\n          description:\n            \"When checked, the model will chat with the user, asking to review work, requesting additional information, etc.\",\n        },\n        \"p-critique\": {\n          type: \"boolean\",\n          title: \"Self-critique\",\n          behavior: [\"config\"],\n          description:\n            \"When checked, the model will critique its output to to improve quality in exchange for taking a little bit more time.\",\n        },\n        ...template.schemas(),\n      },\n      ...template.requireds(),\n      additionalProperties: false,\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n        },\n      },\n      additionalProperties: false,\n    } satisfies Schema,\n    metadata: {\n      icon: \"generative-text\",\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Manages the entry point: describer, passing the inputs, etc.",
        "runnable": true
      }
    },
    "join": {
      "code": "/**\n * @fileoverview Joins user input and Agent Context\n */\nimport {} from \"./common\";\nimport { toText } from \"./utils\";\nexport { invoke as default, describe };\nasync function invoke({ context, request }) {\n    context.userEndedChat = !toText(request);\n    context.userInputs.push(request);\n    if (!context.userEndedChat) {\n        context.work.push(request);\n    }\n    return { context };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    title: \"Agent Context\",\n                    type: \"object\",\n                },\n                request: {\n                    title: \"User Input\",\n                    type: \"object\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    title: \"Agent Context\",\n                    type: \"object\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "join",
        "source": {
          "code": "/**\n * @fileoverview Joins user input and Agent Context\n */\n\nimport { type AgentContext } from \"./common\";\nimport { toText } from \"./utils\";\n\nexport { invoke as default, describe };\n\ntype Inputs = {\n  context: AgentContext;\n  request: LLMContent;\n};\n\ntype Outputs = {\n  context: AgentContext;\n};\n\nasync function invoke({ context, request }: Inputs): Promise<Outputs> {\n  context.userEndedChat = !toText(request);\n  context.userInputs.push(request);\n  if (!context.userEndedChat) {\n    context.work.push(request);\n  }\n  return { context };\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          title: \"Agent Context\",\n          type: \"object\",\n        },\n        request: {\n          title: \"User Input\",\n          type: \"object\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          title: \"Agent Context\",\n          type: \"object\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Joins user input and Agent Context",
        "runnable": true
      }
    },
    "output": {
      "code": "/**\n * @fileoverview Provides an output helper.\n */\nimport output from \"@output\";\nexport { report };\nasync function report(inputs) {\n    const { actor: title, category: description, name, details } = inputs;\n    const detailsSchema = typeof details === \"string\"\n        ? {\n            title: name,\n            type: \"string\",\n            format: \"markdown\",\n        }\n        : {\n            title: name,\n            type: \"object\",\n            behavior: [\"llm-content\"],\n        };\n    const { delivered } = await output({\n        $metadata: {\n            title,\n            description,\n        },\n        schema: {\n            type: \"object\",\n            properties: {\n                details: detailsSchema,\n            },\n        },\n        details,\n    });\n    return delivered;\n}\n",
      "metadata": {
        "title": "output",
        "source": {
          "code": "/**\n * @fileoverview Provides an output helper.\n */\n\nimport output from \"@output\";\n\ntype ReportInputs = {\n  /**\n   * The name of the actor providing the report\n   */\n  actor: string;\n  /**\n   * The general category of the report\n   */\n  category: string;\n  /**\n   * The name of the report\n   */\n  name: string;\n  /**\n   * The details of the report\n   */\n  details: string | LLMContent;\n};\n\nexport { report };\n\nasync function report(inputs: ReportInputs): Promise<boolean> {\n  const { actor: title, category: description, name, details } = inputs;\n\n  const detailsSchema: Schema =\n    typeof details === \"string\"\n      ? {\n          title: name,\n          type: \"string\",\n          format: \"markdown\",\n        }\n      : {\n          title: name,\n          type: \"object\",\n          behavior: [\"llm-content\"],\n        };\n\n  const { delivered } = await output({\n    $metadata: {\n      title,\n      description,\n    },\n    schema: {\n      type: \"object\",\n      properties: {\n        details: detailsSchema,\n      },\n    } satisfies Schema,\n    details,\n  });\n  return delivered;\n}\n",
          "language": "typescript"
        },
        "description": "Provides an output helper.",
        "runnable": false
      }
    },
    "tool-manager": {
      "code": "/**\n * @fileoverview Add a description for your module here.\n */\nimport describeGraph from \"@describe\";\nimport {} from \"./gemini\";\nexport { ToolManager };\nclass ToolManager {\n    tools = new Map();\n    errors = [];\n    #convertSchemas(schema) {\n        return toGeminiSchema(schema);\n        function toGeminiSchema(schema) {\n            switch (schema.type) {\n                case \"object\": {\n                    if (!schema.properties) {\n                        return { type: \"object\" };\n                    }\n                    return {\n                        type: \"object\",\n                        properties: Object.fromEntries(Object.entries(schema.properties).map(([name, schema]) => {\n                            return [name, toGeminiSchema(schema)];\n                        })),\n                    };\n                }\n                case \"array\": {\n                    return {\n                        type: \"array\",\n                        items: toGeminiSchema(schema),\n                    };\n                }\n                default: {\n                    const geminiSchema = { ...schema };\n                    delete geminiSchema.format;\n                    delete geminiSchema.behavior;\n                    delete geminiSchema.examples;\n                    delete geminiSchema.default;\n                    delete geminiSchema.transient;\n                    if (!geminiSchema.description) {\n                        geminiSchema.description = geminiSchema.title;\n                    }\n                    delete geminiSchema.title;\n                    return geminiSchema;\n                }\n            }\n        }\n    }\n    #toName(title) {\n        return title ? title.replace(/\\W/g, \"_\") : \"function\";\n    }\n    async initialize(tools) {\n        if (!tools) {\n            return true;\n        }\n        let hasInvalidTools = false;\n        for (const tool of tools) {\n            const url = typeof tool === \"string\" ? tool : tool.url;\n            const description = await describeGraph({ url });\n            if (description.$error) {\n                this.errors.push(description.$error);\n                // Invalid tool, skip\n                hasInvalidTools = true;\n                continue;\n            }\n            const parameters = this.#convertSchemas(description.inputSchema);\n            const name = this.#toName(description.title);\n            const functionDeclaration = {\n                name,\n                description: description.description || \"\",\n                parameters,\n            };\n            this.tools.set(name, { tool: functionDeclaration, url });\n        }\n        return !hasInvalidTools;\n    }\n    async processResponse(response, callTool) {\n        for (const part of response.parts) {\n            if (\"functionCall\" in part) {\n                const { args, name } = part.functionCall;\n                const url = this.tools.get(name)?.url;\n                if (url) {\n                    await callTool(url, part.functionCall.args);\n                }\n            }\n        }\n    }\n    list() {\n        const entries = [...this.tools.entries()];\n        if (entries.length === 0)\n            return [];\n        return [\n            {\n                functionDeclarations: entries.map(([, value]) => value.tool),\n            },\n        ];\n    }\n}\n",
      "metadata": {
        "title": "tool-manager",
        "source": {
          "code": "/**\n * @fileoverview Add a description for your module here.\n */\n\nimport describeGraph from \"@describe\";\n\nimport {\n  type FunctionDeclaration,\n  type GeminiSchema,\n  type Tool,\n} from \"./gemini\";\n\nexport type CallToolCallback = (tool: string, args: object) => Promise<void>;\n\nexport type ToolHandle = {\n  tool: FunctionDeclaration;\n  url: string;\n};\n\nexport type ToolDescriptor =\n  | string\n  | {\n      kind: \"board\";\n      url: string;\n    };\n\nexport { ToolManager };\n\nclass ToolManager {\n  tools: Map<string, ToolHandle> = new Map();\n  errors: string[] = [];\n\n  #convertSchemas(schema: Schema): GeminiSchema {\n    return toGeminiSchema(schema);\n\n    function toGeminiSchema(schema: Schema): GeminiSchema {\n      switch (schema.type) {\n        case \"object\": {\n          if (!schema.properties) {\n            return { type: \"object\" };\n          }\n          return {\n            type: \"object\",\n            properties: Object.fromEntries(\n              Object.entries(schema.properties).map(([name, schema]) => {\n                return [name, toGeminiSchema(schema)];\n              })\n            ),\n          };\n        }\n        case \"array\": {\n          return {\n            type: \"array\",\n            items: toGeminiSchema(schema),\n          };\n        }\n        default: {\n          const geminiSchema = { ...schema };\n          delete geminiSchema.format;\n          delete geminiSchema.behavior;\n          delete geminiSchema.examples;\n          delete geminiSchema.default;\n          delete geminiSchema.transient;\n          if (!geminiSchema.description) {\n            geminiSchema.description = geminiSchema.title;\n          }\n          delete geminiSchema.title;\n          return geminiSchema as GeminiSchema;\n        }\n      }\n    }\n  }\n\n  #toName(title?: string) {\n    return title ? title.replace(/\\W/g, \"_\") : \"function\";\n  }\n\n  async initialize(tools?: ToolDescriptor[]): Promise<boolean> {\n    if (!tools) {\n      return true;\n    }\n    let hasInvalidTools = false;\n    for (const tool of tools) {\n      const url = typeof tool === \"string\" ? tool : tool.url;\n      const description = await describeGraph({ url });\n      if (description.$error) {\n        this.errors.push(description.$error);\n        // Invalid tool, skip\n        hasInvalidTools = true;\n        continue;\n      }\n      const parameters = this.#convertSchemas(description.inputSchema);\n      const name = this.#toName(description.title);\n      const functionDeclaration = {\n        name,\n        description: description.description || \"\",\n        parameters,\n      };\n      this.tools.set(name, { tool: functionDeclaration, url });\n    }\n    return !hasInvalidTools;\n  }\n\n  async processResponse(response: LLMContent, callTool: CallToolCallback) {\n    for (const part of response.parts) {\n      if (\"functionCall\" in part) {\n        const { args, name } = part.functionCall;\n        const url = this.tools.get(name)?.url;\n        if (url) {\n          await callTool(url, part.functionCall.args);\n        }\n      }\n    }\n  }\n\n  list(): Tool[] {\n    const entries = [...this.tools.entries()];\n    if (entries.length === 0) return [];\n    return [\n      {\n        functionDeclarations: entries.map(([, value]) => value.tool),\n      },\n    ];\n  }\n}\n",
          "language": "typescript"
        },
        "description": "Add a description for your module here.",
        "runnable": false
      }
    },
    "worker-introducer": {
      "code": "/**\n * @fileoverview Summarizes worker's abilities for the purpose of introduction.\n */\nimport { defaultSafetySettings, } from \"./gemini\";\nimport { toText, toLLMContent, err, ok } from \"./utils\";\nimport invokeBoard from \"@invoke\";\nfunction introductionSchema() {\n    return {\n        type: \"object\",\n        properties: {\n            title: {\n                type: \"string\",\n                description: \"The title of the agent\",\n            },\n            abilities: {\n                type: \"string\",\n                description: \"Verb-first, third-person summary of the agent's abilities\",\n            },\n        },\n    };\n}\nfunction introductionInstruction(description, tools) {\n    let toolInstruction = \"You have no access to tools of any kind.\";\n    if (tools.length > 0) {\n        toolInstruction = `You have access to the following tools:\n${tools.map((tool) => JSON.stringify(tool)).join(\"\\n\\n\")}\n\nMake sure to mention your ability to call these particular tools in your abilities.\n`;\n    }\n    return toLLMContent(`You are a project manager for team of AI agents.\nRight now, your job is to summarize what one AI agent does.\nGiven an agent prompt, you distill it to a brief summary of abilities.\nThis summary will be used to decide when to invoke this agent.\n\n# AI Agent Prompt\n\n\\`\\`\\`\n${toText(description)}\n\nYour output is in Markdown format, and you have no means to create \nanything other than text.\n\n${toolInstruction}\n\n\\`\\`\\`\n\nReply in JSON using the provided schema.\n`);\n}\nexport { invoke as default, describe };\nasync function invoke({ description, model, tools }) {\n    const response = (await invokeBoard({\n        $board: model,\n        $metadata: {\n            title: \"Make Introductions\",\n            description: \"Introducing the agent to the team\",\n        },\n        body: {\n            contents: [toLLMContent(\"Write a summary of abilities\")],\n            systemInstruction: introductionInstruction(description, tools),\n            safetySettings: defaultSafetySettings(),\n            generationConfig: {\n                responseSchema: introductionSchema(),\n                responseMimeType: \"application/json\",\n            },\n        },\n    }));\n    if (!ok(response)) {\n        console.error(\"ERROR FROM GEMINI\", response.$error);\n        return response;\n    }\n    if (\"context\" in response) {\n        return err(\"Context should be the output, something's gone wrong.\");\n    }\n    const introduction = response.candidates?.at(0)?.content || toLLMContent(\"No valid response\");\n    return {\n        introduction,\n    };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                description: {\n                    type: \"object\",\n                    behavior: [\"llm-content\"],\n                    title: \"Job Description\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                introduction: {\n                    type: \"object\",\n                    behavior: [\"llm-content\"],\n                    title: \"Introduction\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "worker-introducer",
        "source": {
          "code": "/**\n * @fileoverview Summarizes worker's abilities for the purpose of introduction.\n */\n\nimport {\n  type GeminiSchema,\n  type GeminiOutputs,\n  defaultSafetySettings,\n  type Tool,\n} from \"./gemini\";\nimport { toText, toLLMContent, err, ok } from \"./utils\";\nimport invokeBoard from \"@invoke\";\n\nfunction introductionSchema(): GeminiSchema {\n  return {\n    type: \"object\",\n    properties: {\n      title: {\n        type: \"string\",\n        description: \"The title of the agent\",\n      },\n      abilities: {\n        type: \"string\",\n        description:\n          \"Verb-first, third-person summary of the agent's abilities\",\n      },\n    },\n  };\n}\n\nfunction introductionInstruction(\n  description: LLMContent,\n  tools: Tool[]\n): LLMContent {\n  let toolInstruction = \"You have no access to tools of any kind.\";\n  if (tools.length > 0) {\n    toolInstruction = `You have access to the following tools:\n${tools.map((tool) => JSON.stringify(tool)).join(\"\\n\\n\")}\n\nMake sure to mention your ability to call these particular tools in your abilities.\n`;\n  }\n  return toLLMContent(`You are a project manager for team of AI agents.\nRight now, your job is to summarize what one AI agent does.\nGiven an agent prompt, you distill it to a brief summary of abilities.\nThis summary will be used to decide when to invoke this agent.\n\n# AI Agent Prompt\n\n\\`\\`\\`\n${toText(description)}\n\nYour output is in Markdown format, and you have no means to create \nanything other than text.\n\n${toolInstruction}\n\n\\`\\`\\`\n\nReply in JSON using the provided schema.\n`);\n}\n\nexport { invoke as default, describe };\n\ntype Inputs = {\n  description: LLMContent;\n  model: string;\n  tools: Tool[];\n};\n\ntype Outputs = {\n  $error?: string;\n  introduction?: LLMContent;\n};\n\nasync function invoke({ description, model, tools }: Inputs): Promise<Outputs> {\n  const response = (await invokeBoard({\n    $board: model,\n    $metadata: {\n      title: \"Make Introductions\",\n      description: \"Introducing the agent to the team\",\n    },\n    body: {\n      contents: [toLLMContent(\"Write a summary of abilities\")],\n      systemInstruction: introductionInstruction(description, tools),\n      safetySettings: defaultSafetySettings(),\n      generationConfig: {\n        responseSchema: introductionSchema(),\n        responseMimeType: \"application/json\",\n      },\n    },\n  })) as Outcome<GeminiOutputs>;\n  if (!ok(response)) {\n    console.error(\"ERROR FROM GEMINI\", response.$error);\n    return response;\n  }\n  if (\"context\" in response) {\n    return err(\"Context should be the output, something's gone wrong.\");\n  }\n  const introduction =\n    response.candidates?.at(0)?.content || toLLMContent(\"No valid response\");\n  return {\n    introduction,\n  };\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        description: {\n          type: \"object\",\n          behavior: [\"llm-content\"],\n          title: \"Job Description\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        introduction: {\n          type: \"object\",\n          behavior: [\"llm-content\"],\n          title: \"Introduction\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Summarizes worker's abilities for the purpose of introduction.",
        "runnable": false
      }
    },
    "worker-worker": {
      "code": "/**\n * @fileoverview Performs assigned task. Part of the worker.\n */\nimport invokeBoard from \"@invoke\";\nimport output from \"@output\";\nimport { toText, toLLMContent, ok, err } from \"./utils\";\nimport { defaultSafetySettings, } from \"./gemini\";\nimport { callGemini } from \"./gemini-client\";\nimport { StructuredResponse } from \"./structured-response\";\nexport { invoke as default, describe };\nfunction computeWorkMode(tools, summarize) {\n    if (tools.length > 0) {\n        return \"call-tools\";\n    }\n    if (summarize) {\n        return \"summarize\";\n    }\n    return \"generate\";\n}\nasync function callTools(inputs, model, tools, retries) {\n    inputs.body.tools = tools;\n    inputs.body.toolConfig = {\n        functionCallingConfig: {\n            mode: \"ANY\",\n        },\n    };\n    const response = await callGemini(inputs, model, (response) => {\n        const r = response;\n        if (r.candidates?.at(0)?.content)\n            return;\n        return err(\"No content\");\n    }, retries);\n    if (!ok(response)) {\n        return toLLMContent(\"TODO: Handle Gemini error response\");\n    }\n    const r = response;\n    return r.candidates?.at(0)?.content || toLLMContent(\"No valid response\");\n}\nasync function generate(inputs, model, responseManager, retries) {\n    const response = await callGemini(inputs, model, (response) => {\n        return responseManager.parse(response);\n    }, retries);\n    if (!ok(response)) {\n        return response;\n    }\n    else {\n        console.log(\"RESPONSE MANAGER\", responseManager);\n        return {\n            product: toLLMContent(responseManager.body, \"model\"),\n            response: responseManager.response,\n        };\n    }\n}\nasync function invoke({ id, work, description, model, tools, summarize, chat, }) {\n    // TODO: Make this a parameter.\n    const retries = 5;\n    const mode = computeWorkMode(tools, summarize);\n    const responseManager = new StructuredResponse(id);\n    const inputs = {\n        body: {\n            contents: responseManager.addPrompt(work, chat),\n            systemInstruction: systemInstruction(description, mode, chat),\n            safetySettings: defaultSafetySettings(),\n        },\n    };\n    if (mode === \"call-tools\") {\n        const product = await callTools(inputs, model, tools, retries);\n        return { product, response: undefined };\n    }\n    else {\n        const result = await generate(inputs, model, responseManager, retries);\n        console.log(\"RESULT?\", result);\n        if (\"$error\" in result) {\n            return result;\n        }\n        if (chat) {\n            await output({\n                schema: {\n                    type: \"object\",\n                    properties: {\n                        product: {\n                            type: \"object\",\n                            behavior: [\"llm-content\"],\n                            title: \"Draft\",\n                        },\n                        message: {\n                            type: \"string\",\n                            title: \"Requesting feedback\",\n                            format: \"markdown\",\n                        },\n                    },\n                },\n                message: responseManager.epilog,\n                product: result.product,\n            });\n        }\n        console.log(\"RESULT\", result);\n        return result;\n    }\n}\n/**\n * Returns the system instruction based on on the provided parameters.\n */\nfunction systemInstruction(description, mode, chat) {\n    const preamble = `Here is your job description:\n${toText(description)}\n\n`;\n    const postamble = `\n\nToday is ${new Date().toLocaleString(\"en-US\", {\n        month: \"long\",\n        day: \"numeric\",\n        year: \"numeric\",\n        hour: \"numeric\",\n        minute: \"2-digit\",\n    })}`;\n    switch (mode) {\n        case \"summarize\":\n            return toLLMContent(` \n${preamble}\nSummarize the research results to fulfill the specified task.\n${postamble}`);\n        case \"call-tools\":\n            return toLLMContent(`\n${preamble}\nGenerate multiple function calls to fulfill the specified task.\n${postamble}`);\n        case \"generate\":\n            return toLLMContent(`\n${preamble}\nProvide the response that fulfills the specified task.\n${postamble}`);\n    }\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                work: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Work\",\n                },\n                description: {\n                    type: \"object\",\n                    behavior: [\"llm-content\"],\n                    title: \"Job Description\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"object\",\n                    behavior: [\"llm-content\"],\n                    title: \"Work Product\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "worker-worker",
        "source": {
          "code": "/**\n * @fileoverview Performs assigned task. Part of the worker.\n */\nimport invokeBoard from \"@invoke\";\nimport output from \"@output\";\n\nimport { toText, toLLMContent, ok, err } from \"./utils\";\nimport {\n  type GeminiSchema,\n  type GeminiInputs,\n  type GeminiOutputs,\n  type Tool,\n  defaultSafetySettings,\n  type GeminiAPIOutputs,\n} from \"./gemini\";\nimport { callGemini } from \"./gemini-client\";\nimport { StructuredResponse } from \"./structured-response\";\n\nexport { invoke as default, describe };\n\ntype Inputs = {\n  id: string;\n  work: LLMContent[];\n  description: LLMContent;\n  model: string;\n  tools: Tool[];\n  summarize: boolean;\n  chat: boolean;\n};\n\ntype Outputs = Outcome<{\n  product: LLMContent;\n  response?: LLMContent;\n}>;\n\ntype WorkMode = \"generate\" | \"call-tools\" | \"summarize\";\n\nfunction computeWorkMode(tools: Tool[], summarize: boolean): WorkMode {\n  if (tools.length > 0) {\n    return \"call-tools\";\n  }\n  if (summarize) {\n    return \"summarize\";\n  }\n  return \"generate\";\n}\n\nasync function callTools(\n  inputs: Omit<GeminiInputs, \"model\">,\n  model: string,\n  tools: Tool[],\n  retries: number\n): Promise<LLMContent> {\n  inputs.body.tools = tools;\n  inputs.body.toolConfig = {\n    functionCallingConfig: {\n      mode: \"ANY\",\n    },\n  };\n  const response = await callGemini(\n    inputs,\n    model,\n    (response) => {\n      const r = response as GeminiAPIOutputs;\n      if (r.candidates?.at(0)?.content) return;\n      return err(\"No content\");\n    },\n    retries\n  );\n  if (!ok(response)) {\n    return toLLMContent(\"TODO: Handle Gemini error response\");\n  }\n  const r = response as GeminiAPIOutputs;\n  return r.candidates?.at(0)?.content || toLLMContent(\"No valid response\");\n}\n\nasync function generate(\n  inputs: Omit<GeminiInputs, \"model\">,\n  model: string,\n  responseManager: StructuredResponse,\n  retries: number\n): Promise<Outputs> {\n  const response = await callGemini(\n    inputs,\n    model,\n    (response) => {\n      return responseManager.parse(response);\n    },\n    retries\n  );\n  if (!ok(response)) {\n    return response;\n  } else {\n    console.log(\"RESPONSE MANAGER\", responseManager);\n    return {\n      product: toLLMContent(responseManager.body, \"model\"),\n      response: responseManager.response!,\n    };\n  }\n}\n\nasync function invoke({\n  id,\n  work,\n  description,\n  model,\n  tools,\n  summarize,\n  chat,\n}: Inputs): Promise<Outputs> {\n  // TODO: Make this a parameter.\n  const retries = 5;\n  const mode = computeWorkMode(tools, summarize);\n  const responseManager = new StructuredResponse(id);\n  const inputs: Omit<GeminiInputs, \"model\"> = {\n    body: {\n      contents: responseManager.addPrompt(work, chat),\n      systemInstruction: systemInstruction(description, mode, chat),\n      safetySettings: defaultSafetySettings(),\n    },\n  };\n  if (mode === \"call-tools\") {\n    const product = await callTools(inputs, model, tools, retries);\n    return { product, response: undefined };\n  } else {\n    const result = await generate(inputs, model, responseManager, retries);\n    console.log(\"RESULT?\", result);\n    if (\"$error\" in result) {\n      return result;\n    }\n    if (chat) {\n      await output({\n        schema: {\n          type: \"object\",\n          properties: {\n            product: {\n              type: \"object\",\n              behavior: [\"llm-content\"],\n              title: \"Draft\",\n            },\n            message: {\n              type: \"string\",\n              title: \"Requesting feedback\",\n              format: \"markdown\",\n            },\n          },\n        },\n        message: responseManager.epilog,\n        product: result.product,\n      });\n    }\n    console.log(\"RESULT\", result);\n    return result;\n  }\n}\n\n/**\n * Returns the system instruction based on on the provided parameters.\n */\nfunction systemInstruction(\n  description: LLMContent,\n  mode: WorkMode,\n  chat: boolean\n): LLMContent {\n  const preamble = `Here is your job description:\n${toText(description)}\n\n`;\n  const postamble = `\n\nToday is ${new Date().toLocaleString(\"en-US\", {\n    month: \"long\",\n    day: \"numeric\",\n    year: \"numeric\",\n    hour: \"numeric\",\n    minute: \"2-digit\",\n  })}`;\n\n  switch (mode) {\n    case \"summarize\":\n      return toLLMContent(` \n${preamble}\nSummarize the research results to fulfill the specified task.\n${postamble}`);\n\n    case \"call-tools\":\n      return toLLMContent(`\n${preamble}\nGenerate multiple function calls to fulfill the specified task.\n${postamble}`);\n\n    case \"generate\":\n      return toLLMContent(`\n${preamble}\nProvide the response that fulfills the specified task.\n${postamble}`);\n  }\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        work: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Work\",\n        },\n        description: {\n          type: \"object\",\n          behavior: [\"llm-content\"],\n          title: \"Job Description\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"object\",\n          behavior: [\"llm-content\"],\n          title: \"Work Product\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Performs assigned task. Part of the worker.",
        "runnable": false
      }
    },
    "gemini-client": {
      "code": "import invokeGemini, {} from \"./gemini\";\nimport { ok, err } from \"./utils\";\nexport { callGemini };\nasync function callGemini(inputs, model, validator, retries) {\n    // TODO: Add more nuanced logic around retries\n    for (let i = 0; i < retries; ++i) {\n        const nextStep = i == retries ? \"bailing\" : \"will retry\";\n        const response = await invokeGemini(inputs);\n        if (!ok(response)) {\n            console.error(`Error from model, ${nextStep}`, response.$error);\n        }\n        else {\n            const validating = validator(response);\n            if (!ok(validating)) {\n                console.error(`Validation error, ${nextStep}`, validating.$error);\n                continue;\n            }\n            return response;\n        }\n    }\n    return err(`Failed to get valid response after ${retries} tries`);\n}\n",
      "metadata": {
        "title": "gemini-client",
        "source": {
          "code": "import invokeGemini, { type GeminiInputs, type GeminiOutputs } from \"./gemini\";\nimport { ok, err } from \"./utils\";\n\nexport type ValidatorFunction = (response: GeminiOutputs) => Outcome<void>;\n\nexport { callGemini };\n\nasync function callGemini(\n  inputs: Omit<GeminiInputs, \"model\">,\n  model: string,\n  validator: ValidatorFunction,\n  retries: number\n): Promise<Outcome<GeminiOutputs>> {\n  // TODO: Add more nuanced logic around retries\n  for (let i = 0; i < retries; ++i) {\n    const nextStep = i == retries ? \"bailing\" : \"will retry\";\n    const response = await invokeGemini(inputs);\n    if (!ok(response)) {\n      console.error(`Error from model, ${nextStep}`, response.$error);\n    } else {\n      const validating = validator(response);\n      if (!ok(validating)) {\n        console.error(`Validation error, ${nextStep}`, validating.$error);\n        continue;\n      }\n      return response;\n    }\n  }\n  return err(`Failed to get valid response after ${retries} tries`);\n}\n",
          "language": "typescript"
        },
        "description": "",
        "runnable": false
      }
    },
    "agent-main": {
      "code": "/**\n * @fileoverview The main body of the agent\n */\nimport output from \"@output\";\nimport {} from \"./common\";\nimport { ToolManager } from \"./tool-manager\";\nimport workerIntroducer from \"./worker-introducer\";\nimport workerWorker from \"./worker-worker\";\nimport { report } from \"./output\";\nimport { defaultLLMContent, toText, err, endsWithRole, ok } from \"./utils\";\nimport invokeGraph from \"@invoke\";\nimport { Template, withParameters } from \"./template\";\nexport { invoke as default, describe };\nfunction toLLMContent(text, role = \"user\") {\n    return { parts: [{ text }], role };\n}\nasync function invoke({ context }) {\n    console.log(\"AGENT MAIN\", context);\n    let { id, description, type, context: initialContext, model, defaultModel, tools, chat, work: workContext, params, } = context;\n    if (!description) {\n        const $error = \"No instruction supplied\";\n        await report({\n            actor: \"Text Generator\",\n            name: $error,\n            category: \"Runtime error\",\n            details: `In order to run, I need to have an instruction.`,\n        });\n        return { $error };\n    }\n    const template = new Template(description);\n    const substituting = await template.substitute(withParameters(params));\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    description = substituting;\n    // For now, use the internal Gemini module to invoke\n    // TODO: Add back the capability to invoke model boards\n    //   if (!model) {\n    //     model = defaultModel;\n    //   }\n    //   if (!model) {\n    //     const $error = \"Model was not supplied\";\n    //     await report({\n    //       actor: \"Agent\",\n    //       name: $error,\n    //       category: \"Runtime error\",\n    //       details: `In order to run, Agent neeeds a model to be connected to it.\n    // Please drag one (\"Gemini\" should work) from the list of components over to the \"Model\"\n    // port`,\n    //     });\n    //     return { $error };\n    //   }\n    const toolManager = new ToolManager();\n    if (!(await toolManager.initialize(tools))) {\n        const $error = `Problem initializing tools. \nThe following errors were encountered: ${toolManager.errors.join(\",\")}`;\n        console.error(\"MAIN ERROR\", $error, toolManager.errors);\n        return { $error };\n    }\n    switch (type) {\n        case \"introduction\": {\n            const response = await workerIntroducer({\n                description,\n                model,\n                tools: toolManager.list(),\n            });\n            if (response.$error) {\n                console.error(\"INTRODUCER ERROR\", response.$error);\n                return {\n                    $error: response.$error,\n                };\n            }\n            return {\n                done: [response.introduction || toLLMContent(\"No valid response\")],\n            };\n        }\n        case \"work\": {\n            const { userEndedChat, userInputs, last } = context;\n            if (userEndedChat) {\n                if (!last) {\n                    return err(\"Chat ended without any work\");\n                }\n                return {\n                    done: [...initialContext, last],\n                };\n            }\n            else {\n                const work = [...initialContext, ...workContext];\n                const response = await workerWorker({\n                    id,\n                    description,\n                    work,\n                    model,\n                    tools: toolManager.list(),\n                    summarize: false,\n                    chat,\n                });\n                if (\"$error\" in response) {\n                    console.error(\"ERROR FROM WORKER\", response.$error);\n                    return {\n                        $error: response.$error,\n                    };\n                }\n                const workerResponse = response.product;\n                const toolResults = [];\n                await toolManager.processResponse(workerResponse, async ($board, args) => {\n                    const result = await invokeGraph({\n                        $board,\n                        ...args,\n                    });\n                    toolResults.push(result);\n                });\n                if (toolResults.length > 0) {\n                    const summary = await workerWorker({\n                        id,\n                        description,\n                        work: [\n                            ...toolResults.map((toolResult) => toLLMContent(JSON.stringify(toolResult))),\n                        ],\n                        model,\n                        tools: [],\n                        summarize: true,\n                        chat: false,\n                    });\n                    if (\"$error\" in summary) {\n                        console.error(\"ERROR FROM SUMMARY\", summary.$error);\n                        return {\n                            $error: summary.$error,\n                        };\n                    }\n                    const summaryResponse = summary.product;\n                    return { done: [summaryResponse] };\n                }\n                else if (chat) {\n                    const { userInputs } = context;\n                    if (!userEndedChat) {\n                        const toInput = {\n                            type: \"object\",\n                            properties: {\n                                request: {\n                                    type: \"object\",\n                                    title: \"Please provide feedback\",\n                                    behavior: [\"transient\", \"llm-content\"],\n                                    examples: [defaultLLMContent()],\n                                },\n                            },\n                        };\n                        return {\n                            toInput,\n                            context: {\n                                ...context,\n                                work: [...workContext, response.response],\n                                last: response.product,\n                            },\n                        };\n                    }\n                }\n                return { done: [workerResponse] };\n            }\n        }\n        default:\n            return {\n                done: [\n                    { parts: [{ text: \"Unknown task type\" }] },\n                ],\n            };\n    }\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"object\",\n                    title: \"Agent Context\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                toInput: {\n                    type: \"object\",\n                    title: \"Input Schema\",\n                },\n                context: {\n                    type: \"object\",\n                    title: \"Agent Context\",\n                },\n                done: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Done\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "agent-main",
        "source": {
          "code": "/**\n * @fileoverview The main body of the agent\n */\nimport output from \"@output\";\nimport { type AgentContext } from \"./common\";\nimport { ToolManager } from \"./tool-manager\";\nimport workerIntroducer from \"./worker-introducer\";\nimport workerWorker from \"./worker-worker\";\nimport { report } from \"./output\";\nimport { defaultLLMContent, toText, err, endsWithRole, ok } from \"./utils\";\nimport invokeGraph from \"@invoke\";\nimport { Template, withParameters } from \"./template\";\n\nexport { invoke as default, describe };\n\ntype Inputs = {\n  context: AgentContext;\n};\n\ntype Outputs = {\n  $error?: string;\n  context?: AgentContext;\n  toInput?: Schema;\n  done?: LLMContent[];\n};\n\nfunction toLLMContent(\n  text: string,\n  role: LLMContent[\"role\"] = \"user\"\n): LLMContent {\n  return { parts: [{ text }], role };\n}\n\nasync function invoke({ context }: Inputs): Promise<Outputs> {\n  console.log(\"AGENT MAIN\", context);\n  let {\n    id,\n    description,\n    type,\n    context: initialContext,\n    model,\n    defaultModel,\n    tools,\n    chat,\n    work: workContext,\n    params,\n  } = context;\n  if (!description) {\n    const $error = \"No instruction supplied\";\n    await report({\n      actor: \"Text Generator\",\n      name: $error,\n      category: \"Runtime error\",\n      details: `In order to run, I need to have an instruction.`,\n    });\n    return { $error };\n  }\n  const template = new Template(description);\n  const substituting = await template.substitute(withParameters(params));\n  if (!ok(substituting)) {\n    return substituting;\n  }\n  description = substituting;\n  // For now, use the internal Gemini module to invoke\n  // TODO: Add back the capability to invoke model boards\n  //   if (!model) {\n  //     model = defaultModel;\n  //   }\n  //   if (!model) {\n  //     const $error = \"Model was not supplied\";\n\n  //     await report({\n  //       actor: \"Agent\",\n  //       name: $error,\n  //       category: \"Runtime error\",\n  //       details: `In order to run, Agent neeeds a model to be connected to it.\n  // Please drag one (\"Gemini\" should work) from the list of components over to the \"Model\"\n  // port`,\n  //     });\n  //     return { $error };\n  //   }\n  const toolManager = new ToolManager();\n  if (!(await toolManager.initialize(tools))) {\n    const $error = `Problem initializing tools. \nThe following errors were encountered: ${toolManager.errors.join(\",\")}`;\n    console.error(\"MAIN ERROR\", $error, toolManager.errors);\n    return { $error };\n  }\n\n  switch (type) {\n    case \"introduction\": {\n      const response = await workerIntroducer({\n        description,\n        model,\n        tools: toolManager.list(),\n      });\n      if (response.$error) {\n        console.error(\"INTRODUCER ERROR\", response.$error);\n        return {\n          $error: response.$error,\n        };\n      }\n      return {\n        done: [response.introduction || toLLMContent(\"No valid response\")],\n      };\n    }\n    case \"work\": {\n      const { userEndedChat, userInputs, last } = context;\n      if (userEndedChat) {\n        if (!last) {\n          return err(\"Chat ended without any work\");\n        }\n        return {\n          done: [...initialContext, last],\n        };\n      } else {\n        const work = [...initialContext, ...workContext];\n        const response = await workerWorker({\n          id,\n          description,\n          work,\n          model,\n          tools: toolManager.list(),\n          summarize: false,\n          chat,\n        });\n        if (\"$error\" in response) {\n          console.error(\"ERROR FROM WORKER\", response.$error);\n          return {\n            $error: response.$error,\n          };\n        }\n        const workerResponse = response.product;\n        const toolResults: object[] = [];\n        await toolManager.processResponse(\n          workerResponse,\n          async ($board, args) => {\n            const result = await invokeGraph({\n              $board,\n              ...args,\n            });\n            toolResults.push(result);\n          }\n        );\n        if (toolResults.length > 0) {\n          const summary = await workerWorker({\n            id,\n            description,\n            work: [\n              ...toolResults.map((toolResult) =>\n                toLLMContent(JSON.stringify(toolResult))\n              ),\n            ],\n            model,\n            tools: [],\n            summarize: true,\n            chat: false,\n          });\n          if (\"$error\" in summary) {\n            console.error(\"ERROR FROM SUMMARY\", summary.$error);\n            return {\n              $error: summary.$error,\n            };\n          }\n          const summaryResponse = summary.product;\n          return { done: [summaryResponse] };\n        } else if (chat) {\n          const { userInputs } = context;\n          if (!userEndedChat) {\n            const toInput: Schema = {\n              type: \"object\",\n              properties: {\n                request: {\n                  type: \"object\",\n                  title: \"Please provide feedback\",\n                  behavior: [\"transient\", \"llm-content\"],\n                  examples: [defaultLLMContent()],\n                },\n              },\n            };\n            return {\n              toInput,\n              context: {\n                ...context,\n                work: [...workContext, response.response!],\n                last: response.product,\n              },\n            };\n          }\n        }\n        return { done: [workerResponse] };\n      }\n    }\n    default:\n      return {\n        done: [\n          { parts: [{ text: \"Unknown task type\" }] },\n        ] satisfies LLMContent[],\n      };\n  }\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"object\",\n          title: \"Agent Context\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        toInput: {\n          type: \"object\",\n          title: \"Input Schema\",\n        },\n        context: {\n          type: \"object\",\n          title: \"Agent Context\",\n        },\n        done: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Done\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "The main body of the agent",
        "runnable": true
      }
    },
    "researcher": {
      "code": "/**\n * @fileoverview Scours the Internet according to your plan.\n */\nimport { ToolManager } from \"./tool-manager\";\nimport { Template, withParameters } from \"./template\";\nimport invokeGraph from \"@invoke\";\nimport invokeGemini, { defaultSafetySettings, } from \"./gemini\";\nimport { ok, err, toLLMContent, toText } from \"./utils\";\nimport { report } from \"./output\";\nimport {} from \"./common\";\nexport { invoke as default, describe };\nconst RESEARCH_TOOLS = [\n    \"https://breadboard.live/boards/@dimitri/custom-google-search.bgl.json\",\n    \"https://breadboard.live/boards/@dimitri/wikipedia.bgl.json\",\n    \"https://breadboard.live/boards/@dimitri/tool-page-as-markdown.bgl.json\",\n    \"https://breadboard.live/boards/@dimitri/tool-maps-text-search.bgl.json\",\n];\nconst RESEARCH_MODEL = \"gemini-2.0-flash-exp\";\nconst MAX_ITERATIONS = 7;\nfunction defaultTools() {\n    return JSON.stringify(RESEARCH_TOOLS);\n}\nfunction systemInstruction(plan, first) {\n    const which = first ? \"first\" : \"next\";\n    return `You are a researcher.\n  \nYour job is to use the provided research plan to produce raw research that will be later turned into a detailed research report.\nYou are tasked with finding as much of relevant information as possible.\n\nYour research plan is:\n\n---\n\n${toText(plan)}\n\n---\n\nYou examine the conversation context so far and come up with the ${which} step to produce the report, \nusing the conversation context as the the guide of steps taken so far and the outcomes recorded.\n\nYou do not ask user for feedback. You do not try to have a conversation with the user. \nYou know that the user will only ask you to proceed to next step.\n\nYour next step consists of answering two questions.\n\nFirst, ask yourself \"am I done?\" -- looking back at all that you've researched and the plan, \ndo you have enough to produce the detailed report?\n\nSecond, provide a response. Your response must contain two parts:\nThought: a brief plain text reasoning why this is the right ${which} step and a description of what you will do in plain English.\nAction: invoking the tools are your disposal, more than one if necessary. If you're done, do not invoke any tools.`;\n}\nfunction researcherPrompt(contents, plan, tools, first) {\n    return {\n        model: RESEARCH_MODEL,\n        body: {\n            contents,\n            tools,\n            systemInstruction: toLLMContent(systemInstruction(plan, first)),\n            safetySettings: defaultSafetySettings(),\n        },\n    };\n}\nfunction reportWriterInstruction(plan) {\n    return `You are a research report writer. \nYour teammates produces a wealth of raw research using this research plan as their guide:\n\n---\n\n${toText(plan)}\n\n---\n\nYour task is to take the raw research and write a thorough, detailed research report that captures it in a way that follows the plan.\n\nA report must additionally contain references to the source (always cite your sources).`;\n}\nfunction reportWriterPrompt(plan, research) {\n    return {\n        model: RESEARCH_MODEL,\n        body: {\n            contents: [toLLMContent(research.join(\"\\n\\n\"))],\n            systemInstruction: toLLMContent(reportWriterInstruction(plan)),\n            safetySettings: defaultSafetySettings(),\n        },\n    };\n}\nasync function thought(response, iteration) {\n    const first = response.parts.at(0);\n    if (!first || !(\"text\" in first)) {\n        return;\n    }\n    await report({\n        actor: \"Researcher\",\n        category: `Progress report, iteration ${iteration + 1}`,\n        name: \"Thought\",\n        details: first.text\n            .replace(/^Thought: ?/gm, \"\")\n            .replace(/^Action:.*$/gm, \"\")\n            .trim(),\n    });\n}\nasync function invoke({ context, plan, summarize, tools, ...params }) {\n    tools ??= RESEARCH_TOOLS;\n    const toolManager = new ToolManager();\n    const initializing = await toolManager.initialize(tools);\n    if (!initializing) {\n        return err(\"Unable to initialize tools\");\n    }\n    let content = context;\n    const template = new Template(plan);\n    const substituting = await template.substitute(withParameters(params));\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    plan = substituting;\n    const research = [];\n    for (let i = 0; i <= MAX_ITERATIONS; i++) {\n        const askingGemini = await invokeGemini(researcherPrompt(content, plan, toolManager.list(), i === 0));\n        if (!ok(askingGemini)) {\n            return askingGemini;\n        }\n        if (\"context\" in askingGemini) {\n            return err(`Unexpected \"context\" response`);\n        }\n        const response = askingGemini.candidates.at(0)?.content;\n        if (!response) {\n            return err(\"No actionable response\");\n        }\n        await thought(response, i);\n        const toolResponses = [];\n        await toolManager.processResponse(response, async ($board, args) => {\n            toolResponses.push(JSON.stringify(await invokeGraph({ $board, ...args })));\n        });\n        if (toolResponses.length === 0) {\n            break;\n        }\n        research.push(...toolResponses);\n        content = [...content, response, toLLMContent(toolResponses.join(\"\\n\\n\"))];\n    }\n    if (research.length === 0) {\n        await report({\n            actor: \"Researcher\",\n            category: \"Error\",\n            name: \"Error\",\n            details: \"I was unable to obtain any research results\",\n        });\n        return { context };\n    }\n    if (summarize) {\n        const producingReport = await invokeGemini(reportWriterPrompt(plan, research));\n        if (!ok(producingReport)) {\n            return producingReport;\n        }\n        if (\"context\" in producingReport) {\n            return err(`Unexpected \"context\" response`);\n        }\n        const response = producingReport.candidates.at(0)?.content;\n        if (!response) {\n            return err(\"No actionable response\");\n        }\n        return { context: [...context, response] };\n    }\n    return { context: [...context, toLLMContent(research.join(\"\\n\\n\"))] };\n}\nasync function describe({ inputs: { plan } }) {\n    const template = new Template(plan);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                tools: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"board\"] },\n                    title: \"Tools\",\n                    description: \"The tools to use for research\",\n                    default: defaultTools(),\n                },\n                plan: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\"],\n                    title: \"Research Plan\",\n                    description: \"Provide an outline of what to research, what areas to cover, etc.\",\n                },\n                summarize: {\n                    type: \"boolean\",\n                    behavior: [\"config\"],\n                    title: \"Summarize research\",\n                    description: \"If checked, the Researcher will summarize the results of the research and only pass the research summary along.\",\n                },\n                ...template.schemas(),\n            },\n            ...template.requireds(),\n            additionalProperties: false,\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                },\n            },\n            additionalProperties: false,\n        },\n        metadata: {\n            icon: \"generative\",\n        },\n    };\n}\n",
      "metadata": {
        "title": "Researcher",
        "source": {
          "code": "/**\n * @fileoverview Scours the Internet according to your plan.\n */\nimport { ToolManager, type ToolDescriptor } from \"./tool-manager\";\nimport { Template, withParameters } from \"./template\";\nimport invokeGraph from \"@invoke\";\nimport invokeGemini, {\n  type GeminiInputs,\n  type Tool,\n  defaultSafetySettings,\n} from \"./gemini\";\nimport { ok, err, toLLMContent, toText } from \"./utils\";\nimport { report } from \"./output\";\nimport { type Params } from \"./common\";\n\nexport { invoke as default, describe };\n\nexport type ResearcherInputs = {\n  context: LLMContent[];\n  plan: LLMContent;\n  summarize: boolean;\n  tools: ToolDescriptor[];\n} & Params;\n\nconst RESEARCH_TOOLS: ToolDescriptor[] = [\n  \"https://breadboard.live/boards/@dimitri/custom-google-search.bgl.json\",\n  \"https://breadboard.live/boards/@dimitri/wikipedia.bgl.json\",\n  \"https://breadboard.live/boards/@dimitri/tool-page-as-markdown.bgl.json\",\n  \"https://breadboard.live/boards/@dimitri/tool-maps-text-search.bgl.json\",\n];\n\nconst RESEARCH_MODEL = \"gemini-2.0-flash-exp\";\n\nconst MAX_ITERATIONS = 7;\n\nfunction defaultTools() {\n  return JSON.stringify(RESEARCH_TOOLS);\n}\n\nfunction systemInstruction(plan: LLMContent, first: boolean): string {\n  const which = first ? \"first\" : \"next\";\n  return `You are a researcher.\n  \nYour job is to use the provided research plan to produce raw research that will be later turned into a detailed research report.\nYou are tasked with finding as much of relevant information as possible.\n\nYour research plan is:\n\n---\n\n${toText(plan)}\n\n---\n\nYou examine the conversation context so far and come up with the ${which} step to produce the report, \nusing the conversation context as the the guide of steps taken so far and the outcomes recorded.\n\nYou do not ask user for feedback. You do not try to have a conversation with the user. \nYou know that the user will only ask you to proceed to next step.\n\nYour next step consists of answering two questions.\n\nFirst, ask yourself \"am I done?\" -- looking back at all that you've researched and the plan, \ndo you have enough to produce the detailed report?\n\nSecond, provide a response. Your response must contain two parts:\nThought: a brief plain text reasoning why this is the right ${which} step and a description of what you will do in plain English.\nAction: invoking the tools are your disposal, more than one if necessary. If you're done, do not invoke any tools.`;\n}\n\nfunction researcherPrompt(\n  contents: LLMContent[],\n  plan: LLMContent,\n  tools: Tool[],\n  first: boolean\n): GeminiInputs {\n  return {\n    model: RESEARCH_MODEL,\n    body: {\n      contents,\n      tools,\n      systemInstruction: toLLMContent(systemInstruction(plan, first)),\n      safetySettings: defaultSafetySettings(),\n    },\n  };\n}\n\nfunction reportWriterInstruction(plan: LLMContent) {\n  return `You are a research report writer. \nYour teammates produces a wealth of raw research using this research plan as their guide:\n\n---\n\n${toText(plan)}\n\n---\n\nYour task is to take the raw research and write a thorough, detailed research report that captures it in a way that follows the plan.\n\nA report must additionally contain references to the source (always cite your sources).`;\n}\n\nfunction reportWriterPrompt(\n  plan: LLMContent,\n  research: string[]\n): GeminiInputs {\n  return {\n    model: RESEARCH_MODEL,\n    body: {\n      contents: [toLLMContent(research.join(\"\\n\\n\"))],\n      systemInstruction: toLLMContent(reportWriterInstruction(plan)),\n      safetySettings: defaultSafetySettings(),\n    },\n  };\n}\n\nasync function thought(response: LLMContent, iteration: number) {\n  const first = response.parts.at(0);\n  if (!first || !(\"text\" in first)) {\n    return;\n  }\n  await report({\n    actor: \"Researcher\",\n    category: `Progress report, iteration ${iteration + 1}`,\n    name: \"Thought\",\n    details: first.text\n      .replace(/^Thought: ?/gm, \"\")\n      .replace(/^Action:.*$/gm, \"\")\n      .trim(),\n  });\n}\n\nasync function invoke({\n  context,\n  plan,\n  summarize,\n  tools,\n  ...params\n}: ResearcherInputs) {\n  tools ??= RESEARCH_TOOLS;\n  const toolManager = new ToolManager();\n  const initializing = await toolManager.initialize(tools);\n  if (!initializing) {\n    return err(\"Unable to initialize tools\");\n  }\n  let content = context;\n\n  const template = new Template(plan);\n  const substituting = await template.substitute(withParameters(params));\n  if (!ok(substituting)) {\n    return substituting;\n  }\n  plan = substituting;\n\n  const research: string[] = [];\n  for (let i = 0; i <= MAX_ITERATIONS; i++) {\n    const askingGemini = await invokeGemini(\n      researcherPrompt(content, plan, toolManager.list(), i === 0)\n    );\n\n    if (!ok(askingGemini)) {\n      return askingGemini;\n    }\n    if (\"context\" in askingGemini) {\n      return err(`Unexpected \"context\" response`);\n    }\n    const response = askingGemini.candidates.at(0)?.content;\n    if (!response) {\n      return err(\"No actionable response\");\n    }\n    await thought(response, i);\n\n    const toolResponses: string[] = [];\n    await toolManager.processResponse(response, async ($board, args) => {\n      toolResponses.push(\n        JSON.stringify(await invokeGraph({ $board, ...args }))\n      );\n    });\n    if (toolResponses.length === 0) {\n      break;\n    }\n    research.push(...toolResponses);\n    content = [...content, response, toLLMContent(toolResponses.join(\"\\n\\n\"))];\n  }\n  if (research.length === 0) {\n    await report({\n      actor: \"Researcher\",\n      category: \"Error\",\n      name: \"Error\",\n      details: \"I was unable to obtain any research results\",\n    });\n    return { context };\n  }\n  if (summarize) {\n    const producingReport = await invokeGemini(\n      reportWriterPrompt(plan, research)\n    );\n    if (!ok(producingReport)) {\n      return producingReport;\n    }\n    if (\"context\" in producingReport) {\n      return err(`Unexpected \"context\" response`);\n    }\n    const response = producingReport.candidates.at(0)?.content;\n    if (!response) {\n      return err(\"No actionable response\");\n    }\n    return { context: [...context, response] };\n  }\n  return { context: [...context, toLLMContent(research.join(\"\\n\\n\"))] };\n}\n\ntype DescribeInputs = {\n  inputs: {\n    plan: LLMContent;\n  };\n};\n\nasync function describe({ inputs: { plan } }: DescribeInputs) {\n  const template = new Template(plan);\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n        tools: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"board\"] },\n          title: \"Tools\",\n          description: \"The tools to use for research\",\n          default: defaultTools(),\n        },\n        plan: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\"],\n          title: \"Research Plan\",\n          description:\n            \"Provide an outline of what to research, what areas to cover, etc.\",\n        },\n        summarize: {\n          type: \"boolean\",\n          behavior: [\"config\"],\n          title: \"Summarize research\",\n          description:\n            \"If checked, the Researcher will summarize the results of the research and only pass the research summary along.\",\n        },\n        ...template.schemas(),\n      },\n      ...template.requireds(),\n      additionalProperties: false,\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n        },\n      },\n      additionalProperties: false,\n    } satisfies Schema,\n    metadata: {\n      icon: \"generative\",\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Scours the Internet according to your plan.",
        "runnable": true
      }
    },
    "image-generator": {
      "code": "/**\n * @fileoverview Generates an image using supplied context.\n */\nimport gemini, { defaultSafetySettings, } from \"./gemini\";\nimport { err, ok, toLLMContent, toText } from \"./utils\";\nimport { Template, withParameters } from \"./template\";\nimport {} from \"./common\";\nexport { invoke as default, describe };\nfunction promptRequest(contents, instruction) {\n    return {\n        model: \"gemini-1.5-flash-latest\",\n        body: {\n            contents,\n            systemInstruction: toLLMContent(`\nYou are a creative writer whose specialty is to write prompts for text-to-image models.\nGiven the conversation context and the additional instructions, you produce the prompt.\n\nThe prompt must describe every object in the image in great detail and describe the style \nin terms of color scheme and vibe.\n\n## Additional instructions:\n\n${toText(instruction)}\n\n`),\n        },\n    };\n}\nasync function invoke({ context, instruction, ...params }) {\n    // 0) Substitute params in instruction.\n    const substituting = await new Template(instruction).substitute(withParameters(params));\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    instruction = substituting;\n    // 1) Get last LLMContent from input.\n    const contents = context && Array.isArray(context) && context.length > 0\n        ? [context.at(-1)]\n        : undefined;\n    if (!contents) {\n        return err(\"Must supply context as input\");\n    }\n    // 2) Call Gemini to generate prompt.\n    const generatingPrompt = await gemini(promptRequest(context, instruction));\n    if (!ok(generatingPrompt)) {\n        return generatingPrompt;\n    }\n    if (\"context\" in generatingPrompt) {\n        return err(\"Invalid output from Gemini -- must be candidates\");\n    }\n    const prompt = generatingPrompt.candidates.at(0)?.content;\n    if (!prompt) {\n        return err(\"No content from prompt generator\");\n    }\n    prompt.role = \"user\";\n    prompt.parts.unshift({\n        text: \"Generate an image based on this prompt. Output image only, no text\",\n    });\n    console.log(\"PROMPT\", toText(prompt));\n    // 3) Call Gemini to generate image.\n    const result = await gemini({\n        model: \"gemini-2.0-flash-exp\",\n        body: {\n            contents: [prompt],\n            generationConfig: {\n                responseModalities: [\"TEXT\", \"IMAGE\"],\n            },\n            safetySettings: defaultSafetySettings(),\n        },\n    });\n    if (!ok(result)) {\n        return result;\n    }\n    if (\"context\" in result) {\n        return err(\"Invalid output from Gemini -- must be candidates\");\n    }\n    const content = result.candidates.at(0)?.content;\n    if (!content) {\n        return err(\"No content\");\n    }\n    return { context: [content] };\n}\nasync function describe({ inputs: { instruction } }) {\n    const template = new Template(instruction);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                instruction: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\"],\n                    title: \"Instruction\",\n                    description: \"Describe how to generate the image based on the input: style, interpretation, etc.\",\n                },\n                ...template.schemas(),\n            },\n            ...template.requireds(),\n            additionalProperties: false,\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                },\n            },\n            additionalProperties: false,\n        },\n        metadata: {\n            icon: \"generative-image\",\n        },\n    };\n}\n",
      "metadata": {
        "title": "Image Generator",
        "source": {
          "code": "/**\n * @fileoverview Generates an image using supplied context.\n */\n\nimport gemini, {\n  defaultSafetySettings,\n  type GeminiOutputs,\n  type GeminiInputs,\n} from \"./gemini\";\nimport { err, ok, toLLMContent, toText } from \"./utils\";\nimport { Template, withParameters } from \"./template\";\nimport { type Params } from \"./common\";\n\ntype ImageGeneratorInputs = {\n  context: LLMContent[];\n  instruction: LLMContent;\n} & Params;\n\ntype ImageGeneratorOutputs = {\n  context: LLMContent[];\n};\n\nexport { invoke as default, describe };\n\nfunction promptRequest(\n  contents: LLMContent[],\n  instruction: LLMContent\n): GeminiInputs {\n  return {\n    model: \"gemini-1.5-flash-latest\",\n    body: {\n      contents,\n      systemInstruction: toLLMContent(`\nYou are a creative writer whose specialty is to write prompts for text-to-image models.\nGiven the conversation context and the additional instructions, you produce the prompt.\n\nThe prompt must describe every object in the image in great detail and describe the style \nin terms of color scheme and vibe.\n\n## Additional instructions:\n\n${toText(instruction)}\n\n`),\n    },\n  };\n}\n\nasync function invoke({\n  context,\n  instruction,\n  ...params\n}: ImageGeneratorInputs): Promise<Outcome<ImageGeneratorOutputs>> {\n  // 0) Substitute params in instruction.\n\n  const substituting = await new Template(instruction).substitute(\n    withParameters(params)\n  );\n  if (!ok(substituting)) {\n    return substituting;\n  }\n  instruction = substituting;\n\n  // 1) Get last LLMContent from input.\n  const contents =\n    context && Array.isArray(context) && context.length > 0\n      ? [context.at(-1)!]\n      : undefined;\n  if (!contents) {\n    return err(\"Must supply context as input\");\n  }\n\n  // 2) Call Gemini to generate prompt.\n  const generatingPrompt = await gemini(promptRequest(context, instruction));\n  if (!ok(generatingPrompt)) {\n    return generatingPrompt;\n  }\n  if (\"context\" in generatingPrompt) {\n    return err(\"Invalid output from Gemini -- must be candidates\");\n  }\n  const prompt = generatingPrompt.candidates.at(0)?.content;\n  if (!prompt) {\n    return err(\"No content from prompt generator\");\n  }\n\n  prompt.role = \"user\";\n  prompt.parts.unshift({\n    text: \"Generate an image based on this prompt. Output image only, no text\",\n  });\n\n  console.log(\"PROMPT\", toText(prompt));\n\n  // 3) Call Gemini to generate image.\n  const result = await gemini({\n    model: \"gemini-2.0-flash-exp\",\n    body: {\n      contents: [prompt],\n      generationConfig: {\n        responseModalities: [\"TEXT\", \"IMAGE\"],\n      },\n      safetySettings: defaultSafetySettings(),\n    },\n  });\n  if (!ok(result)) {\n    return result;\n  }\n  if (\"context\" in result) {\n    return err(\"Invalid output from Gemini -- must be candidates\");\n  }\n\n  const content = result.candidates.at(0)?.content;\n  if (!content) {\n    return err(\"No content\");\n  }\n\n  return { context: [content] };\n}\n\ntype DescribeInputs = {\n  inputs: {\n    instruction?: LLMContent;\n  };\n};\n\nasync function describe({ inputs: { instruction } }: DescribeInputs) {\n  const template = new Template(instruction);\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n        instruction: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\"],\n          title: \"Instruction\",\n          description:\n            \"Describe how to generate the image based on the input: style, interpretation, etc.\",\n        },\n        ...template.schemas(),\n      },\n      ...template.requireds(),\n      additionalProperties: false,\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n        },\n      },\n      additionalProperties: false,\n    } satisfies Schema,\n    metadata: {\n      icon: \"generative-image\",\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Generates an image using supplied context.",
        "runnable": true
      }
    },
    "structured-response": {
      "code": "import {} from \"./gemini\";\nimport { err, toLLMContent, endsWithRole } from \"./utils\";\nexport { StructuredResponse };\nclass StructuredResponse {\n    id;\n    prolog = \"\";\n    epilog = \"\";\n    body = \"\";\n    response = undefined;\n    constructor(id) {\n        this.id = id;\n    }\n    get separator() {\n        return `<sep-${this.id}>`;\n    }\n    addPrompt(c, chat) {\n        const part = { text: this.prompt(chat) };\n        if (endsWithRole(c, \"user\")) {\n            const last = c.at(-1);\n            last.parts.push(part);\n        }\n        else {\n            c.push({ parts: [part], role: \"user\" });\n        }\n        return c;\n    }\n    prompt(chat) {\n        const chatOrConclude = chat\n            ? `Finally, ask the user to provide feedback on your output as a friendly assistant might.`\n            : `Finally, you briefly summarize what the work product was and how it fulfills the task.`;\n        return `\nConsider the conversation context so far and generate a response.\n\nYour response must consist of three parts, separated by the ${this.separator} tag.\n\n- Briefly describe the work product, why it fulfills the specified task,\nand any notes or comments you might have about it\n- Insert the ${this.separator} tag\n- Provide the work product only, without any additional conversation \nor comments about your output\n- Insert the ${this.separator} tag\n${chatOrConclude}\n`;\n    }\n    parse(response) {\n        const r = response;\n        const part = r.candidates?.at(0)?.content?.parts?.at(0);\n        if (!part || !(\"text\" in part)) {\n            return err(\"No text in part\");\n        }\n        this.response = r.candidates.at(0).content;\n        const structure = part.text.split(this.separator);\n        if (structure.length !== 3) {\n            return err(\"The output must contain 3 parts\");\n        }\n        this.prolog = structure[0];\n        this.body = structure[1].trim();\n        this.epilog = structure[2].trim();\n    }\n}\n",
      "metadata": {
        "title": "structured-response",
        "source": {
          "code": "import { type GeminiOutputs, type GeminiAPIOutputs } from \"./gemini\";\nimport { err, toLLMContent, endsWithRole } from \"./utils\";\n\nexport { StructuredResponse };\n\nclass StructuredResponse {\n  public prolog: string = \"\";\n  public epilog: string = \"\";\n  public body: string = \"\";\n  response: LLMContent | undefined = undefined;\n\n  constructor(public readonly id: string) {}\n\n  get separator() {\n    return `<sep-${this.id}>`;\n  }\n\n  addPrompt(c: LLMContent[], chat: boolean): LLMContent[] {\n    const part = { text: this.prompt(chat) };\n    if (endsWithRole(c, \"user\")) {\n      const last = c.at(-1)!;\n      last.parts.push(part);\n    } else {\n      c.push({ parts: [part], role: \"user\" });\n    }\n    return c;\n  }\n\n  prompt(chat: boolean): string {\n    const chatOrConclude = chat\n      ? `Finally, ask the user to provide feedback on your output as a friendly assistant might.`\n      : `Finally, you briefly summarize what the work product was and how it fulfills the task.`;\n\n    return `\nConsider the conversation context so far and generate a response.\n\nYour response must consist of three parts, separated by the ${this.separator} tag.\n\n- Briefly describe the work product, why it fulfills the specified task,\nand any notes or comments you might have about it\n- Insert the ${this.separator} tag\n- Provide the work product only, without any additional conversation \nor comments about your output\n- Insert the ${this.separator} tag\n${chatOrConclude}\n`;\n  }\n\n  parse(response: GeminiOutputs): Outcome<void> {\n    const r = response as GeminiAPIOutputs;\n    const part = r.candidates?.at(0)?.content?.parts?.at(0);\n    if (!part || !(\"text\" in part)) {\n      return err(\"No text in part\");\n    }\n    this.response = r.candidates.at(0)!.content!;\n    const structure = part.text.split(this.separator);\n    if (structure.length !== 3) {\n      return err(\"The output must contain 3 parts\");\n    }\n    this.prolog = structure[0];\n    this.body = structure[1].trim();\n    this.epilog = structure[2].trim();\n  }\n}\n",
          "language": "typescript"
        },
        "description": "",
        "runnable": false
      }
    },
    "template": {
      "code": "/**\n * @fileoverview Handles templated content\n */\nexport { invoke as default, describe, Template, withParameters };\nimport {} from \"./common\";\nimport { ok, isLLMContent, isLLMContentArray } from \"./utils\";\nimport readFile from \"@read\";\nfunction unique(params) {\n    return Array.from(new Set(params));\n}\nfunction isTool(param) {\n    return param.param === \"tool\" && param.op === \"url\" && !!param.arg;\n}\nfunction isIn(param) {\n    return param.param === \"in\" && param.op === \"id\" && !!param.arg;\n}\nfunction isAsset(param) {\n    return param.param === \"asset\" && param.op === \"path\" && !!param.arg;\n}\nfunction withParameters(params) {\n    return async function (param) {\n        if (isIn(param)) {\n            const { param: type, op, arg: name } = param;\n            const paramName = `p-z-${name}`;\n            if (paramName in params) {\n                return params[paramName];\n            }\n            return name;\n        }\n        else if (isAsset(param)) {\n            const path = `/assets/${param.arg}`;\n            const reading = await readFile({ path });\n            if (!ok(reading)) {\n                return null;\n            }\n            return reading.data;\n        }\n        return null;\n    };\n}\nclass Template {\n    template;\n    #parts;\n    #role;\n    constructor(template) {\n        this.template = template;\n        if (!template) {\n            this.#role = \"user\";\n            this.#parts = [];\n            return;\n        }\n        this.#parts = this.#splitToTemplateParts(template);\n        this.#role = template.role;\n    }\n    #mergeTextParts(parts) {\n        const merged = [];\n        for (const part of parts) {\n            if (\"text\" in part) {\n                const last = merged[merged.length - 1];\n                if (last && \"text\" in last) {\n                    last.text += part.text;\n                }\n                else {\n                    merged.push(part);\n                }\n            }\n            else {\n                merged.push(part);\n            }\n        }\n        return merged;\n    }\n    /**\n     * Takes an LLM Content and splits it further into parts where\n     * each {{param}} substitution is a separate part.\n     */\n    #splitToTemplateParts(content) {\n        const parts = [];\n        for (const part of content.parts) {\n            if (!(\"text\" in part)) {\n                parts.push(part);\n                continue;\n            }\n            const matches = part.text.matchAll(/{{\\s*(?<name>[\\w-]+)(?:\\s*\\|\\s*(?<op>[\\w-]*)(?::\\s*\"(?<arg>[\\w-]+)\")?)?\\s*}}/g);\n            let start = 0;\n            for (const match of matches) {\n                const name = match.groups?.name || \"\";\n                const op = match.groups?.op;\n                const arg = match.groups?.arg;\n                const end = match.index;\n                if (end > start) {\n                    parts.push({ text: part.text.slice(start, end) });\n                }\n                const maybeParamPart = { param: name, op, arg };\n                if (isAsset(maybeParamPart) ||\n                    isTool(maybeParamPart) ||\n                    isIn(maybeParamPart)) {\n                    parts.push(maybeParamPart);\n                }\n                start = end + match[0].length;\n            }\n            if (start < part.text.length) {\n                parts.push({ text: part.text.slice(start) });\n            }\n        }\n        return parts;\n    }\n    #getLastNonMetadata(value) {\n        const content = value;\n        for (let i = content.length - 1; i >= 0; i--) {\n            if (content[i].role !== \"$metadata\") {\n                return content[i];\n            }\n        }\n        return null;\n    }\n    async substitute(replacer) {\n        const replaced = [];\n        for (const part of this.#parts) {\n            if (\"param\" in part) {\n                const value = await replacer(part);\n                if (value === null) {\n                    // Ignore if null.\n                    continue;\n                }\n                else if (typeof value === \"string\") {\n                    replaced.push({ text: value });\n                }\n                else if (isLLMContent(value)) {\n                    replaced.push(...value.parts);\n                }\n                else if (isLLMContentArray(value)) {\n                    const last = this.#getLastNonMetadata(value);\n                    if (last) {\n                        replaced.push(...last.parts);\n                    }\n                }\n                else {\n                    replaced.push({ text: JSON.stringify(value) });\n                }\n            }\n            else {\n                replaced.push(part);\n            }\n        }\n        const parts = this.#mergeTextParts(replaced);\n        return { parts, role: this.#role };\n    }\n    #toId(param) {\n        return `p-z-${param}`;\n    }\n    #toTitle(id) {\n        const spaced = id?.replace(/[_-]/g, \" \");\n        return ((spaced?.at(0)?.toUpperCase() ?? \"\") +\n            (spaced?.slice(1)?.toLowerCase() ?? \"\"));\n    }\n    #forEachParam(handler) {\n        for (const part of this.#parts) {\n            if (\"param\" in part) {\n                handler(part);\n            }\n        }\n    }\n    requireds() {\n        const required = [];\n        let hasValues = false;\n        this.#forEachParam((param) => {\n            if (!isIn(param))\n                return;\n            hasValues = true;\n            required.push(this.#toId(param.arg));\n        });\n        return hasValues ? { required } : {};\n    }\n    schemas() {\n        const result = [];\n        this.#forEachParam((param) => {\n            const name = param.arg;\n            if (!isIn(param))\n                return;\n            result.push([\n                this.#toId(name),\n                {\n                    title: this.#toTitle(name),\n                    description: `The value to substitute for the parameter \"${name}\"`,\n                    type: \"string\",\n                },\n            ]);\n        });\n        return Object.fromEntries(result);\n    }\n}\n/**\n * API for test harness\n */\nfunction fromTestParams(params) {\n    return Object.fromEntries(Object.entries(params).map(([key, value]) => {\n        return [`p-z-${key}`, value];\n    }));\n}\n/**\n * Only used for testing.\n */\nasync function invoke({ inputs: { content, params }, }) {\n    const template = new Template(content);\n    const result = await template.substitute(withParameters(fromTestParams(params)));\n    if (!ok(result)) {\n        return result;\n    }\n    return { outputs: result };\n}\n/**\n * Only used for testing.\n */\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: { inputs: { type: \"object\", title: \"Test inputs\" } },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: { outputs: { type: \"object\", title: \"Test outputs\" } },\n        },\n    };\n}\n",
      "metadata": {
        "title": "template",
        "source": {
          "code": "/**\n * @fileoverview Handles templated content\n */\n\nexport { invoke as default, describe, Template, withParameters };\n\nimport { type Params } from \"./common\";\nimport { ok, isLLMContent, isLLMContentArray } from \"./utils\";\nimport readFile from \"@read\";\n\ntype LLMContentWithMetadata = LLMContent & {\n  $metadata: unknown;\n};\n\nexport type Requireds = {\n  required?: Schema[\"required\"];\n};\n\nexport type TemplateReplacer = (param: ParamPart) => Promise<unknown>;\n\ntype Location = {\n  part: LLMContent[\"parts\"][0];\n  parts: LLMContent[\"parts\"];\n};\n\nexport type InParamPart = {\n  param: \"in\";\n  op: \"id\";\n  arg: string;\n};\n\nexport type ToolParamPart = {\n  param: \"tool\";\n  op: \"url\";\n  arg: string;\n};\n\nexport type AssetParamPart = {\n  param: \"asset\";\n  op: \"path\";\n  arg: string;\n};\n\nexport type ParamPart = InParamPart | ToolParamPart | AssetParamPart;\n\nexport type TemplatePart = DataPart | ParamPart;\n\nexport type ParamLocationMap = Record<string, Location[]>;\n\nexport type Operation = string;\n\nexport type ParamInfo = {\n  name: string;\n  locations: Location[];\n  op?: Operation;\n  arg?: string;\n};\n\nfunction unique<T>(params: T[]): T[] {\n  return Array.from(new Set(params));\n}\n\nfunction isTool(param: ParamPart): param is ToolParamPart {\n  return param.param === \"tool\" && param.op === \"url\" && !!param.arg;\n}\n\nfunction isIn(param: ParamPart): param is InParamPart {\n  return param.param === \"in\" && param.op === \"id\" && !!param.arg;\n}\n\nfunction isAsset(param: ParamPart): param is AssetParamPart {\n  return param.param === \"asset\" && param.op === \"path\" && !!param.arg;\n}\n\nfunction withParameters(params: Params) {\n  return async function (param: ParamPart) {\n    if (isIn(param)) {\n      const { param: type, op, arg: name } = param;\n      const paramName: `p-z-${string}` = `p-z-${name}`;\n      if (paramName in params) {\n        return params[paramName];\n      }\n      return name;\n    } else if (isAsset(param)) {\n      const path: FileSystemPath = `/assets/${param.arg}`;\n      const reading = await readFile({ path });\n      if (!ok(reading)) {\n        return null;\n      }\n      return reading.data;\n    }\n    return null;\n  };\n}\n\nclass Template {\n  #parts: TemplatePart[];\n  #role: LLMContent[\"role\"];\n\n  constructor(public readonly template: LLMContent | undefined) {\n    if (!template) {\n      this.#role = \"user\";\n      this.#parts = [];\n      return;\n    }\n    this.#parts = this.#splitToTemplateParts(template);\n    this.#role = template.role;\n  }\n\n  #mergeTextParts(parts: TemplatePart[]) {\n    const merged = [];\n    for (const part of parts) {\n      if (\"text\" in part) {\n        const last = merged[merged.length - 1];\n        if (last && \"text\" in last) {\n          last.text += part.text;\n        } else {\n          merged.push(part);\n        }\n      } else {\n        merged.push(part);\n      }\n    }\n    return merged as DataPart[];\n  }\n\n  /**\n   * Takes an LLM Content and splits it further into parts where\n   * each {{param}} substitution is a separate part.\n   */\n  #splitToTemplateParts(content: LLMContent): TemplatePart[] {\n    const parts: TemplatePart[] = [];\n    for (const part of content.parts) {\n      if (!(\"text\" in part)) {\n        parts.push(part);\n        continue;\n      }\n      const matches = part.text.matchAll(\n        /{{\\s*(?<name>[\\w-]+)(?:\\s*\\|\\s*(?<op>[\\w-]*)(?::\\s*\"(?<arg>[\\w-]+)\")?)?\\s*}}/g\n      );\n      let start = 0;\n      for (const match of matches) {\n        const name = match.groups?.name || \"\";\n        const op = match.groups?.op;\n        const arg = match.groups?.arg;\n        const end = match.index;\n        if (end > start) {\n          parts.push({ text: part.text.slice(start, end) });\n        }\n        const maybeParamPart = { param: name, op, arg } as ParamPart;\n        if (\n          isAsset(maybeParamPart) ||\n          isTool(maybeParamPart) ||\n          isIn(maybeParamPart)\n        ) {\n          parts.push(maybeParamPart);\n        }\n        start = end + match[0].length;\n      }\n      if (start < part.text.length) {\n        parts.push({ text: part.text.slice(start) });\n      }\n    }\n    return parts;\n  }\n\n  #getLastNonMetadata(value: LLMContent[]): LLMContent | null {\n    const content = value as LLMContentWithMetadata[];\n    for (let i = content.length - 1; i >= 0; i--) {\n      if (content[i].role !== \"$metadata\") {\n        return content[i] as LLMContent;\n      }\n    }\n    return null;\n  }\n\n  async substitute(replacer: TemplateReplacer): Promise<Outcome<LLMContent>> {\n    const replaced: DataPart[] = [];\n    for (const part of this.#parts) {\n      if (\"param\" in part) {\n        const value = await replacer(part);\n        if (value === null) {\n          // Ignore if null.\n          continue;\n        } else if (typeof value === \"string\") {\n          replaced.push({ text: value });\n        } else if (isLLMContent(value)) {\n          replaced.push(...value.parts);\n        } else if (isLLMContentArray(value)) {\n          const last = this.#getLastNonMetadata(value);\n          if (last) {\n            replaced.push(...last.parts);\n          }\n        } else {\n          replaced.push({ text: JSON.stringify(value) });\n        }\n      } else {\n        replaced.push(part);\n      }\n    }\n    const parts = this.#mergeTextParts(replaced);\n    return { parts, role: this.#role };\n  }\n\n  #toId(param: string) {\n    return `p-z-${param}`;\n  }\n\n  #toTitle(id: string) {\n    const spaced = id?.replace(/[_-]/g, \" \");\n    return (\n      (spaced?.at(0)?.toUpperCase() ?? \"\") +\n      (spaced?.slice(1)?.toLowerCase() ?? \"\")\n    );\n  }\n\n  #forEachParam(handler: (param: ParamPart) => void) {\n    for (const part of this.#parts) {\n      if (\"param\" in part) {\n        handler(part);\n      }\n    }\n  }\n\n  requireds(): Requireds {\n    const required: string[] = [];\n    let hasValues = false;\n    this.#forEachParam((param) => {\n      if (!isIn(param)) return;\n      hasValues = true;\n      required.push(this.#toId(param.arg!));\n    });\n    return hasValues ? { required } : {};\n  }\n\n  schemas(): Record<string, Schema> {\n    const result: [string, Schema][] = [];\n    this.#forEachParam((param) => {\n      const name = param.arg!;\n      if (!isIn(param)) return;\n      result.push([\n        this.#toId(name),\n        {\n          title: this.#toTitle(name),\n          description: `The value to substitute for the parameter \"${name}\"`,\n          type: \"string\",\n        },\n      ]);\n    });\n    return Object.fromEntries(result);\n  }\n}\n\n/**\n * API for test harness\n */\n\nfunction fromTestParams(params: Record<string, string>): Params {\n  return Object.fromEntries(\n    Object.entries(params).map(([key, value]) => {\n      return [`p-z-${key}`, value];\n    })\n  );\n}\n\ntype TestInputs = {\n  inputs: { content: LLMContent; params: Record<string, string> };\n};\n\ntype TestOutputs = {\n  outputs: LLMContent;\n};\n\n/**\n * Only used for testing.\n */\nasync function invoke({\n  inputs: { content, params },\n}: TestInputs): Promise<Outcome<TestOutputs>> {\n  const template = new Template(content);\n  const result = await template.substitute(\n    withParameters(fromTestParams(params))\n  );\n  if (!ok(result)) {\n    return result;\n  }\n  return { outputs: result };\n}\n\n/**\n * Only used for testing.\n */\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: { inputs: { type: \"object\", title: \"Test inputs\" } },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: { outputs: { type: \"object\", title: \"Test outputs\" } },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Handles templated content",
        "runnable": true
      }
    },
    "audio-generator": {
      "code": "/**\n * @fileoverview Generates audio output using supplied context.\n */\nimport gemini, { defaultSafetySettings, } from \"./gemini\";\nimport { err, ok, toLLMContent, toText } from \"./utils\";\nexport { invoke as default, describe };\nasync function invoke({ context, }) {\n    // 1) Get last LLMContent from input.\n    const prompt = context && Array.isArray(context) && context.length > 0\n        ? context.at(-1)\n        : undefined;\n    if (!prompt) {\n        return err(\"Must supply context as input\");\n    }\n    prompt.role = \"user\";\n    // 2) Call Gemini to generate audio.\n    const result = await gemini({\n        model: \"gemini-2.0-flash-exp\",\n        body: {\n            contents: [prompt],\n            generationConfig: {\n                responseModalities: [\"AUDIO\"],\n            },\n            safetySettings: defaultSafetySettings(),\n        },\n    });\n    if (!ok(result)) {\n        return result;\n    }\n    if (\"context\" in result) {\n        return err(\"Invalid output from Gemini -- must be candidates\");\n    }\n    const content = result.candidates.at(0)?.content;\n    if (!content) {\n        return err(\"No content\");\n    }\n    return { context: [content] };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n            },\n            additionalProperties: false,\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                },\n            },\n            additionalProperties: false,\n        },\n        metadata: {\n            icon: \"generative-audio\",\n        },\n    };\n}\n",
      "metadata": {
        "title": "Audio Generator",
        "source": {
          "code": "/**\n * @fileoverview Generates audio output using supplied context.\n */\n\nimport gemini, {\n  defaultSafetySettings,\n  type GeminiOutputs,\n  type GeminiInputs,\n} from \"./gemini\";\nimport { err, ok, toLLMContent, toText } from \"./utils\";\n\ntype AudioGeneratorInputs = {\n  context: LLMContent[];\n};\n\ntype AudioGeneratorOutputs = {\n  context: LLMContent[];\n};\n\nexport { invoke as default, describe };\n\nasync function invoke({\n  context,\n}: AudioGeneratorInputs): Promise<Outcome<AudioGeneratorOutputs>> {\n  // 1) Get last LLMContent from input.\n  const prompt =\n    context && Array.isArray(context) && context.length > 0\n      ? context.at(-1)!\n      : undefined;\n  if (!prompt) {\n    return err(\"Must supply context as input\");\n  }\n  prompt.role = \"user\";\n\n  // 2) Call Gemini to generate audio.\n  const result = await gemini({\n    model: \"gemini-2.0-flash-exp\",\n    body: {\n      contents: [prompt],\n      generationConfig: {\n        responseModalities: [\"AUDIO\"],\n      },\n      safetySettings: defaultSafetySettings(),\n    },\n  });\n  if (!ok(result)) {\n    return result;\n  }\n  if (\"context\" in result) {\n    return err(\"Invalid output from Gemini -- must be candidates\");\n  }\n\n  const content = result.candidates.at(0)?.content;\n  if (!content) {\n    return err(\"No content\");\n  }\n\n  return { context: [content] };\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n      },\n      additionalProperties: false,\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n        },\n      },\n      additionalProperties: false,\n    } satisfies Schema,\n    metadata: {\n      icon: \"generative-audio\",\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Generates audio output using supplied context.",
        "runnable": true
      }
    },
    "text-entry": {
      "code": "/**\n * @fileoverview The entry point and a describer for the \"Text\" component.\n */\nimport { ok, defaultLLMContent, toText } from \"./utils\";\nimport {} from \"./common\";\nimport { Template, withParameters } from \"./template\";\nexport { invoke as default, describe };\nfunction toInput(title = \"Please enter text\") {\n    const toInput = {\n        type: \"object\",\n        properties: {\n            request: {\n                type: \"object\",\n                title,\n                behavior: [\"transient\", \"llm-content\"],\n                examples: [defaultLLMContent()],\n            },\n        },\n    };\n    return toInput;\n}\nasync function invoke({ context, text, description, ...params }) {\n    const haveText = text && toText(text).length > 0;\n    if (haveText) {\n        const template = new Template(text);\n        const substituting = await template.substitute(withParameters(params));\n        if (!ok(substituting)) {\n            return substituting;\n        }\n        return { context: substituting, toMain: \"haveInput\" };\n    }\n    const title = description ? toText(description) : undefined;\n    return { context: \"nothing\", toInput: toInput(title) };\n}\nasync function describe({ inputs: { text } }) {\n    const template = new Template(text);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                description: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\"],\n                    title: \"Description\",\n                    description: \"Optionally provide a description to show to the user when run.\",\n                },\n                text: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\"],\n                    title: \"Text Contents\",\n                    description: \"Put your text here\",\n                },\n                ...template.schemas(),\n            },\n            ...template.requireds(),\n            additionalProperties: false,\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"main-port\"],\n                },\n            },\n            additionalProperties: false,\n        },\n    };\n}\n",
      "metadata": {
        "title": "Text",
        "source": {
          "code": "/**\n * @fileoverview The entry point and a describer for the \"Text\" component.\n */\nimport { ok, defaultLLMContent, toText } from \"./utils\";\nimport { type Params } from \"./common\";\nimport { Template, withParameters } from \"./template\";\n\nexport { invoke as default, describe };\n\ntype TextInputs = {\n  context?: LLMContent[];\n  text?: LLMContent;\n  description?: LLMContent;\n} & Params;\n\ntype TextOutputs =\n  | {\n      toInput: Schema;\n      context: \"nothing\";\n    }\n  | {\n      toMain: string;\n      context: LLMContent;\n    };\n\nfunction toInput(title: string = \"Please enter text\") {\n  const toInput: Schema = {\n    type: \"object\",\n    properties: {\n      request: {\n        type: \"object\",\n        title,\n        behavior: [\"transient\", \"llm-content\"],\n        examples: [defaultLLMContent()],\n      },\n    },\n  };\n  return toInput;\n}\n\nasync function invoke({\n  context,\n  text,\n  description,\n  ...params\n}: TextInputs): Promise<Outcome<TextOutputs>> {\n  const haveText = text && toText(text).length > 0;\n  if (haveText) {\n    const template = new Template(text);\n    const substituting = await template.substitute(withParameters(params));\n    if (!ok(substituting)) {\n      return substituting;\n    }\n    return { context: substituting, toMain: \"haveInput\" };\n  }\n  const title = description ? toText(description) : undefined;\n  return { context: \"nothing\", toInput: toInput(title) };\n}\n\ntype DescribeInputs = {\n  inputs: {\n    text?: LLMContent;\n  };\n};\n\nasync function describe({ inputs: { text } }: DescribeInputs) {\n  const template = new Template(text);\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n        description: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\"],\n          title: \"Description\",\n          description:\n            \"Optionally provide a description to show to the user when run.\",\n        },\n        text: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\"],\n          title: \"Text Contents\",\n          description: \"Put your text here\",\n        },\n        ...template.schemas(),\n      },\n      ...template.requireds(),\n      additionalProperties: false,\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n          behavior: [\"main-port\"],\n        },\n      },\n      additionalProperties: false,\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "The entry point and a describer for the \"Text\" component.",
        "runnable": true
      }
    },
    "text-main": {
      "code": "/**\n * @fileoverview Add a description for your module here.\n */\nimport { err } from \"./utils\";\nexport { invoke as default, describe };\nasync function invoke({ context, request, }) {\n    if (context == \"nothing\") {\n        if (!request) {\n            return err(`No text supplied.`);\n        }\n        return { context: [request] };\n    }\n    return { context: [context] };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"object\",\n                    title: \"Context in\",\n                },\n                request: {\n                    type: \"object\",\n                    title: \"Data From Input\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "text-main",
        "source": {
          "code": "/**\n * @fileoverview Add a description for your module here.\n */\n\nimport { err } from \"./utils\";\n\nexport { invoke as default, describe };\n\ntype TextMainInputs = {\n  context: LLMContent | \"nothing\";\n  request?: LLMContent;\n};\n\ntype TextMainOutputs = {\n  context: LLMContent[];\n};\n\nasync function invoke({\n  context,\n  request,\n}: TextMainInputs): Promise<Outcome<TextMainOutputs>> {\n  if (context == \"nothing\") {\n    if (!request) {\n      return err(`No text supplied.`);\n    }\n    return { context: [request] };\n  }\n  return { context: [context] };\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"object\",\n          title: \"Context in\",\n        },\n        request: {\n          type: \"object\",\n          title: \"Data From Input\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Add a description for your module here.",
        "runnable": true
      }
    }
  },
  "exports": [
    "#daf082ca-c1aa-4aff-b2c8-abeb984ab66c",
    "#module:researcher",
    "#module:image-generator",
    "#module:audio-generator",
    "#21ee02e7-83fa-49d0-964c-0cab10eafc2c"
  ],
  "graphs": {
    "daf082ca-c1aa-4aff-b2c8-abeb984ab66c": {
      "title": "Text Generator",
      "description": "Generates text and so much more.",
      "version": "0.0.1",
      "describer": "module:entry",
      "nodes": [
        {
          "type": "output",
          "id": "output",
          "configuration": {
            "schema": {
              "properties": {
                "context": {
                  "type": "array",
                  "title": "Context",
                  "items": {
                    "type": "object",
                    "behavior": [
                      "llm-content"
                    ]
                  },
                  "default": "null"
                }
              },
              "type": "object",
              "required": []
            }
          },
          "metadata": {
            "visual": {
              "x": 707.9999999999985,
              "y": 44,
              "collapsed": "expanded",
              "outputHeight": 44
            }
          }
        },
        {
          "id": "board-f138aa03",
          "type": "#module:entry",
          "metadata": {
            "visual": {
              "x": -46.99999999999966,
              "y": -72.00000000000045,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "entry"
          }
        },
        {
          "id": "board-d340ad8f",
          "type": "#module:agent-main",
          "metadata": {
            "visual": {
              "x": 331.9999999999998,
              "y": -6.000000000000796,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "agent-main"
          }
        },
        {
          "id": "board-1946064a",
          "type": "#module:join",
          "metadata": {
            "visual": {
              "x": 1059.9999999999986,
              "y": -160.00000000000045,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "join"
          }
        },
        {
          "type": "input",
          "id": "input",
          "metadata": {
            "visual": {
              "x": 712.9999999999997,
              "y": 169.99999999999864,
              "collapsed": "advanced",
              "outputHeight": 44
            },
            "title": "input"
          }
        }
      ],
      "edges": [
        {
          "from": "board-f138aa03",
          "to": "board-d340ad8f",
          "out": "context",
          "in": "context"
        },
        {
          "from": "board-d340ad8f",
          "to": "output",
          "out": "done",
          "in": "context"
        },
        {
          "from": "input",
          "to": "board-1946064a",
          "out": "request",
          "in": "request"
        },
        {
          "from": "board-d340ad8f",
          "to": "input",
          "out": "toInput",
          "in": "schema"
        },
        {
          "from": "board-d340ad8f",
          "to": "board-1946064a",
          "out": "context",
          "in": "context"
        },
        {
          "from": "board-1946064a",
          "to": "board-d340ad8f",
          "out": "context",
          "in": "context"
        }
      ],
      "metadata": {
        "visual": {
          "minimized": false
        },
        "describer": "module:entry",
        "tags": []
      }
    },
    "21ee02e7-83fa-49d0-964c-0cab10eafc2c": {
      "title": "Text",
      "description": "A block of text as input or output",
      "version": "0.0.1",
      "nodes": [
        {
          "type": "input",
          "id": "input",
          "metadata": {
            "visual": {
              "x": 574.0969239608205,
              "y": -533.1431851509614,
              "collapsed": "expanded",
              "outputHeight": 44
            }
          }
        },
        {
          "type": "output",
          "id": "output",
          "configuration": {
            "schema": {
              "properties": {
                "context": {
                  "type": "array",
                  "title": "Context",
                  "items": {
                    "type": "object",
                    "behavior": [
                      "llm-content"
                    ]
                  },
                  "default": "null"
                }
              },
              "type": "object",
              "required": []
            }
          },
          "metadata": {
            "visual": {
              "x": 1234.0969239608205,
              "y": -393.1431851509614,
              "collapsed": "expanded",
              "outputHeight": 44
            }
          }
        },
        {
          "id": "board-64b2c3a8",
          "type": "#module:text-entry",
          "metadata": {
            "visual": {
              "x": 214.09692396082045,
              "y": -633.1431851509615,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "text-entry"
          }
        },
        {
          "id": "board-95a57400",
          "type": "#module:text-main",
          "metadata": {
            "visual": {
              "x": 894.0969239608205,
              "y": -453.1431851509615,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "text-main"
          }
        }
      ],
      "edges": [
        {
          "from": "board-64b2c3a8",
          "out": "toInput",
          "to": "input",
          "in": "schema"
        },
        {
          "from": "board-64b2c3a8",
          "out": "toMain",
          "to": "board-95a57400",
          "in": "request"
        },
        {
          "from": "board-95a57400",
          "to": "output",
          "out": "context",
          "in": "context"
        },
        {
          "from": "board-64b2c3a8",
          "to": "board-95a57400",
          "out": "context",
          "in": "context"
        },
        {
          "from": "input",
          "to": "board-95a57400",
          "out": "request",
          "in": "request"
        }
      ],
      "metadata": {
        "visual": {
          "minimized": false
        },
        "tags": [],
        "describer": "module:text-entry"
      }
    }
  }
}