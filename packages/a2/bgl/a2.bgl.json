{
  "title": "A2",
  "description": "Components that help you build flows.",
  "version": "0.0.1",
  "nodes": [],
  "edges": [],
  "metadata": {
    "comments": [
      {
        "id": "comment-b09617ef",
        "text": "Left Intentionally Blank",
        "metadata": {
          "visual": {
            "x": -37.90624999999966,
            "y": -415.8554687499999,
            "collapsed": "expanded",
            "outputHeight": 0
          }
        }
      }
    ],
    "visual": {
      "presentation": {
        "themes": {
          "54f81cc4-5c04-4d9d-b831-985d556f0ed9": {
            "themeColors": {
              "primaryColor": "#246db5",
              "secondaryColor": "#5cadff",
              "backgroundColor": "#ffffff",
              "textColor": "#1a1a1a",
              "primaryTextColor": "#ffffff"
            },
            "template": "basic",
            "splashScreen": {
              "storedData": {
                "handle": "/images/app/generic-flow.jpg",
                "mimeType": "image/jpeg"
              }
            }
          }
        },
        "theme": "54f81cc4-5c04-4d9d-b831-985d556f0ed9"
      }
    },
    "tags": [
      "published",
      "tool",
      "component"
    ],
    "icon": "text"
  },
  "modules": {
    "common": {
      "code": "/**\n * @fileoverview Common types and code\n */\n",
      "metadata": {
        "title": "common",
        "source": {
          "code": "/**\n * @fileoverview Common types and code\n */\n\nexport type UserInput = LLMContent;\n\nexport type Params = {\n  [key: `p-z-${string}`]: unknown;\n};\n\nexport type DescriberResult = {\n  title?: string;\n  description?: string;\n  inputSchema?: Schema;\n  outputSchema?: Schema;\n};\n\nexport type DescriberResultTransformer = {\n  /**\n   * Returns a transformed result, a `null` if no transformation happened,\n   * or an error as Outcome.\n   */\n  transform(result: DescriberResult): Promise<Outcome<DescriberResult | null>>;\n};\n\nexport type AgentInputs = {\n  /**\n   * Whether (true) or not (false) the agent is allowed to chat with user.\n   */\n  chat: boolean;\n  /**\n   * Whether (true) or not (false) to try to turn the output into a list\n   */\n  makeList: boolean;\n  /**\n   * The incoming conversation context.\n   */\n  context: LLMContent[];\n  /**\n   * Accumulated work context. This is the internal conversation, a result\n   * of talking with the user, for instance.\n   * This context is discarded at the end of interacting with the agent.\n   */\n  work: LLMContent[];\n  /**\n   * The index path to the currently processed list.\n   */\n  listPath: number[];\n  /**\n   * Agent's job description.\n   */\n  description?: LLMContent;\n  /**\n   * Last work product.\n   */\n  last?: LLMContent;\n  /**\n   * Type of the task.\n   */\n  type: \"introduction\" | \"work\";\n  /**\n   * The board URL of the model\n   */\n  model: string;\n  /**\n   * The default model that is passed along by the manager\n   */\n  defaultModel: string;\n  /**\n   * The tools that the worker can use\n   */\n  tools?: string[];\n  /**\n   * params\n   */\n  params: Params;\n};\n\nexport type AgentContext = AgentInputs & {\n  /**\n   * A unique identifier for the session.\n   * Currently used to have a persistent part separator across conversation context\n   */\n  id: string;\n  /**\n   * Accumulating list of user inputs\n   */\n  userInputs: UserInput[];\n  /**\n   * Indicator that the user ended chat.\n   */\n  userEndedChat: boolean;\n};\n\nexport type DescribeInputs = {\n  inputs: {\n    description: LLMContent | undefined;\n  };\n};\n",
          "language": "typescript"
        },
        "description": "Common types and code",
        "runnable": false
      }
    },
    "utils": {
      "code": "/**\n * @fileoverview Common utils for manipulating LLM Content and other relevant types.\n */\nexport { isLLMContent, isLLMContentArray, toLLMContent, toInlineData, toLLMContentInline, toText, contentToJSON, defaultLLMContent, endsWithRole, addUserTurn, isEmpty, llm, ok, err, generateId, mergeTextParts, };\nfunction ok(o) {\n    return !(o && typeof o === \"object\" && \"$error\" in o);\n}\nfunction err($error) {\n    return { $error };\n}\nfunction mergeTextParts(parts) {\n    const merged = [];\n    let text = \"\";\n    for (const part of parts) {\n        if (\"text\" in part) {\n            text += part.text;\n        }\n        else {\n            if (text) {\n                merged.push({ text });\n            }\n            text = \"\";\n            merged.push(part);\n        }\n    }\n    if (text) {\n        merged.push({ text });\n    }\n    return merged;\n}\nclass LLMTemplate {\n    strings;\n    values;\n    constructor(strings, values) {\n        this.strings = strings;\n        this.values = values;\n    }\n    asParts() {\n        return mergeTextParts(this.strings.flatMap((s, i) => {\n            let text = s;\n            const value = this.values.at(i);\n            if (value == undefined) {\n                return { text };\n            }\n            else if (typeof value === \"string\") {\n                text += value;\n                return { text };\n            }\n            else if (value instanceof LLMTemplate) {\n                return value.asParts();\n            }\n            else if (isLLMContent(value)) {\n                return [{ text }, ...value.parts];\n            }\n            else {\n                text += JSON.stringify(value);\n                return { text };\n            }\n        }));\n    }\n    asContent() {\n        const parts = this.asParts();\n        return { parts, role: \"user\" };\n    }\n}\nfunction llm(strings, ...values) {\n    return new LLMTemplate(strings, values);\n}\n/**\n * Copied from @google-labs/breadboard\n */\nfunction isLLMContent(nodeValue) {\n    if (typeof nodeValue !== \"object\" || !nodeValue)\n        return false;\n    if (nodeValue === null || nodeValue === undefined)\n        return false;\n    if (\"role\" in nodeValue && nodeValue.role === \"$metadata\") {\n        return true;\n    }\n    return \"parts\" in nodeValue && Array.isArray(nodeValue.parts);\n}\nfunction isLLMContentArray(nodeValue) {\n    if (!Array.isArray(nodeValue))\n        return false;\n    if (nodeValue.length === 0)\n        return true;\n    return isLLMContent(nodeValue.at(-1));\n}\nfunction toLLMContent(text, role = \"user\") {\n    return { parts: [{ text }], role };\n}\nfunction endsWithRole(c, role) {\n    const last = c.at(-1);\n    if (!last)\n        return false;\n    return last.role === role;\n}\nfunction isEmpty(c) {\n    if (!c.parts.length)\n        return true;\n    for (const part of c.parts) {\n        if (\"text\" in part) {\n            if (part.text.trim().length > 0)\n                return false;\n        }\n        else {\n            return false;\n        }\n    }\n    return true;\n}\nfunction toText(c) {\n    if (isLLMContent(c)) {\n        return contentToText(c);\n    }\n    const last = c.at(-1);\n    if (!last)\n        return \"\";\n    return contentToText(last).trim();\n    function contentToText(content) {\n        return content.parts\n            .map((part) => (\"text\" in part ? part.text : \"\"))\n            .join(\"\\n\\n\");\n    }\n}\nfunction contentToJSON(content) {\n    const part = content?.parts?.at(0);\n    if (!part || !(\"text\" in part)) {\n        throw new Error(\"Invalid response from Gemini\");\n    }\n    return JSON.parse(part.text);\n}\nfunction defaultLLMContent() {\n    return JSON.stringify({\n        parts: [{ text: \"\" }],\n        role: \"user\",\n    });\n}\nfunction addUserTurn(content, context) {\n    context ??= [];\n    const isString = typeof content === \"string\";\n    if (!endsWithRole(context, \"user\")) {\n        return [...context, isString ? toLLMContent(content) : content];\n    }\n    const last = context.at(-1);\n    if (isString) {\n        last.parts.push({ text: content });\n    }\n    else {\n        last.parts.push(...content.parts);\n    }\n    return context;\n}\nfunction toLLMContentInline(mimetype, value, role = \"user\") {\n    return {\n        parts: [\n            {\n                inlineData: {\n                    mimeType: mimetype,\n                    data: value,\n                },\n            },\n        ],\n        role,\n    };\n}\nfunction toInlineData(c) {\n    if (isLLMContent(c)) {\n        return contentToInlineData(c);\n    }\n    const last = c.at(-1);\n    if (!last)\n        return null;\n    return contentToInlineData(last);\n    function contentToInlineData(content) {\n        const part = content.parts.at(-1);\n        if (!part)\n            return \"\";\n        return \"inlineData\" in part && part.inlineData ? part.inlineData : null;\n    }\n}\nfunction generateId() {\n    return Math.random().toString(36).substring(2, 5);\n}\n",
      "metadata": {
        "title": "utils",
        "source": {
          "code": "/**\n * @fileoverview Common utils for manipulating LLM Content and other relevant types.\n */\nexport {\n  isLLMContent,\n  isLLMContentArray,\n  toLLMContent,\n  toInlineData,\n  toLLMContentInline,\n  toText,\n  contentToJSON,\n  defaultLLMContent,\n  endsWithRole,\n  addUserTurn,\n  isEmpty,\n  llm,\n  ok,\n  err,\n  generateId,\n  mergeTextParts,\n};\n\nexport type NonPromise<T> = T extends Promise<unknown> ? never : T;\n\nfunction ok<T>(o: Outcome<NonPromise<T>>): o is NonPromise<T> {\n  return !(o && typeof o === \"object\" && \"$error\" in o);\n}\n\nfunction err($error: string) {\n  return { $error };\n}\n\nfunction mergeTextParts(parts: LLMContent[\"parts\"]): LLMContent[\"parts\"] {\n  const merged: LLMContent[\"parts\"] = [];\n  let text: string = \"\";\n  for (const part of parts) {\n    if (\"text\" in part) {\n      text += part.text;\n    } else {\n      if (text) {\n        merged.push({ text });\n      }\n      text = \"\";\n      merged.push(part);\n    }\n  }\n  if (text) {\n    merged.push({ text });\n  }\n  return merged;\n}\n\nclass LLMTemplate {\n  constructor(\n    public readonly strings: TemplateStringsArray,\n    public readonly values: unknown[]\n  ) {}\n\n  asParts(): LLMContent[\"parts\"] {\n    return mergeTextParts(\n      this.strings.flatMap((s, i) => {\n        let text = s;\n        const value = this.values.at(i);\n        if (value == undefined) {\n          return { text };\n        } else if (typeof value === \"string\") {\n          text += value;\n          return { text };\n        } else if (value instanceof LLMTemplate) {\n          return value.asParts();\n        } else if (isLLMContent(value)) {\n          return [{ text }, ...value.parts];\n        } else {\n          text += JSON.stringify(value);\n          return { text };\n        }\n      })\n    );\n  }\n\n  asContent(): LLMContent {\n    const parts = this.asParts();\n    return { parts, role: \"user\" };\n  }\n}\n\nfunction llm(strings: TemplateStringsArray, ...values: unknown[]): LLMTemplate {\n  return new LLMTemplate(strings, values);\n}\n\n/**\n * Copied from @google-labs/breadboard\n */\nfunction isLLMContent(nodeValue: unknown): nodeValue is LLMContent {\n  if (typeof nodeValue !== \"object\" || !nodeValue) return false;\n  if (nodeValue === null || nodeValue === undefined) return false;\n\n  if (\"role\" in nodeValue && nodeValue.role === \"$metadata\") {\n    return true;\n  }\n\n  return \"parts\" in nodeValue && Array.isArray(nodeValue.parts);\n}\n\nfunction isLLMContentArray(nodeValue: unknown): nodeValue is LLMContent[] {\n  if (!Array.isArray(nodeValue)) return false;\n  if (nodeValue.length === 0) return true;\n  return isLLMContent(nodeValue.at(-1));\n}\n\nfunction toLLMContent(\n  text: string,\n  role: LLMContent[\"role\"] = \"user\"\n): LLMContent {\n  return { parts: [{ text }], role };\n}\n\nfunction endsWithRole(c: LLMContent[], role: \"user\" | \"model\"): boolean {\n  const last = c.at(-1);\n  if (!last) return false;\n  return last.role === role;\n}\n\nfunction isEmpty(c: LLMContent): boolean {\n  if (!c.parts.length) return true;\n  for (const part of c.parts) {\n    if (\"text\" in part) {\n      if (part.text.trim().length > 0) return false;\n    } else {\n      return false;\n    }\n  }\n  return true;\n}\n\nfunction toText(c: LLMContent | LLMContent[]): string {\n  if (isLLMContent(c)) {\n    return contentToText(c);\n  }\n  const last = c.at(-1);\n  if (!last) return \"\";\n  return contentToText(last).trim();\n\n  function contentToText(content: LLMContent) {\n    return content.parts\n      .map((part) => (\"text\" in part ? part.text : \"\"))\n      .join(\"\\n\\n\");\n  }\n}\n\nfunction contentToJSON<T>(content?: LLMContent): T {\n  const part = content?.parts?.at(0);\n  if (!part || !(\"text\" in part)) {\n    throw new Error(\"Invalid response from Gemini\");\n  }\n  return JSON.parse(part.text) as T;\n}\n\nfunction defaultLLMContent(): string {\n  return JSON.stringify({\n    parts: [{ text: \"\" }],\n    role: \"user\",\n  } satisfies LLMContent);\n}\n\nfunction addUserTurn(content: string | LLMContent, context?: LLMContent[]) {\n  context ??= [];\n  const isString = typeof content === \"string\";\n  if (!endsWithRole(context, \"user\")) {\n    return [...context, isString ? toLLMContent(content) : content];\n  }\n  const last = context.at(-1)!;\n  if (isString) {\n    last.parts.push({ text: content });\n  } else {\n    last.parts.push(...content.parts);\n  }\n  return context;\n}\n\nfunction toLLMContentInline(\n  mimetype: string,\n  value: string,\n  role: LLMContent[\"role\"] = \"user\"\n): LLMContent {\n  return {\n    parts: [\n      {\n        inlineData: {\n          mimeType: mimetype,\n          data: value,\n        },\n      },\n    ],\n    role,\n  };\n}\n\nfunction toInlineData(c: LLMContent | LLMContent[]) {\n  if (isLLMContent(c)) {\n    return contentToInlineData(c);\n  }\n  const last = c.at(-1);\n  if (!last) return null;\n  return contentToInlineData(last);\n\n  function contentToInlineData(content: LLMContent) {\n    const part = content.parts.at(-1);\n    if (!part) return \"\";\n    return \"inlineData\" in part && part.inlineData ? part.inlineData : null;\n  }\n}\n\nfunction generateId() {\n  return Math.random().toString(36).substring(2, 5);\n}\n",
          "language": "typescript"
        },
        "description": "Common utils for manipulating LLM Content and other relevant types.",
        "runnable": false
      }
    },
    "gemini": {
      "code": "/**\n * @fileoverview Gemini Model Family.\n */\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\nimport output from \"@output\";\nimport { ok, err, isLLMContentArray } from \"./utils\";\nimport { flattenContext } from \"./lists\";\nconst defaultSafetySettings = () => [\n    {\n        category: \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        threshold: \"OFF\",\n    },\n    {\n        category: \"HARM_CATEGORY_HARASSMENT\",\n        threshold: \"OFF\",\n    },\n    {\n        category: \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        threshold: \"OFF\",\n    },\n];\nasync function endpointURL(model) {\n    const $metadata = {\n        title: \"Get GEMINI_KEY\",\n        description: \"Getting GEMINI_KEY from secrets\",\n    };\n    const key = await secrets({ $metadata, keys: [\"GEMINI_KEY\"] });\n    return `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${key.GEMINI_KEY}`;\n}\nexport { invoke as default, describe, defaultSafetySettings };\nconst VALID_MODALITIES = [\"Text\", \"Text and Image\", \"Audio\"];\nconst MODELS = [\n    \"gemini-2.0-flash\",\n    \"gemini-2.0-flash-001\",\n    \"gemini-2.0-flash-lite-preview-02-05\",\n    \"gemini-2.0-flash-exp\",\n    \"gemini-2.0-flash-thinking-exp\",\n    \"gemini-2.0-flash-thinking-exp-01-21\",\n    \"gemini-2.0-pro-exp-02-05\",\n    \"gemini-1.5-flash-latest\",\n    \"gemini-1.5-pro-latest\",\n    \"gemini-exp-1206\",\n    \"gemini-exp-1121\",\n    \"learnlm-1.5-pro-experimental\",\n    \"gemini-1.5-pro-001\",\n    \"gemini-1.5-pro-002\",\n    \"gemini-1.5-pro-exp-0801\",\n    \"gemini-1.5-pro-exp-0827\",\n    \"gemini-1.5-flash-001\",\n    \"gemini-1.5-flash-002\",\n    \"gemini-1.5-flash-8b-exp-0924\",\n    \"gemini-1.5-flash-8b-exp-0827\",\n    \"gemini-1.5-flash-exp-0827\",\n];\nconst NO_RETRY_CODES = [400, 429, 404];\n/**\n * Modifies the body to remove any\n * Breadboard-specific extensions to LLM Content\n */\nfunction conformBody(body) {\n    return {\n        ...body,\n        contents: flattenContext(body.contents.map((content) => {\n            return {\n                ...content,\n                parts: content.parts.map((part) => {\n                    if (\"json\" in part) {\n                        return { text: JSON.stringify(part.json) };\n                    }\n                    return part;\n                }),\n            };\n        }), true),\n    };\n}\nasync function callAPI(retries, model, body, $metadata) {\n    let $error = \"Unknown error\";\n    while (retries) {\n        const result = await fetch({\n            $metadata,\n            url: await endpointURL(model),\n            method: \"POST\",\n            body: conformBody(body),\n        });\n        if (!ok(result)) {\n            // Fetch is a bit weird, because it returns various props\n            // along with the `$error`. Let's handle that here.\n            const { status, $error: errObject } = result;\n            if (!status) {\n                // This is not an error response, presume fatal error.\n                return { $error };\n            }\n            $error = maybeExtractError(errObject);\n            if (NO_RETRY_CODES.includes(status)) {\n                return { $error };\n            }\n        }\n        else {\n            const outputs = result.response;\n            const candidate = outputs.candidates?.at(0);\n            if (!candidate) {\n                return err(\"Unable to get a good response from Gemini\");\n            }\n            if (\"content\" in candidate) {\n                return outputs;\n            }\n            if (candidate.finishReason === \"IMAGE_SAFETY\") {\n                return err(\"The response candidate content was flagged for image safety reasons.\");\n            }\n        }\n        retries--;\n    }\n    return { $error };\n}\nfunction maybeExtractError(e) {\n    try {\n        const parsed = JSON.parse(e);\n        return parsed.error.message;\n    }\n    catch (error) {\n        return e;\n    }\n}\nfunction isEmptyLLMContent(content) {\n    if (!content || !content.parts || content.parts.length === 0)\n        return true;\n    return content.parts.every((part) => {\n        if (\"text\" in part) {\n            return !part.text?.trim();\n        }\n        return true;\n    });\n}\nfunction addModality(body, modality) {\n    if (!modality)\n        return;\n    switch (modality) {\n        case \"Text\":\n            // No change, defaults.\n            break;\n        case \"Text and Image\":\n            body.generationConfig ??= {};\n            body.generationConfig.responseModalities = [\"TEXT\", \"IMAGE\"];\n            break;\n        case \"Audio\":\n            body.generationConfig ??= {};\n            body.generationConfig.responseModalities = [\"AUDIO\"];\n            break;\n    }\n}\nfunction constructBody(context = [], systemInstruction, prompt, modality) {\n    const contents = [...context];\n    if (!isEmptyLLMContent(prompt)) {\n        contents.push(prompt);\n    }\n    const body = {\n        contents,\n        safetySettings: defaultSafetySettings(),\n    };\n    const canHaveSystemInstruction = modality === \"Text\";\n    if (!isEmptyLLMContent(systemInstruction) && canHaveSystemInstruction) {\n        body.systemInstruction = systemInstruction;\n    }\n    addModality(body, modality);\n    return body;\n}\nfunction augmentBody(body, systemInstruction, prompt, modality) {\n    if (!body.systemInstruction || !isEmptyLLMContent(systemInstruction)) {\n        body.systemInstruction = systemInstruction;\n    }\n    if (!isEmptyLLMContent(prompt)) {\n        body.contents = [...body.contents, prompt];\n    }\n    addModality(body, modality);\n    return body;\n}\nfunction validateInputs(inputs) {\n    if (\"body\" in inputs) {\n        return;\n    }\n    if (inputs.context) {\n        const { context } = inputs;\n        if (!Array.isArray(context)) {\n            return err(\"Incoming context must be an array.\");\n        }\n        if (!isLLMContentArray(context)) {\n            return err(\"Malformed incoming context\");\n        }\n        return;\n    }\n    return err(\"Either body or context is required\");\n}\nasync function invoke(inputs) {\n    const validatingInputs = validateInputs(inputs);\n    if (!ok(validatingInputs)) {\n        return validatingInputs;\n    }\n    let { model } = inputs;\n    if (!model) {\n        model = MODELS[0];\n    }\n    const { context, systemInstruction, prompt, modality, body, $metadata } = inputs;\n    // TODO: Make this configurable.\n    const retries = 5;\n    if (!(\"body\" in inputs)) {\n        // Public API is being used.\n        // Behave as if we're wired in.\n        const result = await callAPI(retries, model, constructBody(context, systemInstruction, prompt, modality));\n        if (!ok(result)) {\n            return result;\n        }\n        const content = result.candidates.at(0)?.content;\n        if (!content) {\n            return err(\"Unable to get a good response from Gemini\");\n        }\n        return { context: [...context, content] };\n    }\n    else {\n        // Private API is being used.\n        // Behave as if we're being invoked.\n        return callAPI(retries, model, augmentBody(body, systemInstruction, prompt, modality), $metadata);\n    }\n}\nasync function describe({ inputs }) {\n    const canHaveModalities = inputs.model === \"gemini-2.0-flash-exp\";\n    const canHaveSystemInstruction = !canHaveModalities || (canHaveModalities && inputs.modality == \"Text\");\n    const maybeAddSystemInstruction = canHaveSystemInstruction\n        ? {\n            systemInstruction: {\n                type: \"object\",\n                behavior: [\"llm-content\", \"config\"],\n                title: \"System Instruction\",\n                default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n                description: \"(Optional) Give the model additional context on what to do,\" +\n                    \"like specific rules/guidelines to adhere to or specify behavior\" +\n                    \"separate from the provided context\",\n            },\n        }\n        : {};\n    const maybeAddModalities = canHaveModalities\n        ? {\n            modality: {\n                type: \"string\",\n                enum: [...VALID_MODALITIES],\n                title: \"Output Modality\",\n                behavior: [\"config\"],\n                description: \"(Optional) Tell the model what kind of output you're looking for.\",\n            },\n        }\n        : {};\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                model: {\n                    type: \"string\",\n                    behavior: [\"config\"],\n                    title: \"Model Name\",\n                    enum: MODELS,\n                    default: MODELS[0],\n                },\n                prompt: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\"],\n                    title: \"Prompt\",\n                    default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n                    description: \"(Optional) A prompt. Will be added to the end of the the conversation context.\",\n                },\n                ...maybeAddSystemInstruction,\n                ...maybeAddModalities,\n                context: {\n                    type: \"array\",\n                    items: {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                    },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                    },\n                    title: \"Context out\",\n                },\n            },\n        },\n        metadata: {\n            icon: \"generative\",\n        },\n    };\n}\n",
      "metadata": {
        "title": "Gemini",
        "source": {
          "code": "/**\n * @fileoverview Gemini Model Family.\n */\n\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\nimport output from \"@output\";\n\nimport { ok, err, isLLMContentArray } from \"./utils\";\nimport { flattenContext } from \"./lists\";\n\nconst defaultSafetySettings = (): SafetySetting[] => [\n  {\n    category: \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n    threshold: \"OFF\",\n  },\n  {\n    category: \"HARM_CATEGORY_HARASSMENT\",\n    threshold: \"OFF\",\n  },\n  {\n    category: \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n    threshold: \"OFF\",\n  },\n];\n\nasync function endpointURL(model: string): Promise<string> {\n  const $metadata = {\n    title: \"Get GEMINI_KEY\",\n    description: \"Getting GEMINI_KEY from secrets\",\n  };\n  const key = await secrets({ $metadata, keys: [\"GEMINI_KEY\"] });\n  return `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${key.GEMINI_KEY}`;\n}\n\nexport { invoke as default, describe, defaultSafetySettings };\n\nconst VALID_MODALITIES = [\"Text\", \"Text and Image\", \"Audio\"] as const;\ntype ValidModalities = (typeof VALID_MODALITIES)[number];\n\nexport type HarmBlockThreshold =\n  // Content with NEGLIGIBLE will be allowed.\n  | \"BLOCK_LOW_AND_ABOVE\"\n  // Content with NEGLIGIBLE and LOW will be allowed.\n  | \"BLOCK_MEDIUM_AND_ABOVE\"\n  // Content with NEGLIGIBLE, LOW, and MEDIUM will be allowed.\n  | \"BLOCK_ONLY_HIGH\"\n  // All content will be allowed.\n  | \"BLOCK_NONE\"\n  // Turn off the safety filter.\n  | \"OFF\";\n\nexport type HarmCategory =\n  // Gemini - Harassment content\n  | \"HARM_CATEGORY_HARASSMENT\"\n  //\tGemini - Hate speech and content.\n  | \"HARM_CATEGORY_HATE_SPEECH\"\n  // Gemini - Sexually explicit content.\n  | \"HARM_CATEGORY_SEXUALLY_EXPLICIT\"\n  // \tGemini - Dangerous content.\n  | \"HARM_CATEGORY_DANGEROUS_CONTENT\"\n  // Gemini - Content that may be used to harm civic integrity.\n  | \"HARM_CATEGORY_CIVIC_INTEGRITY\";\n\nexport type GeminiSchema = {\n  type: \"string\" | \"number\" | \"integer\" | \"boolean\" | \"object\" | \"array\";\n  format?: string;\n  description?: string;\n  nullable?: boolean;\n  enum?: string[];\n  maxItems?: string;\n  minItems?: string;\n  properties?: Record<string, GeminiSchema>;\n  required?: string[];\n  items?: GeminiSchema;\n};\n\nexport type Modality = \"TEXT\" | \"IMAGE\" | \"AUDIO\";\n\nexport type GenerationConfig = {\n  responseMimeType?: \"text/plain\" | \"application/json\" | \"text/x.enum\";\n  responseSchema?: GeminiSchema;\n  responseModalities?: Modality[];\n};\n\nexport type SafetySetting = {\n  category: HarmCategory;\n  threshold: HarmBlockThreshold;\n};\n\nexport type Metadata = {\n  title?: string;\n  description?: string;\n};\n\nexport type GeminiBody = {\n  contents: LLMContent[];\n  tools?: Tool[];\n  toolConfig?: ToolConfig;\n  systemInstruction?: LLMContent;\n  safetySettings?: SafetySetting[];\n  generationConfig?: GenerationConfig;\n};\n\nexport type GeminiInputs = {\n  // The wireable/configurable properties.\n  model?: string;\n  context?: LLMContent[];\n  systemInstruction?: LLMContent;\n  prompt?: LLMContent;\n  modality?: ValidModalities;\n  // The \"private API\" properties\n  $metadata?: Metadata;\n  body: GeminiBody;\n};\n\nexport type Tool = {\n  functionDeclarations?: FunctionDeclaration[];\n  googleSearch?: {};\n  codeExecution?: CodeExecution[];\n};\n\nexport type ToolConfig = {\n  functionCallingConfig?: FunctionCallingConfig;\n};\n\nexport type FunctionCallingConfig = {\n  mode?: \"MODE_UNSPECIFIED\" | \"AUTO\" | \"ANY\" | \"NONE\";\n  allowedFunctionNames?: string[];\n};\n\nexport type FunctionDeclaration = {\n  name: string;\n  description: string;\n  parameters?: GeminiSchema;\n};\n\nexport type CodeExecution = {\n  // Type contains no fields.\n};\n\nexport type FinishReason =\n  // Natural stop point of the model or provided stop sequence.\n  | \"STOP\"\n  // The maximum number of tokens as specified in the request was reached.\n  | \"MAX_TOKENS\"\n  // The response candidate content was flagged for safety reasons.\n  | \"SAFETY\"\n  // The response candidate content was flagged for image safety reasons.\n  | \"IMAGE_SAFETY\"\n  // The response candidate content was flagged for recitation reasons.\n  | \"RECITATION\"\n  // The response candidate content was flagged for using an unsupported language.\n  | \"LANGUAGE\"\n  // Unknown reason.\n  | \"OTHER\"\n  // Token generation stopped because the content contains forbidden terms.\n  | \"BLOCKLIST\"\n  // Token generation stopped for potentially containing prohibited content.\n  | \"PROHIBITED_CONTENT\"\n  // Token generation stopped because the content potentially contains Sensitive Personally Identifiable Information (SPII).\n  | \"SPII\"\n  // The function call generated by the model is invalid.\n  | \"MALFORMED_FUNCTION_CALL\";\n\nexport type GroundingMetadata = {\n  groundingChunks: {\n    web: {\n      uri: string;\n      title: string;\n    };\n  }[];\n  groundingSupports: {\n    groundingChunkIndices: number[];\n    confidenceScores: number[];\n    segment: {\n      partIndex: number;\n      startIndex: number;\n      endIndex: number;\n      text: string;\n    };\n  };\n  webSearchQueries: string[];\n  searchEntryPoint: {\n    renderedContent: string;\n    /**\n     * Base64 encoded JSON representing array of <search term, search url> tuple.\n     * A base64-encoded string.\n     */\n    sdkBlob: string;\n  };\n  retrievalMetadata: {\n    googleSearchDynamicRetrievalScore: number;\n  };\n};\n\nexport type Candidate = {\n  content?: LLMContent;\n  finishReason?: FinishReason;\n  safetyRatings?: SafetySetting[];\n  tokenOutput: number;\n  groundingMetadata: GroundingMetadata;\n};\n\nexport type GeminiAPIOutputs = {\n  candidates: Candidate[];\n};\n\nexport type GeminiOutputs =\n  | GeminiAPIOutputs\n  | {\n      context: LLMContent[];\n    };\n\nconst MODELS: readonly string[] = [\n  \"gemini-2.0-flash\",\n  \"gemini-2.0-flash-001\",\n  \"gemini-2.0-flash-lite-preview-02-05\",\n  \"gemini-2.0-flash-exp\",\n  \"gemini-2.0-flash-thinking-exp\",\n  \"gemini-2.0-flash-thinking-exp-01-21\",\n  \"gemini-2.0-pro-exp-02-05\",\n  \"gemini-1.5-flash-latest\",\n  \"gemini-1.5-pro-latest\",\n  \"gemini-exp-1206\",\n  \"gemini-exp-1121\",\n  \"learnlm-1.5-pro-experimental\",\n  \"gemini-1.5-pro-001\",\n  \"gemini-1.5-pro-002\",\n  \"gemini-1.5-pro-exp-0801\",\n  \"gemini-1.5-pro-exp-0827\",\n  \"gemini-1.5-flash-001\",\n  \"gemini-1.5-flash-002\",\n  \"gemini-1.5-flash-8b-exp-0924\",\n  \"gemini-1.5-flash-8b-exp-0827\",\n  \"gemini-1.5-flash-exp-0827\",\n];\n\nconst NO_RETRY_CODES: readonly number[] = [400, 429, 404];\n\ntype FetchErrorResponse = {\n  $error: string;\n  status: number;\n  statusText: string;\n  contentType: string;\n  responseHeaders: Record<string, string>;\n};\n\n/**\n * Using\n * `{\"error\":{\"code\":400,\"message\":\"Invalid JSON paylo…'contents[0].parts[0]': Cannot find field.\"}]}]}\n * as template for this type.\n */\ntype GeminiError = {\n  error: {\n    code: number;\n    details: {\n      type: string;\n      fieldViolations: {\n        description: string;\n        field: string;\n      }[];\n    }[];\n    message: string;\n    status: string;\n  };\n};\n\n/**\n * Modifies the body to remove any\n * Breadboard-specific extensions to LLM Content\n */\nfunction conformBody(body: GeminiBody): GeminiBody {\n  return {\n    ...body,\n    contents: flattenContext(\n      body.contents.map((content) => {\n        return {\n          ...content,\n          parts: content.parts.map((part) => {\n            if (\"json\" in part) {\n              return { text: JSON.stringify(part.json) };\n            }\n            return part;\n          }),\n        };\n      }),\n      true\n    ),\n  };\n}\n\nasync function callAPI(\n  retries: number,\n  model: string,\n  body: GeminiBody,\n  $metadata?: Metadata\n): Promise<Outcome<GeminiAPIOutputs>> {\n  let $error: string = \"Unknown error\";\n  while (retries) {\n    const result = await fetch({\n      $metadata,\n      url: await endpointURL(model),\n      method: \"POST\",\n      body: conformBody(body),\n    });\n    if (!ok(result)) {\n      // Fetch is a bit weird, because it returns various props\n      // along with the `$error`. Let's handle that here.\n      const { status, $error: errObject } = result as FetchErrorResponse;\n      if (!status) {\n        // This is not an error response, presume fatal error.\n        return { $error };\n      }\n      $error = maybeExtractError(errObject);\n      if (NO_RETRY_CODES.includes(status)) {\n        return { $error };\n      }\n    } else {\n      const outputs = result.response as GeminiAPIOutputs;\n      const candidate = outputs.candidates?.at(0);\n      if (!candidate) {\n        return err(\"Unable to get a good response from Gemini\");\n      }\n      if (\"content\" in candidate) {\n        return outputs;\n      }\n      if (candidate.finishReason === \"IMAGE_SAFETY\") {\n        return err(\n          \"The response candidate content was flagged for image safety reasons.\"\n        );\n      }\n    }\n    retries--;\n  }\n  return { $error };\n}\n\nfunction maybeExtractError(e: string): string {\n  try {\n    const parsed = JSON.parse(e) as GeminiError;\n    return parsed.error.message;\n  } catch (error) {\n    return e;\n  }\n}\n\nfunction isEmptyLLMContent(content?: LLMContent): content is undefined {\n  if (!content || !content.parts || content.parts.length === 0) return true;\n  return content.parts.every((part) => {\n    if (\"text\" in part) {\n      return !part.text?.trim();\n    }\n    return true;\n  });\n}\n\nfunction addModality(body: GeminiBody, modality?: ValidModalities) {\n  if (!modality) return;\n  switch (modality) {\n    case \"Text\":\n      // No change, defaults.\n      break;\n    case \"Text and Image\":\n      body.generationConfig ??= {};\n      body.generationConfig.responseModalities = [\"TEXT\", \"IMAGE\"];\n      break;\n    case \"Audio\":\n      body.generationConfig ??= {};\n      body.generationConfig.responseModalities = [\"AUDIO\"];\n      break;\n  }\n}\n\nfunction constructBody(\n  context: LLMContent[] = [],\n  systemInstruction?: LLMContent,\n  prompt?: LLMContent,\n  modality?: ValidModalities\n): GeminiBody {\n  const contents = [...context];\n  if (!isEmptyLLMContent(prompt)) {\n    contents.push(prompt);\n  }\n  const body: GeminiBody = {\n    contents,\n    safetySettings: defaultSafetySettings(),\n  };\n  const canHaveSystemInstruction = modality === \"Text\";\n  if (!isEmptyLLMContent(systemInstruction) && canHaveSystemInstruction) {\n    body.systemInstruction = systemInstruction;\n  }\n  addModality(body, modality);\n  return body;\n}\n\nfunction augmentBody(\n  body: GeminiBody,\n  systemInstruction?: LLMContent,\n  prompt?: LLMContent,\n  modality?: ValidModalities\n): GeminiBody {\n  if (!body.systemInstruction || !isEmptyLLMContent(systemInstruction)) {\n    body.systemInstruction = systemInstruction;\n  }\n  if (!isEmptyLLMContent(prompt)) {\n    body.contents = [...body.contents, prompt];\n  }\n  addModality(body, modality);\n  return body;\n}\n\nfunction validateInputs(inputs: GeminiInputs): Outcome<void> {\n  if (\"body\" in (inputs as object)) {\n    return;\n  }\n  if (inputs.context) {\n    const { context } = inputs;\n    if (!Array.isArray(context)) {\n      return err(\"Incoming context must be an array.\");\n    }\n    if (!isLLMContentArray(context)) {\n      return err(\"Malformed incoming context\");\n    }\n    return;\n  }\n  return err(\"Either body or context is required\");\n}\n\nasync function invoke(inputs: GeminiInputs): Promise<Outcome<GeminiOutputs>> {\n  const validatingInputs = validateInputs(inputs);\n  if (!ok(validatingInputs)) {\n    return validatingInputs;\n  }\n  let { model } = inputs;\n  if (!model) {\n    model = MODELS[0];\n  }\n  const { context, systemInstruction, prompt, modality, body, $metadata } =\n    inputs;\n  // TODO: Make this configurable.\n  const retries = 5;\n  if (!(\"body\" in inputs)) {\n    // Public API is being used.\n    // Behave as if we're wired in.\n    const result = await callAPI(\n      retries,\n      model,\n      constructBody(context, systemInstruction, prompt, modality)\n    );\n    if (!ok(result)) {\n      return result;\n    }\n    const content = result.candidates.at(0)?.content;\n    if (!content) {\n      return err(\"Unable to get a good response from Gemini\");\n    }\n    return { context: [...context!, content] };\n  } else {\n    // Private API is being used.\n    // Behave as if we're being invoked.\n    return callAPI(\n      retries,\n      model,\n      augmentBody(body, systemInstruction, prompt, modality),\n      $metadata\n    );\n  }\n}\n\ntype DescribeInputs = {\n  inputs: {\n    modality?: ValidModalities;\n    model: string;\n  };\n};\n\nasync function describe({ inputs }: DescribeInputs) {\n  const canHaveModalities = inputs.model === \"gemini-2.0-flash-exp\";\n  const canHaveSystemInstruction =\n    !canHaveModalities || (canHaveModalities && inputs.modality == \"Text\");\n  const maybeAddSystemInstruction: Schema[\"properties\"] =\n    canHaveSystemInstruction\n      ? {\n          systemInstruction: {\n            type: \"object\",\n            behavior: [\"llm-content\", \"config\"],\n            title: \"System Instruction\",\n            default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n            description:\n              \"(Optional) Give the model additional context on what to do,\" +\n              \"like specific rules/guidelines to adhere to or specify behavior\" +\n              \"separate from the provided context\",\n          },\n        }\n      : {};\n  const maybeAddModalities: Schema[\"properties\"] = canHaveModalities\n    ? {\n        modality: {\n          type: \"string\",\n          enum: [...VALID_MODALITIES],\n          title: \"Output Modality\",\n          behavior: [\"config\"],\n          description:\n            \"(Optional) Tell the model what kind of output you're looking for.\",\n        },\n      }\n    : {};\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        model: {\n          type: \"string\",\n          behavior: [\"config\"],\n          title: \"Model Name\",\n          enum: MODELS as string[],\n          default: MODELS[0],\n        },\n        prompt: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\"],\n          title: \"Prompt\",\n          default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n          description:\n            \"(Optional) A prompt. Will be added to the end of the the conversation context.\",\n        },\n        ...maybeAddSystemInstruction,\n        ...maybeAddModalities,\n        context: {\n          type: \"array\",\n          items: {\n            type: \"object\",\n            behavior: [\"llm-content\"],\n          },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: {\n            type: \"object\",\n            behavior: [\"llm-content\"],\n          },\n          title: \"Context out\",\n        },\n      },\n    } satisfies Schema,\n    metadata: {\n      icon: \"generative\",\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Gemini Model Family.",
        "runnable": false
      }
    },
    "entry": {
      "code": "/**\n * @fileoverview Manages the entry point: describer, passing the inputs, etc.\n */\nimport {} from \"./common\";\nimport { toLLMContent, defaultLLMContent } from \"./utils\";\nimport { Template } from \"./template\";\nexport { invoke as default, describe };\nasync function invoke({ context, \"p-chat\": chat, \"p-list\": makeList, description, ...params }) {\n    // Make sure it's a boolean.\n    chat = !!chat;\n    context ??= [];\n    const defaultModel = \"\";\n    const type = \"work\";\n    return {\n        context: {\n            id: Math.random().toString(36).substring(2, 5),\n            chat,\n            makeList,\n            listPath: [],\n            context,\n            userInputs: [],\n            defaultModel,\n            model: \"\",\n            description,\n            tools: [],\n            type,\n            work: [],\n            userEndedChat: false,\n            params,\n        },\n    };\n}\nasync function describe({ inputs: { description } }) {\n    const template = new Template(description);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                description: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"Instruction\",\n                    description: \"Give the model additional context on what to do, like specific rules/guidelines to adhere to or specify behavior separate from the provided context.\",\n                    default: defaultLLMContent(),\n                },\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                \"p-chat\": {\n                    type: \"boolean\",\n                    title: \"Chat with User\",\n                    behavior: [\"config\", \"hint-preview\"],\n                    icon: \"chat\",\n                    description: \"When checked, this step will chat with the user, asking to review work, requesting additional information, etc.\",\n                },\n                \"p-list\": {\n                    type: \"boolean\",\n                    title: \"Make a list\",\n                    behavior: [\"config\", \"hint-preview\"],\n                    icon: \"summarize\",\n                    description: \"When checked, this step will try to create a list as its output. Make sure that the prompt asks for a list of some sort\",\n                },\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"main-port\", \"hint-text\"],\n                },\n            },\n        },\n        title: \"Make Text\",\n        metadata: {\n            icon: \"generative-text\",\n            tags: [\"quick-access\", \"generative\"],\n            order: 1,\n        },\n    };\n}\n",
      "metadata": {
        "title": "Make Text",
        "source": {
          "code": "/**\n * @fileoverview Manages the entry point: describer, passing the inputs, etc.\n */\n\nimport {\n  type AgentContext,\n  type AgentInputs,\n  type DescribeInputs,\n} from \"./common\";\nimport { toLLMContent, defaultLLMContent } from \"./utils\";\nimport { Template } from \"./template\";\n\nexport { invoke as default, describe };\n\nexport type EntryInputs = {\n  context: LLMContent[];\n  description: LLMContent;\n  \"p-chat\": boolean;\n  \"p-list\": boolean;\n  [key: `p-z-${string}`]: unknown;\n};\n\ntype Outputs = {\n  context: AgentContext;\n};\n\nasync function invoke({\n  context,\n  \"p-chat\": chat,\n  \"p-list\": makeList,\n  description,\n  ...params\n}: EntryInputs): Promise<Outputs> {\n  // Make sure it's a boolean.\n  chat = !!chat;\n  context ??= [];\n  const defaultModel = \"\";\n  const type = \"work\";\n  return {\n    context: {\n      id: Math.random().toString(36).substring(2, 5),\n      chat,\n      makeList,\n      listPath: [],\n      context,\n      userInputs: [],\n      defaultModel,\n      model: \"\",\n      description,\n      tools: [],\n      type,\n      work: [],\n      userEndedChat: false,\n      params,\n    },\n  };\n}\n\nasync function describe({ inputs: { description } }: DescribeInputs) {\n  const template = new Template(description);\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        description: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n          title: \"Instruction\",\n          description:\n            \"Give the model additional context on what to do, like specific rules/guidelines to adhere to or specify behavior separate from the provided context.\",\n          default: defaultLLMContent(),\n        },\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n        \"p-chat\": {\n          type: \"boolean\",\n          title: \"Chat with User\",\n          behavior: [\"config\", \"hint-preview\"],\n          icon: \"chat\",\n          description:\n            \"When checked, this step will chat with the user, asking to review work, requesting additional information, etc.\",\n        },\n        \"p-list\": {\n          type: \"boolean\",\n          title: \"Make a list\",\n          behavior: [\"config\", \"hint-preview\"],\n          icon: \"summarize\",\n          description:\n            \"When checked, this step will try to create a list as its output. Make sure that the prompt asks for a list of some sort\",\n        },\n        ...template.schemas(),\n      },\n      behavior: [\"at-wireable\"],\n      ...template.requireds(),\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n          behavior: [\"main-port\", \"hint-text\"],\n        },\n      },\n    } satisfies Schema,\n    title: \"Make Text\",\n    metadata: {\n      icon: \"generative-text\",\n      tags: [\"quick-access\", \"generative\"],\n      order: 1,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Manages the entry point: describer, passing the inputs, etc.",
        "runnable": true
      }
    },
    "join": {
      "code": "/**\n * @fileoverview Joins user input and Agent Context\n */\nimport {} from \"./common\";\nimport { isEmpty } from \"./utils\";\nimport { addContent } from \"./lists\";\nexport { invoke as default, describe };\nasync function invoke({ context, request }) {\n    context.userEndedChat = isEmpty(request);\n    context.userInputs.push(request);\n    if (!context.userEndedChat) {\n        context.work = addContent(context.work, request);\n    }\n    return { context };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    title: \"Agent Context\",\n                    type: \"object\",\n                },\n                request: {\n                    title: \"User Input\",\n                    type: \"object\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    title: \"Agent Context\",\n                    type: \"object\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "join",
        "source": {
          "code": "/**\n * @fileoverview Joins user input and Agent Context\n */\n\nimport { type AgentContext } from \"./common\";\nimport { isEmpty } from \"./utils\";\nimport { addContent } from \"./lists\";\n\nexport { invoke as default, describe };\n\ntype Inputs = {\n  context: AgentContext;\n  request: LLMContent;\n};\n\ntype Outputs = {\n  context: AgentContext;\n};\n\nasync function invoke({ context, request }: Inputs): Promise<Outputs> {\n  context.userEndedChat = isEmpty(request);\n  context.userInputs.push(request);\n  if (!context.userEndedChat) {\n    context.work = addContent(context.work, request);\n  }\n  return { context };\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          title: \"Agent Context\",\n          type: \"object\",\n        },\n        request: {\n          title: \"User Input\",\n          type: \"object\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          title: \"Agent Context\",\n          type: \"object\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Joins user input and Agent Context",
        "runnable": true
      }
    },
    "output": {
      "code": "/**\n * @fileoverview Provides an output helper.\n */\nimport output from \"@output\";\nexport { report };\nasync function report(inputs) {\n    const { actor: title, category: description, name, details, icon } = inputs;\n    const detailsSchema = typeof details === \"string\"\n        ? {\n            title: name,\n            type: \"string\",\n            format: \"markdown\",\n        }\n        : {\n            title: name,\n            type: \"object\",\n            behavior: [\"llm-content\"],\n        };\n    if (icon) {\n        detailsSchema.icon = icon;\n    }\n    const schema = {\n        type: \"object\",\n        properties: {\n            details: detailsSchema,\n        },\n    };\n    const { delivered } = await output({\n        $metadata: {\n            title,\n            description,\n            icon,\n        },\n        schema,\n        details,\n    });\n    return delivered;\n}\n",
      "metadata": {
        "title": "output",
        "source": {
          "code": "/**\n * @fileoverview Provides an output helper.\n */\n\nimport output from \"@output\";\n\ntype ReportInputs = {\n  /**\n   * The name of the actor providing the report\n   */\n  actor: string;\n  /**\n   * The general category of the report\n   */\n  category: string;\n  /**\n   * The name of the report\n   */\n  name: string;\n  /**\n   * The details of the report\n   */\n  details: string | LLMContent;\n  /**\n   * The icon to use\n   */\n  icon?: string;\n};\n\nexport { report };\n\nasync function report(inputs: ReportInputs): Promise<boolean> {\n  const { actor: title, category: description, name, details, icon } = inputs;\n\n  const detailsSchema: Schema =\n    typeof details === \"string\"\n      ? {\n          title: name,\n          type: \"string\",\n          format: \"markdown\",\n        }\n      : {\n          title: name,\n          type: \"object\",\n          behavior: [\"llm-content\"],\n        };\n\n  if (icon) {\n    detailsSchema.icon = icon;\n  }\n\n  const schema: Schema = {\n    type: \"object\",\n    properties: {\n      details: detailsSchema,\n    },\n  };\n\n  const { delivered } = await output({\n    $metadata: {\n      title,\n      description,\n      icon,\n    },\n    schema,\n    details,\n  });\n  return delivered;\n}\n",
          "language": "typescript"
        },
        "description": "Provides an output helper.",
        "runnable": false
      }
    },
    "tool-manager": {
      "code": "/**\n * @fileoverview Add a description for your module here.\n */\nimport describeGraph from \"@describe\";\nimport invokeGraph from \"@invoke\";\nimport { ok, err } from \"./utils\";\nimport {} from \"./common\";\nimport {} from \"./gemini\";\nexport { ToolManager };\nclass ToolManager {\n    describerResultTransformer;\n    #hasSearch = false;\n    tools = new Map();\n    errors = [];\n    constructor(describerResultTransformer) {\n        this.describerResultTransformer = describerResultTransformer;\n    }\n    #convertSchemas(schema) {\n        return toGeminiSchema(schema);\n        function toGeminiSchema(schema) {\n            switch (schema.type) {\n                case \"object\": {\n                    if (schema.behavior?.includes(\"llm-content\")) {\n                        return {\n                            type: \"string\",\n                            description: schema.description || schema.title,\n                        };\n                    }\n                    if (!schema.properties) {\n                        return { type: \"object\" };\n                    }\n                    return {\n                        type: \"object\",\n                        properties: Object.fromEntries(Object.entries(schema.properties).map(([name, schema]) => {\n                            return [name, toGeminiSchema(schema)];\n                        })),\n                        required: schema.required,\n                    };\n                }\n                case \"array\": {\n                    const items = schema.items;\n                    if (items.behavior?.includes(\"llm-content\")) {\n                        return {\n                            type: \"string\",\n                            description: schema.description,\n                        };\n                    }\n                    return {\n                        type: \"array\",\n                        items: toGeminiSchema(schema.items),\n                    };\n                }\n                default: {\n                    const geminiSchema = { ...schema };\n                    delete geminiSchema.format;\n                    delete geminiSchema.behavior;\n                    delete geminiSchema.examples;\n                    delete geminiSchema.default;\n                    delete geminiSchema.transient;\n                    if (!geminiSchema.description) {\n                        geminiSchema.description = geminiSchema.title;\n                    }\n                    delete geminiSchema.title;\n                    return geminiSchema;\n                }\n            }\n        }\n    }\n    #toName(title) {\n        return title ? title.replace(/\\W/g, \"_\") : \"function\";\n    }\n    addSearch() {\n        this.#hasSearch = true;\n    }\n    async addTool(url) {\n        let description = (await describeGraph({\n            url,\n        }));\n        let passContext = false;\n        if (!ok(description))\n            return description;\n        if (this.describerResultTransformer) {\n            const transforming = await this.describerResultTransformer.transform(description);\n            if (!ok(transforming))\n                return transforming;\n            if (transforming) {\n                description = transforming;\n                passContext = true;\n            }\n        }\n        const name = this.#toName(description.title);\n        const functionDeclaration = {\n            name,\n            description: description.description || \"\",\n        };\n        const parameters = this.#convertSchemas(description.inputSchema);\n        if (parameters.properties) {\n            console.log(\"FUNCTION CALL PARAMETERS\", parameters, description.inputSchema);\n            functionDeclaration.parameters = parameters;\n        }\n        this.tools.set(name, { tool: functionDeclaration, url, passContext });\n        return description.title || name;\n    }\n    async initialize(tools) {\n        if (!tools) {\n            return true;\n        }\n        let hasInvalidTools = false;\n        for (const tool of tools) {\n            const url = typeof tool === \"string\" ? tool : tool.url;\n            const description = (await describeGraph({\n                url,\n            }));\n            if (!ok(description)) {\n                this.errors.push(description.$error);\n                // Invalid tool, skip\n                hasInvalidTools = true;\n                continue;\n            }\n            const parameters = this.#convertSchemas(description.inputSchema);\n            const name = this.#toName(description.title);\n            const functionDeclaration = {\n                name,\n                description: description.description || \"\",\n                parameters,\n            };\n            this.tools.set(name, {\n                tool: functionDeclaration,\n                url,\n                passContext: false,\n            });\n        }\n        return !hasInvalidTools;\n    }\n    async processResponse(response, callTool) {\n        for (const part of response.parts) {\n            if (\"functionCall\" in part) {\n                const { args, name } = part.functionCall;\n                const handle = this.tools.get(name);\n                if (handle) {\n                    const { url, passContext } = handle;\n                    await callTool(url, part.functionCall.args, passContext);\n                }\n            }\n        }\n    }\n    hasTools() {\n        return this.tools.size !== 0;\n    }\n    list() {\n        const declaration = {};\n        const entries = [...this.tools.entries()];\n        if (entries.length !== 0) {\n            declaration.functionDeclarations = entries.map(([, value]) => value.tool);\n        }\n        if (this.#hasSearch) {\n            declaration.googleSearch = {};\n        }\n        if (Object.keys(declaration).length === 0)\n            return [];\n        return [declaration];\n    }\n}\n",
      "metadata": {
        "title": "tool-manager",
        "source": {
          "code": "/**\n * @fileoverview Add a description for your module here.\n */\n\nimport describeGraph from \"@describe\";\nimport invokeGraph from \"@invoke\";\nimport { ok, err } from \"./utils\";\nimport {\n  type DescriberResult,\n  type DescriberResultTransformer,\n} from \"./common\";\nimport {\n  type FunctionDeclaration,\n  type GeminiSchema,\n  type Tool,\n} from \"./gemini\";\n\nexport type CallToolCallback = (\n  tool: string,\n  args: object,\n  passContext?: boolean\n) => Promise<void>;\n\nexport type ToolHandle = {\n  tool: FunctionDeclaration;\n  url: string;\n  passContext: boolean;\n};\n\nexport type ToolDescriptor =\n  | string\n  | {\n      kind: \"board\";\n      url: string;\n    };\n\nexport { ToolManager };\n\nclass ToolManager {\n  #hasSearch = false;\n  tools: Map<string, ToolHandle> = new Map();\n  errors: string[] = [];\n\n  constructor(\n    private readonly describerResultTransformer?: DescriberResultTransformer\n  ) {}\n\n  #convertSchemas(schema: Schema): GeminiSchema {\n    return toGeminiSchema(schema);\n\n    function toGeminiSchema(schema: Schema): GeminiSchema {\n      switch (schema.type) {\n        case \"object\": {\n          if (schema.behavior?.includes(\"llm-content\")) {\n            return {\n              type: \"string\",\n              description: schema.description || schema.title,\n            };\n          }\n          if (!schema.properties) {\n            return { type: \"object\" };\n          }\n          return {\n            type: \"object\",\n            properties: Object.fromEntries(\n              Object.entries(schema.properties).map(([name, schema]) => {\n                return [name, toGeminiSchema(schema)];\n              })\n            ),\n            required: schema.required,\n          };\n        }\n        case \"array\": {\n          const items = schema.items as Schema;\n          if (items.behavior?.includes(\"llm-content\")) {\n            return {\n              type: \"string\",\n              description: schema.description,\n            };\n          }\n          return {\n            type: \"array\",\n            items: toGeminiSchema(schema.items as Schema),\n          };\n        }\n        default: {\n          const geminiSchema = { ...schema };\n          delete geminiSchema.format;\n          delete geminiSchema.behavior;\n          delete geminiSchema.examples;\n          delete geminiSchema.default;\n          delete geminiSchema.transient;\n          if (!geminiSchema.description) {\n            geminiSchema.description = geminiSchema.title;\n          }\n          delete geminiSchema.title;\n          return geminiSchema as GeminiSchema;\n        }\n      }\n    }\n  }\n\n  #toName(title?: string) {\n    return title ? title.replace(/\\W/g, \"_\") : \"function\";\n  }\n\n  addSearch() {\n    this.#hasSearch = true;\n  }\n\n  async addTool(url: string): Promise<Outcome<string>> {\n    let description = (await describeGraph({\n      url,\n    })) as Outcome<DescriberResult>;\n    let passContext = false;\n    if (!ok(description)) return description;\n    if (this.describerResultTransformer) {\n      const transforming =\n        await this.describerResultTransformer.transform(description);\n      if (!ok(transforming)) return transforming;\n      if (transforming) {\n        description = transforming;\n        passContext = true;\n      }\n    }\n    const name = this.#toName(description.title);\n    const functionDeclaration: FunctionDeclaration = {\n      name,\n      description: description.description || \"\",\n    };\n    const parameters = this.#convertSchemas(description.inputSchema!);\n    if (parameters.properties) {\n      console.log(\n        \"FUNCTION CALL PARAMETERS\",\n        parameters,\n        description.inputSchema\n      );\n      functionDeclaration.parameters = parameters;\n    }\n    this.tools.set(name, { tool: functionDeclaration, url, passContext });\n    return description.title || name;\n  }\n\n  async initialize(tools?: ToolDescriptor[]): Promise<boolean> {\n    if (!tools) {\n      return true;\n    }\n    let hasInvalidTools = false;\n    for (const tool of tools) {\n      const url = typeof tool === \"string\" ? tool : tool.url;\n      const description = (await describeGraph({\n        url,\n      })) as Outcome<DescriberResult>;\n      if (!ok(description)) {\n        this.errors.push(description.$error);\n        // Invalid tool, skip\n        hasInvalidTools = true;\n        continue;\n      }\n      const parameters = this.#convertSchemas(description.inputSchema!);\n      const name = this.#toName(description.title);\n      const functionDeclaration = {\n        name,\n        description: description.description || \"\",\n        parameters,\n      };\n      this.tools.set(name, {\n        tool: functionDeclaration,\n        url,\n        passContext: false,\n      });\n    }\n    return !hasInvalidTools;\n  }\n\n  async processResponse(response: LLMContent, callTool: CallToolCallback) {\n    for (const part of response.parts) {\n      if (\"functionCall\" in part) {\n        const { args, name } = part.functionCall;\n        const handle = this.tools.get(name);\n        if (handle) {\n          const { url, passContext } = handle;\n          await callTool(url, part.functionCall.args, passContext);\n        }\n      }\n    }\n  }\n\n  hasTools(): boolean {\n    return this.tools.size !== 0;\n  }\n\n  list(): Tool[] {\n    const declaration: Tool = {};\n    const entries = [...this.tools.entries()];\n    if (entries.length !== 0) {\n      declaration.functionDeclarations = entries.map(([, value]) => value.tool);\n    }\n    if (this.#hasSearch) {\n      declaration.googleSearch = {};\n    }\n    if (Object.keys(declaration).length === 0) return [];\n    return [declaration];\n  }\n}\n",
          "language": "typescript"
        },
        "description": "Add a description for your module here.",
        "runnable": false
      }
    },
    "worker-worker": {
      "code": "/**\n * @fileoverview Performs assigned task. Part of the worker.\n */\nimport invokeBoard from \"@invoke\";\nimport output from \"@output\";\nimport { toText, toLLMContent, ok, err, llm, generateId } from \"./utils\";\nimport { defaultSafetySettings, } from \"./gemini\";\nimport { callGemini } from \"./gemini-client\";\nimport { GeminiPrompt } from \"./gemini-prompt\";\nimport { StructuredResponse } from \"./structured-response\";\nimport { ToolManager } from \"./tool-manager\";\nimport { fanOutContext } from \"./lists\";\nexport { invoke as default, describe };\nfunction computeWorkMode(tools, summarize) {\n    if (tools.length > 0) {\n        return \"call-tools\";\n    }\n    if (summarize) {\n        return \"summarize\";\n    }\n    return \"generate\";\n}\nasync function callTools(inputs, model, tools, retries) {\n    inputs.body.tools = tools;\n    inputs.body.toolConfig = {\n        functionCallingConfig: {\n            mode: \"ANY\",\n        },\n    };\n    const response = await callGemini(inputs, model, (response) => {\n        const r = response;\n        if (r.candidates?.at(0)?.content)\n            return;\n        return err(\"No content\");\n    }, retries);\n    if (!ok(response)) {\n        return toLLMContent(\"TODO: Handle Gemini error response\");\n    }\n    const r = response;\n    return r.candidates?.at(0)?.content || toLLMContent(\"No valid response\");\n}\nasync function generate(inputs, model, responseManager, retries) {\n    const response = await callGemini(inputs, model, (response) => {\n        return responseManager.parse(response);\n    }, retries);\n    if (!ok(response)) {\n        return response;\n    }\n    else {\n        return {\n            product: toLLMContent(responseManager.body, \"model\"),\n            response: responseManager.response,\n        };\n    }\n}\nasync function invoke({ id, work: context, description: instruction, model, toolManager, summarize, chat, makeList, }) {\n    // TODO: Make this a parameter.\n    const retries = 5;\n    const tools = toolManager.list();\n    const mode = computeWorkMode(tools, summarize);\n    const responseManager = new StructuredResponse(id, chat);\n    const inputs = {\n        body: {\n            contents: responseManager.addPrompt(context, prompt(instruction, mode, chat)),\n            systemInstruction: responseManager.instruction(),\n            safetySettings: defaultSafetySettings(),\n        },\n    };\n    if (mode === \"call-tools\") {\n        const product = await callTools(inputs, model, tools, retries);\n        return { product };\n    }\n    else {\n        let product = null;\n        if (makeList) {\n            const generating = await new GeminiPrompt({\n                body: {\n                    contents: [...context, listPrompt(prompt(instruction, mode, chat))],\n                    safetySettings: defaultSafetySettings(),\n                    generationConfig: {\n                        responseSchema: listSchema(),\n                        responseMimeType: \"application/json\",\n                    },\n                },\n            }, {\n                toolManager,\n            }).invoke();\n            if (!ok(generating))\n                return generating;\n            const list = toList(generating.last);\n            if (!ok(list))\n                return list;\n            product = list;\n        }\n        else {\n            const result = await generate(inputs, model, responseManager, retries);\n            if (\"$error\" in result) {\n                return result;\n            }\n            product = result.product;\n        }\n        return { product, epilog: responseManager.epilog };\n    }\n}\nfunction toList(content) {\n    const jsonPart = content.parts.at(0);\n    if (!jsonPart || !(\"json\" in jsonPart)) {\n        // TODO: Error recovery\n        return err(`Gemini generated invalid list`);\n    }\n    const response = jsonPart.json;\n    return {\n        parts: [\n            {\n                id: generateId(),\n                list: response.list.map((item) => {\n                    return { content: [toLLMContent(item, \"model\")] };\n                }),\n            },\n        ],\n    };\n}\nfunction listSchema() {\n    return {\n        type: \"object\",\n        properties: {\n            list: {\n                type: \"array\",\n                description: \"The list of results\",\n                items: {\n                    type: \"string\",\n                    description: \"Result list item as markdown text\",\n                },\n            },\n        },\n        required: [\"list\"],\n    };\n}\nfunction listPrompt(content) {\n    return llm `\n  ${content}\n\n  Output as a list of items, each item must be markdown text.\n`.asContent();\n}\nfunction prompt(description, mode, chat) {\n    const preamble = llm `\n${description}\n\n`;\n    const postamble = `\n\nToday is ${new Date().toLocaleString(\"en-US\", {\n        month: \"long\",\n        day: \"numeric\",\n        year: \"numeric\",\n        hour: \"numeric\",\n        minute: \"2-digit\",\n    })}`;\n    switch (mode) {\n        case \"summarize\":\n            return llm ` \n${preamble}\nSummarize the research results to fulfill the specified task.\n${postamble}`.asContent();\n        case \"call-tools\":\n            return llm `\n${preamble}\nGenerate multiple function calls to fulfill the specified task.\n${postamble}`.asContent();\n        case \"generate\":\n            return llm `\n${preamble}\nProvide the response that fulfills the specified task.\n${postamble}`.asContent();\n    }\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                work: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Work\",\n                },\n                description: {\n                    type: \"object\",\n                    behavior: [\"llm-content\"],\n                    title: \"Job Description\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"object\",\n                    behavior: [\"llm-content\"],\n                    title: \"Work Product\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "worker-worker",
        "source": {
          "code": "/**\n * @fileoverview Performs assigned task. Part of the worker.\n */\nimport invokeBoard from \"@invoke\";\nimport output from \"@output\";\n\nimport { toText, toLLMContent, ok, err, llm, generateId } from \"./utils\";\nimport {\n  type GeminiSchema,\n  type GeminiInputs,\n  type GeminiOutputs,\n  type Tool,\n  defaultSafetySettings,\n  type GeminiAPIOutputs,\n} from \"./gemini\";\nimport { callGemini } from \"./gemini-client\";\nimport { GeminiPrompt } from \"./gemini-prompt\";\nimport { StructuredResponse } from \"./structured-response\";\nimport { ToolManager } from \"./tool-manager\";\nimport { fanOutContext } from \"./lists\";\n\nexport { invoke as default, describe };\n\ntype Inputs = {\n  id: string;\n  work: LLMContent[];\n  description: LLMContent;\n  model: string;\n  toolManager: ToolManager;\n  summarize: boolean;\n  chat: boolean;\n  makeList: boolean;\n};\n\ntype Outputs = {\n  product: LLMContent;\n  response?: LLMContent;\n  epilog?: string;\n};\n\ntype WorkMode = \"generate\" | \"call-tools\" | \"summarize\";\n\nfunction computeWorkMode(tools: Tool[], summarize: boolean): WorkMode {\n  if (tools.length > 0) {\n    return \"call-tools\";\n  }\n  if (summarize) {\n    return \"summarize\";\n  }\n  return \"generate\";\n}\n\nasync function callTools(\n  inputs: Omit<GeminiInputs, \"model\">,\n  model: string,\n  tools: Tool[],\n  retries: number\n): Promise<LLMContent> {\n  inputs.body.tools = tools;\n  inputs.body.toolConfig = {\n    functionCallingConfig: {\n      mode: \"ANY\",\n    },\n  };\n  const response = await callGemini(\n    inputs,\n    model,\n    (response) => {\n      const r = response as GeminiAPIOutputs;\n      if (r.candidates?.at(0)?.content) return;\n      return err(\"No content\");\n    },\n    retries\n  );\n  if (!ok(response)) {\n    return toLLMContent(\"TODO: Handle Gemini error response\");\n  }\n  const r = response as GeminiAPIOutputs;\n  return r.candidates?.at(0)?.content || toLLMContent(\"No valid response\");\n}\n\nasync function generate(\n  inputs: Omit<GeminiInputs, \"model\">,\n  model: string,\n  responseManager: StructuredResponse,\n  retries: number\n): Promise<Outcome<Outputs>> {\n  const response = await callGemini(\n    inputs,\n    model,\n    (response) => {\n      return responseManager.parse(response);\n    },\n    retries\n  );\n  if (!ok(response)) {\n    return response;\n  } else {\n    return {\n      product: toLLMContent(responseManager.body, \"model\"),\n      response: responseManager.response!,\n    };\n  }\n}\n\nasync function invoke({\n  id,\n  work: context,\n  description: instruction,\n  model,\n  toolManager,\n  summarize,\n  chat,\n  makeList,\n}: Inputs): Promise<Outcome<Outputs>> {\n  // TODO: Make this a parameter.\n  const retries = 5;\n  const tools = toolManager.list();\n  const mode = computeWorkMode(tools, summarize);\n  const responseManager = new StructuredResponse(id, chat);\n  const inputs: Omit<GeminiInputs, \"model\"> = {\n    body: {\n      contents: responseManager.addPrompt(\n        context,\n        prompt(instruction, mode, chat)\n      ),\n      systemInstruction: responseManager.instruction(),\n      safetySettings: defaultSafetySettings(),\n    },\n  };\n  if (mode === \"call-tools\") {\n    const product = await callTools(inputs, model, tools, retries);\n    return { product };\n  } else {\n    let product: LLMContent | null = null;\n    if (makeList) {\n      const generating = await new GeminiPrompt(\n        {\n          body: {\n            contents: [...context, listPrompt(prompt(instruction, mode, chat))],\n            safetySettings: defaultSafetySettings(),\n            generationConfig: {\n              responseSchema: listSchema(),\n              responseMimeType: \"application/json\",\n            },\n          },\n        },\n        {\n          toolManager,\n        }\n      ).invoke();\n      if (!ok(generating)) return generating;\n\n      const list = toList(generating.last);\n      if (!ok(list)) return list;\n\n      product = list;\n    } else {\n      const result = await generate(inputs, model, responseManager, retries);\n      if (\"$error\" in result) {\n        return result;\n      }\n      product = result.product;\n    }\n    return { product, epilog: responseManager.epilog };\n  }\n}\n\ntype ListResponse = {\n  list: string[];\n};\n\nfunction toList(content: LLMContent): Outcome<LLMContent> {\n  const jsonPart = content.parts.at(0);\n  if (!jsonPart || !(\"json\" in jsonPart)) {\n    // TODO: Error recovery\n    return err(`Gemini generated invalid list`);\n  }\n  const response = jsonPart.json as ListResponse;\n  return {\n    parts: [\n      {\n        id: generateId(),\n        list: response.list.map((item) => {\n          return { content: [toLLMContent(item, \"model\")] };\n        }),\n      },\n    ],\n  };\n}\n\nfunction listSchema(): GeminiSchema {\n  return {\n    type: \"object\",\n    properties: {\n      list: {\n        type: \"array\",\n        description: \"The list of results\",\n        items: {\n          type: \"string\",\n          description: \"Result list item as markdown text\",\n        },\n      },\n    },\n    required: [\"list\"],\n  };\n}\n\nfunction listPrompt(content: LLMContent): LLMContent {\n  return llm`\n  ${content}\n\n  Output as a list of items, each item must be markdown text.\n`.asContent();\n}\n\nfunction prompt(\n  description: LLMContent,\n  mode: WorkMode,\n  chat: boolean\n): LLMContent {\n  const preamble = llm`\n${description}\n\n`;\n  const postamble = `\n\nToday is ${new Date().toLocaleString(\"en-US\", {\n    month: \"long\",\n    day: \"numeric\",\n    year: \"numeric\",\n    hour: \"numeric\",\n    minute: \"2-digit\",\n  })}`;\n\n  switch (mode) {\n    case \"summarize\":\n      return llm` \n${preamble}\nSummarize the research results to fulfill the specified task.\n${postamble}`.asContent();\n\n    case \"call-tools\":\n      return llm`\n${preamble}\nGenerate multiple function calls to fulfill the specified task.\n${postamble}`.asContent();\n\n    case \"generate\":\n      return llm`\n${preamble}\nProvide the response that fulfills the specified task.\n${postamble}`.asContent();\n  }\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        work: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Work\",\n        },\n        description: {\n          type: \"object\",\n          behavior: [\"llm-content\"],\n          title: \"Job Description\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"object\",\n          behavior: [\"llm-content\"],\n          title: \"Work Product\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Performs assigned task. Part of the worker.",
        "runnable": false
      }
    },
    "gemini-client": {
      "code": "import invokeGemini, {} from \"./gemini\";\nimport { ok, err } from \"./utils\";\nexport { callGemini };\nasync function callGemini(inputs, model, validator, retries) {\n    // TODO: Add more nuanced logic around retries\n    for (let i = 0; i < retries; ++i) {\n        const nextStep = i == retries ? \"bailing\" : \"will retry\";\n        const response = await invokeGemini(inputs);\n        if (!ok(response)) {\n            console.error(`Error from model, ${nextStep}`, response.$error);\n        }\n        else {\n            const validating = validator(response);\n            if (!ok(validating)) {\n                console.error(`Validation error, ${nextStep}`, validating.$error);\n                continue;\n            }\n            return response;\n        }\n    }\n    return err(`Failed to get valid response after ${retries} tries`);\n}\n",
      "metadata": {
        "title": "gemini-client",
        "source": {
          "code": "import invokeGemini, { type GeminiInputs, type GeminiOutputs } from \"./gemini\";\nimport { ok, err } from \"./utils\";\n\nexport type ValidatorFunction = (response: GeminiOutputs) => Outcome<void>;\n\nexport { callGemini };\n\nasync function callGemini(\n  inputs: Omit<GeminiInputs, \"model\">,\n  model: string,\n  validator: ValidatorFunction,\n  retries: number\n): Promise<Outcome<GeminiOutputs>> {\n  // TODO: Add more nuanced logic around retries\n  for (let i = 0; i < retries; ++i) {\n    const nextStep = i == retries ? \"bailing\" : \"will retry\";\n    const response = await invokeGemini(inputs);\n    if (!ok(response)) {\n      console.error(`Error from model, ${nextStep}`, response.$error);\n    } else {\n      const validating = validator(response);\n      if (!ok(validating)) {\n        console.error(`Validation error, ${nextStep}`, validating.$error);\n        continue;\n      }\n      return response;\n    }\n  }\n  return err(`Failed to get valid response after ${retries} tries`);\n}\n",
          "language": "typescript"
        },
        "description": "",
        "runnable": false
      }
    },
    "agent-main": {
      "code": "/**\n * @fileoverview The main body of the agent\n */\nimport output from \"@output\";\nimport {} from \"./common\";\nimport { ToolManager } from \"./tool-manager\";\nimport workerWorker from \"./worker-worker\";\nimport { report } from \"./output\";\nimport { defaultLLMContent, toText, err, endsWithRole, ok, llm } from \"./utils\";\nimport invokeGraph from \"@invoke\";\nimport { Template } from \"./template\";\nimport { ArgumentNameGenerator } from \"./introducer\";\nimport { fanOutContext } from \"./lists\";\nexport { invoke as default, describe };\nfunction toLLMContent(text, role = \"user\") {\n    return { parts: [{ text }], role };\n}\nasync function invoke({ context }) {\n    console.log(\"AGENT MAIN\", context);\n    let { id, description, context: initialContext, model, defaultModel, tools, chat, makeList, work: workContext, params, } = context;\n    if (!description) {\n        const $error = \"No instruction supplied\";\n        await report({\n            actor: \"Text Generator\",\n            name: $error,\n            category: \"Runtime error\",\n            details: `In order to run, I need to have an instruction.`,\n        });\n        return { $error };\n    }\n    const template = new Template(description);\n    const toolManager = new ToolManager(new ArgumentNameGenerator());\n    const substituting = await template.substitute(params, async ({ path: url }) => toolManager.addTool(url));\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    description = substituting;\n    if (!(await toolManager.initialize(tools))) {\n        const $error = `Problem initializing tools. \nThe following errors were encountered: ${toolManager.errors.join(\",\")}`;\n        console.error(\"MAIN ERROR\", $error, toolManager.errors);\n        return { $error };\n    }\n    const { userEndedChat, userInputs, last } = context;\n    if (userEndedChat) {\n        if (!last) {\n            return err(\"Chat ended without any work\");\n        }\n        return {\n            done: [...initialContext, last],\n        };\n    }\n    let epilog;\n    const result = await fanOutContext(description, workContext.length > 0 ? workContext : initialContext, async (description, work) => {\n        // 1) Make first attempt to make text\n        const response = await workerWorker({\n            id,\n            description,\n            work,\n            model,\n            toolManager,\n            summarize: false,\n            chat,\n            makeList,\n        });\n        if (!ok(response)) {\n            console.error(\"ERROR FROM WORKER\", response.$error);\n            return response;\n        }\n        const workerResponse = response.product;\n        epilog = response.epilog;\n        // 2) Call tools\n        const toolResults = [];\n        await toolManager.processResponse(workerResponse, async ($board, args) => {\n            const result = await invokeGraph({\n                $board,\n                ...args,\n            });\n            toolResults.push(result);\n        });\n        // 3) Handle tool results\n        if (toolResults.length > 0) {\n            const summary = await workerWorker({\n                id,\n                description,\n                work: [\n                    ...toolResults.map((toolResult) => toLLMContent(JSON.stringify(toolResult))),\n                ],\n                model,\n                toolManager,\n                summarize: true,\n                chat: false,\n                makeList,\n            });\n            if (!ok(summary)) {\n                console.error(\"ERROR FROM SUMMARY\", summary.$error);\n                return summary;\n            }\n            const summaryResponse = summary.product;\n            epilog = summary.epilog;\n            return summaryResponse;\n        }\n        return workerResponse;\n    });\n    if (!ok(result))\n        return result;\n    // 4) Handle chat.\n    if (chat) {\n        const last = result.at(-1);\n        epilog ??= \"Please provide feedback on the draft\";\n        await output({\n            schema: {\n                type: \"object\",\n                properties: {\n                    \"a-product\": {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                        title: \"Draft\",\n                    },\n                    \"b-message\": {\n                        type: \"string\",\n                        title: \"\",\n                        format: \"markdown\",\n                    },\n                },\n            },\n            $metadata: {\n                title: \"Writer\",\n                description: \"Asking for feedback on a draft\",\n                icon: \"generative-text\",\n            },\n            \"b-message\": epilog,\n            \"a-product\": last,\n        });\n        const { userInputs } = context;\n        if (!userEndedChat) {\n            const toInput = {\n                type: \"object\",\n                properties: {\n                    request: {\n                        type: \"object\",\n                        title: \"Please provide feedback\",\n                        description: \"Provide feedback or click submit to continue\",\n                        behavior: [\"transient\", \"llm-content\"],\n                        examples: [defaultLLMContent()],\n                    },\n                },\n            };\n            return {\n                toInput,\n                context: {\n                    ...context,\n                    work: result,\n                    last,\n                },\n            };\n        }\n    }\n    // 5) Fall through to default response.\n    return { done: result };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"object\",\n                    title: \"Agent Context\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                toInput: {\n                    type: \"object\",\n                    title: \"Input Schema\",\n                },\n                context: {\n                    type: \"object\",\n                    title: \"Agent Context\",\n                },\n                done: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Done\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "agent-main",
        "source": {
          "code": "/**\n * @fileoverview The main body of the agent\n */\nimport output from \"@output\";\nimport { type AgentContext, type DescriberResult } from \"./common\";\nimport { ToolManager } from \"./tool-manager\";\nimport workerWorker from \"./worker-worker\";\nimport { report } from \"./output\";\nimport { defaultLLMContent, toText, err, endsWithRole, ok, llm } from \"./utils\";\nimport invokeGraph from \"@invoke\";\nimport { Template } from \"./template\";\nimport { ArgumentNameGenerator } from \"./introducer\";\nimport { fanOutContext } from \"./lists\";\n\nexport { invoke as default, describe };\n\ntype Inputs = {\n  context: AgentContext;\n};\n\ntype Outputs = {\n  $error?: string;\n  context?: AgentContext;\n  toInput?: Schema;\n  done?: LLMContent[];\n};\n\nfunction toLLMContent(\n  text: string,\n  role: LLMContent[\"role\"] = \"user\"\n): LLMContent {\n  return { parts: [{ text }], role };\n}\n\nasync function invoke({ context }: Inputs): Promise<Outputs> {\n  console.log(\"AGENT MAIN\", context);\n  let {\n    id,\n    description,\n    context: initialContext,\n    model,\n    defaultModel,\n    tools,\n    chat,\n    makeList,\n    work: workContext,\n    params,\n  } = context;\n  if (!description) {\n    const $error = \"No instruction supplied\";\n    await report({\n      actor: \"Text Generator\",\n      name: $error,\n      category: \"Runtime error\",\n      details: `In order to run, I need to have an instruction.`,\n    });\n    return { $error };\n  }\n  const template = new Template(description);\n  const toolManager = new ToolManager(new ArgumentNameGenerator());\n  const substituting = await template.substitute(\n    params,\n    async ({ path: url }) => toolManager.addTool(url)\n  );\n  if (!ok(substituting)) {\n    return substituting;\n  }\n  description = substituting;\n\n  if (!(await toolManager.initialize(tools))) {\n    const $error = `Problem initializing tools. \nThe following errors were encountered: ${toolManager.errors.join(\",\")}`;\n    console.error(\"MAIN ERROR\", $error, toolManager.errors);\n    return { $error };\n  }\n\n  const { userEndedChat, userInputs, last } = context;\n  if (userEndedChat) {\n    if (!last) {\n      return err(\"Chat ended without any work\");\n    }\n    return {\n      done: [...initialContext, last],\n    };\n  }\n  let epilog: string | undefined;\n  const result = await fanOutContext(\n    description,\n    workContext.length > 0 ? workContext : initialContext,\n    async (description, work) => {\n      // 1) Make first attempt to make text\n      const response = await workerWorker({\n        id,\n        description,\n        work,\n        model,\n        toolManager,\n        summarize: false,\n        chat,\n        makeList,\n      });\n      if (!ok(response)) {\n        console.error(\"ERROR FROM WORKER\", response.$error);\n        return response;\n      }\n      const workerResponse = response.product;\n      epilog = response.epilog;\n\n      // 2) Call tools\n      const toolResults: object[] = [];\n      await toolManager.processResponse(\n        workerResponse,\n        async ($board, args) => {\n          const result = await invokeGraph({\n            $board,\n            ...args,\n          });\n          toolResults.push(result);\n        }\n      );\n\n      // 3) Handle tool results\n      if (toolResults.length > 0) {\n        const summary = await workerWorker({\n          id,\n          description,\n          work: [\n            ...toolResults.map((toolResult) =>\n              toLLMContent(JSON.stringify(toolResult))\n            ),\n          ],\n          model,\n          toolManager,\n          summarize: true,\n          chat: false,\n          makeList,\n        });\n        if (!ok(summary)) {\n          console.error(\"ERROR FROM SUMMARY\", summary.$error);\n          return summary;\n        }\n        const summaryResponse = summary.product;\n        epilog = summary.epilog;\n        return summaryResponse;\n      }\n\n      return workerResponse;\n    }\n  );\n  if (!ok(result)) return result;\n\n  // 4) Handle chat.\n  if (chat) {\n    const last = result.at(-1)!;\n    epilog ??= \"Please provide feedback on the draft\";\n    await output({\n      schema: {\n        type: \"object\",\n        properties: {\n          \"a-product\": {\n            type: \"object\",\n            behavior: [\"llm-content\"],\n            title: \"Draft\",\n          },\n          \"b-message\": {\n            type: \"string\",\n            title: \"\",\n            format: \"markdown\",\n          },\n        },\n      },\n      $metadata: {\n        title: \"Writer\",\n        description: \"Asking for feedback on a draft\",\n        icon: \"generative-text\",\n      },\n      \"b-message\": epilog,\n      \"a-product\": last,\n    });\n\n    const { userInputs } = context;\n    if (!userEndedChat) {\n      const toInput: Schema = {\n        type: \"object\",\n        properties: {\n          request: {\n            type: \"object\",\n            title: \"Please provide feedback\",\n            description: \"Provide feedback or click submit to continue\",\n            behavior: [\"transient\", \"llm-content\"],\n            examples: [defaultLLMContent()],\n          },\n        },\n      };\n      return {\n        toInput,\n        context: {\n          ...context,\n          work: result,\n          last,\n        },\n      };\n    }\n  }\n\n  // 5) Fall through to default response.\n  return { done: result };\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"object\",\n          title: \"Agent Context\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        toInput: {\n          type: \"object\",\n          title: \"Input Schema\",\n        },\n        context: {\n          type: \"object\",\n          title: \"Agent Context\",\n        },\n        done: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Done\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "The main body of the agent",
        "runnable": true
      }
    },
    "researcher": {
      "code": "/**\n * @fileoverview Searching the Internet according to your plan.\n */\nimport { ToolManager } from \"./tool-manager\";\nimport { Template } from \"./template\";\nimport invokeGraph from \"@invoke\";\nimport invokeGemini, { defaultSafetySettings, } from \"./gemini\";\nimport { ok, err, llm, toLLMContent, toText, addUserTurn } from \"./utils\";\nimport { report } from \"./output\";\nimport {} from \"./common\";\nimport { ArgumentNameGenerator } from \"./introducer\";\nexport { invoke as default, describe };\nconst RESEARCH_TOOLS = [\n    {\n        url: \"./tools.bgl.json#module:search-web\",\n        title: \"Search Web\",\n    },\n    {\n        url: \"./tools.bgl.json#module:search-wikipedia\",\n        title: \"Search Wikipedia\",\n    },\n    {\n        url: \"./tools.bgl.json#module:get-webpage\",\n        title: \"Get Webpage\",\n    },\n    {\n        url: \"./tools.bgl.json#module:search-maps\",\n        title: \"Search Maps\",\n    },\n];\nconst RESEARCH_MODEL = \"gemini-2.0-flash\";\nconst MAX_ITERATIONS = 7;\nfunction systemInstruction(first) {\n    const which = first ? \"first\" : \"next\";\n    return `You are a researcher.\n  \nYour job is to use the provided research plan to produce raw research that will be later turned into a detailed research report.\nYou are tasked with finding as much of relevant information as possible.\n\nYou examine the conversation context so far and come up with the ${which} step to produce the report, \nusing the conversation context as the the guide of steps taken so far and the outcomes recorded.\n\nYou do not ask user for feedback. You do not try to have a conversation with the user. \nYou know that the user will only ask you to proceed to next step.\n\nYour next step consists of answering two questions.\n\nFirst, ask yourself \"am I done?\" -- looking back at all that you've researched and the plan, \ndo you have enough to produce the detailed report?\n\nSecond, provide a response. Your response must contain two parts:\nThought: a brief plain text reasoning why this is the right ${which} step and a description of what you will do in plain English.\nAction: invoking the tools are your disposal, more than one if necessary. If you're done, do not invoke any tools.`;\n}\nfunction researcherPrompt(contents, plan, tools, first) {\n    return {\n        model: RESEARCH_MODEL,\n        body: {\n            contents: addUserTurn(llm `\nDo the research according to this plan:\n\n---\n\n${plan}\n\n---\n`.asContent(), contents),\n            tools,\n            systemInstruction: toLLMContent(systemInstruction(first)),\n            safetySettings: defaultSafetySettings(),\n        },\n    };\n}\nfunction reportWriterInstruction() {\n    return `You are a research report writer. \nYour teammates produced a wealth of raw research according to the supplied plan.\n\nYour task is to take the raw research and write a thorough, detailed research report that captures it in a way that follows the plan. Use markdown.\n\nA report must additionally contain references to the source (always cite your sources).`;\n}\nfunction reportWriterPrompt(plan, research) {\n    return {\n        model: RESEARCH_MODEL,\n        body: {\n            contents: [toLLMContent(research.join(\"\\n\\n\"))],\n            systemInstruction: toLLMContent(reportWriterInstruction()),\n            safetySettings: defaultSafetySettings(),\n        },\n    };\n}\nasync function thought(response, iteration) {\n    const first = response.parts?.at(0);\n    if (!first || !(\"text\" in first)) {\n        return;\n    }\n    await report({\n        actor: \"Researcher\",\n        category: `Progress report, iteration ${iteration + 1}`,\n        name: \"Thought\",\n        icon: \"generative\",\n        details: first.text\n            .replace(/^Thought: ?/gm, \"\")\n            .replace(/^Action:.*$/gm, \"\")\n            .trim(),\n    });\n}\nasync function invoke({ context, plan, summarize, ...params }) {\n    const tools = RESEARCH_TOOLS.map((descriptor) => descriptor.url);\n    const toolManager = new ToolManager(new ArgumentNameGenerator());\n    let content = context || [toLLMContent(\"Start the research\")];\n    const template = new Template(plan);\n    const substituting = await template.substitute(params, async ({ path: url }) => toolManager.addTool(url));\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    if (!toolManager.hasTools()) {\n        // If no tools supplied (legacy case, actually), initialize\n        // with a set of default tools.\n        const initializing = await toolManager.initialize(tools);\n        if (!initializing) {\n            return err(\"Unable to initialize tools\");\n        }\n    }\n    plan = substituting;\n    const research = [];\n    for (let i = 0; i <= MAX_ITERATIONS; i++) {\n        const askingGemini = await invokeGemini(researcherPrompt(content, plan, toolManager.list(), i === 0));\n        if (!ok(askingGemini)) {\n            return askingGemini;\n        }\n        if (\"context\" in askingGemini) {\n            return err(`Unexpected \"context\" response`);\n        }\n        const response = askingGemini.candidates.at(0)?.content;\n        if (!response) {\n            return err(\"No actionable response\");\n        }\n        await thought(response, i);\n        const toolResponses = [];\n        await toolManager.processResponse(response, async ($board, args) => {\n            toolResponses.push(JSON.stringify(await invokeGraph({ $board, ...args })));\n        });\n        if (toolResponses.length === 0) {\n            break;\n        }\n        research.push(...toolResponses);\n        content = [...content, response, toLLMContent(toolResponses.join(\"\\n\\n\"))];\n    }\n    if (research.length === 0) {\n        await report({\n            actor: \"Researcher\",\n            category: \"Error\",\n            name: \"Error\",\n            details: \"I was unable to obtain any research results\",\n        });\n        return { context };\n    }\n    if (summarize) {\n        const producingReport = await invokeGemini(reportWriterPrompt(plan, research));\n        if (!ok(producingReport)) {\n            return producingReport;\n        }\n        if (\"context\" in producingReport) {\n            return err(`Unexpected \"context\" response`);\n        }\n        const response = producingReport.candidates.at(0)?.content;\n        if (!response) {\n            return err(\"No actionable response\");\n        }\n        return { context: [...(context || []), response] };\n    }\n    return { context: [...(context || []), toLLMContent(research.join(\"\\n\\n\"))] };\n}\nfunction toOxfordList(items) {\n    if (items.length === 0)\n        return \"\";\n    if (items.length === 1)\n        return items[0];\n    if (items.length === 2)\n        return items.join(\" and \");\n    const lastItem = items.pop();\n    return `${items.join(\", \")}, and ${lastItem}`;\n}\nfunction researchExample() {\n    const type = \"tool\";\n    const tools = RESEARCH_TOOLS.map(({ url: path, title }) => Template.part({ title, path, type }));\n    return [\n        JSON.stringify({\n            plan: toLLMContent(`Research the topic provided using ${toOxfordList(tools)} tools`),\n        }),\n    ];\n}\nasync function describe({ inputs: { plan } }) {\n    const template = new Template(plan);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                plan: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"Research Plan\",\n                    description: \"Provide an outline of what to research, what areas to cover, etc.\",\n                },\n                summarize: {\n                    type: \"boolean\",\n                    behavior: [\"config\", \"hint-preview\"],\n                    icon: \"summarize\",\n                    title: \"Summarize research\",\n                    description: \"If checked, the Researcher will summarize the results of the research and only pass the research summary along.\",\n                },\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n            additionalProperties: false,\n            examples: researchExample(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"main-port\", \"hint-text\"],\n                },\n            },\n            additionalProperties: false,\n        },\n        title: \"Do deep research\",\n        description: \"Do deep research according to your plan\",\n        metadata: {\n            icon: \"generative\",\n            tags: [\"quick-access\", \"generative\"],\n            order: 101,\n        },\n    };\n}\n",
      "metadata": {
        "title": "Do deep research",
        "source": {
          "code": "/**\n * @fileoverview Searching the Internet according to your plan.\n */\nimport { ToolManager, type ToolDescriptor } from \"./tool-manager\";\nimport { Template } from \"./template\";\nimport invokeGraph from \"@invoke\";\nimport invokeGemini, {\n  type GeminiInputs,\n  type Tool,\n  defaultSafetySettings,\n} from \"./gemini\";\nimport { ok, err, llm, toLLMContent, toText, addUserTurn } from \"./utils\";\nimport { report } from \"./output\";\nimport { type Params } from \"./common\";\nimport { ArgumentNameGenerator } from \"./introducer\";\n\nexport { invoke as default, describe };\n\nexport type ResearcherInputs = {\n  context?: LLMContent[];\n  plan: LLMContent;\n  summarize: boolean;\n} & Params;\n\nexport type DefaultToolDescriptor = {\n  url: string;\n  title: string;\n};\n\nconst RESEARCH_TOOLS: DefaultToolDescriptor[] = [\n  {\n    url: \"./tools.bgl.json#module:search-web\",\n    title: \"Search Web\",\n  },\n  {\n    url: \"./tools.bgl.json#module:search-wikipedia\",\n    title: \"Search Wikipedia\",\n  },\n  {\n    url: \"./tools.bgl.json#module:get-webpage\",\n    title: \"Get Webpage\",\n  },\n  {\n    url: \"./tools.bgl.json#module:search-maps\",\n    title: \"Search Maps\",\n  },\n];\n\nconst RESEARCH_MODEL = \"gemini-2.0-flash\";\n\nconst MAX_ITERATIONS = 7;\n\nfunction systemInstruction(first: boolean): string {\n  const which = first ? \"first\" : \"next\";\n  return `You are a researcher.\n  \nYour job is to use the provided research plan to produce raw research that will be later turned into a detailed research report.\nYou are tasked with finding as much of relevant information as possible.\n\nYou examine the conversation context so far and come up with the ${which} step to produce the report, \nusing the conversation context as the the guide of steps taken so far and the outcomes recorded.\n\nYou do not ask user for feedback. You do not try to have a conversation with the user. \nYou know that the user will only ask you to proceed to next step.\n\nYour next step consists of answering two questions.\n\nFirst, ask yourself \"am I done?\" -- looking back at all that you've researched and the plan, \ndo you have enough to produce the detailed report?\n\nSecond, provide a response. Your response must contain two parts:\nThought: a brief plain text reasoning why this is the right ${which} step and a description of what you will do in plain English.\nAction: invoking the tools are your disposal, more than one if necessary. If you're done, do not invoke any tools.`;\n}\n\nfunction researcherPrompt(\n  contents: LLMContent[],\n  plan: LLMContent,\n  tools: Tool[],\n  first: boolean\n): GeminiInputs {\n  return {\n    model: RESEARCH_MODEL,\n    body: {\n      contents: addUserTurn(\n        llm`\nDo the research according to this plan:\n\n---\n\n${plan}\n\n---\n`.asContent(),\n        contents\n      ),\n      tools,\n      systemInstruction: toLLMContent(systemInstruction(first)),\n      safetySettings: defaultSafetySettings(),\n    },\n  };\n}\n\nfunction reportWriterInstruction() {\n  return `You are a research report writer. \nYour teammates produced a wealth of raw research according to the supplied plan.\n\nYour task is to take the raw research and write a thorough, detailed research report that captures it in a way that follows the plan. Use markdown.\n\nA report must additionally contain references to the source (always cite your sources).`;\n}\n\nfunction reportWriterPrompt(\n  plan: LLMContent,\n  research: string[]\n): GeminiInputs {\n  return {\n    model: RESEARCH_MODEL,\n    body: {\n      contents: [toLLMContent(research.join(\"\\n\\n\"))],\n      systemInstruction: toLLMContent(reportWriterInstruction()),\n      safetySettings: defaultSafetySettings(),\n    },\n  };\n}\n\nasync function thought(response: LLMContent, iteration: number) {\n  const first = response.parts?.at(0);\n  if (!first || !(\"text\" in first)) {\n    return;\n  }\n  await report({\n    actor: \"Researcher\",\n    category: `Progress report, iteration ${iteration + 1}`,\n    name: \"Thought\",\n    icon: \"generative\",\n    details: first.text\n      .replace(/^Thought: ?/gm, \"\")\n      .replace(/^Action:.*$/gm, \"\")\n      .trim(),\n  });\n}\n\nasync function invoke({\n  context,\n  plan,\n  summarize,\n  ...params\n}: ResearcherInputs) {\n  const tools = RESEARCH_TOOLS.map((descriptor) => descriptor.url);\n  const toolManager = new ToolManager(new ArgumentNameGenerator());\n  let content = context || [toLLMContent(\"Start the research\")];\n\n  const template = new Template(plan);\n  const substituting = await template.substitute(\n    params,\n    async ({ path: url }) => toolManager.addTool(url)\n  );\n  if (!ok(substituting)) {\n    return substituting;\n  }\n  if (!toolManager.hasTools()) {\n    // If no tools supplied (legacy case, actually), initialize\n    // with a set of default tools.\n    const initializing = await toolManager.initialize(tools);\n    if (!initializing) {\n      return err(\"Unable to initialize tools\");\n    }\n  }\n  plan = substituting;\n\n  const research: string[] = [];\n  for (let i = 0; i <= MAX_ITERATIONS; i++) {\n    const askingGemini = await invokeGemini(\n      researcherPrompt(content, plan, toolManager.list(), i === 0)\n    );\n\n    if (!ok(askingGemini)) {\n      return askingGemini;\n    }\n    if (\"context\" in askingGemini) {\n      return err(`Unexpected \"context\" response`);\n    }\n    const response = askingGemini.candidates.at(0)?.content;\n    if (!response) {\n      return err(\"No actionable response\");\n    }\n    await thought(response, i);\n\n    const toolResponses: string[] = [];\n    await toolManager.processResponse(response, async ($board, args) => {\n      toolResponses.push(\n        JSON.stringify(await invokeGraph({ $board, ...args }))\n      );\n    });\n    if (toolResponses.length === 0) {\n      break;\n    }\n    research.push(...toolResponses);\n    content = [...content, response, toLLMContent(toolResponses.join(\"\\n\\n\"))];\n  }\n  if (research.length === 0) {\n    await report({\n      actor: \"Researcher\",\n      category: \"Error\",\n      name: \"Error\",\n      details: \"I was unable to obtain any research results\",\n    });\n    return { context };\n  }\n  if (summarize) {\n    const producingReport = await invokeGemini(\n      reportWriterPrompt(plan, research)\n    );\n    if (!ok(producingReport)) {\n      return producingReport;\n    }\n    if (\"context\" in producingReport) {\n      return err(`Unexpected \"context\" response`);\n    }\n    const response = producingReport.candidates.at(0)?.content;\n    if (!response) {\n      return err(\"No actionable response\");\n    }\n    return { context: [...(context || []), response] };\n  }\n  return { context: [...(context || []), toLLMContent(research.join(\"\\n\\n\"))] };\n}\n\ntype DescribeInputs = {\n  inputs: {\n    plan: LLMContent;\n  };\n};\n\nfunction toOxfordList(items: string[]): string {\n  if (items.length === 0) return \"\";\n  if (items.length === 1) return items[0];\n  if (items.length === 2) return items.join(\" and \");\n  const lastItem = items.pop();\n  return `${items.join(\", \")}, and ${lastItem}`;\n}\n\nfunction researchExample(): string[] {\n  const type = \"tool\";\n  const tools = RESEARCH_TOOLS.map(({ url: path, title }) =>\n    Template.part({ title, path, type })\n  );\n  return [\n    JSON.stringify({\n      plan: toLLMContent(\n        `Research the topic provided using ${toOxfordList(tools)} tools`\n      ),\n    }),\n  ];\n}\n\nasync function describe({ inputs: { plan } }: DescribeInputs) {\n  const template = new Template(plan);\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n        plan: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n          title: \"Research Plan\",\n          description:\n            \"Provide an outline of what to research, what areas to cover, etc.\",\n        },\n        summarize: {\n          type: \"boolean\",\n          behavior: [\"config\", \"hint-preview\"],\n          icon: \"summarize\",\n          title: \"Summarize research\",\n          description:\n            \"If checked, the Researcher will summarize the results of the research and only pass the research summary along.\",\n        },\n        ...template.schemas(),\n      },\n      behavior: [\"at-wireable\"],\n      ...template.requireds(),\n      additionalProperties: false,\n      examples: researchExample(),\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n          behavior: [\"main-port\", \"hint-text\"],\n        },\n      },\n      additionalProperties: false,\n    } satisfies Schema,\n    title: \"Do deep research\",\n    description: \"Do deep research according to your plan\",\n    metadata: {\n      icon: \"generative\",\n      tags: [\"quick-access\", \"generative\"],\n      order: 101,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Searching the Internet according to your plan.",
        "runnable": true
      }
    },
    "image-generator": {
      "code": "/**\n * @fileoverview Generates an image using supplied context.\n */\nimport invokeBoard from \"@invoke\";\nimport gemini, { defaultSafetySettings, } from \"./gemini\";\nimport { GeminiPrompt } from \"./gemini-prompt\";\nimport { err, ok, toLLMContent, toLLMContentInline, toText, addUserTurn, llm, } from \"./utils\";\nimport { Template } from \"./template\";\nimport { ToolManager } from \"./tool-manager\";\nimport { executeStep, } from \"./step-executor\";\nimport {} from \"./common\";\nimport { report } from \"./output\";\nimport { ArgumentNameGenerator } from \"./introducer\";\nimport { fanOutContext } from \"./lists\";\nconst MAKE_IMAGE_ICON = \"generative-image\";\nexport { invoke as default, describe };\nasync function callImageGen(imageInstruction) {\n    const executionInputs = {};\n    const encodedInstruction = btoa(unescape(encodeURIComponent(imageInstruction)));\n    executionInputs[\"image_prompt\"] = {\n        chunks: [\n            {\n                mimetype: \"text/plain\",\n                data: encodedInstruction,\n            },\n        ],\n    };\n    const inputParameters = [\"image_prompt\"];\n    const body = {\n        planStep: {\n            stepName: \"GenerateImage\",\n            modelApi: \"image_generation\",\n            inputParameters: inputParameters,\n            systemPrompt: \"\",\n        },\n        execution_inputs: executionInputs,\n    };\n    const response = await executeStep(body);\n    if (!ok(response)) {\n        return toLLMContent(\"Image generation failed: \" + response.$error);\n    }\n    let returnVal;\n    for (let value of Object.values(response.executionOutputs)) {\n        const mimetype = value.chunks[0].mimetype;\n        if (mimetype.startsWith(\"image\")) {\n            returnVal = toLLMContentInline(mimetype, value.chunks[0].data);\n        }\n    }\n    if (!returnVal) {\n        return toLLMContent(\"Error: No image returned from backend\");\n    }\n    return returnVal;\n}\nfunction gatheringRequest(contents, instruction, toolManager) {\n    const promptText = llm `\nAnalyze the instruction below and rather than following it, determine what information needs to be gathered to \ngenerate an accurate prompt for a text-to-image model in the next turn:\n-- begin instruction --\n${instruction}\n-- end instruction --\n\nCall the tools to gather the necessary information that could be used to create an accurate prompt.`;\n    return new GeminiPrompt({\n        body: {\n            contents: addUserTurn(promptText.asContent(), contents),\n            tools: toolManager.list(),\n            systemInstruction: toLLMContent(`\nYou are a researcher whose specialty is to call tools whose output helps gather the necessary information\nto be used to create an accurate prompt for a text-to-image model.\n`),\n        },\n    }, toolManager);\n}\nfunction promptRequest(contents, instruction) {\n    const context = contents?.length\n        ? \"using conversation context and these additional\"\n        : \"with these\";\n    const promptText = llm `Generate a single text-to-image prompt ${context} instructions:\n${instruction}\n\nTypical output format:\n\n## Setting/background\n\nDetailed description of everything that is in the background of the image.\n\n## Foreground/focus\n\nDetailed description of object and/or shapes that are in the foreground and are the main focal point of the image\n\n## Style\n\nDetailed description of the style, color scheme, vibe, kind of drawing (illustration, photorealistic, etc.)\n\nYou output will be fed directly into the text-to-image model, so it must be prompt only, no additional chit-chat\n`;\n    return new GeminiPrompt({\n        body: {\n            contents: addUserTurn(promptText.asContent(), contents),\n            systemInstruction: toLLMContent(`\nYou are a creative writer whose specialty is to write prompts for text-to-image models.\n\nThe prompt must describe every object in the image in great detail and describe the style \nin terms of color scheme and vibe.\n`),\n        },\n    });\n}\nfunction gracefulExit(notOk) {\n    report({\n        actor: \"Make Image\",\n        category: \"Warning\",\n        name: \"Graceful exit\",\n        details: `I tried a couple of times, but the Gemini API failed to generate the image you requested with the following error:\n\n### ${notOk.$error}\n\nTo keep things moving, I will return a blank result. My apologies!`,\n        icon: MAKE_IMAGE_ICON,\n    });\n    return toLLMContent(\" \");\n}\nconst MAX_RETRIES = 5;\nasync function invoke({ context: incomingContext, instruction, \"p-disable-prompt-rewrite\": disablePromptRewrite, ...params }) {\n    // 1) Substitute params in instruction.\n    const toolManager = new ToolManager(new ArgumentNameGenerator());\n    const substituting = await new Template(instruction).substitute(params, async ({ path: url }) => toolManager.addTool(url));\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    const fanningOut = await fanOutContext(substituting, incomingContext, async (instruction, context) => {\n        // 2) If there are tools in instruction, add an extra step of preparing\n        // information via tools.\n        if (toolManager.hasTools()) {\n            const gatheringInformation = await gatheringRequest(context, instruction, toolManager).invoke();\n            if (!ok(gatheringInformation))\n                return gatheringInformation;\n            context.push(...gatheringInformation.all);\n        }\n        let retryCount = MAX_RETRIES;\n        while (retryCount--) {\n            let imagePrompt;\n            // 3) Call Gemini to generate prompt.\n            if (disablePromptRewrite) {\n                imagePrompt = toLLMContent(toText(addUserTurn(instruction, context)));\n            }\n            else {\n                const generatingPrompt = await promptRequest(context, instruction).invoke();\n                if (!ok(generatingPrompt))\n                    return generatingPrompt;\n                imagePrompt = generatingPrompt.last;\n            }\n            console.log(\"PROMPT\", toText(imagePrompt));\n            // 4) Call Gemini to generate image.\n            const output = await callImageGen(toText(imagePrompt));\n            return output;\n        }\n        return gracefulExit(err(`Failed to generate an image after ${MAX_RETRIES} tries.`));\n    });\n    if (!ok(fanningOut))\n        return fanningOut;\n    return { context: fanningOut };\n}\nasync function describe({ inputs: { instruction } }) {\n    const template = new Template(instruction);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                instruction: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"Instruction\",\n                    description: \"Describe how to generate the image based on the input: style, interpretation, etc.\",\n                },\n                \"p-disable-prompt-rewrite\": {\n                    type: \"boolean\",\n                    title: \"Disable prompt expansion\",\n                    behavior: [\"config\", \"hint-preview\"],\n                    description: \"By default, inputs and instructions will be automatically expanded into a high quality image prompt. Check to disable this re-writing behavior.\",\n                },\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"hint-image\", \"main-port\"],\n                },\n            },\n        },\n        title: \"Make Image\",\n        metadata: {\n            icon: MAKE_IMAGE_ICON,\n            tags: [\"quick-access\", \"generative\"],\n            order: 2,\n        },\n    };\n}\n",
      "metadata": {
        "title": "Make Image",
        "source": {
          "code": "/**\n * @fileoverview Generates an image using supplied context.\n */\n\nimport invokeBoard from \"@invoke\";\n\nimport gemini, {\n  defaultSafetySettings,\n  type GeminiOutputs,\n  type GeminiInputs,\n  type Tool,\n} from \"./gemini\";\nimport { GeminiPrompt } from \"./gemini-prompt\";\nimport {\n  err,\n  ok,\n  toLLMContent,\n  toLLMContentInline,\n  toText,\n  addUserTurn,\n  llm,\n} from \"./utils\";\nimport { Template } from \"./template\";\nimport { ToolManager } from \"./tool-manager\";\nimport {\n  type ContentMap,\n  type ExecuteStepRequest,\n  executeStep,\n} from \"./step-executor\";\nimport { type Params, type DescriberResult } from \"./common\";\nimport { report } from \"./output\";\nimport { ArgumentNameGenerator } from \"./introducer\";\nimport { fanOutContext } from \"./lists\";\n\nconst MAKE_IMAGE_ICON = \"generative-image\";\n\ntype ImageGeneratorInputs = {\n  context: LLMContent[];\n  instruction: LLMContent;\n  \"p-disable-prompt-rewrite\": boolean;\n} & Params;\n\ntype ImageGeneratorOutputs = {\n  context: LLMContent[] | DescriberResult;\n};\n\nexport { invoke as default, describe };\n\nasync function callImageGen(imageInstruction: string): Promise<LLMContent> {\n  const executionInputs: ContentMap = {};\n  const encodedInstruction = btoa(\n    unescape(encodeURIComponent(imageInstruction))\n  );\n  executionInputs[\"image_prompt\"] = {\n    chunks: [\n      {\n        mimetype: \"text/plain\",\n        data: encodedInstruction,\n      },\n    ],\n  };\n  const inputParameters: string[] = [\"image_prompt\"];\n  const body = {\n    planStep: {\n      stepName: \"GenerateImage\",\n      modelApi: \"image_generation\",\n      inputParameters: inputParameters,\n      systemPrompt: \"\",\n    },\n    execution_inputs: executionInputs,\n  } satisfies ExecuteStepRequest;\n  const response = await executeStep(body);\n  if (!ok(response)) {\n    return toLLMContent(\"Image generation failed: \" + response.$error);\n  }\n\n  let returnVal;\n  for (let value of Object.values(response.executionOutputs)) {\n    const mimetype = value.chunks[0].mimetype;\n    if (mimetype.startsWith(\"image\")) {\n      returnVal = toLLMContentInline(mimetype, value.chunks[0].data);\n    }\n  }\n  if (!returnVal) {\n    return toLLMContent(\"Error: No image returned from backend\");\n  }\n  return returnVal;\n}\n\nfunction gatheringRequest(\n  contents: LLMContent[] | undefined,\n  instruction: LLMContent,\n  toolManager: ToolManager\n): GeminiPrompt {\n  const promptText = llm`\nAnalyze the instruction below and rather than following it, determine what information needs to be gathered to \ngenerate an accurate prompt for a text-to-image model in the next turn:\n-- begin instruction --\n${instruction}\n-- end instruction --\n\nCall the tools to gather the necessary information that could be used to create an accurate prompt.`;\n  return new GeminiPrompt(\n    {\n      body: {\n        contents: addUserTurn(promptText.asContent(), contents),\n        tools: toolManager.list(),\n        systemInstruction: toLLMContent(`\nYou are a researcher whose specialty is to call tools whose output helps gather the necessary information\nto be used to create an accurate prompt for a text-to-image model.\n`),\n      },\n    },\n    toolManager\n  );\n}\n\nfunction promptRequest(\n  contents: LLMContent[] | undefined,\n  instruction: LLMContent\n): GeminiPrompt {\n  const context = contents?.length\n    ? \"using conversation context and these additional\"\n    : \"with these\";\n  const promptText = llm`Generate a single text-to-image prompt ${context} instructions:\n${instruction}\n\nTypical output format:\n\n## Setting/background\n\nDetailed description of everything that is in the background of the image.\n\n## Foreground/focus\n\nDetailed description of object and/or shapes that are in the foreground and are the main focal point of the image\n\n## Style\n\nDetailed description of the style, color scheme, vibe, kind of drawing (illustration, photorealistic, etc.)\n\nYou output will be fed directly into the text-to-image model, so it must be prompt only, no additional chit-chat\n`;\n  return new GeminiPrompt({\n    body: {\n      contents: addUserTurn(promptText.asContent(), contents),\n      systemInstruction: toLLMContent(`\nYou are a creative writer whose specialty is to write prompts for text-to-image models.\n\nThe prompt must describe every object in the image in great detail and describe the style \nin terms of color scheme and vibe.\n`),\n    },\n  });\n}\n\nfunction gracefulExit(notOk: { $error: string }): Outcome<LLMContent> {\n  report({\n    actor: \"Make Image\",\n    category: \"Warning\",\n    name: \"Graceful exit\",\n    details: `I tried a couple of times, but the Gemini API failed to generate the image you requested with the following error:\n\n### ${notOk.$error}\n\nTo keep things moving, I will return a blank result. My apologies!`,\n    icon: MAKE_IMAGE_ICON,\n  });\n  return toLLMContent(\" \");\n}\n\nconst MAX_RETRIES = 5;\n\nasync function invoke({\n  context: incomingContext,\n  instruction,\n  \"p-disable-prompt-rewrite\": disablePromptRewrite,\n  ...params\n}: ImageGeneratorInputs): Promise<Outcome<ImageGeneratorOutputs>> {\n  // 1) Substitute params in instruction.\n  const toolManager = new ToolManager(new ArgumentNameGenerator());\n  const substituting = await new Template(instruction).substitute(\n    params,\n    async ({ path: url }) => toolManager.addTool(url)\n  );\n  if (!ok(substituting)) {\n    return substituting;\n  }\n\n  const fanningOut = await fanOutContext(\n    substituting,\n    incomingContext,\n    async (instruction, context) => {\n      // 2) If there are tools in instruction, add an extra step of preparing\n      // information via tools.\n      if (toolManager.hasTools()) {\n        const gatheringInformation = await gatheringRequest(\n          context,\n          instruction,\n          toolManager\n        ).invoke();\n        if (!ok(gatheringInformation)) return gatheringInformation;\n        context.push(...gatheringInformation.all);\n      }\n\n      let retryCount = MAX_RETRIES;\n\n      while (retryCount--) {\n        let imagePrompt: LLMContent;\n        // 3) Call Gemini to generate prompt.\n        if (disablePromptRewrite) {\n          imagePrompt = toLLMContent(toText(addUserTurn(instruction, context)));\n        } else {\n          const generatingPrompt = await promptRequest(\n            context,\n            instruction\n          ).invoke();\n          if (!ok(generatingPrompt)) return generatingPrompt;\n          imagePrompt = generatingPrompt.last;\n        }\n        console.log(\"PROMPT\", toText(imagePrompt));\n        // 4) Call Gemini to generate image.\n        const output = await callImageGen(toText(imagePrompt));\n        return output;\n      }\n      return gracefulExit(\n        err(`Failed to generate an image after ${MAX_RETRIES} tries.`)\n      );\n    }\n  );\n\n  if (!ok(fanningOut)) return fanningOut;\n  return { context: fanningOut };\n}\n\ntype DescribeInputs = {\n  inputs: {\n    instruction?: LLMContent;\n  };\n};\n\nasync function describe({ inputs: { instruction } }: DescribeInputs) {\n  const template = new Template(instruction);\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n        instruction: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n          title: \"Instruction\",\n          description:\n            \"Describe how to generate the image based on the input: style, interpretation, etc.\",\n        },\n        \"p-disable-prompt-rewrite\": {\n          type: \"boolean\",\n          title: \"Disable prompt expansion\",\n          behavior: [\"config\", \"hint-preview\"],\n          description:\n            \"By default, inputs and instructions will be automatically expanded into a high quality image prompt. Check to disable this re-writing behavior.\",\n        },\n        ...template.schemas(),\n      },\n      behavior: [\"at-wireable\"],\n      ...template.requireds(),\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n          behavior: [\"hint-image\", \"main-port\"],\n        },\n      },\n    } satisfies Schema,\n    title: \"Make Image\",\n    metadata: {\n      icon: MAKE_IMAGE_ICON,\n      tags: [\"quick-access\", \"generative\"],\n      order: 2,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Generates an image using supplied context.",
        "runnable": true
      }
    },
    "structured-response": {
      "code": "import {} from \"./gemini\";\nimport { err, toLLMContent, endsWithRole } from \"./utils\";\nexport { StructuredResponse };\nclass StructuredResponse {\n    id;\n    chat;\n    prolog = \"\";\n    epilog = \"\";\n    body = \"\";\n    response = undefined;\n    constructor(id, chat) {\n        this.id = id;\n        this.chat = chat;\n    }\n    get separator() {\n        return `<sep-${this.id}>`;\n    }\n    addPrompt(c, prompt) {\n        const { parts: p } = prompt;\n        const context = [...c];\n        const parts = [\n            {\n                text: this.instructionText(),\n            },\n            ...p,\n        ];\n        if (endsWithRole(c, \"user\")) {\n            const last = context.pop();\n            context.push({\n                ...last,\n                parts: [...last.parts, ...parts],\n            });\n        }\n        else {\n            context.push({ parts, role: \"user\" });\n        }\n        return context;\n    }\n    instructionText() {\n        const chatOrConclude = this.chat\n            ? `Finally, ask the user to provide feedback on your output as a friendly assistant might.`\n            : `Finally, you briefly summarize what the work product was and how it fulfills the task.`;\n        return `\nConsider the conversation context so far and generate a response.\n\nYour response must consist of three parts, separated by the ${this.separator} tag.\n\n- Briefly describe the work product, why it fulfills the specified task,\nand any notes or comments you might have about it\n- Insert the ${this.separator} tag\n- Provide the work product only, without any additional conversation \nor comments about your output\n- Insert the ${this.separator} tag\n${chatOrConclude}\n`;\n    }\n    instruction() {\n        return toLLMContent(this.instructionText());\n    }\n    parseContent(content) {\n        const part = content.parts?.at(0);\n        if (!part || !(\"text\" in part)) {\n            return err(\"No text in part\");\n        }\n        this.response = content;\n        const structure = part.text.split(this.separator);\n        if (structure.length !== 3) {\n            console.warn(`The output must contain 3 parts, but only ${structure.length} were found`);\n            if (structure.length == 2) {\n                // Assume that the prolog and body are here, but the epilog was gone.\n                // This can happen sometimes when we go past the output token window.\n                this.prolog = structure[0];\n                this.body = structure[1].trim();\n                this.epilog = this.chat ? \"Please provide feedback\" : \"\";\n                return;\n            }\n            return err(`No structure response delimiters were found. This is likely an invalid reponse.`);\n        }\n        this.prolog = structure[0];\n        this.body = structure[1].trim();\n        this.epilog = structure[2].trim();\n    }\n    bodyAsContent() {\n        return toLLMContent(this.body, \"model\");\n    }\n    parse(response) {\n        const r = response;\n        const content = r.candidates?.at(0)?.content;\n        if (!content) {\n            return err(\"No content\");\n        }\n        return this.parseContent(content);\n    }\n}\n",
      "metadata": {
        "title": "structured-response",
        "source": {
          "code": "import { type GeminiOutputs, type GeminiAPIOutputs } from \"./gemini\";\nimport { err, toLLMContent, endsWithRole } from \"./utils\";\n\nexport { StructuredResponse };\n\nclass StructuredResponse {\n  public prolog: string = \"\";\n  public epilog: string = \"\";\n  public body: string = \"\";\n  response: LLMContent | undefined = undefined;\n\n  constructor(\n    public readonly id: string,\n    public readonly chat: boolean\n  ) {}\n\n  get separator() {\n    return `<sep-${this.id}>`;\n  }\n\n  addPrompt(c: LLMContent[], prompt: LLMContent): LLMContent[] {\n    const { parts: p } = prompt;\n    const context: LLMContent[] = [...c];\n    const parts = [\n      {\n        text: this.instructionText(),\n      },\n      ...p,\n    ];\n    if (endsWithRole(c, \"user\")) {\n      const last = context.pop()!;\n      context.push({\n        ...last,\n        parts: [...last.parts, ...parts],\n      });\n    } else {\n      context.push({ parts, role: \"user\" });\n    }\n    return context;\n  }\n\n  instructionText(): string {\n    const chatOrConclude = this.chat\n      ? `Finally, ask the user to provide feedback on your output as a friendly assistant might.`\n      : `Finally, you briefly summarize what the work product was and how it fulfills the task.`;\n\n    return `\nConsider the conversation context so far and generate a response.\n\nYour response must consist of three parts, separated by the ${this.separator} tag.\n\n- Briefly describe the work product, why it fulfills the specified task,\nand any notes or comments you might have about it\n- Insert the ${this.separator} tag\n- Provide the work product only, without any additional conversation \nor comments about your output\n- Insert the ${this.separator} tag\n${chatOrConclude}\n`;\n  }\n\n  instruction(): LLMContent {\n    return toLLMContent(this.instructionText());\n  }\n\n  parseContent(content: LLMContent): Outcome<void> {\n    const part = content.parts?.at(0);\n    if (!part || !(\"text\" in part)) {\n      return err(\"No text in part\");\n    }\n    this.response = content;\n    const structure = part.text.split(this.separator);\n    if (structure.length !== 3) {\n      console.warn(\n        `The output must contain 3 parts, but only ${structure.length} were found`\n      );\n      if (structure.length == 2) {\n        // Assume that the prolog and body are here, but the epilog was gone.\n        // This can happen sometimes when we go past the output token window.\n        this.prolog = structure[0];\n        this.body = structure[1].trim();\n        this.epilog = this.chat ? \"Please provide feedback\" : \"\";\n        return;\n      }\n      return err(\n        `No structure response delimiters were found. This is likely an invalid reponse.`\n      );\n    }\n    this.prolog = structure[0];\n    this.body = structure[1].trim();\n    this.epilog = structure[2].trim();\n  }\n\n  bodyAsContent(): LLMContent {\n    return toLLMContent(this.body, \"model\");\n  }\n\n  parse(response: GeminiOutputs): Outcome<void> {\n    const r = response as GeminiAPIOutputs;\n    const content = r.candidates?.at(0)?.content;\n    if (!content) {\n      return err(\"No content\");\n    }\n    return this.parseContent(content);\n  }\n}\n",
          "language": "typescript"
        },
        "description": "",
        "runnable": false
      }
    },
    "template": {
      "code": "/**\n * @fileoverview Handles templated content\n */\nexport { invoke as default, describe, Template };\nimport {} from \"./common\";\nimport { ok, err, isLLMContent, isLLMContentArray } from \"./utils\";\nimport readFile from \"@read\";\nfunction unique(params) {\n    return Array.from(new Set(params));\n}\nfunction isTool(param) {\n    return param.type === \"tool\" && !!param.path;\n}\nfunction isIn(param) {\n    return param.type === \"in\" && !!param.path;\n}\nfunction isAsset(param) {\n    return param.type === \"asset\" && !!param.path;\n}\nfunction isParameter(param) {\n    return param.type === \"param\" && !!param.path;\n}\nfunction isParamPart(param) {\n    return isTool(param) || isIn(param) || isAsset(param) || isParameter(param);\n}\nconst PARSING_REGEX = /{(?<json>{(?:.*?)})}/gim;\nclass Template {\n    template;\n    #parts;\n    #role;\n    constructor(template) {\n        this.template = template;\n        if (!template) {\n            this.#role = \"user\";\n            this.#parts = [];\n            return;\n        }\n        this.#parts = this.#splitToTemplateParts(template);\n        this.#role = template.role;\n    }\n    #mergeTextParts(parts) {\n        const merged = [];\n        for (const part of parts) {\n            if (\"text\" in part) {\n                const last = merged[merged.length - 1];\n                if (last && \"text\" in last) {\n                    last.text += part.text;\n                }\n                else {\n                    merged.push(part);\n                }\n            }\n            else {\n                merged.push(part);\n            }\n        }\n        return merged;\n    }\n    /**\n     * Takes an LLM Content and splits it further into parts where\n     * each {{param}} substitution is a separate part.\n     */\n    #splitToTemplateParts(content) {\n        const parts = [];\n        for (const part of content.parts) {\n            if (!(\"text\" in part)) {\n                parts.push(part);\n                continue;\n            }\n            const matches = part.text.matchAll(PARSING_REGEX);\n            let start = 0;\n            for (const match of matches) {\n                const json = match.groups?.json;\n                const op = match.groups?.op;\n                const arg = match.groups?.arg;\n                const end = match.index;\n                if (end > start) {\n                    parts.push({ text: part.text.slice(start, end) });\n                }\n                if (json) {\n                    let maybeTemplatePart;\n                    try {\n                        maybeTemplatePart = JSON.parse(json);\n                        if (isParamPart(maybeTemplatePart)) {\n                            parts.push(maybeTemplatePart);\n                        }\n                        else {\n                            maybeTemplatePart = null;\n                        }\n                    }\n                    catch (e) {\n                        // do nothing\n                    }\n                    finally {\n                        if (!maybeTemplatePart) {\n                            parts.push({ text: part.text.slice(end, end + match[0].length) });\n                        }\n                    }\n                }\n                start = end + match[0].length;\n            }\n            if (start < part.text.length) {\n                parts.push({ text: part.text.slice(start) });\n            }\n        }\n        return parts;\n    }\n    #getLastNonMetadata(value) {\n        const content = value;\n        for (let i = content.length - 1; i >= 0; i--) {\n            if (content[i].role !== \"$metadata\") {\n                return content[i];\n            }\n        }\n        return null;\n    }\n    async #replaceParam(param, params, whenTool) {\n        if (isIn(param)) {\n            const { type, title: name, path } = param;\n            const paramName = `p-z-${path}`;\n            if (paramName in params) {\n                return params[paramName];\n            }\n            return name;\n        }\n        else if (isAsset(param)) {\n            const path = `/assets/${param.path}`;\n            const reading = await readFile({ path });\n            if (!ok(reading)) {\n                return err(`Unable to find asset \"${param.title}\"`);\n            }\n            return reading.data;\n        }\n        else if (isTool(param)) {\n            return await whenTool(param);\n        }\n        else if (isParameter(param)) {\n            const path = `/env/parameters/${param.path}`;\n            const reading = await readFile({ path });\n            if (!ok(reading)) {\n                console.error(`Unknown parameter \"${param.title}\"`);\n                return null;\n            }\n            return reading.data;\n        }\n        return null;\n    }\n    async substitute(params, whenTool) {\n        const replaced = [];\n        for (const part of this.#parts) {\n            if (\"type\" in part) {\n                const value = await this.#replaceParam(part, params, whenTool);\n                if (value === null) {\n                    // Ignore if null.\n                    continue;\n                }\n                else if (!ok(value)) {\n                    return value;\n                }\n                else if (typeof value === \"string\") {\n                    replaced.push({ text: value });\n                }\n                else if (isLLMContent(value)) {\n                    replaced.push(...value.parts);\n                }\n                else if (isLLMContentArray(value)) {\n                    const last = this.#getLastNonMetadata(value);\n                    console.log(\"LAST\", last);\n                    if (last) {\n                        replaced.push(...last.parts);\n                    }\n                }\n                else {\n                    replaced.push({ text: JSON.stringify(value) });\n                }\n            }\n            else {\n                replaced.push(part);\n            }\n        }\n        const parts = this.#mergeTextParts(replaced);\n        return { parts, role: this.#role };\n    }\n    #toId(param) {\n        return `p-z-${param}`;\n    }\n    #toTitle(id) {\n        const spaced = id?.replace(/[_-]/g, \" \");\n        return ((spaced?.at(0)?.toUpperCase() ?? \"\") +\n            (spaced?.slice(1)?.toLowerCase() ?? \"\"));\n    }\n    #forEachParam(handler) {\n        for (const part of this.#parts) {\n            if (\"type\" in part) {\n                handler(part);\n            }\n        }\n    }\n    requireds() {\n        const required = [];\n        let hasValues = false;\n        this.#forEachParam((param) => {\n            if (!isIn(param))\n                return;\n            hasValues = true;\n            required.push(this.#toId(param.title));\n        });\n        return hasValues ? { required } : {};\n    }\n    schemas() {\n        const result = [];\n        this.#forEachParam((param) => {\n            const name = param.title;\n            const id = this.#toId(param.path);\n            if (!isIn(param))\n                return;\n            result.push([\n                id,\n                {\n                    title: this.#toTitle(name),\n                    description: `The value to substitute for the parameter \"${name}\"`,\n                    type: \"object\",\n                    behavior: [\"llm-content\"],\n                },\n            ]);\n        });\n        return Object.fromEntries(result);\n    }\n    static part(part) {\n        return `{${JSON.stringify(part)}}`;\n    }\n}\n/**\n * API for test harness\n */\nfunction fromTestParams(params) {\n    return Object.fromEntries(Object.entries(params).map(([key, value]) => {\n        return [`p-z-${key}`, value];\n    }));\n}\n/**\n * Only used for testing.\n */\nasync function invoke({ inputs: { content, params }, }) {\n    const template = new Template(content);\n    const result = await template.substitute(fromTestParams(params), async (params) => {\n        return params.path;\n    });\n    if (!ok(result)) {\n        return result;\n    }\n    return { outputs: result };\n}\n/**\n * Only used for testing.\n */\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: { inputs: { type: \"object\", title: \"Test inputs\" } },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: { outputs: { type: \"object\", title: \"Test outputs\" } },\n        },\n    };\n}\n",
      "metadata": {
        "title": "template",
        "source": {
          "code": "/**\n * @fileoverview Handles templated content\n */\n\nexport { invoke as default, describe, Template };\n\nimport { type Params } from \"./common\";\nimport { ok, err, isLLMContent, isLLMContentArray } from \"./utils\";\nimport readFile from \"@read\";\n\ntype LLMContentWithMetadata = LLMContent & {\n  $metadata: unknown;\n};\n\nexport type Requireds = {\n  required?: Schema[\"required\"];\n};\n\ntype Location = {\n  part: LLMContent[\"parts\"][0];\n  parts: LLMContent[\"parts\"];\n};\n\nexport type InParamPart = {\n  type: \"in\";\n  path: string;\n  title: string;\n};\n\nexport type ToolParamPart = {\n  type: \"tool\";\n  path: string;\n  title: string;\n};\n\nexport type AssetParamPart = {\n  type: \"asset\";\n  path: string;\n  title: string;\n};\n\nexport type ParameterParamPart = {\n  type: \"param\";\n  path: string;\n  title: string;\n};\n\nexport type ParamPart =\n  | InParamPart\n  | ToolParamPart\n  | AssetParamPart\n  | ParameterParamPart;\n\nexport type TemplatePart = DataPart | ParamPart;\n\nexport type ToolCallback = (param: ToolParamPart) => Promise<Outcome<string>>;\n\nfunction unique<T>(params: T[]): T[] {\n  return Array.from(new Set(params));\n}\n\nfunction isTool(param: ParamPart): param is ToolParamPart {\n  return param.type === \"tool\" && !!param.path;\n}\n\nfunction isIn(param: ParamPart): param is InParamPart {\n  return param.type === \"in\" && !!param.path;\n}\n\nfunction isAsset(param: ParamPart): param is AssetParamPart {\n  return param.type === \"asset\" && !!param.path;\n}\n\nfunction isParameter(param: ParamPart): param is ParameterParamPart {\n  return param.type === \"param\" && !!param.path;\n}\n\nfunction isParamPart(param: ParamPart): param is ParamPart {\n  return isTool(param) || isIn(param) || isAsset(param) || isParameter(param);\n}\n\nconst PARSING_REGEX = /{(?<json>{(?:.*?)})}/gim;\n\nclass Template {\n  #parts: TemplatePart[];\n  #role: LLMContent[\"role\"];\n\n  constructor(public readonly template: LLMContent | undefined) {\n    if (!template) {\n      this.#role = \"user\";\n      this.#parts = [];\n      return;\n    }\n    this.#parts = this.#splitToTemplateParts(template);\n    this.#role = template.role;\n  }\n\n  #mergeTextParts(parts: TemplatePart[]) {\n    const merged = [];\n    for (const part of parts) {\n      if (\"text\" in part) {\n        const last = merged[merged.length - 1];\n        if (last && \"text\" in last) {\n          last.text += part.text;\n        } else {\n          merged.push(part);\n        }\n      } else {\n        merged.push(part);\n      }\n    }\n    return merged as DataPart[];\n  }\n\n  /**\n   * Takes an LLM Content and splits it further into parts where\n   * each {{param}} substitution is a separate part.\n   */\n  #splitToTemplateParts(content: LLMContent): TemplatePart[] {\n    const parts: TemplatePart[] = [];\n    for (const part of content.parts) {\n      if (!(\"text\" in part)) {\n        parts.push(part);\n        continue;\n      }\n      const matches = part.text.matchAll(PARSING_REGEX);\n      let start = 0;\n      for (const match of matches) {\n        const json = match.groups?.json;\n        const op = match.groups?.op;\n        const arg = match.groups?.arg;\n        const end = match.index;\n        if (end > start) {\n          parts.push({ text: part.text.slice(start, end) });\n        }\n        if (json) {\n          let maybeTemplatePart;\n          try {\n            maybeTemplatePart = JSON.parse(json);\n            if (isParamPart(maybeTemplatePart)) {\n              parts.push(maybeTemplatePart);\n            } else {\n              maybeTemplatePart = null;\n            }\n          } catch (e) {\n            // do nothing\n          } finally {\n            if (!maybeTemplatePart) {\n              parts.push({ text: part.text.slice(end, end + match[0].length) });\n            }\n          }\n        }\n        start = end + match[0].length;\n      }\n      if (start < part.text.length) {\n        parts.push({ text: part.text.slice(start) });\n      }\n    }\n    return parts;\n  }\n\n  #getLastNonMetadata(value: LLMContent[]): LLMContent | null {\n    const content = value as LLMContentWithMetadata[];\n    for (let i = content.length - 1; i >= 0; i--) {\n      if (content[i].role !== \"$metadata\") {\n        return content[i] as LLMContent;\n      }\n    }\n    return null;\n  }\n\n  async #replaceParam(\n    param: ParamPart,\n    params: Params,\n    whenTool: ToolCallback\n  ): Promise<Outcome<unknown>> {\n    if (isIn(param)) {\n      const { type, title: name, path } = param;\n      const paramName: `p-z-${string}` = `p-z-${path}`;\n      if (paramName in params) {\n        return params[paramName];\n      }\n      return name;\n    } else if (isAsset(param)) {\n      const path: FileSystemPath = `/assets/${param.path}`;\n      const reading = await readFile({ path });\n      if (!ok(reading)) {\n        return err(`Unable to find asset \"${param.title}\"`);\n      }\n      return reading.data;\n    } else if (isTool(param)) {\n      return await whenTool(param);\n    } else if (isParameter(param)) {\n      const path: FileSystemPath = `/env/parameters/${param.path}`;\n      const reading = await readFile({ path });\n      if (!ok(reading)) {\n        console.error(`Unknown parameter \"${param.title}\"`);\n        return null;\n      }\n      return reading.data;\n    }\n    return null;\n  }\n\n  async substitute(\n    params: Params,\n    whenTool: ToolCallback\n  ): Promise<Outcome<LLMContent>> {\n    const replaced: DataPart[] = [];\n    for (const part of this.#parts) {\n      if (\"type\" in part) {\n        const value = await this.#replaceParam(part, params, whenTool);\n        if (value === null) {\n          // Ignore if null.\n          continue;\n        } else if (!ok(value)) {\n          return value;\n        } else if (typeof value === \"string\") {\n          replaced.push({ text: value });\n        } else if (isLLMContent(value)) {\n          replaced.push(...value.parts);\n        } else if (isLLMContentArray(value)) {\n          const last = this.#getLastNonMetadata(value);\n          console.log(\"LAST\", last);\n          if (last) {\n            replaced.push(...last.parts);\n          }\n        } else {\n          replaced.push({ text: JSON.stringify(value) });\n        }\n      } else {\n        replaced.push(part);\n      }\n    }\n    const parts = this.#mergeTextParts(replaced);\n    return { parts, role: this.#role };\n  }\n\n  #toId(param: string) {\n    return `p-z-${param}`;\n  }\n\n  #toTitle(id: string) {\n    const spaced = id?.replace(/[_-]/g, \" \");\n    return (\n      (spaced?.at(0)?.toUpperCase() ?? \"\") +\n      (spaced?.slice(1)?.toLowerCase() ?? \"\")\n    );\n  }\n\n  #forEachParam(handler: (param: ParamPart) => void) {\n    for (const part of this.#parts) {\n      if (\"type\" in part) {\n        handler(part);\n      }\n    }\n  }\n\n  requireds(): Requireds {\n    const required: string[] = [];\n    let hasValues = false;\n    this.#forEachParam((param) => {\n      if (!isIn(param)) return;\n      hasValues = true;\n      required.push(this.#toId(param.title!));\n    });\n    return hasValues ? { required } : {};\n  }\n\n  schemas(): Record<string, Schema> {\n    const result: [string, Schema][] = [];\n    this.#forEachParam((param) => {\n      const name = param.title!;\n      const id = this.#toId(param.path!);\n      if (!isIn(param)) return;\n      result.push([\n        id,\n        {\n          title: this.#toTitle(name),\n          description: `The value to substitute for the parameter \"${name}\"`,\n          type: \"object\",\n          behavior: [\"llm-content\"],\n        },\n      ]);\n    });\n    return Object.fromEntries(result);\n  }\n\n  static part(part: ParamPart) {\n    return `{${JSON.stringify(part)}}`;\n  }\n}\n\n/**\n * API for test harness\n */\n\nfunction fromTestParams(params: Record<string, string>): Params {\n  return Object.fromEntries(\n    Object.entries(params).map(([key, value]) => {\n      return [`p-z-${key}`, value];\n    })\n  );\n}\n\ntype TestInputs = {\n  inputs: { content: LLMContent; params: Record<string, string> };\n};\n\ntype TestOutputs = {\n  outputs: LLMContent;\n};\n\n/**\n * Only used for testing.\n */\nasync function invoke({\n  inputs: { content, params },\n}: TestInputs): Promise<Outcome<TestOutputs>> {\n  const template = new Template(content);\n  const result = await template.substitute(\n    fromTestParams(params),\n    async (params) => {\n      return params.path;\n    }\n  );\n  if (!ok(result)) {\n    return result;\n  }\n  return { outputs: result };\n}\n\n/**\n * Only used for testing.\n */\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: { inputs: { type: \"object\", title: \"Test inputs\" } },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: { outputs: { type: \"object\", title: \"Test outputs\" } },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Handles templated content",
        "runnable": true
      }
    },
    "audio-generator": {
      "code": "/**\n * @fileoverview Generates audio output using supplied context.\n */\nimport gemini, { defaultSafetySettings, } from \"./gemini\";\nimport { err, ok, llm, toLLMContent, toText } from \"./utils\";\nimport {} from \"./common\";\nexport { invoke as default, describe };\nasync function invoke({ context, }) {\n    // 1) Get last LLMContent from input.\n    const prompt = context && Array.isArray(context) && context.length > 0\n        ? context.at(-1)\n        : undefined;\n    if (!prompt) {\n        return err(\"Must supply context as input\");\n    }\n    prompt.role = \"user\";\n    // 2) Call Gemini to generate audio.\n    const result = await gemini({\n        model: \"gemini-2.0-flash-exp\",\n        body: {\n            contents: [prompt],\n            generationConfig: {\n                responseModalities: [\"AUDIO\"],\n            },\n            safetySettings: defaultSafetySettings(),\n        },\n    });\n    if (!ok(result)) {\n        return result;\n    }\n    if (\"context\" in result) {\n        return err(\"Invalid output from Gemini -- must be candidates\");\n    }\n    const content = result.candidates.at(0)?.content;\n    if (!content) {\n        return err(\"No content\");\n    }\n    return { context: [content] };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n            },\n            additionalProperties: false,\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"hint-audio\"],\n                },\n            },\n            additionalProperties: false,\n        },\n        title: \"Make Audio\",\n        metadata: {\n            icon: \"generative-audio\",\n            tags: [\"quick-access\", \"generative\"],\n            order: 3,\n        },\n    };\n}\n",
      "metadata": {
        "title": "Audio Generator",
        "source": {
          "code": "/**\n * @fileoverview Generates audio output using supplied context.\n */\n\nimport gemini, {\n  defaultSafetySettings,\n  type GeminiOutputs,\n  type GeminiInputs,\n} from \"./gemini\";\nimport { err, ok, llm, toLLMContent, toText } from \"./utils\";\nimport { type DescriberResult } from \"./common\";\n\ntype AudioGeneratorInputs = {\n  context: LLMContent[];\n};\n\ntype AudioGeneratorOutputs = {\n  context: LLMContent[] | DescriberResult;\n};\n\nexport { invoke as default, describe };\n\nasync function invoke({\n  context,\n}: AudioGeneratorInputs): Promise<Outcome<AudioGeneratorOutputs>> {\n  // 1) Get last LLMContent from input.\n  const prompt =\n    context && Array.isArray(context) && context.length > 0\n      ? context.at(-1)!\n      : undefined;\n  if (!prompt) {\n    return err(\"Must supply context as input\");\n  }\n  prompt.role = \"user\";\n\n  // 2) Call Gemini to generate audio.\n  const result = await gemini({\n    model: \"gemini-2.0-flash-exp\",\n    body: {\n      contents: [prompt],\n      generationConfig: {\n        responseModalities: [\"AUDIO\"],\n      },\n      safetySettings: defaultSafetySettings(),\n    },\n  });\n  if (!ok(result)) {\n    return result;\n  }\n  if (\"context\" in result) {\n    return err(\"Invalid output from Gemini -- must be candidates\");\n  }\n\n  const content = result.candidates.at(0)?.content;\n  if (!content) {\n    return err(\"No content\");\n  }\n\n  return { context: [content] };\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n      },\n      additionalProperties: false,\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n          behavior: [\"hint-audio\"],\n        },\n      },\n      additionalProperties: false,\n    } satisfies Schema,\n    title: \"Make Audio\",\n    metadata: {\n      icon: \"generative-audio\",\n      tags: [\"quick-access\", \"generative\"],\n      order: 3,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Generates audio output using supplied context.",
        "runnable": true
      }
    },
    "text-entry": {
      "code": "/**\n * @fileoverview Allows asking user for input that could be then used in next steps.\n */\nimport { ok, defaultLLMContent, toText } from \"./utils\";\nimport {} from \"./common\";\nimport { Template } from \"./template\";\nimport { report } from \"./output\";\nexport { invoke as default, describe };\nconst MODALITY = [\n    \"Any\",\n    \"Audio\",\n    \"Image\",\n    \"Video\",\n    \"Upload File\",\n];\nfunction toInput(title, modality) {\n    const toInput = {\n        type: \"object\",\n        properties: {\n            request: {\n                type: \"object\",\n                title,\n                description: \"Provide input to proceed\",\n                behavior: [\"transient\", \"llm-content\"],\n                examples: [defaultLLMContent()],\n                format: computeIcon(modality),\n            },\n        },\n    };\n    return toInput;\n}\nconst ICONS = {\n    Any: \"multimodal\",\n    Audio: \"audio\",\n    Video: \"video\",\n    Image: \"image\",\n    \"Upload File\": \"file\",\n};\nconst HINTS = {\n    Any: \"hint-multimodal\",\n    Audio: \"hint-audio\",\n    Video: \"hint-image\",\n    Image: \"hint-image\",\n    \"Upload File\": \"hint-text\",\n};\nfunction computeIcon(modality) {\n    return (modality && ICONS[modality]) || \"multimodal\";\n}\nfunction computeHint(modality) {\n    return (modality && HINTS[modality]) || \"hint-multimodal\";\n}\nasync function invoke({ description, \"p-modality\": modality, }) {\n    const title = description ? toText(description) : \"Please provide input\";\n    await report({\n        actor: \"Ask User\",\n        category: \"Requesting Input\",\n        name: \"\",\n        details: title,\n        icon: \"input\",\n    });\n    return { context: \"nothing\", toInput: toInput(title, modality) };\n}\nasync function describe({ inputs: { [\"p-modality\"]: modality }, }) {\n    const icon = computeIcon(modality);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                description: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"What to ask of user\",\n                    description: \"Provide a request prompt that will be shown to the user.\",\n                },\n                \"p-modality\": {\n                    type: \"string\",\n                    enum: MODALITY,\n                    behavior: [\"config\", \"hint-preview\"],\n                    icon,\n                    title: \"Input type\",\n                    description: \"Set the type of input the user can provide\",\n                },\n            },\n            additionalProperties: true,\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"main-port\", computeHint(modality)],\n                },\n            },\n            additionalProperties: false,\n        },\n        title: \"Ask User\",\n        metadata: {\n            icon: \"input\",\n            tags: [\"quick-access\", \"core\"],\n            order: 1,\n        },\n    };\n}\n",
      "metadata": {
        "title": "Ask User",
        "icon": "text",
        "source": {
          "code": "/**\n * @fileoverview Allows asking user for input that could be then used in next steps.\n */\nimport { ok, defaultLLMContent, toText } from \"./utils\";\nimport { type Params } from \"./common\";\nimport { Template } from \"./template\";\nimport { report } from \"./output\";\n\nexport { invoke as default, describe };\n\nconst MODALITY: readonly string[] = [\n  \"Any\",\n  \"Audio\",\n  \"Image\",\n  \"Video\",\n  \"Upload File\",\n] as const;\n\ntype Modality = (typeof MODALITY)[number];\n\ntype TextInputs = {\n  description?: LLMContent;\n  \"p-modality\"?: Modality;\n};\n\ntype TextOutputs =\n  | {\n      toInput: Schema;\n      context: \"nothing\";\n    }\n  | {\n      toMain: string;\n      context: LLMContent;\n    };\n\nfunction toInput(title: string, modality: Modality | undefined) {\n  const toInput: Schema = {\n    type: \"object\",\n    properties: {\n      request: {\n        type: \"object\",\n        title,\n        description: \"Provide input to proceed\",\n        behavior: [\"transient\", \"llm-content\"],\n        examples: [defaultLLMContent()],\n        format: computeIcon(modality),\n      },\n    },\n  };\n  return toInput;\n}\n\nconst ICONS: Record<Modality, string> = {\n  Any: \"multimodal\",\n  Audio: \"audio\",\n  Video: \"video\",\n  Image: \"image\",\n  \"Upload File\": \"file\",\n};\n\nconst HINTS: Record<Modality, BehaviorSchema> = {\n  Any: \"hint-multimodal\",\n  Audio: \"hint-audio\",\n  Video: \"hint-image\",\n  Image: \"hint-image\",\n  \"Upload File\": \"hint-text\",\n};\n\nfunction computeIcon(modality?: Modality): string {\n  return (modality && ICONS[modality]) || \"multimodal\";\n}\n\nfunction computeHint(modality: Modality): BehaviorSchema {\n  return (modality && HINTS[modality]) || \"hint-multimodal\";\n}\n\nasync function invoke({\n  description,\n  \"p-modality\": modality,\n}: TextInputs): Promise<Outcome<TextOutputs>> {\n  const title = description ? toText(description) : \"Please provide input\";\n  await report({\n    actor: \"Ask User\",\n    category: \"Requesting Input\",\n    name: \"\",\n    details: title,\n    icon: \"input\",\n  });\n  return { context: \"nothing\", toInput: toInput(title, modality) };\n}\n\ntype DescribeInputs = {\n  inputs: {\n    \"p-modality\": Modality;\n  };\n};\n\nasync function describe({\n  inputs: { [\"p-modality\"]: modality },\n}: DescribeInputs) {\n  const icon = computeIcon(modality);\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        description: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n          title: \"What to ask of user\",\n          description:\n            \"Provide a request prompt that will be shown to the user.\",\n        },\n        \"p-modality\": {\n          type: \"string\",\n          enum: MODALITY as string[],\n          behavior: [\"config\", \"hint-preview\"],\n          icon,\n          title: \"Input type\",\n          description: \"Set the type of input the user can provide\",\n        },\n      },\n      additionalProperties: true,\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n          behavior: [\"main-port\", computeHint(modality)],\n        },\n      },\n      additionalProperties: false,\n    } satisfies Schema,\n    title: \"Ask User\",\n    metadata: {\n      icon: \"input\",\n      tags: [\"quick-access\", \"core\"],\n      order: 1,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Allows asking user for input that could be then used in next steps.",
        "runnable": true
      }
    },
    "text-main": {
      "code": "/**\n * @fileoverview Add a description for your module here.\n */\nimport { err } from \"./utils\";\nexport { invoke as default, describe };\nasync function invoke({ context, request, }) {\n    if (context == \"nothing\") {\n        if (!request) {\n            return err(`No text supplied.`);\n        }\n        return { context: [request] };\n    }\n    return { context: [context] };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"object\",\n                    title: \"Context in\",\n                },\n                request: {\n                    type: \"object\",\n                    title: \"Data From Input\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "text-main",
        "source": {
          "code": "/**\n * @fileoverview Add a description for your module here.\n */\n\nimport { err } from \"./utils\";\n\nexport { invoke as default, describe };\n\ntype TextMainInputs = {\n  context: LLMContent | \"nothing\";\n  request?: LLMContent;\n};\n\ntype TextMainOutputs = {\n  context: LLMContent[];\n};\n\nasync function invoke({\n  context,\n  request,\n}: TextMainInputs): Promise<Outcome<TextMainOutputs>> {\n  if (context == \"nothing\") {\n    if (!request) {\n      return err(`No text supplied.`);\n    }\n    return { context: [request] };\n  }\n  return { context: [context] };\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"object\",\n          title: \"Context in\",\n        },\n        request: {\n          type: \"object\",\n          title: \"Data From Input\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Add a description for your module here.",
        "runnable": true
      }
    },
    "combine-outputs": {
      "code": "/**\n * @fileoverview Combines multiple outputs into one.\n */\nimport { Template } from \"./template\";\nimport { ok } from \"./utils\";\nimport { fanOutContext, flattenContext } from \"./lists\";\nexport { invoke as default, describe };\nasync function invoke({ text, \"z-flatten-list\": flatten, ...params }) {\n    const template = new Template(text);\n    const substituting = await template.substitute(params, async () => \"\");\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    let context = await fanOutContext(substituting, undefined, async (instruction) => instruction);\n    if (!ok(context))\n        return context;\n    if (flatten) {\n        context = flattenContext(context);\n    }\n    return { context };\n}\nasync function describe({ inputs: { text } }) {\n    const template = new Template(text);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                text: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"hint-preview\", \"config\"],\n                    title: \"Text\",\n                    description: \"Type the @ character to select the outputs to combine\",\n                },\n                \"z-flatten-list\": {\n                    type: \"boolean\",\n                    behavior: [\"hint-preview\", \"config\"],\n                    icon: \"summarize\",\n                    title: \"Flatten the list\",\n                    description: \"When checked, the step will flatten the incoming list into a single outputs\",\n                },\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                    },\n                    title: \"Context out\",\n                    behavior: [\"main-port\", \"hint-multimodal\"],\n                },\n            },\n        },\n        title: \"Combine Outputs\",\n        metadata: {\n            icon: \"combine-outputs\",\n            tags: [\"quick-access\", \"core\", \"experimental\"],\n            order: 100,\n        },\n    };\n}\n",
      "metadata": {
        "title": "Combine Outputs",
        "source": {
          "code": "/**\n * @fileoverview Combines multiple outputs into one.\n */\n\nimport { Template } from \"./template\";\nimport { ok } from \"./utils\";\nimport { fanOutContext, flattenContext } from \"./lists\";\n\nexport { invoke as default, describe };\n\ntype InvokeInputs = {\n  text?: LLMContent;\n  \"z-flatten-list\": boolean;\n};\n\ntype Outputs = {\n  context: LLMContent[];\n};\n\ntype DescribeInputs = {\n  inputs: {\n    text?: LLMContent;\n  };\n};\n\nasync function invoke({\n  text,\n  \"z-flatten-list\": flatten,\n  ...params\n}: InvokeInputs): Promise<Outcome<Outputs>> {\n  const template = new Template(text);\n  const substituting = await template.substitute(params, async () => \"\");\n  if (!ok(substituting)) {\n    return substituting;\n  }\n  let context = await fanOutContext(\n    substituting,\n    undefined,\n    async (instruction) => instruction\n  );\n  if (!ok(context)) return context;\n  if (flatten) {\n    context = flattenContext(context);\n  }\n  return { context };\n}\n\nasync function describe({ inputs: { text } }: DescribeInputs) {\n  const template = new Template(text);\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        text: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"hint-preview\", \"config\"],\n          title: \"Text\",\n          description: \"Type the @ character to select the outputs to combine\",\n        },\n        \"z-flatten-list\": {\n          type: \"boolean\",\n          behavior: [\"hint-preview\", \"config\"],\n          icon: \"summarize\",\n          title: \"Flatten the list\",\n          description:\n            \"When checked, the step will flatten the incoming list into a single outputs\",\n        },\n        ...template.schemas(),\n      },\n      behavior: [\"at-wireable\"],\n      ...template.requireds(),\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: {\n            type: \"object\",\n            behavior: [\"llm-content\"],\n          },\n          title: \"Context out\",\n          behavior: [\"main-port\", \"hint-multimodal\"],\n        },\n      },\n    } satisfies Schema,\n    title: \"Combine Outputs\",\n    metadata: {\n      icon: \"combine-outputs\",\n      tags: [\"quick-access\", \"core\", \"experimental\"],\n      order: 100,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Combines multiple outputs into one.",
        "runnable": true
      }
    },
    "make-code": {
      "code": "/**\n * @fileoverview Generates code using supplied context.\n */\nimport invokeBoard from \"@invoke\";\nimport gemini, { defaultSafetySettings, } from \"./gemini\";\nimport { err, ok, toLLMContent, toText, addUserTurn } from \"./utils\";\nimport { Template } from \"./template\";\nimport { ToolManager } from \"./tool-manager\";\nimport {} from \"./common\";\nimport { report } from \"./output\";\nimport { ArgumentNameGenerator } from \"./introducer\";\nconst MAKE_CODE_ICON = \"generative-code\";\nexport { invoke as default, describe };\nfunction gatheringRequest(contents, instruction, language, toolManager) {\n    const promptText = `\nAnalyze the instruction below and rather than following it, determine what information needs to be gathered to \ngenerate an accurate prompt for a text-to-${language} model in the next turn:\n-- begin instruction --\n${toText(instruction)}\n-- end instruction --\n\nCall the tools to gather the necessary information that could be used to create an accurate prompt.`;\n    return new GeminiPrompt({\n        model: \"gemini-1.5-flash-latest\",\n        body: {\n            contents: addUserTurn(promptText, contents),\n            tools: toolManager.list(),\n            systemInstruction: toLLMContent(`\nYou are a researcher whose specialty is to call tools whose output helps gather the necessary information\nto be used to create an accurate prompt for a text-to-${language} model.\n`),\n        },\n    }, toolManager);\n}\nfunction promptRequest(contents, instruction, language) {\n    const context = contents?.length\n        ? \"using conversation context and these additional\"\n        : \"with these\";\n    const promptText = `Generate a single text-to-${language} prompt ${context} instructions:\n${toText(instruction)}\n\nTypical output format:\n\n## Setting/background\n\nDetailed description of everything that is understood about the ${language} code that is being requested.\n\n## Primary focus\n\nDetailed description of the primary functionality or the main focal point of the JavaScript code.\n\n## Style\n\nDetailed description of the style and approach of the code (defensive, TDD, creative, etc.). The output should\nalways an invariably be ${language === \"JavaScript\" ? \"EcmaScript JavaScript Modules\" : language} and be fully functional \nwithout any placeholders. \n\nIf you are dealing with JavaScript you may use imports if and only if the instruction indicates, otherwise you must\ncreate the functionality as a standalone piece of EcmaScript JavaScript.\n\nYou output will be fed directly into the text-to-${language} model, so it must be prompt only, no additional chit-chat\n`;\n    return new GeminiPrompt({\n        model: \"gemini-1.5-flash-latest\",\n        body: {\n            contents: addUserTurn(promptText, contents),\n            systemInstruction: toLLMContent(`\nYou are a world-class ${language} developer whose specialty is to write prompts for text-to-${language} models that \nalways generate valid outputs.\n\nThe prompt must describe every aspect of the functionality in great detail and describe the problem being solved \nin terms of data structures, algorithms, and style. You must use the instruction to fully understand and replicate\nany reference implementations you've been given and you must not augment or deviate from that style. You should\nensure that the prompt includes enough information to fully replicate that style with examples and maximal clarity.\n\nIf the code pertains to user interface work, you must also maximize the accessibility of the code generated with\nappropriate titles and labels for buttons, controls, inputs, etc, and they should never be empty.\n\nBe sure to export all relevant symbols so that the code can be used outside of the EcmaScript Module. Always do\nthis as named symbols rather than using default exports.\n\nIf writing JavaScript, and where a variable is private, use private fields (#field) rather than an underscore at the start.\n`),\n        },\n    });\n}\nfunction codeRequest(prompt, language) {\n    prompt.role = \"user\";\n    prompt.parts.unshift({\n        text: `Generate ${language} code based on this prompt. Output code only, no chit-chat`,\n    });\n    return new GeminiPrompt({\n        model: \"gemini-2.0-flash-exp\",\n        body: {\n            contents: [prompt],\n            generationConfig: {\n                responseModalities: [\"TEXT\"],\n            },\n            safetySettings: defaultSafetySettings(),\n        },\n    });\n}\nclass GeminiPrompt {\n    inputs;\n    toolManager;\n    constructor(inputs, toolManager) {\n        this.inputs = inputs;\n        this.toolManager = toolManager;\n    }\n    async invoke() {\n        const invoking = await gemini(this.inputs);\n        if (!ok(invoking))\n            return invoking;\n        if (\"context\" in invoking) {\n            return err(\"Invalid output from Gemini -- must be candidates\");\n        }\n        const content = invoking.candidates.at(0)?.content;\n        if (!content) {\n            return err(\"No content from Gemini\");\n        }\n        const results = [];\n        const errors = [];\n        await this.toolManager?.processResponse(content, async ($board, args) => {\n            const callingTool = await invokeBoard({ $board, ...args });\n            if (\"$error\" in callingTool) {\n                errors.push(JSON.stringify(callingTool.$error));\n            }\n            else {\n                results.push(JSON.stringify(callingTool));\n            }\n        });\n        if (errors.length) {\n            return err(`Calling tools generated the following errors: ${errors.join(\",\")}`);\n        }\n        const result = [content];\n        if (results.length) {\n            result.push(toLLMContent(results.join(\"\\n\\n\")));\n        }\n        return { all: result, last: result.at(-1) };\n    }\n}\nfunction gracefulExit(notOk) {\n    report({\n        actor: \"Make Code\",\n        category: \"Warning\",\n        name: \"Graceful exit\",\n        details: `I tried a couple of times, but the Gemini API failed to generate the code you requested with the following error:\n\n### ${notOk.$error}\n\nTo keep things moving, I will return a blank result. My apologies!`,\n        icon: MAKE_CODE_ICON,\n    });\n    return { context: [toLLMContent(\" \")] };\n}\nconst MAX_RETRIES = 5;\nasync function invoke({ context, instruction, language, ...params }) {\n    context ??= [];\n    // 1) Substitute params in instruction.\n    const toolManager = new ToolManager(new ArgumentNameGenerator());\n    const substituting = await new Template(instruction).substitute(params, async ({ path: url }) => toolManager.addTool(url));\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    instruction = substituting;\n    // 2) If there are tools in instruction, add an extra step of preparing\n    // information via tools.\n    if (toolManager.hasTools()) {\n        const gatheringInformation = await gatheringRequest(context, instruction, language, toolManager).invoke();\n        if (!ok(gatheringInformation))\n            return gatheringInformation;\n        context.push(...gatheringInformation.all);\n    }\n    let retryCount = MAX_RETRIES;\n    while (retryCount--) {\n        // 3) Call Gemini to generate prompt.\n        const generatingPrompt = await promptRequest(context, instruction, language).invoke();\n        if (!ok(generatingPrompt))\n            return generatingPrompt;\n        console.log(\"PROMPT\", toText(generatingPrompt.last));\n        // 4) Call Gemini to generate image.\n        const generatingCode = await codeRequest(generatingPrompt.last, language).invoke();\n        if (!ok(generatingCode)) {\n            return generatingCode;\n        }\n        return { context: generatingCode.all };\n    }\n    return gracefulExit(err(`Failed to generate ${language} after ${MAX_RETRIES} tries.`));\n}\nasync function describe({ inputs: { instruction } }) {\n    const template = new Template(instruction);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                instruction: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"Instruction\",\n                    description: \"Describe how to generate the JavaScript based on the input: focus, functionality, aim of the code\",\n                },\n                language: {\n                    type: \"string\",\n                    behavior: [\"hint-text\", \"config\", \"hint-preview\"],\n                    title: \"Language\",\n                    enum: [\"JavaScript\", \"HTML\", \"CSS\"],\n                    description: \"The language you'd like to generate\",\n                    default: \"JavaScript\",\n                },\n                ...template.schemas(),\n            },\n            ...template.requireds(),\n            additionalProperties: false,\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"hint-code\", \"main-port\"],\n                },\n            },\n            additionalProperties: false,\n        },\n        title: \"Make Code\",\n        metadata: {\n            icon: MAKE_CODE_ICON,\n            tags: [\"quick-access\", \"generative\", \"experimental\"],\n            order: 2,\n        },\n    };\n}\n",
      "metadata": {
        "title": "Make Code",
        "source": {
          "code": "/**\n * @fileoverview Generates code using supplied context.\n */\n\nimport invokeBoard from \"@invoke\";\n\nimport gemini, {\n  defaultSafetySettings,\n  type GeminiOutputs,\n  type GeminiInputs,\n  type Tool,\n} from \"./gemini\";\nimport { err, ok, toLLMContent, toText, addUserTurn } from \"./utils\";\nimport { Template } from \"./template\";\nimport { ToolManager } from \"./tool-manager\";\nimport { type Params } from \"./common\";\nimport { report } from \"./output\";\nimport { ArgumentNameGenerator } from \"./introducer\";\n\nconst MAKE_CODE_ICON = \"generative-code\";\n\ntype CodeGeneratorInputs = {\n  context: LLMContent[];\n  instruction: LLMContent;\n  language: string;\n} & Params;\n\ntype CodeGeneratorOutputs = {\n  context: LLMContent[];\n};\n\nexport { invoke as default, describe };\n\nfunction gatheringRequest(\n  contents: LLMContent[] | undefined,\n  instruction: LLMContent,\n  language: string,\n  toolManager: ToolManager\n): GeminiPrompt {\n  const promptText = `\nAnalyze the instruction below and rather than following it, determine what information needs to be gathered to \ngenerate an accurate prompt for a text-to-${language} model in the next turn:\n-- begin instruction --\n${toText(instruction)}\n-- end instruction --\n\nCall the tools to gather the necessary information that could be used to create an accurate prompt.`;\n  return new GeminiPrompt(\n    {\n      model: \"gemini-1.5-flash-latest\",\n      body: {\n        contents: addUserTurn(promptText, contents),\n        tools: toolManager.list(),\n        systemInstruction: toLLMContent(`\nYou are a researcher whose specialty is to call tools whose output helps gather the necessary information\nto be used to create an accurate prompt for a text-to-${language} model.\n`),\n      },\n    },\n    toolManager\n  );\n}\n\nfunction promptRequest(\n  contents: LLMContent[] | undefined,\n  instruction: LLMContent,\n  language: string\n): GeminiPrompt {\n  const context = contents?.length\n    ? \"using conversation context and these additional\"\n    : \"with these\";\n  const promptText = `Generate a single text-to-${language} prompt ${context} instructions:\n${toText(instruction)}\n\nTypical output format:\n\n## Setting/background\n\nDetailed description of everything that is understood about the ${language} code that is being requested.\n\n## Primary focus\n\nDetailed description of the primary functionality or the main focal point of the JavaScript code.\n\n## Style\n\nDetailed description of the style and approach of the code (defensive, TDD, creative, etc.). The output should\nalways an invariably be ${language === \"JavaScript\" ? \"EcmaScript JavaScript Modules\" : language} and be fully functional \nwithout any placeholders. \n\nIf you are dealing with JavaScript you may use imports if and only if the instruction indicates, otherwise you must\ncreate the functionality as a standalone piece of EcmaScript JavaScript.\n\nYou output will be fed directly into the text-to-${language} model, so it must be prompt only, no additional chit-chat\n`;\n  return new GeminiPrompt({\n    model: \"gemini-1.5-flash-latest\",\n    body: {\n      contents: addUserTurn(promptText, contents),\n      systemInstruction: toLLMContent(`\nYou are a world-class ${language} developer whose specialty is to write prompts for text-to-${language} models that \nalways generate valid outputs.\n\nThe prompt must describe every aspect of the functionality in great detail and describe the problem being solved \nin terms of data structures, algorithms, and style. You must use the instruction to fully understand and replicate\nany reference implementations you've been given and you must not augment or deviate from that style. You should\nensure that the prompt includes enough information to fully replicate that style with examples and maximal clarity.\n\nIf the code pertains to user interface work, you must also maximize the accessibility of the code generated with\nappropriate titles and labels for buttons, controls, inputs, etc, and they should never be empty.\n\nBe sure to export all relevant symbols so that the code can be used outside of the EcmaScript Module. Always do\nthis as named symbols rather than using default exports.\n\nIf writing JavaScript, and where a variable is private, use private fields (#field) rather than an underscore at the start.\n`),\n    },\n  });\n}\n\nfunction codeRequest(prompt: LLMContent, language: string): GeminiPrompt {\n  prompt.role = \"user\";\n  prompt.parts.unshift({\n    text: `Generate ${language} code based on this prompt. Output code only, no chit-chat`,\n  });\n\n  return new GeminiPrompt({\n    model: \"gemini-2.0-flash-exp\",\n    body: {\n      contents: [prompt],\n      generationConfig: {\n        responseModalities: [\"TEXT\"],\n      },\n      safetySettings: defaultSafetySettings(),\n    },\n  });\n}\n\nexport type GeminiPromptOutput = {\n  last: LLMContent;\n  all: LLMContent[];\n};\n\nclass GeminiPrompt {\n  constructor(\n    public readonly inputs: GeminiInputs,\n    public readonly toolManager?: ToolManager\n  ) {}\n\n  async invoke(): Promise<Outcome<GeminiPromptOutput>> {\n    const invoking = await gemini(this.inputs);\n    if (!ok(invoking)) return invoking;\n    if (\"context\" in invoking) {\n      return err(\"Invalid output from Gemini -- must be candidates\");\n    }\n    const content = invoking.candidates.at(0)?.content;\n    if (!content) {\n      return err(\"No content from Gemini\");\n    }\n    const results: string[] = [];\n    const errors: string[] = [];\n    await this.toolManager?.processResponse(content, async ($board, args) => {\n      const callingTool = await invokeBoard({ $board, ...args });\n      if (\"$error\" in callingTool) {\n        errors.push(JSON.stringify(callingTool.$error));\n      } else {\n        results.push(JSON.stringify(callingTool));\n      }\n    });\n    if (errors.length) {\n      return err(\n        `Calling tools generated the following errors: ${errors.join(\",\")}`\n      );\n    }\n    const result = [content];\n    if (results.length) {\n      result.push(toLLMContent(results.join(\"\\n\\n\")));\n    }\n    return { all: result, last: result.at(-1)! };\n  }\n}\n\nfunction gracefulExit(notOk: {\n  $error: string;\n}): Outcome<CodeGeneratorOutputs> {\n  report({\n    actor: \"Make Code\",\n    category: \"Warning\",\n    name: \"Graceful exit\",\n    details: `I tried a couple of times, but the Gemini API failed to generate the code you requested with the following error:\n\n### ${notOk.$error}\n\nTo keep things moving, I will return a blank result. My apologies!`,\n    icon: MAKE_CODE_ICON,\n  });\n  return { context: [toLLMContent(\" \")] };\n}\n\nconst MAX_RETRIES = 5;\n\nasync function invoke({\n  context,\n  instruction,\n  language,\n  ...params\n}: CodeGeneratorInputs): Promise<Outcome<CodeGeneratorOutputs>> {\n  context ??= [];\n\n  // 1) Substitute params in instruction.\n  const toolManager = new ToolManager(new ArgumentNameGenerator());\n  const substituting = await new Template(instruction).substitute(\n    params,\n    async ({ path: url }) => toolManager.addTool(url)\n  );\n  if (!ok(substituting)) {\n    return substituting;\n  }\n  instruction = substituting;\n\n  // 2) If there are tools in instruction, add an extra step of preparing\n  // information via tools.\n  if (toolManager.hasTools()) {\n    const gatheringInformation = await gatheringRequest(\n      context,\n      instruction,\n      language,\n      toolManager\n    ).invoke();\n    if (!ok(gatheringInformation)) return gatheringInformation;\n    context.push(...gatheringInformation.all);\n  }\n\n  let retryCount = MAX_RETRIES;\n\n  while (retryCount--) {\n    // 3) Call Gemini to generate prompt.\n    const generatingPrompt = await promptRequest(\n      context,\n      instruction,\n      language\n    ).invoke();\n    if (!ok(generatingPrompt)) return generatingPrompt;\n\n    console.log(\"PROMPT\", toText(generatingPrompt.last));\n\n    // 4) Call Gemini to generate image.\n    const generatingCode = await codeRequest(\n      generatingPrompt.last,\n      language\n    ).invoke();\n    if (!ok(generatingCode)) {\n      return generatingCode;\n    }\n\n    return { context: generatingCode.all };\n  }\n  return gracefulExit(\n    err(`Failed to generate ${language} after ${MAX_RETRIES} tries.`)\n  );\n}\n\ntype DescribeInputs = {\n  inputs: {\n    instruction?: LLMContent;\n  };\n};\n\nasync function describe({ inputs: { instruction } }: DescribeInputs) {\n  const template = new Template(instruction);\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n        instruction: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n          title: \"Instruction\",\n          description:\n            \"Describe how to generate the JavaScript based on the input: focus, functionality, aim of the code\",\n        },\n        language: {\n          type: \"string\",\n          behavior: [\"hint-text\", \"config\", \"hint-preview\"],\n          title: \"Language\",\n          enum: [\"JavaScript\", \"HTML\", \"CSS\"],\n          description: \"The language you'd like to generate\",\n          default: \"JavaScript\",\n        },\n        ...template.schemas(),\n      },\n      ...template.requireds(),\n      additionalProperties: false,\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n          behavior: [\"hint-code\", \"main-port\"],\n        },\n      },\n      additionalProperties: false,\n    } satisfies Schema,\n    title: \"Make Code\",\n    metadata: {\n      icon: MAKE_CODE_ICON,\n      tags: [\"quick-access\", \"generative\", \"experimental\"],\n      order: 2,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Generates code using supplied context.",
        "runnable": true
      }
    },
    "gemini-prompt": {
      "code": "/**\n * @fileoverview Add a description for your module here.\n */\nimport invokeBoard from \"@invoke\";\nimport gemini, {} from \"./gemini\";\nimport { ToolManager } from \"./tool-manager\";\nimport { ok, err, toLLMContent, addUserTurn } from \"./utils\";\nexport { GeminiPrompt };\nfunction textToJson(content) {\n    return {\n        ...content,\n        parts: content.parts.map((part) => {\n            if (\"text\" in part) {\n                try {\n                    return { json: JSON.parse(part.text) };\n                }\n                catch (e) {\n                    // fall through.\n                }\n            }\n            return part;\n        }),\n    };\n}\nfunction mergeLastParts(contexts) {\n    const parts = [];\n    for (const context of contexts) {\n        const last = context.at(-1);\n        if (!last)\n            continue;\n        if (!last.parts)\n            continue;\n        parts.push(...last.parts);\n    }\n    return {\n        parts,\n        role: \"user\",\n    };\n}\nclass GeminiPrompt {\n    inputs;\n    options;\n    constructor(inputs, options) {\n        this.inputs = inputs;\n        this.options = this.#reconcileOptions(options);\n    }\n    #reconcileOptions(options) {\n        if (!options)\n            return {};\n        if (options instanceof ToolManager) {\n            return { toolManager: options };\n        }\n        return options;\n    }\n    #normalizeArgs(a, passContext) {\n        if (!passContext)\n            return a;\n        const args = a;\n        const context = [...this.inputs.body.contents];\n        const hasContext = \"context\" in args;\n        let contextArg = hasContext\n            ? {}\n            : {\n                context,\n            };\n        return {\n            ...contextArg,\n            ...Object.fromEntries(Object.entries(args).map(([name, value]) => {\n                if (hasContext) {\n                    value = addUserTurn(value, [\n                        ...this.inputs.body.contents,\n                    ]);\n                }\n                return [name, value];\n            })),\n        };\n    }\n    async invoke() {\n        const { allowToolErrors, validator } = this.options;\n        const invoking = await gemini(this.inputs);\n        if (!ok(invoking))\n            return invoking;\n        if (\"context\" in invoking) {\n            return err(\"Invalid output from Gemini -- must be candidates\");\n        }\n        const candidate = invoking.candidates.at(0);\n        const content = candidate?.content;\n        if (!content) {\n            return err(\"No content from Gemini\");\n        }\n        if (!content.parts) {\n            return err(`Gemini failed to generate result due to ${candidate.finishReason}`);\n        }\n        const results = [];\n        const errors = [];\n        if (validator) {\n            const validating = validator(content);\n            if (!ok(validating))\n                return validating;\n        }\n        await this.options.toolManager?.processResponse(content, async ($board, args, passContext) => {\n            console.log(\"CALLING TOOL\", $board, args, passContext);\n            const callingTool = await invokeBoard({\n                $board,\n                ...this.#normalizeArgs(args, passContext),\n            });\n            if (\"$error\" in callingTool) {\n                errors.push(JSON.stringify(callingTool.$error));\n            }\n            else {\n                if (passContext) {\n                    if (!(\"context\" in callingTool)) {\n                        errors.push(`No \"context\" port in outputs of \"${$board}\"`);\n                    }\n                    else {\n                        results.push(callingTool.context);\n                    }\n                }\n                else {\n                    results.push([toLLMContent(JSON.stringify(callingTool))]);\n                }\n            }\n        });\n        console.log(\"ERRORS\", errors);\n        if (errors.length && !allowToolErrors) {\n            return err(`Calling tools generated the following errors: ${errors.join(\",\")}`);\n        }\n        const isJSON = this.inputs.body.generationConfig?.responseMimeType == \"application/json\";\n        const result = isJSON ? [textToJson(content)] : [content];\n        if (results.length) {\n            result.push(mergeLastParts(results));\n        }\n        return { all: result, last: result.at(-1), candidate };\n    }\n}\n",
      "metadata": {
        "title": "gemini-prompt",
        "source": {
          "code": "/**\n * @fileoverview Add a description for your module here.\n */\n\nimport invokeBoard from \"@invoke\";\n\nimport gemini, {\n  type GeminiInputs,\n  type GeminiOutputs,\n  type Candidate,\n} from \"./gemini\";\nimport { ToolManager } from \"./tool-manager\";\nimport { ok, err, toLLMContent, addUserTurn } from \"./utils\";\n\nexport { GeminiPrompt };\n\nfunction textToJson(content: LLMContent): LLMContent {\n  return {\n    ...content,\n    parts: content.parts.map((part) => {\n      if (\"text\" in part) {\n        try {\n          return { json: JSON.parse(part.text) };\n        } catch (e) {\n          // fall through.\n        }\n      }\n      return part;\n    }),\n  };\n}\n\nfunction mergeLastParts(contexts: LLMContent[][]): LLMContent {\n  const parts: DataPart[] = [];\n  for (const context of contexts) {\n    const last = context.at(-1);\n    if (!last) continue;\n    if (!last.parts) continue;\n    parts.push(...last.parts);\n  }\n  return {\n    parts,\n    role: \"user\",\n  };\n}\n\nexport type ValidatorFunction = (response: LLMContent) => Outcome<void>;\n\nexport type GeminiPromptOutput = {\n  last: LLMContent;\n  all: LLMContent[];\n  candidate: Candidate;\n};\n\nexport type GeminiPromptInvokeOptions = GeminiPromptOptions;\n\nexport type GeminiPromptOptions = {\n  allowToolErrors?: boolean;\n  validator?: ValidatorFunction;\n  toolManager?: ToolManager;\n};\nclass GeminiPrompt {\n  readonly options: GeminiPromptOptions;\n\n  constructor(\n    public readonly inputs: GeminiInputs,\n    options?: ToolManager | GeminiPromptOptions\n  ) {\n    this.options = this.#reconcileOptions(options);\n  }\n\n  #reconcileOptions(\n    options?: ToolManager | GeminiPromptOptions\n  ): GeminiPromptOptions {\n    if (!options) return {};\n    if (options instanceof ToolManager) {\n      return { toolManager: options };\n    }\n    return options;\n  }\n\n  #normalizeArgs(a: object, passContext?: boolean) {\n    if (!passContext) return a;\n    const args = a as Record<string, unknown>;\n    const context = [...this.inputs.body.contents];\n    const hasContext = \"context\" in args;\n    let contextArg = hasContext\n      ? {}\n      : {\n          context,\n        };\n    return {\n      ...contextArg,\n      ...Object.fromEntries(\n        Object.entries(args).map(([name, value]) => {\n          if (hasContext) {\n            value = addUserTurn(value as string, [\n              ...this.inputs.body.contents,\n            ]);\n          }\n          return [name, value];\n        })\n      ),\n    };\n  }\n\n  async invoke(): Promise<Outcome<GeminiPromptOutput>> {\n    const { allowToolErrors, validator } = this.options;\n    const invoking = await gemini(this.inputs);\n    if (!ok(invoking)) return invoking;\n    if (\"context\" in invoking) {\n      return err(\"Invalid output from Gemini -- must be candidates\");\n    }\n    const candidate = invoking.candidates.at(0);\n    const content = candidate?.content;\n    if (!content) {\n      return err(\"No content from Gemini\");\n    }\n    if (!content.parts) {\n      return err(\n        `Gemini failed to generate result due to ${candidate.finishReason}`\n      );\n    }\n    const results: LLMContent[][] = [];\n    const errors: string[] = [];\n    if (validator) {\n      const validating = validator(content);\n      if (!ok(validating)) return validating;\n    }\n    await this.options.toolManager?.processResponse(\n      content,\n      async ($board, args, passContext) => {\n        console.log(\"CALLING TOOL\", $board, args, passContext);\n        const callingTool = await invokeBoard({\n          $board,\n          ...this.#normalizeArgs(args, passContext),\n        });\n        if (\"$error\" in callingTool) {\n          errors.push(JSON.stringify(callingTool.$error));\n        } else {\n          if (passContext) {\n            if (!(\"context\" in callingTool)) {\n              errors.push(`No \"context\" port in outputs of \"${$board}\"`);\n            } else {\n              results.push(callingTool.context as LLMContent[]);\n            }\n          } else {\n            results.push([toLLMContent(JSON.stringify(callingTool))]);\n          }\n        }\n      }\n    );\n    console.log(\"ERRORS\", errors);\n    if (errors.length && !allowToolErrors) {\n      return err(\n        `Calling tools generated the following errors: ${errors.join(\",\")}`\n      );\n    }\n    const isJSON =\n      this.inputs.body.generationConfig?.responseMimeType == \"application/json\";\n    const result = isJSON ? [textToJson(content)] : [content];\n    if (results.length) {\n      result.push(mergeLastParts(results));\n    }\n    return { all: result, last: result.at(-1)!, candidate };\n  }\n}\n",
          "language": "typescript"
        },
        "description": "Add a description for your module here.",
        "runnable": false
      }
    },
    "step-executor": {
      "code": "/**\n * @fileoverview Utilities to execute tools on the AppCatalyst backend server.\n */\nexport { executeStep };\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\nimport { ok, err } from \"./utils\";\nfunction maybeExtractError(e) {\n    try {\n        const parsed = JSON.parse(e);\n        return parsed.error.message;\n    }\n    catch (error) {\n        return e;\n    }\n}\nasync function executeStep(body) {\n    // Get the API key.\n    const apiKeys = await secrets({ keys: [\"APP_CATALYST_GEMINI_KEY\"] });\n    const apiKey = apiKeys[\"APP_CATALYST_GEMINI_KEY\"];\n    // Get an authentication token.\n    const key = \"connection:$sign-in\";\n    const token = (await secrets({ keys: [key] }))[key];\n    // Call the API.\n    const url = \"https://staging-appcatalyst.sandbox.googleapis.com/v1beta1/executeStep\";\n    const fetchResult = await fetch({\n        url: url,\n        method: \"POST\",\n        headers: {\n            \"Content-Type\": \"application/json\",\n            Authorization: `Bearer ${token}`,\n        },\n        body: body,\n    });\n    let $error = \"Unknown error\";\n    if (!ok(fetchResult)) {\n        const { status, $error: errObject } = fetchResult;\n        console.warn($error);\n        if (!status) {\n            // This is not an error response, presume fatal error.\n            return { $error };\n        }\n        $error = maybeExtractError(errObject);\n        return { $error };\n    }\n    const response = fetchResult.response;\n    return response;\n}\n",
      "metadata": {
        "title": "step-executor",
        "source": {
          "code": "/**\n * @fileoverview Utilities to execute tools on the AppCatalyst backend server.\n */\n\nexport { executeStep };\n\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\nimport { ok, err } from \"./utils\";\n\ntype FetchErrorResponse = {\n  $error: string;\n  status: number;\n  statusText: string;\n  contentType: string;\n  responseHeaders: Record<string, string>;\n};\n\ntype Chunk = {\n  mimetype: string;\n  data: string;\n};\n\nexport type Content = {\n  chunks: Chunk[];\n};\n\nexport interface ContentMap {\n  [key: string]: Content;\n}\n\nexport interface ExecuteStepRequest {\n  planStep: {\n    stepName: string;\n    modelApi: string;\n    inputParameters: string[];\n    systemPrompt: string;\n    stepIntent?: string;\n  };\n  execution_inputs: ContentMap;\n}\n\nexport interface ExecuteStepResponse {\n  executionOutputs: ContentMap;\n}\n\nfunction maybeExtractError(e: string): string {\n  try {\n    const parsed = JSON.parse(e);\n    return parsed.error.message;\n  } catch (error) {\n    return e;\n  }\n}\n\nasync function executeStep(\n  body: ExecuteStepRequest\n): Promise<Outcome<ExecuteStepResponse>> {\n  // Get the API key.\n  const apiKeys = await secrets({ keys: [\"APP_CATALYST_GEMINI_KEY\"] });\n  const apiKey = apiKeys[\"APP_CATALYST_GEMINI_KEY\"];\n\n  // Get an authentication token.\n  const key = \"connection:$sign-in\";\n  const token = (await secrets({ keys: [key] }))[key];\n  // Call the API.\n  const url =\n    \"https://staging-appcatalyst.sandbox.googleapis.com/v1beta1/executeStep\";\n  const fetchResult = await fetch({\n    url: url,\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n      Authorization: `Bearer ${token}`,\n    },\n    body: body,\n  });\n  let $error: string = \"Unknown error\";\n  if (!ok(fetchResult)) {\n    const { status, $error: errObject } = fetchResult as FetchErrorResponse;\n    console.warn($error);\n    if (!status) {\n      // This is not an error response, presume fatal error.\n      return { $error };\n    }\n    $error = maybeExtractError(errObject);\n    return { $error };\n  }\n  const response = fetchResult.response as ExecuteStepResponse;\n  return response;\n}\n",
          "language": "typescript"
        },
        "description": "Utilities to execute tools on the AppCatalyst backend server.",
        "runnable": false
      }
    },
    "image-editor": {
      "code": "/**\n * @fileoverview Edits an image using the supplied context.\n */\nimport invokeBoard from \"@invoke\";\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\nimport { err, ok, isEmpty, toLLMContent, toLLMContentInline, toText, toInlineData, addUserTurn, llm, } from \"./utils\";\nimport { Template } from \"./template\";\nimport { executeStep } from \"./step-executor\";\nimport { ToolManager } from \"./tool-manager\";\nimport {} from \"./common\";\nimport { report } from \"./output\";\nimport { ArgumentNameGenerator } from \"./introducer\";\nconst MAKE_IMAGE_ICON = \"generative-image-edit\";\nconst MAGIC_KEY = \"<ORIGINAL_INPUT_IMAGE>\";\nconst STEP_NAME = \"AI Edit Image\";\nconst API_NAME = \"ai_image_editing\";\nexport { invoke as default, describe };\nasync function callImageEdit(instruction, image_content) {\n    const imageChunk = toInlineData(image_content);\n    if (!imageChunk) {\n        throw new Error(\"No image provided to image edit call\");\n    }\n    const encodedInstruction = btoa(unescape(encodeURIComponent(instruction)));\n    const body = {\n        planStep: {\n            stepName: STEP_NAME,\n            modelApi: API_NAME,\n            inputParameters: [\"input_image\", \"input_instruction\"],\n            systemPrompt: \"\",\n        },\n        execution_inputs: {\n            input_image: {\n                chunks: [\n                    {\n                        mimetype: imageChunk.mimeType,\n                        data: imageChunk.data,\n                    },\n                ],\n            },\n            input_instruction: {\n                chunks: [\n                    {\n                        mimetype: \"text/plain\",\n                        data: encodedInstruction,\n                    },\n                ],\n            },\n        },\n    };\n    // TODO(askerryryan): Remove once functional.\n    console.log(\"request body\");\n    console.log(body);\n    const response = await executeStep(body);\n    // TODO(askerryryan): Remove once functional.\n    console.log(\"response\");\n    console.log(response);\n    if (!ok(response)) {\n        return toLLMContent(\"Image Editing failed: \" + response.$error);\n    }\n    const outContent = response.executionOutputs[STEP_NAME];\n    if (!outContent) {\n        return toLLMContent(\"Error: No image returned from backend\");\n    }\n    return toLLMContentInline(outContent.chunks[0].mimetype, outContent.chunks[0].data);\n}\nfunction gracefulExit(notOk) {\n    report({\n        actor: \"Make Image\",\n        category: \"Warning\",\n        name: \"Graceful exit\",\n        details: `I tried a couple of times, but the Image Editing API failed to generate the image you requested with the following error:\n\n### ${notOk.$error}\n\nTo keep things moving, I will return a blank result. My apologies!`,\n        icon: MAKE_IMAGE_ICON,\n    });\n    return { context: [toLLMContent(\" \")] };\n}\nconst MAX_RETRIES = 5;\nfunction maybeAddMagicImageReference(inputString) {\n    if (inputString.includes(MAGIC_KEY)) {\n        return inputString;\n    }\n    else {\n        return inputString + \" \" + MAGIC_KEY;\n    }\n}\nfunction extractInlineData(context) {\n    const results = [];\n    for (let el of context) {\n        for (let part of el.parts) {\n            if (part) {\n                if (\"inlineData\" in part && part.inlineData) {\n                    results.push(toLLMContentInline(part.inlineData.mimeType, part.inlineData.data));\n                }\n            }\n        }\n    }\n    return results;\n}\nfunction extractTextData(context) {\n    const results = [];\n    for (let el of context) {\n        for (let part of el.parts) {\n            if (part) {\n                if (\"text\" in part && part.text) {\n                    results.push(toLLMContent(part.text));\n                }\n            }\n        }\n    }\n    return results;\n}\n/**\n * Handles 4 distinct cases:\n * 1) The editing directive (without explicit reference) is provided as static instruction\n * 2) The editing directive is provided as static instruction w/ interleaved @ reference to the image\n * 3) The editing directive is provided without explicit reference as context\n * 4) The editing directive is provided as context with explicit interleaved @reference to the image.\n * 3 + 4 Currently assume the directive is provided as @ in the instruction, but the image is not.\n * **/\nasync function invoke({ context, instruction, ...params }) {\n    context ??= [];\n    let instructionContext;\n    // 1) Extract any image and text data from context (with history).\n    let imageContext = extractInlineData(context);\n    const textContext = extractTextData(context);\n    // 2) Substitute variables and magic image reference.\n    // Note: it is important that images are not subsituted in here as they will\n    // not be handled properly. At this point, only text variables should be left.\n    const toolManager = new ToolManager(new ArgumentNameGenerator());\n    const substituting = await new Template(toLLMContent(toText(instruction).trim())).substitute(params, async ({ path: url }) => toolManager.addTool(url));\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    // 3) Extract image and text data from (non-history) references.\n    const refImages = extractInlineData([substituting]);\n    const refText = extractTextData([substituting]);\n    // 4) Combine with whatever data was extracted from context. Validate that\n    // we have exactly one image and some textual instruction.\n    imageContext = imageContext.concat(refImages);\n    if (imageContext.length != 1) {\n        return {\n            context: [\n                toLLMContent(\"AI image editing needs exactly one image input, please!\"),\n            ],\n        };\n    }\n    const combinedInstruction = toText(addUserTurn(toText(refText), textContext));\n    if (!combinedInstruction) {\n        return {\n            context: [toLLMContent(\"An image editing instruction must be provided.\")],\n        };\n    }\n    const updatedInstruction = maybeAddMagicImageReference(combinedInstruction);\n    console.log(\"PROMPT: \" + updatedInstruction);\n    let retryCount = MAX_RETRIES;\n    while (retryCount--) {\n        const generatedImage = await callImageEdit(updatedInstruction, imageContext[0]);\n        if (!ok(generatedImage)) {\n            return generatedImage;\n        }\n        return { context: [generatedImage] };\n    }\n    return gracefulExit(err(`Failed to generate a edited image after ${MAX_RETRIES} tries.`));\n}\nasync function describe({ inputs: { instruction } }) {\n    const template = new Template(instruction);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Image to edit\",\n                    behavior: [\"main-port\"],\n                },\n                instruction: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"Instruction\",\n                    description: \"Describe how to change or edit an image. Use @ to add the image to edit. Example: 'Make the person from @<your reference image>' have pink hair'\",\n                },\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"hint-image\", \"main-port\"],\n                },\n            },\n        },\n        title: \"Edit Image\",\n        metadata: {\n            icon: MAKE_IMAGE_ICON,\n            tags: [\"quick-access\", \"generative\", \"experimental\"],\n            order: 2,\n        },\n    };\n}\n",
      "metadata": {
        "title": "Image Edit",
        "source": {
          "code": "/**\n * @fileoverview Edits an image using the supplied context.\n */\n\nimport invokeBoard from \"@invoke\";\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\n\nimport {\n  err,\n  ok,\n  isEmpty,\n  toLLMContent,\n  toLLMContentInline,\n  toText,\n  toInlineData,\n  addUserTurn,\n  llm,\n} from \"./utils\";\nimport { Template } from \"./template\";\nimport { executeStep } from \"./step-executor\";\nimport type { ExecuteStepRequest, Content } from \"./step-executor\";\nimport { ToolManager } from \"./tool-manager\";\nimport { type Params } from \"./common\";\nimport { report } from \"./output\";\nimport { ArgumentNameGenerator } from \"./introducer\";\n\nconst MAKE_IMAGE_ICON = \"generative-image-edit\";\nconst MAGIC_KEY = \"<ORIGINAL_INPUT_IMAGE>\";\nconst STEP_NAME = \"AI Edit Image\";\nconst API_NAME = \"ai_image_editing\";\n\ntype ImageGeneratorInputs = {\n  context: LLMContent[];\n  instruction: LLMContent;\n} & Params;\n\ntype ImageGeneratorOutputs = {\n  context: LLMContent[];\n};\n\nexport { invoke as default, describe };\n\nasync function callImageEdit(\n  instruction: string,\n  image_content: LLMContent\n): Promise<LLMContent> {\n  const imageChunk = toInlineData(image_content);\n  if (!imageChunk) {\n    throw new Error(\"No image provided to image edit call\");\n  }\n  const encodedInstruction = btoa(unescape(encodeURIComponent(instruction)));\n  const body = {\n    planStep: {\n      stepName: STEP_NAME,\n      modelApi: API_NAME,\n      inputParameters: [\"input_image\", \"input_instruction\"],\n      systemPrompt: \"\",\n    },\n    execution_inputs: {\n      input_image: {\n        chunks: [\n          {\n            mimetype: imageChunk.mimeType,\n            data: imageChunk.data,\n          },\n        ],\n      },\n      input_instruction: {\n        chunks: [\n          {\n            mimetype: \"text/plain\",\n            data: encodedInstruction,\n          },\n        ],\n      },\n    },\n  } satisfies ExecuteStepRequest;\n  // TODO(askerryryan): Remove once functional.\n  console.log(\"request body\");\n  console.log(body);\n  const response = await executeStep(body);\n  // TODO(askerryryan): Remove once functional.\n  console.log(\"response\");\n  console.log(response);\n  if (!ok(response)) {\n    return toLLMContent(\"Image Editing failed: \" + response.$error);\n  }\n\n  const outContent = response.executionOutputs[STEP_NAME];\n  if (!outContent) {\n    return toLLMContent(\"Error: No image returned from backend\");\n  }\n  return toLLMContentInline(\n    outContent.chunks[0].mimetype,\n    outContent.chunks[0].data\n  );\n}\n\nfunction gracefulExit(notOk: {\n  $error: string;\n}): Outcome<ImageGeneratorOutputs> {\n  report({\n    actor: \"Make Image\",\n    category: \"Warning\",\n    name: \"Graceful exit\",\n    details: `I tried a couple of times, but the Image Editing API failed to generate the image you requested with the following error:\n\n### ${notOk.$error}\n\nTo keep things moving, I will return a blank result. My apologies!`,\n    icon: MAKE_IMAGE_ICON,\n  });\n  return { context: [toLLMContent(\" \")] };\n}\n\nconst MAX_RETRIES = 5;\n\nfunction maybeAddMagicImageReference(inputString: string): string {\n  if (inputString.includes(MAGIC_KEY)) {\n    return inputString;\n  } else {\n    return inputString + \" \" + MAGIC_KEY;\n  }\n}\n\nfunction extractInlineData(context: LLMContent[]): LLMContent[] {\n  const results = [];\n  for (let el of context) {\n    for (let part of el.parts) {\n      if (part) {\n        if (\"inlineData\" in part && part.inlineData) {\n          results.push(\n            toLLMContentInline(part.inlineData.mimeType, part.inlineData.data)\n          );\n        }\n      }\n    }\n  }\n  return results;\n}\n\nfunction extractTextData(context: LLMContent[]): LLMContent[] {\n  const results = [];\n  for (let el of context) {\n    for (let part of el.parts) {\n      if (part) {\n        if (\"text\" in part && part.text) {\n          results.push(toLLMContent(part.text));\n        }\n      }\n    }\n  }\n  return results;\n}\n\n/**\n * Handles 4 distinct cases:\n * 1) The editing directive (without explicit reference) is provided as static instruction\n * 2) The editing directive is provided as static instruction w/ interleaved @ reference to the image\n * 3) The editing directive is provided without explicit reference as context\n * 4) The editing directive is provided as context with explicit interleaved @reference to the image.\n * 3 + 4 Currently assume the directive is provided as @ in the instruction, but the image is not.\n * **/\nasync function invoke({\n  context,\n  instruction,\n  ...params\n}: ImageGeneratorInputs): Promise<Outcome<ImageGeneratorOutputs>> {\n  context ??= [];\n  let instructionContext;\n  // 1) Extract any image and text data from context (with history).\n  let imageContext = extractInlineData(context);\n  const textContext = extractTextData(context);\n\n  // 2) Substitute variables and magic image reference.\n  // Note: it is important that images are not subsituted in here as they will\n  // not be handled properly. At this point, only text variables should be left.\n  const toolManager = new ToolManager(new ArgumentNameGenerator());\n  const substituting = await new Template(\n    toLLMContent(toText(instruction).trim())\n  ).substitute(params, async ({ path: url }) => toolManager.addTool(url));\n  if (!ok(substituting)) {\n    return substituting;\n  }\n  // 3) Extract image and text data from (non-history) references.\n  const refImages = extractInlineData([substituting]);\n  const refText = extractTextData([substituting]);\n\n  // 4) Combine with whatever data was extracted from context. Validate that\n  // we have exactly one image and some textual instruction.\n  imageContext = imageContext.concat(refImages);\n  if (imageContext.length != 1) {\n    return {\n      context: [\n        toLLMContent(\"AI image editing needs exactly one image input, please!\"),\n      ],\n    };\n  }\n\n  const combinedInstruction = toText(addUserTurn(toText(refText), textContext));\n  if (!combinedInstruction) {\n    return {\n      context: [toLLMContent(\"An image editing instruction must be provided.\")],\n    };\n  }\n  const updatedInstruction = maybeAddMagicImageReference(combinedInstruction);\n  console.log(\"PROMPT: \" + updatedInstruction);\n\n  let retryCount = MAX_RETRIES;\n  while (retryCount--) {\n    const generatedImage = await callImageEdit(\n      updatedInstruction,\n      imageContext[0]\n    );\n    if (!ok(generatedImage)) {\n      return generatedImage;\n    }\n\n    return { context: [generatedImage] };\n  }\n  return gracefulExit(\n    err(`Failed to generate a edited image after ${MAX_RETRIES} tries.`)\n  );\n}\n\ntype DescribeInputs = {\n  inputs: {\n    instruction?: LLMContent;\n  };\n};\n\nasync function describe({ inputs: { instruction } }: DescribeInputs) {\n  const template = new Template(instruction);\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Image to edit\",\n          behavior: [\"main-port\"],\n        },\n        instruction: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n          title: \"Instruction\",\n          description:\n            \"Describe how to change or edit an image. Use @ to add the image to edit. Example: 'Make the person from @<your reference image>' have pink hair'\",\n        },\n        ...template.schemas(),\n      },\n      behavior: [\"at-wireable\"],\n      ...template.requireds(),\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n          behavior: [\"hint-image\", \"main-port\"],\n        },\n      },\n    } satisfies Schema,\n    title: \"Edit Image\",\n    metadata: {\n      icon: MAKE_IMAGE_ICON,\n      tags: [\"quick-access\", \"generative\", \"experimental\"],\n      order: 2,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Edits an image using the supplied context.",
        "runnable": true
      }
    },
    "introducer": {
      "code": "/**\n * @fileoverview Handles introduction of the step.\n */\nimport { defaultSafetySettings } from \"./gemini\";\nimport { toLLMContent, toText, ok, err, llm } from \"./utils\";\nimport { ToolManager } from \"./tool-manager\";\nimport { GeminiPrompt } from \"./gemini-prompt\";\nimport {} from \"./common\";\nexport { Introducer, ArgumentNameGenerator };\nfunction introductionSchema() {\n    return {\n        type: \"object\",\n        properties: {\n            title: {\n                type: \"string\",\n                description: \"The title of the agent\",\n            },\n            abilities: {\n                type: \"string\",\n                description: \"Verb-first, third-person summary of the agent's abilities\",\n            },\n            argument: {\n                type: \"string\",\n                description: \"The description of the single text argument that the agent takes as input\",\n            },\n        },\n    };\n}\n/**\n * Attempts to adjust the describer result for subgraphs.\n * Accounts for LLMContent[] `context` property\n * and parameters\n */\nclass ArgumentNameGenerator {\n    #containsContext(describerResult) {\n        if (!describerResult.inputSchema?.properties)\n            return true;\n        const context = describerResult.inputSchema.properties[\"context\"];\n        if (!context)\n            return false;\n        if (context.type === \"array\" && context.items) {\n            return !!context.items.behavior?.includes(\"llm-content\");\n        }\n        return false;\n    }\n    async transform(describerResult) {\n        // If there's no `context` property, exit early.\n        if (!this.#containsContext(describerResult)) {\n            return null;\n        }\n        const { title, description } = describerResult;\n        // Fail transform when there's no title or description.\n        // The resulting function declaration will be a dud anyway.\n        if (!title || !description) {\n            return err(`Custom tool must have a title and a description`);\n        }\n        // Add parameters to the describer.\n        const required = [];\n        const params = Object.fromEntries(Object.entries(describerResult.inputSchema?.properties || {})\n            .filter(([name]) => {\n            if (name === \"context\")\n                return false;\n            required.push(name);\n            return true;\n        })\n            .map(([name, value]) => {\n            return [\n                name,\n                {\n                    ...value,\n                    type: \"string\",\n                },\n            ];\n        }));\n        if (required.length > 0) {\n            return {\n                ...describerResult,\n                inputSchema: {\n                    type: \"object\",\n                    properties: params,\n                    required,\n                },\n            };\n        }\n        // When no parameters found, try to discern the parameter name\n        // from description and title.\n        const naming = await new GeminiPrompt({\n            body: {\n                contents: [this.prompt(describerResult)],\n                safetySettings: defaultSafetySettings(),\n                generationConfig: {\n                    responseSchema: this.schema(),\n                    responseMimeType: \"application/json\",\n                },\n            },\n        }).invoke();\n        if (!ok(naming))\n            return naming;\n        const result = naming.last.parts.at(0).json;\n        return {\n            ...describerResult,\n            inputSchema: {\n                type: \"object\",\n                properties: {\n                    context: {\n                        type: \"string\",\n                        description: result.description,\n                    },\n                },\n            },\n        };\n    }\n    schema() {\n        return {\n            type: \"object\",\n            properties: {\n                description: {\n                    type: \"string\",\n                    description: \"One-sentence description of a function argument\",\n                },\n            },\n        };\n    }\n    prompt(describerResult) {\n        return llm `\nYou are amazing at describing things. Today, you will be coming up a one-sentence description \nof a function argument.\n\nThe function's title is: ${describerResult.title}\n\nThe function's description is ${describerResult.description}\n\nIt takes a single argument.\n\nCome up with a one-sentence description of this argument based on the title/description,\nwith the aim of using this description in a JSON Schema.\n`.asContent();\n    }\n}\nclass Introducer {\n    instruction;\n    toolManager;\n    constructor(instruction, toolManager) {\n        this.instruction = instruction;\n        this.toolManager = toolManager;\n    }\n    prompt() {\n        const tools = this.toolManager?.list() || [];\n        let toolInstruction = \"You have no access to tools of any kind.\";\n        if (tools.length > 0) {\n            toolInstruction = `You have access to the following tools:\n${tools.map((tool) => JSON.stringify(tool)).join(\"\\n\\n\")}\nMake sure to mention your ability to call these particular tools in your abilities.\n`;\n        }\n        return toLLMContent(`You are a project manager for team of AI agents.\nRight now, your job is to summarize what one AI agent does.\nGiven an agent prompt, you distill it to a brief summary of abilities.\nThis summary will be used to decide when to invoke this agent.\n# AI Agent Prompt\n\\`\\`\\`\n${toText(this.instruction)}\n${toolInstruction}\n\\`\\`\\`\nReply in JSON using the provided schema.\n`);\n    }\n    async invoke() {\n        const introducing = await new GeminiPrompt({\n            body: {\n                contents: [this.prompt()],\n                safetySettings: defaultSafetySettings(),\n                generationConfig: {\n                    responseSchema: introductionSchema(),\n                    responseMimeType: \"application/json\",\n                },\n            },\n        }).invoke();\n        if (!ok(introducing))\n            return introducing;\n        const intro = introducing.last.parts.at(0)\n            .json;\n        return {\n            title: intro.title,\n            description: intro.abilities,\n            inputSchema: {\n                type: \"object\",\n                properties: {\n                    context: {\n                        type: \"string\",\n                        description: intro.argument,\n                    },\n                },\n            },\n        };\n    }\n}\n",
      "metadata": {
        "title": "introducer",
        "source": {
          "code": "/**\n * @fileoverview Handles introduction of the step.\n */\n\nimport { type Tool, defaultSafetySettings, type GeminiSchema } from \"./gemini\";\nimport { toLLMContent, toText, ok, err, llm } from \"./utils\";\nimport { ToolManager } from \"./tool-manager\";\nimport { GeminiPrompt } from \"./gemini-prompt\";\nimport {\n  type DescriberResult,\n  type DescriberResultTransformer,\n} from \"./common\";\n\nexport { Introducer, ArgumentNameGenerator };\n\nexport type IntroPort = {\n  $intro: boolean;\n};\n\ntype Introduction = {\n  title: string;\n  abilities: string;\n  argument: string;\n};\n\nfunction introductionSchema(): GeminiSchema {\n  return {\n    type: \"object\",\n    properties: {\n      title: {\n        type: \"string\",\n        description: \"The title of the agent\",\n      },\n      abilities: {\n        type: \"string\",\n        description:\n          \"Verb-first, third-person summary of the agent's abilities\",\n      },\n      argument: {\n        type: \"string\",\n        description:\n          \"The description of the single text argument that the agent takes as input\",\n      },\n    },\n  };\n}\n\ntype NamingResult = {\n  description: string;\n};\n\n/**\n * Attempts to adjust the describer result for subgraphs.\n * Accounts for LLMContent[] `context` property\n * and parameters\n */\nclass ArgumentNameGenerator implements DescriberResultTransformer {\n  #containsContext(describerResult: DescriberResult): boolean {\n    if (!describerResult.inputSchema?.properties) return true;\n    const context = describerResult.inputSchema.properties[\"context\"];\n    if (!context) return false;\n    if (context.type === \"array\" && context.items) {\n      return !!(context.items as Schema).behavior?.includes(\"llm-content\");\n    }\n    return false;\n  }\n\n  async transform(\n    describerResult: DescriberResult\n  ): Promise<Outcome<DescriberResult | null>> {\n    // If there's no `context` property, exit early.\n    if (!this.#containsContext(describerResult)) {\n      return null;\n    }\n    const { title, description } = describerResult;\n\n    // Fail transform when there's no title or description.\n    // The resulting function declaration will be a dud anyway.\n    if (!title || !description) {\n      return err(`Custom tool must have a title and a description`);\n    }\n\n    // Add parameters to the describer.\n    const required: string[] = [];\n    const params = Object.fromEntries(\n      Object.entries(describerResult.inputSchema?.properties || {})\n        .filter(([name]) => {\n          if (name === \"context\") return false;\n          required.push(name);\n          return true;\n        })\n        .map(([name, value]) => {\n          return [\n            name,\n            {\n              ...value,\n              type: \"string\",\n            },\n          ];\n        })\n    );\n    if (required.length > 0) {\n      return {\n        ...describerResult,\n        inputSchema: {\n          type: \"object\",\n          properties: params,\n          required,\n        },\n      };\n    }\n\n    // When no parameters found, try to discern the parameter name\n    // from description and title.\n    const naming = await new GeminiPrompt({\n      body: {\n        contents: [this.prompt(describerResult)],\n        safetySettings: defaultSafetySettings(),\n        generationConfig: {\n          responseSchema: this.schema(),\n          responseMimeType: \"application/json\",\n        },\n      },\n    }).invoke();\n    if (!ok(naming)) return naming;\n    const result = (naming.last.parts.at(0) as JSONPart).json as NamingResult;\n\n    return {\n      ...describerResult,\n      inputSchema: {\n        type: \"object\",\n        properties: {\n          context: {\n            type: \"string\",\n            description: result.description,\n          },\n        },\n      },\n    };\n  }\n\n  schema(): GeminiSchema {\n    return {\n      type: \"object\",\n      properties: {\n        description: {\n          type: \"string\",\n          description: \"One-sentence description of a function argument\",\n        },\n      },\n    };\n  }\n\n  prompt(describerResult: DescriberResult): LLMContent {\n    return llm`\nYou are amazing at describing things. Today, you will be coming up a one-sentence description \nof a function argument.\n\nThe function's title is: ${describerResult.title}\n\nThe function's description is ${describerResult.description}\n\nIt takes a single argument.\n\nCome up with a one-sentence description of this argument based on the title/description,\nwith the aim of using this description in a JSON Schema.\n`.asContent();\n  }\n}\n\nclass Introducer {\n  constructor(\n    public readonly instruction: LLMContent,\n    public readonly toolManager?: ToolManager\n  ) {}\n\n  prompt(): LLMContent {\n    const tools = this.toolManager?.list() || [];\n    let toolInstruction = \"You have no access to tools of any kind.\";\n    if (tools.length > 0) {\n      toolInstruction = `You have access to the following tools:\n${tools.map((tool) => JSON.stringify(tool)).join(\"\\n\\n\")}\nMake sure to mention your ability to call these particular tools in your abilities.\n`;\n    }\n    return toLLMContent(`You are a project manager for team of AI agents.\nRight now, your job is to summarize what one AI agent does.\nGiven an agent prompt, you distill it to a brief summary of abilities.\nThis summary will be used to decide when to invoke this agent.\n# AI Agent Prompt\n\\`\\`\\`\n${toText(this.instruction)}\n${toolInstruction}\n\\`\\`\\`\nReply in JSON using the provided schema.\n`);\n  }\n\n  async invoke(): Promise<Outcome<DescriberResult>> {\n    const introducing = await new GeminiPrompt({\n      body: {\n        contents: [this.prompt()],\n        safetySettings: defaultSafetySettings(),\n        generationConfig: {\n          responseSchema: introductionSchema(),\n          responseMimeType: \"application/json\",\n        },\n      },\n    }).invoke();\n    if (!ok(introducing)) return introducing;\n    const intro = (introducing.last.parts.at(0) as JSONPart)\n      .json as Introduction;\n\n    return {\n      title: intro.title,\n      description: intro.abilities,\n      inputSchema: {\n        type: \"object\",\n        properties: {\n          context: {\n            type: \"string\",\n            description: intro.argument,\n          },\n        },\n      },\n    };\n  }\n}\n",
          "language": "typescript"
        },
        "description": "Handles introduction of the step.",
        "runnable": false
      }
    },
    "html-generator": {
      "code": "/**\n * @fileoverview Utility for calling generate_webpage tool.\n */\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\nimport { err, ok, toLLMContent, toLLMContentInline, toText } from \"./utils\";\nimport { executeStep } from \"./step-executor\";\nimport { report } from \"./output\";\nexport { callGenWebpage };\nasync function callGenWebpage(instruction, content) {\n    const executionInputs = {};\n    const inputParameters = [];\n    let i = 0;\n    for (let val of content) {\n        for (let part of val.parts) {\n            i++;\n            if (\"text\" in part) {\n                const key = `text_${i}`;\n                inputParameters.push(key);\n                const encodedText = btoa(unescape(encodeURIComponent(part.text)));\n                executionInputs[key] = {\n                    chunks: [\n                        {\n                            mimetype: \"text/plain\",\n                            data: encodedText,\n                        },\n                    ],\n                };\n            }\n            else if (\"inlineData\" in part) {\n                const key = `media_${i}`;\n                inputParameters.push(key);\n                executionInputs[key] = {\n                    chunks: [\n                        {\n                            mimetype: part.inlineData.mimeType,\n                            data: part.inlineData.data,\n                        },\n                    ],\n                };\n            }\n            else {\n                console.error(\"Skipping unexpected content part\");\n            }\n        }\n    }\n    const body = {\n        planStep: {\n            stepName: \"GenerateWebpage\",\n            modelApi: \"generate_webpage\",\n            inputParameters: inputParameters,\n            systemPrompt: instruction,\n            stepIntent: instruction,\n        },\n        execution_inputs: executionInputs,\n    };\n    // Add the contents\n    // TODO(askerryryan): Remove once functional.\n    console.log(\"request body\");\n    console.log(body);\n    const response = await executeStep(body);\n    // TODO(askerryryan): Remove once functional.\n    console.log(\"response\");\n    console.log(response);\n    if (!ok(response)) {\n        return toLLMContent(\"Webpage generation failed: \" + response.$error);\n    }\n    let returnVal;\n    const outputChunk = response.executionOutputs[\"GenerateWebpage\"];\n    if (!outputChunk) {\n        return toLLMContent(\"Error: Malformed response\");\n    }\n    const mimetype = outputChunk.chunks[0].mimetype;\n    if (mimetype == \"text/html\") {\n        returnVal = toLLMContentInline(mimetype, atob(outputChunk.chunks[0].data));\n    }\n    else {\n        returnVal = toLLMContent(atob(outputChunk.chunks[0].data));\n    }\n    if (!returnVal) {\n        return toLLMContent(\"Error: No webpage returned from backend\");\n    }\n    return returnVal;\n}\n",
      "metadata": {
        "title": "html-generator",
        "source": {
          "code": "/**\n * @fileoverview Utility for calling generate_webpage tool.\n */\n\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\n\nimport { err, ok, toLLMContent, toLLMContentInline, toText } from \"./utils\";\nimport { executeStep } from \"./step-executor\";\nimport type { ExecuteStepRequest, Content, ContentMap } from \"./step-executor\";\nimport { report } from \"./output\";\n\nexport { callGenWebpage };\n\nasync function callGenWebpage(\n  instruction: string,\n  content: LLMContent[]\n): Promise<LLMContent> {\n  const executionInputs: ContentMap = {};\n  const inputParameters: string[] = [];\n  let i = 0;\n  for (let val of content) {\n    for (let part of val.parts) {\n      i++;\n      if (\"text\" in part) {\n        const key = `text_${i}`;\n        inputParameters.push(key);\n        const encodedText = btoa(unescape(encodeURIComponent(part.text)));\n        executionInputs[key] = {\n          chunks: [\n            {\n              mimetype: \"text/plain\",\n              data: encodedText,\n            },\n          ],\n        };\n      } else if (\"inlineData\" in part) {\n        const key = `media_${i}`;\n        inputParameters.push(key);\n        executionInputs[key] = {\n          chunks: [\n            {\n              mimetype: part.inlineData.mimeType,\n              data: part.inlineData.data,\n            },\n          ],\n        };\n      } else {\n        console.error(\"Skipping unexpected content part\");\n      }\n    }\n  }\n  const body = {\n    planStep: {\n      stepName: \"GenerateWebpage\",\n      modelApi: \"generate_webpage\",\n      inputParameters: inputParameters,\n      systemPrompt: instruction,\n      stepIntent: instruction,\n    },\n    execution_inputs: executionInputs,\n  } satisfies ExecuteStepRequest;\n  // Add the contents\n  // TODO(askerryryan): Remove once functional.\n  console.log(\"request body\");\n  console.log(body);\n  const response = await executeStep(body);\n  // TODO(askerryryan): Remove once functional.\n  console.log(\"response\");\n  console.log(response);\n  if (!ok(response)) {\n    return toLLMContent(\"Webpage generation failed: \" + response.$error);\n  }\n\n  let returnVal;\n  const outputChunk = response.executionOutputs[\"GenerateWebpage\"];\n  if (!outputChunk) {\n    return toLLMContent(\"Error: Malformed response\");\n  }\n  const mimetype = outputChunk.chunks[0].mimetype;\n  if (mimetype == \"text/html\") {\n    returnVal = toLLMContentInline(mimetype, atob(outputChunk.chunks[0].data));\n  } else {\n    returnVal = toLLMContent(atob(outputChunk.chunks[0].data));\n  }\n  if (!returnVal) {\n    return toLLMContent(\"Error: No webpage returned from backend\");\n  }\n  return returnVal;\n}\n",
          "language": "typescript"
        },
        "description": "Utility for calling generate_webpage tool.",
        "runnable": false
      }
    },
    "render-outputs": {
      "code": "/**\n * @fileoverview Renders multiple outputs into single display.\n */\nimport { Template } from \"./template\";\nimport { ok, toText, isEmpty } from \"./utils\";\nimport { callGenWebpage } from \"./html-generator\";\nexport { invoke as default, describe };\nasync function invoke({ text, instruction, \"p-auto-render\": autoRender, ...params }) {\n    const template = new Template(text);\n    const substituting = await template.substitute(params, async () => \"\");\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    let out = substituting;\n    if (autoRender) {\n        if (!instruction) {\n            instruction = \"Render content with markdown format.\";\n        }\n        instruction +=\n            \" Assume content will render on a mobile device. Use a responsive or mobile-friendly layout whenever possible and minimize unnecessary padding or margins.\";\n        console.log(\"Generating output based on instruction: \", instruction);\n        const webPage = callGenWebpage(instruction, [substituting]);\n        if (!ok(webPage)) {\n            console.error(\"Failed to generated output\");\n        }\n        else {\n            out = await webPage;\n            console.log(out);\n        }\n    }\n    return { context: [out] };\n}\nasync function describe({ inputs: { text } }) {\n    const template = new Template(text);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                text: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"hint-preview\", \"config\", \"at-wireable\"],\n                    title: \"Outputs to render\",\n                    description: \"Type the @ character to select the outputs to combine\",\n                },\n                instruction: {\n                    type: \"text\",\n                    behavior: [\"hint-preview\", \"config\"],\n                    title: \"Display instructions\",\n                    description: \"Give any style or layout guidelines for how the content should be rendered. Requires Auto-Combine to be enabled.\",\n                },\n                \"p-auto-render\": {\n                    type: \"boolean\",\n                    title: \"Auto Combine\",\n                    behavior: [\"config\", \"hint-preview\"],\n                    description: \"If enabled, Gemini will automatically decide how to best combine and render the outputs. Otherwise, outputs are displayed in exactly the order specified.\",\n                },\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                    },\n                    title: \"Context out\",\n                    behavior: [\"main-port\", \"hint-multimodal\"],\n                },\n            },\n        },\n        title: \"Render Outputs\",\n        metadata: {\n            icon: \"combine-outputs\",\n            tags: [\"quick-access\", \"core\"],\n            order: 100,\n        },\n    };\n}\n",
      "metadata": {
        "title": "render-outputs",
        "source": {
          "code": "/**\n * @fileoverview Renders multiple outputs into single display.\n */\n\nimport { Template } from \"./template\";\nimport { ok, toText, isEmpty } from \"./utils\";\nimport { callGenWebpage } from \"./html-generator\";\n\nexport { invoke as default, describe };\n\ntype InvokeInputs = {\n  text?: LLMContent;\n  instruction?: string;\n  \"p-auto-render\": boolean;\n};\n\ntype DescribeInputs = {\n  inputs: {\n    text?: LLMContent;\n  };\n};\n\nasync function invoke({\n  text,\n  instruction,\n  \"p-auto-render\": autoRender,\n  ...params\n}: InvokeInputs) {\n  const template = new Template(text);\n  const substituting = await template.substitute(params, async () => \"\");\n  if (!ok(substituting)) {\n    return substituting;\n  }\n  let out = substituting;\n  if (autoRender) {\n    if (!instruction) {\n      instruction = \"Render content with markdown format.\";\n    }\n    instruction +=\n      \" Assume content will render on a mobile device. Use a responsive or mobile-friendly layout whenever possible and minimize unnecessary padding or margins.\";\n    console.log(\"Generating output based on instruction: \", instruction);\n    const webPage = callGenWebpage(instruction, [substituting]);\n    if (!ok(webPage)) {\n      console.error(\"Failed to generated output\");\n    } else {\n      out = await webPage;\n      console.log(out);\n    }\n  }\n  return { context: [out] };\n}\n\nasync function describe({ inputs: { text } }: DescribeInputs) {\n  const template = new Template(text);\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        text: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"hint-preview\", \"config\", \"at-wireable\"],\n          title: \"Outputs to render\",\n          description: \"Type the @ character to select the outputs to combine\",\n        },\n        instruction: {\n          type: \"text\",\n          behavior: [\"hint-preview\", \"config\"],\n          title: \"Display instructions\",\n          description:\n            \"Give any style or layout guidelines for how the content should be rendered. Requires Auto-Combine to be enabled.\",\n        },\n        \"p-auto-render\": {\n          type: \"boolean\",\n          title: \"Auto Combine\",\n          behavior: [\"config\", \"hint-preview\"],\n          description:\n            \"If enabled, Gemini will automatically decide how to best combine and render the outputs. Otherwise, outputs are displayed in exactly the order specified.\",\n        },\n        ...template.schemas(),\n      },\n      behavior: [\"at-wireable\"],\n      ...template.requireds(),\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: {\n            type: \"object\",\n            behavior: [\"llm-content\"],\n          },\n          title: \"Context out\",\n          behavior: [\"main-port\", \"hint-multimodal\"],\n        },\n      },\n    } satisfies Schema,\n    title: \"Render Outputs\",\n    metadata: {\n      icon: \"combine-outputs\",\n      tags: [\"quick-access\", \"core\"],\n      order: 100,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Renders multiple outputs into single display.",
        "runnable": true
      }
    },
    "lists": {
      "code": "/**\n * @fileoverview Handles lists according to https://github.com/breadboard-ai/breadboard/wiki/Step-Listification\n */\nimport { ok, err, generateId, mergeTextParts } from \"./utils\";\nexport { fanOutContext, flattenContext, hasLists, addContent };\nfunction isListPart(o) {\n    return !!o && \"list\" in o;\n}\nfunction emptyContent() {\n    return { parts: [{ text: \"\" }] };\n}\nfunction addContent(context, content) {\n    const last = context.at(-1);\n    const maybeList = last?.parts?.at(0);\n    const remainder = context.slice(0, -1);\n    if (isListPart(maybeList)) {\n        const list = maybeList.list.map((item) => ({\n            ...item,\n            content: [...item.content, content],\n        }));\n        return [...remainder, { ...last, parts: [{ ...maybeList, list }] }];\n    }\n    return [...context, content];\n}\nfunction unzipContent(content) {\n    // 1) Scan content for lists.\n    const info = new Set();\n    const ids = new Set();\n    let maxLength = 0;\n    content.parts.forEach((part, index) => {\n        if (!isListPart(part))\n            return;\n        const length = part.list.length;\n        if (length > maxLength)\n            maxLength = length;\n        info.add(index);\n        ids.add(part.id);\n    });\n    if (ids.size > 1) {\n        console.warn(\"Multiple list sources aren't yet supported in instruction, using the first one\");\n    }\n    const id = [...ids].at(0) || \"\";\n    if (info.size === 0) {\n        return { contents: [content], id };\n    }\n    // 2) Create a list and replace lists with list entries.\n    return {\n        id,\n        contents: new Array(maxLength).fill(0).map((_, entryIndex) => {\n            const parts = mergeTextParts(content.parts.flatMap((part, partIndex) => {\n                if (!info.has(partIndex))\n                    return part;\n                // We know this exists, so we're ok with not checking\n                // for existence.\n                const item = part.list.at(entryIndex);\n                if (!item) {\n                    return [];\n                }\n                const last = item.content.at(-1);\n                return last ? last.parts : [];\n            }));\n            return { ...content, parts };\n        }),\n    };\n}\nfunction hasLists(context) {\n    return isListPart(context.at(-1)?.parts?.at(0));\n}\nasync function fanOutContext(instruction, context, transformer, path) {\n    const instructions = unzipContent(instruction);\n    context ??= [];\n    let list = [];\n    let id;\n    // Look at the first part of the last context and see if it's a list.\n    const maybeList = context.at(-1)?.parts?.at(0);\n    const remainder = context.slice(0, -1);\n    const originalListItems = [];\n    if (isListPart(maybeList)) {\n        id = maybeList.id;\n        for (const item of maybeList.list) {\n            originalListItems.push(item.content);\n            list.push([...remainder, ...item.content]);\n        }\n    }\n    else {\n        id = instructions.id;\n        list = new Array(instructions.contents.length).fill(context);\n    }\n    const results = await Promise.all(list.map(async (item, index) => {\n        const itemInstruction = instructions.contents.at(index) ||\n            instructions.contents.at(0) ||\n            emptyContent();\n        return transformer(itemInstruction, item);\n    }));\n    const errors = results.filter((result) => \"$error\" in result);\n    if (errors.length > 0) {\n        return err(`List operation failed with the following errors:\n${errors\n            .map((error) => {\n            return error.$error;\n        })\n            .join(\"\\n\")}`);\n    }\n    if (results.length > 1) {\n        const newListItem = {\n            parts: [\n                {\n                    id,\n                    list: results.map((item, i) => ({\n                        content: [...(originalListItems[i] || []), item],\n                    })),\n                },\n            ],\n        };\n        return [...remainder, newListItem];\n    }\n    const newContextItem = results.at(-1);\n    return [...context, newContextItem];\n}\nfunction flattenContext(context, all = false) {\n    context ??= []; // Look at the first part of the last context and see if it's a list.\n    const last = context.at(-1);\n    if (!last)\n        return context;\n    if (all) {\n        return context.map((content) => flattenContent(content, all)).flat();\n    }\n    const remainder = context.slice(0, -1);\n    return [...remainder, ...flattenContent(last, all)];\n}\nfunction zipContexts(contexts) {\n    let maxLength = 0;\n    contexts.forEach((context) => {\n        if (maxLength < context.length)\n            maxLength = context.length;\n    });\n    return new Array(maxLength).fill(0).map((_, index) => {\n        let role;\n        const parts = mergeTextParts(contexts\n            .map((context) => {\n            const item = context.at(index);\n            if (!item)\n                return null;\n            if (!role)\n                role = item.role;\n            return item.parts;\n        })\n            .filter((item) => item !== null)\n            .flat());\n        role ??= \"user\";\n        return {\n            parts,\n            role,\n        };\n    });\n}\nfunction flattenContent(content, all = false) {\n    let hadList = false;\n    const flattened = content.parts\n        .map((part) => {\n        if (isListPart(part)) {\n            hadList = true;\n            return zipContexts(part.list.map((item) => item.content));\n        }\n        return {\n            parts: [part],\n            role: content.role,\n        };\n    })\n        .flat();\n    if (!hadList)\n        return [content];\n    return flattened;\n}\n",
      "metadata": {
        "title": "lists",
        "source": {
          "code": "/**\n * @fileoverview Handles lists according to https://github.com/breadboard-ai/breadboard/wiki/Step-Listification\n */\nimport { ok, err, generateId, mergeTextParts } from \"./utils\";\n\nexport { fanOutContext, flattenContext, hasLists, addContent };\n\ntype ContentTransformer = (\n  instruction: LLMContent,\n  context: LLMContent[]\n) => Promise<Outcome<LLMContent>>;\n\ntype UnzippedResult = {\n  contents: LLMContent[];\n  id: string;\n};\n\nfunction isListPart(o: DataPart | undefined): o is ListPart {\n  return !!o && \"list\" in o;\n}\n\nfunction emptyContent(): LLMContent {\n  return { parts: [{ text: \"\" }] };\n}\n\nfunction addContent(context: LLMContent[], content: LLMContent): LLMContent[] {\n  const last = context.at(-1);\n  const maybeList = last?.parts?.at(0);\n  const remainder = context.slice(0, -1);\n  if (isListPart(maybeList)) {\n    const list = maybeList.list.map((item) => ({\n      ...item,\n      content: [...item.content, content],\n    }));\n    return [...remainder, { ...last, parts: [{ ...maybeList, list }] }];\n  }\n  return [...context, content];\n}\n\nfunction unzipContent(content: LLMContent): UnzippedResult {\n  // 1) Scan content for lists.\n  const info: Set<number> = new Set();\n  const ids: Set<string> = new Set();\n  let maxLength = 0;\n  content.parts.forEach((part, index) => {\n    if (!isListPart(part)) return;\n    const length = part.list.length;\n    if (length > maxLength) maxLength = length;\n    info.add(index);\n    ids.add(part.id);\n  });\n  if (ids.size > 1) {\n    console.warn(\n      \"Multiple list sources aren't yet supported in instruction, using the first one\"\n    );\n  }\n  const id = [...ids].at(0) || \"\";\n  if (info.size === 0) {\n    return { contents: [content], id };\n  }\n  // 2) Create a list and replace lists with list entries.\n  return {\n    id,\n    contents: new Array(maxLength).fill(0).map((_, entryIndex) => {\n      const parts = mergeTextParts(\n        content.parts.flatMap((part, partIndex) => {\n          if (!info.has(partIndex)) return part;\n\n          // We know this exists, so we're ok with not checking\n          // for existence.\n          const item = (part as ListPart).list.at(entryIndex);\n          if (!item) {\n            return [];\n          }\n          const last = item.content.at(-1);\n          return last ? last.parts : [];\n        })\n      );\n      return { ...content, parts };\n    }),\n  };\n}\n\nfunction hasLists(context: LLMContent[]): boolean {\n  return isListPart(context.at(-1)?.parts?.at(0));\n}\n\nasync function fanOutContext(\n  instruction: LLMContent,\n  context: LLMContent[] | undefined,\n  transformer: ContentTransformer,\n  path?: number[]\n): Promise<Outcome<LLMContent[]>> {\n  const instructions = unzipContent(instruction);\n  context ??= [];\n  let list: LLMContent[][] = [];\n  let id: string;\n  // Look at the first part of the last context and see if it's a list.\n  const maybeList = context.at(-1)?.parts?.at(0);\n  const remainder = context.slice(0, -1);\n  const originalListItems: LLMContent[][] = [];\n  if (isListPart(maybeList)) {\n    id = maybeList.id;\n    for (const item of maybeList.list) {\n      originalListItems.push(item.content);\n      list.push([...remainder, ...item.content]);\n    }\n  } else {\n    id = instructions.id;\n    list = new Array(instructions.contents.length).fill(context);\n  }\n  const results = await Promise.all(\n    list.map(async (item, index) => {\n      const itemInstruction =\n        instructions.contents.at(index) ||\n        instructions.contents.at(0) ||\n        emptyContent();\n      return transformer(itemInstruction, item);\n    })\n  );\n  const errors = results.filter((result) => \"$error\" in result);\n  if (errors.length > 0) {\n    return err(`List operation failed with the following errors:\n${errors\n  .map((error) => {\n    return error.$error;\n  })\n  .join(\"\\n\")}`);\n  }\n  if (results.length > 1) {\n    const newListItem = {\n      parts: [\n        {\n          id,\n          list: (results as LLMContent[]).map((item, i) => ({\n            content: [...(originalListItems[i] || []), item],\n          })),\n        },\n      ],\n    };\n    return [...remainder, newListItem];\n  }\n  const newContextItem = results.at(-1)! as LLMContent;\n  return [...context, newContextItem];\n}\n\nfunction flattenContext(\n  context: LLMContent[] | undefined,\n  all = false\n): LLMContent[] {\n  context ??= []; // Look at the first part of the last context and see if it's a list.\n  const last = context.at(-1);\n  if (!last) return context;\n  if (all) {\n    return context.map((content) => flattenContent(content, all)).flat();\n  }\n  const remainder = context.slice(0, -1);\n  return [...remainder, ...flattenContent(last, all)];\n}\n\nfunction zipContexts(contexts: LLMContent[][]): LLMContent[] {\n  let maxLength = 0;\n  contexts.forEach((context) => {\n    if (maxLength < context.length) maxLength = context.length;\n  });\n  return new Array(maxLength).fill(0).map((_, index) => {\n    let role: string | undefined;\n    const parts = mergeTextParts(\n      contexts\n        .map((context) => {\n          const item = context.at(index);\n          if (!item) return null;\n          if (!role) role = item.role;\n          return item.parts;\n        })\n        .filter((item) => item !== null)\n        .flat()\n    );\n    role ??= \"user\";\n    return {\n      parts,\n      role,\n    };\n  });\n}\n\nfunction flattenContent(content: LLMContent, all = false): LLMContent[] {\n  let hadList = false;\n  const flattened = content.parts\n    .map((part) => {\n      if (isListPart(part)) {\n        hadList = true;\n        return zipContexts(part.list.map((item) => item.content));\n      }\n      return {\n        parts: [part],\n        role: content.role,\n      } as LLMContent;\n    })\n    .flat();\n  if (!hadList) return [content];\n  return flattened;\n}\n",
          "language": "typescript"
        },
        "description": "Handles lists according to https://github.com/breadboard-ai/breadboard/wiki/Step-Listification",
        "runnable": false
      }
    }
  },
  "exports": [
    "#daf082ca-c1aa-4aff-b2c8-abeb984ab66c",
    "#module:researcher",
    "#module:image-generator",
    "#module:image-editor",
    "#module:render-outputs",
    "#module:audio-generator",
    "#21ee02e7-83fa-49d0-964c-0cab10eafc2c",
    "#module:combine-outputs",
    "#module:make-code"
  ],
  "graphs": {
    "daf082ca-c1aa-4aff-b2c8-abeb984ab66c": {
      "title": "Make Text",
      "description": "Generates text and so much more.",
      "version": "0.0.1",
      "describer": "module:entry",
      "nodes": [
        {
          "type": "output",
          "id": "output",
          "configuration": {
            "schema": {
              "properties": {
                "context": {
                  "type": "array",
                  "title": "Context",
                  "items": {
                    "type": "object",
                    "behavior": [
                      "llm-content"
                    ]
                  },
                  "default": "null"
                }
              },
              "type": "object",
              "required": []
            }
          },
          "metadata": {
            "visual": {
              "x": 720,
              "y": 0,
              "collapsed": "expanded",
              "outputHeight": 44
            }
          }
        },
        {
          "id": "board-f138aa03",
          "type": "#module:entry",
          "metadata": {
            "visual": {
              "x": -46.99999999999966,
              "y": -71.99999999999898,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "entry"
          }
        },
        {
          "id": "board-d340ad8f",
          "type": "#module:agent-main",
          "metadata": {
            "visual": {
              "x": 340,
              "y": 0,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "Generating draft",
            "logLevel": "info"
          },
          "configuration": {}
        },
        {
          "id": "board-1946064a",
          "type": "#module:join",
          "metadata": {
            "visual": {
              "x": 1059.9999999999986,
              "y": -159.99999999999886,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "join"
          }
        },
        {
          "type": "input",
          "id": "input",
          "metadata": {
            "visual": {
              "x": 720.0000000000005,
              "y": 160.00000000000114,
              "collapsed": "advanced",
              "outputHeight": 44
            },
            "title": "Waiting for user feedback",
            "logLevel": "info"
          },
          "configuration": {}
        }
      ],
      "edges": [
        {
          "from": "board-f138aa03",
          "to": "board-d340ad8f",
          "out": "context",
          "in": "context"
        },
        {
          "from": "board-d340ad8f",
          "to": "output",
          "out": "done",
          "in": "context"
        },
        {
          "from": "input",
          "to": "board-1946064a",
          "out": "request",
          "in": "request"
        },
        {
          "from": "board-d340ad8f",
          "to": "input",
          "out": "toInput",
          "in": "schema"
        },
        {
          "from": "board-d340ad8f",
          "to": "board-1946064a",
          "out": "context",
          "in": "context"
        },
        {
          "from": "board-1946064a",
          "to": "board-d340ad8f",
          "out": "context",
          "in": "context"
        }
      ],
      "metadata": {
        "visual": {
          "minimized": false
        },
        "describer": "module:entry",
        "tags": []
      }
    },
    "21ee02e7-83fa-49d0-964c-0cab10eafc2c": {
      "title": "Ask User",
      "description": "A block of text as input or output",
      "version": "0.0.1",
      "nodes": [
        {
          "type": "input",
          "id": "input",
          "metadata": {
            "visual": {
              "x": 580.0000000000005,
              "y": -539.9999999999994,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "Waiting for user input",
            "logLevel": "info"
          },
          "configuration": {}
        },
        {
          "type": "output",
          "id": "output",
          "configuration": {
            "schema": {
              "properties": {
                "context": {
                  "type": "array",
                  "title": "Context",
                  "items": {
                    "type": "object",
                    "behavior": [
                      "llm-content"
                    ]
                  },
                  "default": "null"
                }
              },
              "type": "object",
              "required": []
            }
          },
          "metadata": {
            "visual": {
              "x": 1240.0000000000005,
              "y": -399.99999999999943,
              "collapsed": "expanded",
              "outputHeight": 44
            }
          }
        },
        {
          "id": "board-64b2c3a8",
          "type": "#module:text-entry",
          "metadata": {
            "visual": {
              "x": 225.9030760391795,
              "y": -646.8568148490385,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "text-entry"
          }
        },
        {
          "id": "board-95a57400",
          "type": "#module:text-main",
          "metadata": {
            "visual": {
              "x": 900,
              "y": -459.99999999999943,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "text-main"
          }
        }
      ],
      "edges": [
        {
          "from": "board-64b2c3a8",
          "out": "toInput",
          "to": "input",
          "in": "schema"
        },
        {
          "from": "board-64b2c3a8",
          "out": "toMain",
          "to": "board-95a57400",
          "in": "request"
        },
        {
          "from": "board-95a57400",
          "to": "output",
          "out": "context",
          "in": "context"
        },
        {
          "from": "board-64b2c3a8",
          "to": "board-95a57400",
          "out": "context",
          "in": "context"
        },
        {
          "from": "input",
          "to": "board-95a57400",
          "out": "request",
          "in": "request"
        }
      ],
      "metadata": {
        "visual": {
          "minimized": false
        },
        "tags": [],
        "describer": "module:text-entry",
        "icon": "text"
      }
    }
  },
  "assets": {
    "@@thumbnail": {
      "metadata": {
        "title": "Thumbnail",
        "type": "file"
      },
      "data": "data:image/svg+xml;base64,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"
    }
  }
}