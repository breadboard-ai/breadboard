{
  "title": "A2",
  "description": "Components that help you build flows.",
  "version": "0.0.1",
  "nodes": [],
  "edges": [],
  "metadata": {
    "comments": [
      {
        "id": "comment-b09617ef",
        "text": "Left Intentionally Blank",
        "metadata": {
          "visual": {
            "x": -37.90624999999966,
            "y": -415.8554687499999,
            "collapsed": "expanded",
            "outputHeight": 0
          }
        }
      }
    ],
    "visual": {
      "presentation": {
        "themes": {
          "54f81cc4-5c04-4d9d-b831-985d556f0ed9": {
            "themeColors": {
              "primaryColor": "#246db5",
              "secondaryColor": "#5cadff",
              "backgroundColor": "#ffffff",
              "textColor": "#1a1a1a",
              "primaryTextColor": "#ffffff"
            },
            "template": "basic",
            "splashScreen": {
              "storedData": {
                "handle": "/images/app/generic-flow.jpg",
                "mimeType": "image/jpeg"
              }
            }
          }
        },
        "theme": "54f81cc4-5c04-4d9d-b831-985d556f0ed9"
      }
    },
    "tags": [
      "published",
      "tool",
      "component"
    ],
    "icon": "text"
  },
  "modules": {
    "common": {
      "code": "/**\n * @fileoverview Common types and code\n */\n",
      "metadata": {
        "title": "common",
        "source": {
          "code": "/**\n * @fileoverview Common types and code\n */\n\nexport type UserInput = LLMContent;\n\nexport type Params = {\n  [key: `p-z-${string}`]: unknown;\n};\n\nexport type GraphTag =\n  | \"connector\"\n  | \"connector-configure\"\n  | \"connector-load\"\n  | \"connector-save\";\n\nexport type ExportDescriberResult = {\n  title?: string;\n  description?: string;\n  metadata?: {\n    icon?: string;\n    tags?: GraphTag[];\n  };\n  inputSchema?: Schema;\n};\n\nexport type DescriberResult = {\n  title?: string;\n  metadata?: {\n    tags?: GraphTag[];\n  };\n  description?: string;\n  inputSchema?: Schema;\n  outputSchema?: Schema;\n  exports?: Record<string, ExportDescriberResult>;\n};\n\nexport type DescriberResultTransformer = {\n  /**\n   * Returns a transformed result, a `null` if no transformation happened,\n   * or an error as Outcome.\n   */\n  transform(result: DescriberResult): Promise<Outcome<DescriberResult | null>>;\n};\n\nexport type CallToolCallback = (\n  tool: string,\n  args: object,\n  passContext?: boolean\n) => Promise<void>;\n\nexport type AgentInputs = {\n  /**\n   * Whether (true) or not (false) the agent is allowed to chat with user.\n   */\n  chat: boolean;\n  /**\n   * Whether (true) or not (false) to try to turn the output into a list\n   */\n  makeList: boolean;\n  /**\n   * The incoming conversation context.\n   */\n  context: LLMContent[];\n  /**\n   * Accumulated work context. This is the internal conversation, a result\n   * of talking with the user, for instance.\n   * This context is discarded at the end of interacting with the agent.\n   */\n  work: LLMContent[];\n  /**\n   * The index path to the currently processed list.\n   */\n  listPath: number[];\n  /**\n   * Agent's job description.\n   */\n  description?: LLMContent;\n  /**\n   * Last work product.\n   */\n  last?: LLMContent;\n  /**\n   * Type of the task.\n   */\n  type: \"introduction\" | \"work\";\n  /**\n   * The board URL of the model\n   */\n  model: string;\n  /**\n   * The default model that is passed along by the manager\n   */\n  defaultModel: string;\n  /**\n   * The tools that the worker can use\n   */\n  tools?: string[];\n  /**\n   * params\n   */\n  params: Params;\n};\n\nexport type AgentContext = AgentInputs & {\n  /**\n   * A unique identifier for the session.\n   * Currently used to have a persistent part separator across conversation context\n   */\n  id: string;\n  /**\n   * Accumulating list of user inputs\n   */\n  userInputs: UserInput[];\n  /**\n   * Indicator that the user ended chat.\n   */\n  userEndedChat: boolean;\n};\n\nexport type DescribeInputs = {\n  inputs: {\n    description: LLMContent | undefined;\n  };\n};\n",
          "language": "typescript"
        },
        "description": "Common types and code",
        "runnable": false
      }
    },
    "utils": {
      "code": "/**\n * @fileoverview Common utils for manipulating LLM Content and other relevant types.\n */\nexport { isLLMContent, isLLMContentArray, toLLMContent, toInlineData, toLLMContentInline, toInlineReference, toLLMContentStored, toText, joinContent, contentToJSON, defaultLLMContent, endsWithRole, addUserTurn, isEmpty, isStoredData, llm, ok, err, generateId, mergeTextParts, toTextConcat, extractTextData, extractInlineData, extractMediaData, };\nfunction ok(o) {\n    return !(o && typeof o === \"object\" && \"$error\" in o);\n}\nfunction err($error) {\n    return { $error };\n}\nfunction mergeTextParts(parts, separator = \"\") {\n    const merged = [];\n    let text = \"\";\n    for (const part of parts) {\n        if (\"text\" in part) {\n            text += `${part.text}${separator}`;\n        }\n        else {\n            if (text) {\n                merged.push({ text });\n            }\n            text = \"\";\n            merged.push(part);\n        }\n    }\n    if (text) {\n        merged.push({ text });\n    }\n    return merged;\n}\nclass LLMTemplate {\n    strings;\n    values;\n    constructor(strings, values) {\n        this.strings = strings;\n        this.values = values;\n    }\n    asParts() {\n        return mergeTextParts(this.strings.flatMap((s, i) => {\n            let text = s;\n            const value = this.values.at(i);\n            if (value == undefined) {\n                return { text };\n            }\n            else if (typeof value === \"string\") {\n                text += value;\n                return { text };\n            }\n            else if (value instanceof LLMTemplate) {\n                return value.asParts();\n            }\n            else if (isLLMContent(value)) {\n                return [{ text }, ...value.parts];\n            }\n            else {\n                text += JSON.stringify(value);\n                return { text };\n            }\n        }));\n    }\n    asContent() {\n        const parts = this.asParts();\n        return { parts, role: \"user\" };\n    }\n}\nfunction llm(strings, ...values) {\n    return new LLMTemplate(strings, values);\n}\n/**\n * Copied from @google-labs/breadboard\n */\nfunction isLLMContent(nodeValue) {\n    if (typeof nodeValue !== \"object\" || !nodeValue)\n        return false;\n    if (nodeValue === null || nodeValue === undefined)\n        return false;\n    if (\"role\" in nodeValue && nodeValue.role === \"$metadata\") {\n        return true;\n    }\n    return \"parts\" in nodeValue && Array.isArray(nodeValue.parts);\n}\nfunction isLLMContentArray(nodeValue) {\n    if (!Array.isArray(nodeValue))\n        return false;\n    if (nodeValue.length === 0)\n        return true;\n    return isLLMContent(nodeValue.at(-1));\n}\nfunction toLLMContent(text, role = \"user\") {\n    return { parts: [{ text }], role };\n}\nfunction endsWithRole(c, role) {\n    const last = c.at(-1);\n    if (!last)\n        return false;\n    return last.role === role;\n}\nfunction isEmpty(c) {\n    if (!c.parts.length)\n        return true;\n    for (const part of c.parts) {\n        if (\"text\" in part) {\n            if (part.text.trim().length > 0)\n                return false;\n        }\n        else {\n            return false;\n        }\n    }\n    return true;\n}\nfunction isStoredData(c) {\n    const part = c.parts.at(-1);\n    if (!part) {\n        return false;\n    }\n    return \"storedData\" in part && part.storedData != null;\n}\nfunction toText(c) {\n    if (isLLMContent(c)) {\n        return contentToText(c);\n    }\n    const last = c.at(-1);\n    if (!last)\n        return \"\";\n    return contentToText(last).trim();\n    function contentToText(content) {\n        return content.parts\n            .map((part) => (\"text\" in part ? part.text : \"\"))\n            .join(\"\\n\\n\");\n    }\n}\nfunction contentToJSON(content) {\n    const part = content?.parts?.at(0);\n    if (!part || !(\"text\" in part)) {\n        throw new Error(\"Invalid response from Gemini\");\n    }\n    return JSON.parse(part.text);\n}\nfunction defaultLLMContent() {\n    return JSON.stringify({\n        parts: [{ text: \"\" }],\n        role: \"user\",\n    });\n}\nfunction joinContent(content, context, dropHistory = false) {\n    if (dropHistory && context) {\n        let last = context.at(-1);\n        let retainedContext;\n        if (last) {\n            retainedContext = [last];\n        }\n        return addUserTurn(content, retainedContext);\n    }\n    return addUserTurn(content, context);\n}\nfunction extractInlineData(context) {\n    const results = [];\n    for (let el of context) {\n        for (let part of el.parts) {\n            if (part) {\n                if (\"inlineData\" in part && part.inlineData) {\n                    results.push(toLLMContentInline(part.inlineData.mimeType, part.inlineData.data));\n                }\n            }\n        }\n    }\n    return results;\n}\nfunction extractMediaData(context) {\n    const results = [];\n    for (let el of context) {\n        for (let part of el.parts) {\n            if (part) {\n                if (\"inlineData\" in part && part.inlineData) {\n                    results.push(toLLMContentInline(part.inlineData.mimeType, part.inlineData.data));\n                }\n                if (\"storedData\" in part && part.storedData) {\n                    results.push(toLLMContentStored(part.storedData.mimeType, part.storedData.handle));\n                }\n            }\n        }\n    }\n    return results;\n}\nfunction extractTextData(context) {\n    const results = [];\n    for (let el of context) {\n        for (let part of el.parts) {\n            if (part) {\n                if (\"text\" in part && part.text) {\n                    results.push(toLLMContent(part.text));\n                }\n            }\n        }\n    }\n    return results;\n}\nfunction toTextConcat(content) {\n    return content.map(toText).join(\"\\n\\n\");\n}\nfunction addUserTurn(content, context) {\n    context ??= [];\n    const isString = typeof content === \"string\";\n    if (!endsWithRole(context, \"user\")) {\n        return [...context, isString ? toLLMContent(content) : content];\n    }\n    const last = context.at(-1);\n    if (isString) {\n        last.parts.push({ text: content });\n    }\n    else {\n        last.parts.push(...content.parts);\n    }\n    return context;\n}\nfunction toLLMContentInline(mimetype, value, role = \"user\") {\n    return {\n        parts: [\n            {\n                inlineData: {\n                    mimeType: mimetype,\n                    data: value,\n                },\n            },\n        ],\n        role,\n    };\n}\nfunction toLLMContentStored(mimetype, handle, role = \"user\") {\n    return {\n        parts: [\n            {\n                storedData: {\n                    mimeType: mimetype,\n                    handle: handle,\n                },\n            },\n        ],\n        role,\n    };\n}\nfunction toInlineData(c) {\n    if (isLLMContent(c)) {\n        return contentToInlineData(c);\n    }\n    const last = c.at(-1);\n    if (!last)\n        return null;\n    return contentToInlineData(last);\n    function contentToInlineData(content) {\n        const part = content.parts.at(-1);\n        if (!part)\n            return \"\";\n        return \"inlineData\" in part && part.inlineData ? part.inlineData : null;\n    }\n}\n// TODO(askerryryan): Move this to the middleware.\nfunction toInlineReference(c) {\n    const last = c.parts.at(-1);\n    if (last == undefined || !(\"storedData\" in last)) {\n        return toInlineData(c);\n    }\n    const blobId = last.storedData.handle.split(\"/\").slice(-1)[0];\n    const gcs_handle = \"bb-blob-store/\" + blobId;\n    return toInlineData(toLLMContentInline(\"text/gcs-path\", btoa(gcs_handle)));\n}\nexport function mergeContent(content, role) {\n    const parts = [];\n    for (const el of content) {\n        for (const part of el.parts) {\n            parts.push(part);\n        }\n    }\n    return {\n        parts: parts,\n        role: role,\n    };\n}\nfunction generateId() {\n    return Math.random().toString(36).substring(2, 5);\n}\n",
      "metadata": {
        "title": "utils",
        "source": {
          "code": "/**\n * @fileoverview Common utils for manipulating LLM Content and other relevant types.\n */\nexport {\n  isLLMContent,\n  isLLMContentArray,\n  toLLMContent,\n  toInlineData,\n  toLLMContentInline,\n  toInlineReference,\n  toLLMContentStored,\n  toText,\n  joinContent,\n  contentToJSON,\n  defaultLLMContent,\n  endsWithRole,\n  addUserTurn,\n  isEmpty,\n  isStoredData,\n  llm,\n  ok,\n  err,\n  generateId,\n  mergeTextParts,\n  toTextConcat,\n  extractTextData,\n  extractInlineData,\n  extractMediaData,\n};\n\nexport type NonPromise<T> = T extends Promise<unknown> ? never : T;\n\nfunction ok<T>(o: Outcome<NonPromise<T>>): o is NonPromise<T> {\n  return !(o && typeof o === \"object\" && \"$error\" in o);\n}\n\nfunction err($error: string) {\n  return { $error };\n}\n\nfunction mergeTextParts(\n  parts: LLMContent[\"parts\"],\n  separator = \"\"\n): LLMContent[\"parts\"] {\n  const merged: LLMContent[\"parts\"] = [];\n  let text: string = \"\";\n  for (const part of parts) {\n    if (\"text\" in part) {\n      text += `${part.text}${separator}`;\n    } else {\n      if (text) {\n        merged.push({ text });\n      }\n      text = \"\";\n      merged.push(part);\n    }\n  }\n  if (text) {\n    merged.push({ text });\n  }\n  return merged;\n}\n\nclass LLMTemplate {\n  constructor(\n    public readonly strings: TemplateStringsArray,\n    public readonly values: unknown[]\n  ) {}\n\n  asParts(): LLMContent[\"parts\"] {\n    return mergeTextParts(\n      this.strings.flatMap((s, i) => {\n        let text = s;\n        const value = this.values.at(i);\n        if (value == undefined) {\n          return { text };\n        } else if (typeof value === \"string\") {\n          text += value;\n          return { text };\n        } else if (value instanceof LLMTemplate) {\n          return value.asParts();\n        } else if (isLLMContent(value)) {\n          return [{ text }, ...value.parts];\n        } else {\n          text += JSON.stringify(value);\n          return { text };\n        }\n      })\n    );\n  }\n\n  asContent(): LLMContent {\n    const parts = this.asParts();\n    return { parts, role: \"user\" };\n  }\n}\n\nfunction llm(strings: TemplateStringsArray, ...values: unknown[]): LLMTemplate {\n  return new LLMTemplate(strings, values);\n}\n\n/**\n * Copied from @google-labs/breadboard\n */\nfunction isLLMContent(nodeValue: unknown): nodeValue is LLMContent {\n  if (typeof nodeValue !== \"object\" || !nodeValue) return false;\n  if (nodeValue === null || nodeValue === undefined) return false;\n\n  if (\"role\" in nodeValue && nodeValue.role === \"$metadata\") {\n    return true;\n  }\n\n  return \"parts\" in nodeValue && Array.isArray(nodeValue.parts);\n}\n\nfunction isLLMContentArray(nodeValue: unknown): nodeValue is LLMContent[] {\n  if (!Array.isArray(nodeValue)) return false;\n  if (nodeValue.length === 0) return true;\n  return isLLMContent(nodeValue.at(-1));\n}\n\nfunction toLLMContent(\n  text: string,\n  role: LLMContent[\"role\"] = \"user\"\n): LLMContent {\n  return { parts: [{ text }], role };\n}\n\nfunction endsWithRole(c: LLMContent[], role: \"user\" | \"model\"): boolean {\n  const last = c.at(-1);\n  if (!last) return false;\n  return last.role === role;\n}\n\nfunction isEmpty(c: LLMContent): boolean {\n  if (!c.parts.length) return true;\n  for (const part of c.parts) {\n    if (\"text\" in part) {\n      if (part.text.trim().length > 0) return false;\n    } else {\n      return false;\n    }\n  }\n  return true;\n}\n\nfunction isStoredData(c: LLMContent) {\n  const part = c.parts.at(-1);\n  if (!part) {\n    return false;\n  }\n  return \"storedData\" in part && part.storedData != null;\n}\n\nfunction toText(c: LLMContent | LLMContent[]): string {\n  if (isLLMContent(c)) {\n    return contentToText(c);\n  }\n  const last = c.at(-1);\n  if (!last) return \"\";\n  return contentToText(last).trim();\n\n  function contentToText(content: LLMContent) {\n    return content.parts\n      .map((part) => (\"text\" in part ? part.text : \"\"))\n      .join(\"\\n\\n\");\n  }\n}\n\nfunction contentToJSON<T>(content?: LLMContent): T {\n  const part = content?.parts?.at(0);\n  if (!part || !(\"text\" in part)) {\n    throw new Error(\"Invalid response from Gemini\");\n  }\n  return JSON.parse(part.text) as T;\n}\n\nfunction defaultLLMContent(): string {\n  return JSON.stringify({\n    parts: [{ text: \"\" }],\n    role: \"user\",\n  } satisfies LLMContent);\n}\n\nfunction joinContent(\n  content: string | LLMContent,\n  context?: LLMContent[],\n  dropHistory: boolean = false\n): LLMContent[] {\n  if (dropHistory && context) {\n    let last = context.at(-1);\n    let retainedContext;\n    if (last) {\n      retainedContext = [last];\n    }\n    return addUserTurn(content, retainedContext);\n  }\n  return addUserTurn(content, context);\n}\n\nfunction extractInlineData(context: LLMContent[]): LLMContent[] {\n  const results = [];\n  for (let el of context) {\n    for (let part of el.parts) {\n      if (part) {\n        if (\"inlineData\" in part && part.inlineData) {\n          results.push(\n            toLLMContentInline(part.inlineData.mimeType, part.inlineData.data)\n          );\n        }\n      }\n    }\n  }\n  return results;\n}\n\nfunction extractMediaData(context: LLMContent[]): LLMContent[] {\n  const results = [];\n  for (let el of context) {\n    for (let part of el.parts) {\n      if (part) {\n        if (\"inlineData\" in part && part.inlineData) {\n          results.push(\n            toLLMContentInline(part.inlineData.mimeType, part.inlineData.data)\n          );\n        }\n        if (\"storedData\" in part && part.storedData) {\n          results.push(\n            toLLMContentStored(part.storedData.mimeType, part.storedData.handle)\n          );\n        }\n      }\n    }\n  }\n  return results;\n}\n\nfunction extractTextData(context: LLMContent[]): LLMContent[] {\n  const results = [];\n  for (let el of context) {\n    for (let part of el.parts) {\n      if (part) {\n        if (\"text\" in part && part.text) {\n          results.push(toLLMContent(part.text));\n        }\n      }\n    }\n  }\n  return results;\n}\n\nfunction toTextConcat(content: LLMContent[]): string {\n  return content.map(toText).join(\"\\n\\n\");\n}\n\nfunction addUserTurn(content: string | LLMContent, context?: LLMContent[]) {\n  context ??= [];\n  const isString = typeof content === \"string\";\n  if (!endsWithRole(context, \"user\")) {\n    return [...context, isString ? toLLMContent(content) : content];\n  }\n  const last = context.at(-1)!;\n  if (isString) {\n    last.parts.push({ text: content });\n  } else {\n    last.parts.push(...content.parts);\n  }\n  return context;\n}\n\nfunction toLLMContentInline(\n  mimetype: string,\n  value: string,\n  role: LLMContent[\"role\"] = \"user\"\n): LLMContent {\n  return {\n    parts: [\n      {\n        inlineData: {\n          mimeType: mimetype,\n          data: value,\n        },\n      },\n    ],\n    role,\n  };\n}\n\nfunction toLLMContentStored(\n  mimetype: string,\n  handle: string,\n  role: LLMContent[\"role\"] = \"user\"\n) {\n  return {\n    parts: [\n      {\n        storedData: {\n          mimeType: mimetype,\n          handle: handle,\n        },\n      },\n    ],\n    role,\n  };\n}\n\nfunction toInlineData(c: LLMContent | LLMContent[]) {\n  if (isLLMContent(c)) {\n    return contentToInlineData(c);\n  }\n  const last = c.at(-1);\n  if (!last) return null;\n  return contentToInlineData(last);\n\n  function contentToInlineData(content: LLMContent) {\n    const part = content.parts.at(-1);\n    if (!part) return \"\";\n    return \"inlineData\" in part && part.inlineData ? part.inlineData : null;\n  }\n}\n\n// TODO(askerryryan): Move this to the middleware.\nfunction toInlineReference(c: LLMContent) {\n  const last = c.parts.at(-1);\n  if (last == undefined || !(\"storedData\" in last)) {\n    return toInlineData(c);\n  }\n  const blobId = last.storedData.handle.split(\"/\").slice(-1)[0];\n  const gcs_handle = \"bb-blob-store/\" + blobId;\n  return toInlineData(toLLMContentInline(\"text/gcs-path\", btoa(gcs_handle)));\n}\n\nexport function mergeContent(content: LLMContent[], role: string): LLMContent {\n  const parts: DataPart[] = [];\n  for (const el of content) {\n    for (const part of el.parts) {\n      parts.push(part);\n    }\n  }\n  return {\n    parts: parts,\n    role: role,\n  } satisfies LLMContent;\n}\n\nfunction generateId() {\n  return Math.random().toString(36).substring(2, 5);\n}\n",
          "language": "typescript"
        },
        "description": "Common utils for manipulating LLM Content and other relevant types.",
        "runnable": false
      }
    },
    "gemini": {
      "code": "/**\n * @fileoverview Gemini Model Family.\n */\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\nimport output from \"@output\";\nimport { ok, err, isLLMContentArray } from \"./utils\";\nimport { flattenContext } from \"./lists\";\nconst defaultSafetySettings = () => [\n    {\n        category: \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n        threshold: \"OFF\",\n    },\n    {\n        category: \"HARM_CATEGORY_HARASSMENT\",\n        threshold: \"OFF\",\n    },\n    {\n        category: \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n        threshold: \"OFF\",\n    },\n];\nfunction endpointURL(model) {\n    // const $metadata = {\n    //   title: \"Get GEMINI_KEY\",\n    //   description: \"Getting GEMINI_KEY from secrets\",\n    // };\n    // const key = await secrets({ $metadata, keys: [\"GEMINI_KEY\"] });\n    return `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent`;\n}\nexport { invoke as default, describe, defaultSafetySettings };\nconst VALID_MODALITIES = [\"Text\", \"Text and Image\", \"Audio\"];\nconst MODELS = [\n    \"gemini-2.0-flash\",\n    \"gemini-2.0-flash-001\",\n    \"gemini-2.0-flash-lite-preview-02-05\",\n    \"gemini-2.0-flash-exp\",\n    \"gemini-2.0-flash-thinking-exp\",\n    \"gemini-2.0-flash-thinking-exp-01-21\",\n    \"gemini-2.0-pro-exp-02-05\",\n    \"gemini-1.5-flash-latest\",\n    \"gemini-1.5-pro-latest\",\n    \"gemini-exp-1206\",\n    \"gemini-exp-1121\",\n    \"learnlm-1.5-pro-experimental\",\n    \"gemini-1.5-pro-001\",\n    \"gemini-1.5-pro-002\",\n    \"gemini-1.5-pro-exp-0801\",\n    \"gemini-1.5-pro-exp-0827\",\n    \"gemini-1.5-flash-001\",\n    \"gemini-1.5-flash-002\",\n    \"gemini-1.5-flash-8b-exp-0924\",\n    \"gemini-1.5-flash-8b-exp-0827\",\n    \"gemini-1.5-flash-exp-0827\",\n];\nconst NO_RETRY_CODES = [400, 429, 404, 403];\n/**\n * Modifies the body to remove any\n * Breadboard-specific extensions to LLM Content\n */\nfunction conformBody(body) {\n    return {\n        ...body,\n        contents: flattenContext(body.contents.map((content) => {\n            return {\n                ...content,\n                parts: content.parts.map((part) => {\n                    if (\"json\" in part) {\n                        return { text: JSON.stringify(part.json) };\n                    }\n                    return part;\n                }),\n            };\n        }), true),\n    };\n}\nasync function getAccessToken() {\n    const signInSecretName = \"connection:$sign-in\";\n    const result = await secrets({ keys: [signInSecretName] });\n    return result[signInSecretName];\n}\nasync function callAPI(retries, model, body, $metadata) {\n    const accessToken = await getAccessToken();\n    let $error = \"Unknown error\";\n    while (retries) {\n        const result = await fetch({\n            $metadata,\n            url: endpointURL(model),\n            method: \"POST\",\n            headers: {\n                Authorization: `Bearer ${accessToken}`,\n            },\n            body: conformBody(body),\n        });\n        if (!ok(result)) {\n            // Fetch is a bit weird, because it returns various props\n            // along with the `$error`. Let's handle that here.\n            const { status, $error: errObject } = result;\n            if (!status) {\n                // This is not an error response, presume fatal error.\n                return { $error };\n            }\n            $error = maybeExtractError(errObject);\n            if (NO_RETRY_CODES.includes(status)) {\n                return { $error };\n            }\n        }\n        else {\n            const outputs = result.response;\n            const candidate = outputs.candidates?.at(0);\n            if (!candidate) {\n                return err(\"Unable to get a good response from Gemini\");\n            }\n            if (\"content\" in candidate) {\n                return outputs;\n            }\n            if (candidate.finishReason === \"IMAGE_SAFETY\") {\n                return err(\"The response candidate content was flagged for image safety reasons.\");\n            }\n        }\n        retries--;\n    }\n    return { $error };\n}\nfunction maybeExtractError(e) {\n    try {\n        const parsed = JSON.parse(e);\n        return parsed.error.message;\n    }\n    catch (error) {\n        return e;\n    }\n}\nfunction isEmptyLLMContent(content) {\n    if (!content || !content.parts || content.parts.length === 0)\n        return true;\n    return content.parts.every((part) => {\n        if (\"text\" in part) {\n            return !part.text?.trim();\n        }\n        return true;\n    });\n}\nfunction addModality(body, modality) {\n    if (!modality)\n        return;\n    switch (modality) {\n        case \"Text\":\n            // No change, defaults.\n            break;\n        case \"Text and Image\":\n            body.generationConfig ??= {};\n            body.generationConfig.responseModalities = [\"TEXT\", \"IMAGE\"];\n            break;\n        case \"Audio\":\n            body.generationConfig ??= {};\n            body.generationConfig.responseModalities = [\"AUDIO\"];\n            break;\n    }\n}\nfunction constructBody(context = [], systemInstruction, prompt, modality) {\n    const contents = [...context];\n    if (!isEmptyLLMContent(prompt)) {\n        contents.push(prompt);\n    }\n    const body = {\n        contents,\n        safetySettings: defaultSafetySettings(),\n    };\n    const canHaveSystemInstruction = modality === \"Text\";\n    if (!isEmptyLLMContent(systemInstruction) && canHaveSystemInstruction) {\n        body.systemInstruction = systemInstruction;\n    }\n    addModality(body, modality);\n    return body;\n}\nfunction augmentBody(body, systemInstruction, prompt, modality) {\n    if (!body.systemInstruction || !isEmptyLLMContent(systemInstruction)) {\n        body.systemInstruction = systemInstruction;\n    }\n    if (!isEmptyLLMContent(prompt)) {\n        body.contents = [...body.contents, prompt];\n    }\n    addModality(body, modality);\n    return body;\n}\nfunction validateInputs(inputs) {\n    if (\"body\" in inputs) {\n        return;\n    }\n    if (inputs.context) {\n        const { context } = inputs;\n        if (!Array.isArray(context)) {\n            return err(\"Incoming context must be an array.\");\n        }\n        if (!isLLMContentArray(context)) {\n            return err(\"Malformed incoming context\");\n        }\n        return;\n    }\n    return err(\"Either body or context is required\");\n}\nasync function invoke(inputs) {\n    const validatingInputs = validateInputs(inputs);\n    if (!ok(validatingInputs)) {\n        return validatingInputs;\n    }\n    let { model } = inputs;\n    if (!model) {\n        model = MODELS[0];\n    }\n    const { context, systemInstruction, prompt, modality, body, $metadata } = inputs;\n    // TODO: Make this configurable.\n    const retries = 5;\n    if (!(\"body\" in inputs)) {\n        // Public API is being used.\n        // Behave as if we're wired in.\n        const result = await callAPI(retries, model, constructBody(context, systemInstruction, prompt, modality));\n        if (!ok(result)) {\n            return result;\n        }\n        const content = result.candidates.at(0)?.content;\n        if (!content) {\n            return err(\"Unable to get a good response from Gemini\");\n        }\n        return { context: [...context, content] };\n    }\n    else {\n        // Private API is being used.\n        // Behave as if we're being invoked.\n        return callAPI(retries, model, augmentBody(body, systemInstruction, prompt, modality), $metadata);\n    }\n}\nasync function describe({ inputs }) {\n    const canHaveModalities = inputs.model === \"gemini-2.0-flash-exp\";\n    const canHaveSystemInstruction = !canHaveModalities || (canHaveModalities && inputs.modality == \"Text\");\n    const maybeAddSystemInstruction = canHaveSystemInstruction\n        ? {\n            systemInstruction: {\n                type: \"object\",\n                behavior: [\"llm-content\", \"config\"],\n                title: \"System Instruction\",\n                default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n                description: \"(Optional) Give the model additional context on what to do,\" +\n                    \"like specific rules/guidelines to adhere to or specify behavior\" +\n                    \"separate from the provided context\",\n            },\n        }\n        : {};\n    const maybeAddModalities = canHaveModalities\n        ? {\n            modality: {\n                type: \"string\",\n                enum: [...VALID_MODALITIES],\n                title: \"Output Modality\",\n                behavior: [\"config\"],\n                description: \"(Optional) Tell the model what kind of output you're looking for.\",\n            },\n        }\n        : {};\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                model: {\n                    type: \"string\",\n                    behavior: [\"config\"],\n                    title: \"Model Name\",\n                    enum: MODELS,\n                    default: MODELS[0],\n                },\n                prompt: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\"],\n                    title: \"Prompt\",\n                    default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n                    description: \"(Optional) A prompt. Will be added to the end of the the conversation context.\",\n                },\n                ...maybeAddSystemInstruction,\n                ...maybeAddModalities,\n                context: {\n                    type: \"array\",\n                    items: {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                    },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                    },\n                    title: \"Context out\",\n                },\n            },\n        },\n        metadata: {\n            icon: \"generative\",\n        },\n    };\n}\n",
      "metadata": {
        "title": "Gemini",
        "source": {
          "code": "/**\n * @fileoverview Gemini Model Family.\n */\n\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\nimport output from \"@output\";\n\nimport { ok, err, isLLMContentArray } from \"./utils\";\nimport { flattenContext } from \"./lists\";\n\nconst defaultSafetySettings = (): SafetySetting[] => [\n  {\n    category: \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n    threshold: \"OFF\",\n  },\n  {\n    category: \"HARM_CATEGORY_HARASSMENT\",\n    threshold: \"OFF\",\n  },\n  {\n    category: \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n    threshold: \"OFF\",\n  },\n];\n\nfunction endpointURL(model: string) {\n  // const $metadata = {\n  //   title: \"Get GEMINI_KEY\",\n  //   description: \"Getting GEMINI_KEY from secrets\",\n  // };\n  // const key = await secrets({ $metadata, keys: [\"GEMINI_KEY\"] });\n  return `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent`;\n}\n\nexport { invoke as default, describe, defaultSafetySettings };\n\nconst VALID_MODALITIES = [\"Text\", \"Text and Image\", \"Audio\"] as const;\ntype ValidModalities = (typeof VALID_MODALITIES)[number];\n\nexport type HarmBlockThreshold =\n  // Content with NEGLIGIBLE will be allowed.\n  | \"BLOCK_LOW_AND_ABOVE\"\n  // Content with NEGLIGIBLE and LOW will be allowed.\n  | \"BLOCK_MEDIUM_AND_ABOVE\"\n  // Content with NEGLIGIBLE, LOW, and MEDIUM will be allowed.\n  | \"BLOCK_ONLY_HIGH\"\n  // All content will be allowed.\n  | \"BLOCK_NONE\"\n  // Turn off the safety filter.\n  | \"OFF\";\n\nexport type HarmCategory =\n  // Gemini - Harassment content\n  | \"HARM_CATEGORY_HARASSMENT\"\n  //\tGemini - Hate speech and content.\n  | \"HARM_CATEGORY_HATE_SPEECH\"\n  // Gemini - Sexually explicit content.\n  | \"HARM_CATEGORY_SEXUALLY_EXPLICIT\"\n  // \tGemini - Dangerous content.\n  | \"HARM_CATEGORY_DANGEROUS_CONTENT\"\n  // Gemini - Content that may be used to harm civic integrity.\n  | \"HARM_CATEGORY_CIVIC_INTEGRITY\";\n\nexport type GeminiSchema = {\n  type: \"string\" | \"number\" | \"integer\" | \"boolean\" | \"object\" | \"array\";\n  format?: string;\n  description?: string;\n  nullable?: boolean;\n  enum?: string[];\n  maxItems?: string;\n  minItems?: string;\n  properties?: Record<string, GeminiSchema>;\n  required?: string[];\n  items?: GeminiSchema;\n};\n\nexport type Modality = \"TEXT\" | \"IMAGE\" | \"AUDIO\";\n\nexport type GenerationConfig = {\n  responseMimeType?: \"text/plain\" | \"application/json\" | \"text/x.enum\";\n  responseSchema?: GeminiSchema;\n  responseModalities?: Modality[];\n};\n\nexport type SafetySetting = {\n  category: HarmCategory;\n  threshold: HarmBlockThreshold;\n};\n\nexport type Metadata = {\n  title?: string;\n  description?: string;\n};\n\nexport type GeminiBody = {\n  contents: LLMContent[];\n  tools?: Tool[];\n  toolConfig?: ToolConfig;\n  systemInstruction?: LLMContent;\n  safetySettings?: SafetySetting[];\n  generationConfig?: GenerationConfig;\n};\n\nexport type GeminiInputs = {\n  // The wireable/configurable properties.\n  model?: string;\n  context?: LLMContent[];\n  systemInstruction?: LLMContent;\n  prompt?: LLMContent;\n  modality?: ValidModalities;\n  // The \"private API\" properties\n  $metadata?: Metadata;\n  body: GeminiBody;\n};\n\nexport type Tool = {\n  functionDeclarations?: FunctionDeclaration[];\n  googleSearch?: {};\n  codeExecution?: CodeExecution[];\n};\n\nexport type ToolConfig = {\n  functionCallingConfig?: FunctionCallingConfig;\n};\n\nexport type FunctionCallingConfig = {\n  mode?: \"MODE_UNSPECIFIED\" | \"AUTO\" | \"ANY\" | \"NONE\";\n  allowedFunctionNames?: string[];\n};\n\nexport type FunctionDeclaration = {\n  name: string;\n  description: string;\n  parameters?: GeminiSchema;\n};\n\nexport type CodeExecution = {\n  // Type contains no fields.\n};\n\nexport type FinishReason =\n  // Natural stop point of the model or provided stop sequence.\n  | \"STOP\"\n  // The maximum number of tokens as specified in the request was reached.\n  | \"MAX_TOKENS\"\n  // The response candidate content was flagged for safety reasons.\n  | \"SAFETY\"\n  // The response candidate content was flagged for image safety reasons.\n  | \"IMAGE_SAFETY\"\n  // The response candidate content was flagged for recitation reasons.\n  | \"RECITATION\"\n  // The response candidate content was flagged for using an unsupported language.\n  | \"LANGUAGE\"\n  // Unknown reason.\n  | \"OTHER\"\n  // Token generation stopped because the content contains forbidden terms.\n  | \"BLOCKLIST\"\n  // Token generation stopped for potentially containing prohibited content.\n  | \"PROHIBITED_CONTENT\"\n  // Token generation stopped because the content potentially contains Sensitive Personally Identifiable Information (SPII).\n  | \"SPII\"\n  // The function call generated by the model is invalid.\n  | \"MALFORMED_FUNCTION_CALL\";\n\nexport type GroundingMetadata = {\n  groundingChunks: {\n    web: {\n      uri: string;\n      title: string;\n    };\n  }[];\n  groundingSupports: {\n    groundingChunkIndices: number[];\n    confidenceScores: number[];\n    segment: {\n      partIndex: number;\n      startIndex: number;\n      endIndex: number;\n      text: string;\n    };\n  };\n  webSearchQueries: string[];\n  searchEntryPoint: {\n    renderedContent: string;\n    /**\n     * Base64 encoded JSON representing array of <search term, search url> tuple.\n     * A base64-encoded string.\n     */\n    sdkBlob: string;\n  };\n  retrievalMetadata: {\n    googleSearchDynamicRetrievalScore: number;\n  };\n};\n\nexport type Candidate = {\n  content?: LLMContent;\n  finishReason?: FinishReason;\n  safetyRatings?: SafetySetting[];\n  tokenOutput: number;\n  groundingMetadata: GroundingMetadata;\n};\n\nexport type GeminiAPIOutputs = {\n  candidates: Candidate[];\n};\n\nexport type GeminiOutputs =\n  | GeminiAPIOutputs\n  | {\n      context: LLMContent[];\n    };\n\nconst MODELS: readonly string[] = [\n  \"gemini-2.0-flash\",\n  \"gemini-2.0-flash-001\",\n  \"gemini-2.0-flash-lite-preview-02-05\",\n  \"gemini-2.0-flash-exp\",\n  \"gemini-2.0-flash-thinking-exp\",\n  \"gemini-2.0-flash-thinking-exp-01-21\",\n  \"gemini-2.0-pro-exp-02-05\",\n  \"gemini-1.5-flash-latest\",\n  \"gemini-1.5-pro-latest\",\n  \"gemini-exp-1206\",\n  \"gemini-exp-1121\",\n  \"learnlm-1.5-pro-experimental\",\n  \"gemini-1.5-pro-001\",\n  \"gemini-1.5-pro-002\",\n  \"gemini-1.5-pro-exp-0801\",\n  \"gemini-1.5-pro-exp-0827\",\n  \"gemini-1.5-flash-001\",\n  \"gemini-1.5-flash-002\",\n  \"gemini-1.5-flash-8b-exp-0924\",\n  \"gemini-1.5-flash-8b-exp-0827\",\n  \"gemini-1.5-flash-exp-0827\",\n];\n\nconst NO_RETRY_CODES: readonly number[] = [400, 429, 404, 403];\n\ntype FetchErrorResponse = {\n  $error: string;\n  status: number;\n  statusText: string;\n  contentType: string;\n  responseHeaders: Record<string, string>;\n};\n\n/**\n * Using\n * `{\"error\":{\"code\":400,\"message\":\"Invalid JSON payloâ€¦'contents[0].parts[0]': Cannot find field.\"}]}]}\n * as template for this type.\n */\ntype GeminiError = {\n  error: {\n    code: number;\n    details: {\n      type: string;\n      fieldViolations: {\n        description: string;\n        field: string;\n      }[];\n    }[];\n    message: string;\n    status: string;\n  };\n};\n\n/**\n * Modifies the body to remove any\n * Breadboard-specific extensions to LLM Content\n */\nfunction conformBody(body: GeminiBody): GeminiBody {\n  return {\n    ...body,\n    contents: flattenContext(\n      body.contents.map((content) => {\n        return {\n          ...content,\n          parts: content.parts.map((part) => {\n            if (\"json\" in part) {\n              return { text: JSON.stringify(part.json) };\n            }\n            return part;\n          }),\n        };\n      }),\n      true\n    ),\n  };\n}\n\nasync function getAccessToken() {\n  const signInSecretName = \"connection:$sign-in\";\n  const result = await secrets({ keys: [signInSecretName] });\n  return result[signInSecretName];\n}\n\nasync function callAPI(\n  retries: number,\n  model: string,\n  body: GeminiBody,\n  $metadata?: Metadata\n): Promise<Outcome<GeminiAPIOutputs>> {\n  const accessToken = await getAccessToken();\n  let $error: string = \"Unknown error\";\n  while (retries) {\n    const result = await fetch({\n      $metadata,\n      url: endpointURL(model),\n      method: \"POST\",\n      headers: {\n        Authorization: `Bearer ${accessToken}`,\n      },\n      body: conformBody(body),\n    });\n    if (!ok(result)) {\n      // Fetch is a bit weird, because it returns various props\n      // along with the `$error`. Let's handle that here.\n      const { status, $error: errObject } = result as FetchErrorResponse;\n      if (!status) {\n        // This is not an error response, presume fatal error.\n        return { $error };\n      }\n      $error = maybeExtractError(errObject);\n      if (NO_RETRY_CODES.includes(status)) {\n        return { $error };\n      }\n    } else {\n      const outputs = result.response as GeminiAPIOutputs;\n      const candidate = outputs.candidates?.at(0);\n      if (!candidate) {\n        return err(\"Unable to get a good response from Gemini\");\n      }\n      if (\"content\" in candidate) {\n        return outputs;\n      }\n      if (candidate.finishReason === \"IMAGE_SAFETY\") {\n        return err(\n          \"The response candidate content was flagged for image safety reasons.\"\n        );\n      }\n    }\n    retries--;\n  }\n  return { $error };\n}\n\nfunction maybeExtractError(e: string): string {\n  try {\n    const parsed = JSON.parse(e) as GeminiError;\n    return parsed.error.message;\n  } catch (error) {\n    return e;\n  }\n}\n\nfunction isEmptyLLMContent(content?: LLMContent): content is undefined {\n  if (!content || !content.parts || content.parts.length === 0) return true;\n  return content.parts.every((part) => {\n    if (\"text\" in part) {\n      return !part.text?.trim();\n    }\n    return true;\n  });\n}\n\nfunction addModality(body: GeminiBody, modality?: ValidModalities) {\n  if (!modality) return;\n  switch (modality) {\n    case \"Text\":\n      // No change, defaults.\n      break;\n    case \"Text and Image\":\n      body.generationConfig ??= {};\n      body.generationConfig.responseModalities = [\"TEXT\", \"IMAGE\"];\n      break;\n    case \"Audio\":\n      body.generationConfig ??= {};\n      body.generationConfig.responseModalities = [\"AUDIO\"];\n      break;\n  }\n}\n\nfunction constructBody(\n  context: LLMContent[] = [],\n  systemInstruction?: LLMContent,\n  prompt?: LLMContent,\n  modality?: ValidModalities\n): GeminiBody {\n  const contents = [...context];\n  if (!isEmptyLLMContent(prompt)) {\n    contents.push(prompt);\n  }\n  const body: GeminiBody = {\n    contents,\n    safetySettings: defaultSafetySettings(),\n  };\n  const canHaveSystemInstruction = modality === \"Text\";\n  if (!isEmptyLLMContent(systemInstruction) && canHaveSystemInstruction) {\n    body.systemInstruction = systemInstruction;\n  }\n  addModality(body, modality);\n  return body;\n}\n\nfunction augmentBody(\n  body: GeminiBody,\n  systemInstruction?: LLMContent,\n  prompt?: LLMContent,\n  modality?: ValidModalities\n): GeminiBody {\n  if (!body.systemInstruction || !isEmptyLLMContent(systemInstruction)) {\n    body.systemInstruction = systemInstruction;\n  }\n  if (!isEmptyLLMContent(prompt)) {\n    body.contents = [...body.contents, prompt];\n  }\n  addModality(body, modality);\n  return body;\n}\n\nfunction validateInputs(inputs: GeminiInputs): Outcome<void> {\n  if (\"body\" in (inputs as object)) {\n    return;\n  }\n  if (inputs.context) {\n    const { context } = inputs;\n    if (!Array.isArray(context)) {\n      return err(\"Incoming context must be an array.\");\n    }\n    if (!isLLMContentArray(context)) {\n      return err(\"Malformed incoming context\");\n    }\n    return;\n  }\n  return err(\"Either body or context is required\");\n}\n\nasync function invoke(inputs: GeminiInputs): Promise<Outcome<GeminiOutputs>> {\n  const validatingInputs = validateInputs(inputs);\n  if (!ok(validatingInputs)) {\n    return validatingInputs;\n  }\n  let { model } = inputs;\n  if (!model) {\n    model = MODELS[0];\n  }\n  const { context, systemInstruction, prompt, modality, body, $metadata } =\n    inputs;\n  // TODO: Make this configurable.\n  const retries = 5;\n  if (!(\"body\" in inputs)) {\n    // Public API is being used.\n    // Behave as if we're wired in.\n    const result = await callAPI(\n      retries,\n      model,\n      constructBody(context, systemInstruction, prompt, modality)\n    );\n    if (!ok(result)) {\n      return result;\n    }\n    const content = result.candidates.at(0)?.content;\n    if (!content) {\n      return err(\"Unable to get a good response from Gemini\");\n    }\n    return { context: [...context!, content] };\n  } else {\n    // Private API is being used.\n    // Behave as if we're being invoked.\n    return callAPI(\n      retries,\n      model,\n      augmentBody(body, systemInstruction, prompt, modality),\n      $metadata\n    );\n  }\n}\n\ntype DescribeInputs = {\n  inputs: {\n    modality?: ValidModalities;\n    model: string;\n  };\n};\n\nasync function describe({ inputs }: DescribeInputs) {\n  const canHaveModalities = inputs.model === \"gemini-2.0-flash-exp\";\n  const canHaveSystemInstruction =\n    !canHaveModalities || (canHaveModalities && inputs.modality == \"Text\");\n  const maybeAddSystemInstruction: Schema[\"properties\"] =\n    canHaveSystemInstruction\n      ? {\n          systemInstruction: {\n            type: \"object\",\n            behavior: [\"llm-content\", \"config\"],\n            title: \"System Instruction\",\n            default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n            description:\n              \"(Optional) Give the model additional context on what to do,\" +\n              \"like specific rules/guidelines to adhere to or specify behavior\" +\n              \"separate from the provided context\",\n          },\n        }\n      : {};\n  const maybeAddModalities: Schema[\"properties\"] = canHaveModalities\n    ? {\n        modality: {\n          type: \"string\",\n          enum: [...VALID_MODALITIES],\n          title: \"Output Modality\",\n          behavior: [\"config\"],\n          description:\n            \"(Optional) Tell the model what kind of output you're looking for.\",\n        },\n      }\n    : {};\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        model: {\n          type: \"string\",\n          behavior: [\"config\"],\n          title: \"Model Name\",\n          enum: MODELS as string[],\n          default: MODELS[0],\n        },\n        prompt: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\"],\n          title: \"Prompt\",\n          default: '{\"role\":\"user\",\"parts\":[{\"text\":\"\"}]}',\n          description:\n            \"(Optional) A prompt. Will be added to the end of the the conversation context.\",\n        },\n        ...maybeAddSystemInstruction,\n        ...maybeAddModalities,\n        context: {\n          type: \"array\",\n          items: {\n            type: \"object\",\n            behavior: [\"llm-content\"],\n          },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: {\n            type: \"object\",\n            behavior: [\"llm-content\"],\n          },\n          title: \"Context out\",\n        },\n      },\n    } satisfies Schema,\n    metadata: {\n      icon: \"generative\",\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Gemini Model Family.",
        "runnable": false
      }
    },
    "entry": {
      "code": "/**\n * @fileoverview Manages the entry point: describer, passing the inputs, etc.\n */\nimport {} from \"./common\";\nimport { ok, toLLMContent, defaultLLMContent } from \"./utils\";\nimport { Template } from \"./template\";\nimport { readSettings } from \"./settings\";\nexport { invoke as default, describe };\nasync function invoke({ context, \"p-chat\": chat, \"p-list\": makeList, description, ...params }) {\n    // Make sure it's a boolean.\n    chat = !!chat;\n    context ??= [];\n    const defaultModel = \"\";\n    const type = \"work\";\n    return {\n        context: {\n            id: Math.random().toString(36).substring(2, 5),\n            chat,\n            makeList,\n            listPath: [],\n            context,\n            userInputs: [],\n            defaultModel,\n            model: \"\",\n            description,\n            tools: [],\n            type,\n            work: [],\n            userEndedChat: false,\n            params,\n        },\n    };\n}\nasync function describe({ inputs: { description } }) {\n    const settings = await readSettings();\n    const experimental = ok(settings) && !!settings[\"Show Experimental Components\"];\n    const template = new Template(description);\n    let extra = {};\n    if (experimental) {\n        extra = {\n            \"p-chat\": {\n                type: \"boolean\",\n                title: \"Chat with User\",\n                behavior: [\"config\", \"hint-preview\"],\n                icon: \"chat\",\n                description: \"When checked, this step will chat with the user, asking to review work, requesting additional information, etc.\",\n            },\n            \"p-list\": {\n                type: \"boolean\",\n                title: \"Make a list\",\n                behavior: [\"config\", \"hint-preview\"],\n                icon: \"summarize\",\n                description: \"When checked, this step will try to create a list as its output. Make sure that the prompt asks for a list of some sort\",\n            },\n        };\n    }\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                description: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"Instruction\",\n                    description: \"Give the model additional context on what to do, like specific rules/guidelines to adhere to or specify behavior separate from the provided context.\",\n                    default: defaultLLMContent(),\n                },\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                ...extra,\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"main-port\", \"hint-text\"],\n                },\n            },\n        },\n        title: \"Make Text\",\n        metadata: {\n            icon: \"generative-text\",\n            tags: [\"quick-access\", \"generative\"],\n            order: 1,\n        },\n    };\n}\n",
      "metadata": {
        "title": "Make Text",
        "source": {
          "code": "/**\n * @fileoverview Manages the entry point: describer, passing the inputs, etc.\n */\n\nimport {\n  type AgentContext,\n  type AgentInputs,\n  type DescribeInputs,\n} from \"./common\";\nimport { ok, toLLMContent, defaultLLMContent } from \"./utils\";\nimport { Template } from \"./template\";\nimport { readSettings } from \"./settings\";\n\nexport { invoke as default, describe };\n\nexport type EntryInputs = {\n  context: LLMContent[];\n  description: LLMContent;\n  \"p-chat\": boolean;\n  \"p-list\": boolean;\n  [key: `p-z-${string}`]: unknown;\n};\n\ntype Outputs = {\n  context: AgentContext;\n};\n\nasync function invoke({\n  context,\n  \"p-chat\": chat,\n  \"p-list\": makeList,\n  description,\n  ...params\n}: EntryInputs): Promise<Outputs> {\n  // Make sure it's a boolean.\n  chat = !!chat;\n  context ??= [];\n  const defaultModel = \"\";\n  const type = \"work\";\n  return {\n    context: {\n      id: Math.random().toString(36).substring(2, 5),\n      chat,\n      makeList,\n      listPath: [],\n      context,\n      userInputs: [],\n      defaultModel,\n      model: \"\",\n      description,\n      tools: [],\n      type,\n      work: [],\n      userEndedChat: false,\n      params,\n    },\n  };\n}\n\nasync function describe({ inputs: { description } }: DescribeInputs) {\n  const settings = await readSettings();\n  const experimental =\n    ok(settings) && !!settings[\"Show Experimental Components\"];\n  const template = new Template(description);\n  let extra: Record<string, Schema> = {};\n  if (experimental) {\n    extra = {\n      \"p-chat\": {\n        type: \"boolean\",\n        title: \"Chat with User\",\n        behavior: [\"config\", \"hint-preview\"],\n        icon: \"chat\",\n        description:\n          \"When checked, this step will chat with the user, asking to review work, requesting additional information, etc.\",\n      },\n      \"p-list\": {\n        type: \"boolean\",\n        title: \"Make a list\",\n        behavior: [\"config\", \"hint-preview\"],\n        icon: \"summarize\",\n        description:\n          \"When checked, this step will try to create a list as its output. Make sure that the prompt asks for a list of some sort\",\n      },\n    };\n  }\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        description: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n          title: \"Instruction\",\n          description:\n            \"Give the model additional context on what to do, like specific rules/guidelines to adhere to or specify behavior separate from the provided context.\",\n          default: defaultLLMContent(),\n        },\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n        ...extra,\n        ...template.schemas(),\n      },\n      behavior: [\"at-wireable\"],\n      ...template.requireds(),\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n          behavior: [\"main-port\", \"hint-text\"],\n        },\n      },\n    } satisfies Schema,\n    title: \"Make Text\",\n    metadata: {\n      icon: \"generative-text\",\n      tags: [\"quick-access\", \"generative\"],\n      order: 1,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Manages the entry point: describer, passing the inputs, etc.",
        "runnable": true
      }
    },
    "join": {
      "code": "/**\n * @fileoverview Joins user input and Agent Context\n */\nimport {} from \"./common\";\nimport { isEmpty } from \"./utils\";\nimport { addContent } from \"./lists\";\nexport { invoke as default, describe };\nasync function invoke({ context, request }) {\n    context.userEndedChat = isEmpty(request);\n    context.userInputs.push(request);\n    if (!context.userEndedChat) {\n        context.work = addContent(context.work, request);\n    }\n    return { context };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    title: \"Agent Context\",\n                    type: \"object\",\n                },\n                request: {\n                    title: \"User Input\",\n                    type: \"object\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    title: \"Agent Context\",\n                    type: \"object\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "join",
        "source": {
          "code": "/**\n * @fileoverview Joins user input and Agent Context\n */\n\nimport { type AgentContext } from \"./common\";\nimport { isEmpty } from \"./utils\";\nimport { addContent } from \"./lists\";\n\nexport { invoke as default, describe };\n\ntype Inputs = {\n  context: AgentContext;\n  request: LLMContent;\n};\n\ntype Outputs = {\n  context: AgentContext;\n};\n\nasync function invoke({ context, request }: Inputs): Promise<Outputs> {\n  context.userEndedChat = isEmpty(request);\n  context.userInputs.push(request);\n  if (!context.userEndedChat) {\n    context.work = addContent(context.work, request);\n  }\n  return { context };\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          title: \"Agent Context\",\n          type: \"object\",\n        },\n        request: {\n          title: \"User Input\",\n          type: \"object\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          title: \"Agent Context\",\n          type: \"object\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Joins user input and Agent Context",
        "runnable": true
      }
    },
    "output": {
      "code": "/**\n * @fileoverview Provides an output helper.\n */\nimport output from \"@output\";\nexport { report };\nasync function report(inputs) {\n    const { actor: title, category: description, name, details, icon } = inputs;\n    const detailsSchema = typeof details === \"string\"\n        ? {\n            title: name,\n            type: \"string\",\n            format: \"markdown\",\n        }\n        : {\n            title: name,\n            type: \"object\",\n            behavior: [\"llm-content\"],\n        };\n    if (icon) {\n        detailsSchema.icon = icon;\n    }\n    const schema = {\n        type: \"object\",\n        properties: {\n            details: detailsSchema,\n        },\n    };\n    const { delivered } = await output({\n        $metadata: {\n            title,\n            description,\n            icon,\n        },\n        schema,\n        details,\n    });\n    return delivered;\n}\n",
      "metadata": {
        "title": "output",
        "source": {
          "code": "/**\n * @fileoverview Provides an output helper.\n */\n\nimport output from \"@output\";\n\ntype ReportInputs = {\n  /**\n   * The name of the actor providing the report\n   */\n  actor: string;\n  /**\n   * The general category of the report\n   */\n  category: string;\n  /**\n   * The name of the report\n   */\n  name: string;\n  /**\n   * The details of the report\n   */\n  details: string | LLMContent;\n  /**\n   * The icon to use\n   */\n  icon?: string;\n};\n\nexport { report };\n\nasync function report(inputs: ReportInputs): Promise<boolean> {\n  const { actor: title, category: description, name, details, icon } = inputs;\n\n  const detailsSchema: Schema =\n    typeof details === \"string\"\n      ? {\n          title: name,\n          type: \"string\",\n          format: \"markdown\",\n        }\n      : {\n          title: name,\n          type: \"object\",\n          behavior: [\"llm-content\"],\n        };\n\n  if (icon) {\n    detailsSchema.icon = icon;\n  }\n\n  const schema: Schema = {\n    type: \"object\",\n    properties: {\n      details: detailsSchema,\n    },\n  };\n\n  const { delivered } = await output({\n    $metadata: {\n      title,\n      description,\n      icon,\n    },\n    schema,\n    details,\n  });\n  return delivered;\n}\n",
          "language": "typescript"
        },
        "description": "Provides an output helper.",
        "runnable": false
      }
    },
    "tool-manager": {
      "code": "/**\n * @fileoverview Manages tools.\n */\nimport describeGraph from \"@describe\";\nimport invokeGraph from \"@invoke\";\nimport { ok, err } from \"./utils\";\nimport {} from \"./gemini\";\nimport { ConnectorManager } from \"./connector-manager\";\nexport { ToolManager };\nclass ToolManager {\n    describerResultTransformer;\n    #hasSearch = false;\n    tools = new Map();\n    connectors = new Map();\n    errors = [];\n    constructor(describerResultTransformer) {\n        this.describerResultTransformer = describerResultTransformer;\n    }\n    #convertSchemas(schema) {\n        return toGeminiSchema(schema);\n        function toGeminiSchema(schema) {\n            switch (schema.type) {\n                case \"object\": {\n                    if (schema.behavior?.includes(\"llm-content\")) {\n                        return {\n                            type: \"string\",\n                            description: schema.description || schema.title,\n                        };\n                    }\n                    if (!schema.properties) {\n                        return { type: \"object\" };\n                    }\n                    return {\n                        type: \"object\",\n                        properties: Object.fromEntries(Object.entries(schema.properties).map(([name, schema]) => {\n                            return [name, toGeminiSchema(schema)];\n                        })),\n                        required: schema.required,\n                    };\n                }\n                case \"array\": {\n                    const items = schema.items;\n                    if (items.behavior?.includes(\"llm-content\")) {\n                        return {\n                            type: \"string\",\n                            description: schema.description,\n                        };\n                    }\n                    return {\n                        type: \"array\",\n                        items: toGeminiSchema(schema.items),\n                    };\n                }\n                default: {\n                    const geminiSchema = { ...schema };\n                    delete geminiSchema.format;\n                    delete geminiSchema.behavior;\n                    delete geminiSchema.examples;\n                    delete geminiSchema.default;\n                    delete geminiSchema.transient;\n                    if (!geminiSchema.description) {\n                        geminiSchema.description = geminiSchema.title;\n                    }\n                    delete geminiSchema.title;\n                    return geminiSchema;\n                }\n            }\n        }\n    }\n    #toName(title) {\n        return title ? title.replace(/\\W/g, \"_\") : \"function\";\n    }\n    addSearch() {\n        this.#hasSearch = true;\n    }\n    #createToolHandle(url, description, passContext, connector) {\n        const name = this.#toName(description.title);\n        const functionDeclaration = {\n            name,\n            description: description.description || \"\",\n        };\n        const parameters = this.#convertSchemas(description.inputSchema);\n        if (parameters.properties) {\n            functionDeclaration.parameters = parameters;\n        }\n        return [name, { tool: functionDeclaration, url, passContext, connector }];\n    }\n    #addOneTool(url, description, passContext, connector) {\n        const [name, handle] = this.#createToolHandle(url, description, passContext, connector);\n        this.tools.set(name, handle);\n        return description.title || name;\n    }\n    addCustomTool(name, handle) {\n        this.tools.set(name, handle);\n    }\n    async addTool(url, instance) {\n        if (instance) {\n            // This is a connector.\n            const connector = new ConnectorManager({ path: instance });\n            const tools = await connector.listTools();\n            if (!ok(tools))\n                return tools;\n            for (const tool of tools) {\n                const { url, description } = tool;\n                this.#addOneTool(url, description, false, connector);\n            }\n            // Return empty string, which will inform the\n            // substitution machinery to just reuse title.\n            return \"\";\n        }\n        let description = (await describeGraph({\n            url,\n        }));\n        let passContext = false;\n        if (!ok(description))\n            return description;\n        // TODO: Remove this altogether?\n        // Let's see if there are exports. If yes, let's add the exports\n        // instead of the tool.\n        if (description.exports) {\n            let connector = null;\n            if (description.metadata?.tags?.includes(\"connector\")) {\n                // This is a connector\n                connector = { tools: new Map() };\n            }\n            Object.entries(description.exports).forEach(([id, exportDescription]) => {\n                // TODO: Figure out what to do with passContext\n                const idAndHandle = this.#createToolHandle(id, exportDescription, passContext);\n                const [name, handle] = idAndHandle;\n                console.log(\"EXPORT DESCRIPTION\", exportDescription);\n                if (connector) {\n                    if (exportDescription.metadata?.tags?.includes(\"connector-configure\")) {\n                        connector.configure = idAndHandle;\n                        return;\n                    }\n                    else if (exportDescription.metadata?.tags?.includes(\"connector-load\")) {\n                        connector.load = idAndHandle;\n                        return;\n                    }\n                    else {\n                        connector.tools.set(name, handle);\n                    }\n                }\n                this.tools.set(name, handle);\n            });\n            return this.#toName(description.title);\n        }\n        // Otherwise, let's add the tool itself.\n        if (this.describerResultTransformer) {\n            const transforming = await this.describerResultTransformer.transform(description);\n            if (!ok(transforming))\n                return transforming;\n            if (transforming) {\n                description = transforming;\n                passContext = true;\n            }\n        }\n        return this.#addOneTool(url, description, passContext);\n    }\n    async initialize(tools) {\n        if (!tools) {\n            return true;\n        }\n        let hasInvalidTools = false;\n        for (const tool of tools) {\n            const url = typeof tool === \"string\" ? tool : tool.url;\n            const description = (await describeGraph({\n                url,\n            }));\n            if (!ok(description)) {\n                this.errors.push(description.$error);\n                // Invalid tool, skip\n                hasInvalidTools = true;\n                continue;\n            }\n            const parameters = this.#convertSchemas(description.inputSchema);\n            const name = this.#toName(description.title);\n            const functionDeclaration = {\n                name,\n                description: description.description || \"\",\n                parameters,\n            };\n            this.tools.set(name, {\n                tool: functionDeclaration,\n                url,\n                passContext: false,\n            });\n        }\n        return !hasInvalidTools;\n    }\n    async processResponse(response, callTool) {\n        for (const part of response.parts) {\n            if (\"functionCall\" in part) {\n                const { args, name } = part.functionCall;\n                const handle = this.tools.get(name);\n                if (handle) {\n                    if (handle.invoke) {\n                        await handle.invoke(args);\n                    }\n                    else {\n                        const { url, passContext, connector } = handle;\n                        if (connector) {\n                            await connector.invokeTool(name, args, callTool);\n                        }\n                        else {\n                            await callTool(url, part.functionCall.args, passContext);\n                        }\n                    }\n                }\n            }\n        }\n    }\n    hasTools() {\n        return this.tools.size !== 0;\n    }\n    list() {\n        const declaration = {};\n        const entries = [...this.tools.entries()];\n        if (entries.length !== 0) {\n            declaration.functionDeclarations = entries.map(([, value]) => value.tool);\n        }\n        if (this.#hasSearch) {\n            declaration.googleSearch = {};\n        }\n        if (Object.keys(declaration).length === 0)\n            return [];\n        return [declaration];\n    }\n}\n",
      "metadata": {
        "title": "tool-manager",
        "source": {
          "code": "/**\n * @fileoverview Manages tools.\n */\n\nimport describeGraph from \"@describe\";\nimport invokeGraph from \"@invoke\";\nimport { ok, err } from \"./utils\";\nimport type {\n  DescriberResult,\n  ExportDescriberResult,\n  DescriberResultTransformer,\n  CallToolCallback,\n} from \"./common\";\nimport {\n  type FunctionDeclaration,\n  type GeminiSchema,\n  type Tool,\n} from \"./gemini\";\nimport { ConnectorManager } from \"./connector-manager\";\n\nexport type ToolHandle = {\n  tool: FunctionDeclaration;\n  url: string;\n  passContext: boolean;\n  connector?: ConnectorManager;\n  invoke?: (args: Record<string, unknown>) => Promise<Outcome<void>>;\n};\n\nexport type ConnectorHandle = {\n  tools: Map<string, ToolHandle>;\n  configure?: [string, ToolHandle];\n  load?: [string, ToolHandle];\n};\n\nexport type ToolDescriptor =\n  | string\n  | {\n      kind: \"board\";\n      url: string;\n    };\n\nexport { ToolManager };\n\nclass ToolManager {\n  #hasSearch = false;\n  tools: Map<string, ToolHandle> = new Map();\n  connectors: Map<string, ConnectorHandle> = new Map();\n  errors: string[] = [];\n\n  constructor(\n    private readonly describerResultTransformer?: DescriberResultTransformer\n  ) {}\n\n  #convertSchemas(schema: Schema): GeminiSchema {\n    return toGeminiSchema(schema);\n\n    function toGeminiSchema(schema: Schema): GeminiSchema {\n      switch (schema.type) {\n        case \"object\": {\n          if (schema.behavior?.includes(\"llm-content\")) {\n            return {\n              type: \"string\",\n              description: schema.description || schema.title,\n            };\n          }\n          if (!schema.properties) {\n            return { type: \"object\" };\n          }\n          return {\n            type: \"object\",\n            properties: Object.fromEntries(\n              Object.entries(schema.properties).map(([name, schema]) => {\n                return [name, toGeminiSchema(schema)];\n              })\n            ),\n            required: schema.required,\n          };\n        }\n        case \"array\": {\n          const items = schema.items as Schema;\n          if (items.behavior?.includes(\"llm-content\")) {\n            return {\n              type: \"string\",\n              description: schema.description,\n            };\n          }\n          return {\n            type: \"array\",\n            items: toGeminiSchema(schema.items as Schema),\n          };\n        }\n        default: {\n          const geminiSchema = { ...schema };\n          delete geminiSchema.format;\n          delete geminiSchema.behavior;\n          delete geminiSchema.examples;\n          delete geminiSchema.default;\n          delete geminiSchema.transient;\n          if (!geminiSchema.description) {\n            geminiSchema.description = geminiSchema.title;\n          }\n          delete geminiSchema.title;\n          return geminiSchema as GeminiSchema;\n        }\n      }\n    }\n  }\n\n  #toName(title?: string) {\n    return title ? title.replace(/\\W/g, \"_\") : \"function\";\n  }\n\n  addSearch() {\n    this.#hasSearch = true;\n  }\n\n  #createToolHandle(\n    url: string,\n    description: ExportDescriberResult,\n    passContext: boolean,\n    connector?: ConnectorManager\n  ): [string, ToolHandle] {\n    const name = this.#toName(description.title);\n    const functionDeclaration: FunctionDeclaration = {\n      name,\n      description: description.description || \"\",\n    };\n    const parameters = this.#convertSchemas(description.inputSchema!);\n    if (parameters.properties) {\n      functionDeclaration.parameters = parameters;\n    }\n    return [name, { tool: functionDeclaration, url, passContext, connector }];\n  }\n\n  #addOneTool(\n    url: string,\n    description: ExportDescriberResult,\n    passContext: boolean,\n    connector?: ConnectorManager\n  ): Outcome<string> {\n    const [name, handle] = this.#createToolHandle(\n      url,\n      description,\n      passContext,\n      connector\n    );\n    this.tools.set(name, handle);\n    return description.title || name;\n  }\n\n  addCustomTool(name: string, handle: ToolHandle) {\n    this.tools.set(name, handle);\n  }\n\n  async addTool(url: string, instance?: string): Promise<Outcome<string>> {\n    if (instance) {\n      // This is a connector.\n      const connector = new ConnectorManager({ path: instance });\n      const tools = await connector.listTools();\n      if (!ok(tools)) return tools;\n      for (const tool of tools) {\n        const { url, description } = tool;\n        this.#addOneTool(url, description, false, connector);\n      }\n      // Return empty string, which will inform the\n      // substitution machinery to just reuse title.\n      return \"\";\n    }\n\n    let description = (await describeGraph({\n      url,\n    })) as Outcome<DescriberResult>;\n    let passContext = false;\n    if (!ok(description)) return description;\n\n    // TODO: Remove this altogether?\n    // Let's see if there are exports. If yes, let's add the exports\n    // instead of the tool.\n    if (description.exports) {\n      let connector: ConnectorHandle | null = null;\n      if (description.metadata?.tags?.includes(\"connector\")) {\n        // This is a connector\n        connector = { tools: new Map() };\n      }\n      Object.entries(description.exports).forEach(([id, exportDescription]) => {\n        // TODO: Figure out what to do with passContext\n        const idAndHandle = this.#createToolHandle(\n          id,\n          exportDescription,\n          passContext\n        );\n        const [name, handle] = idAndHandle;\n        console.log(\"EXPORT DESCRIPTION\", exportDescription);\n        if (connector) {\n          if (\n            exportDescription.metadata?.tags?.includes(\"connector-configure\")\n          ) {\n            connector.configure = idAndHandle;\n            return;\n          } else if (\n            exportDescription.metadata?.tags?.includes(\"connector-load\")\n          ) {\n            connector.load = idAndHandle;\n            return;\n          } else {\n            connector.tools.set(name, handle);\n          }\n        }\n        this.tools.set(name, handle);\n      });\n      return this.#toName(description.title);\n    }\n\n    // Otherwise, let's add the tool itself.\n    if (this.describerResultTransformer) {\n      const transforming =\n        await this.describerResultTransformer.transform(description);\n      if (!ok(transforming)) return transforming;\n      if (transforming) {\n        description = transforming;\n        passContext = true;\n      }\n    }\n    return this.#addOneTool(url, description, passContext);\n  }\n\n  async initialize(tools?: ToolDescriptor[]): Promise<boolean> {\n    if (!tools) {\n      return true;\n    }\n    let hasInvalidTools = false;\n    for (const tool of tools) {\n      const url = typeof tool === \"string\" ? tool : tool.url;\n      const description = (await describeGraph({\n        url,\n      })) as Outcome<DescriberResult>;\n      if (!ok(description)) {\n        this.errors.push(description.$error);\n        // Invalid tool, skip\n        hasInvalidTools = true;\n        continue;\n      }\n      const parameters = this.#convertSchemas(description.inputSchema!);\n      const name = this.#toName(description.title);\n      const functionDeclaration = {\n        name,\n        description: description.description || \"\",\n        parameters,\n      };\n      this.tools.set(name, {\n        tool: functionDeclaration,\n        url,\n        passContext: false,\n      });\n    }\n    return !hasInvalidTools;\n  }\n\n  async processResponse(response: LLMContent, callTool: CallToolCallback) {\n    for (const part of response.parts) {\n      if (\"functionCall\" in part) {\n        const { args, name } = part.functionCall;\n        const handle = this.tools.get(name);\n        if (handle) {\n          if (handle.invoke) {\n            await handle.invoke(args as Record<string, unknown>);\n          } else {\n            const { url, passContext, connector } = handle;\n            if (connector) {\n              await connector.invokeTool(\n                name,\n                args as Record<string, unknown>,\n                callTool\n              );\n            } else {\n              await callTool(url, part.functionCall.args, passContext);\n            }\n          }\n        }\n      }\n    }\n  }\n\n  hasTools(): boolean {\n    return this.tools.size !== 0;\n  }\n\n  list(): Tool[] {\n    const declaration: Tool = {};\n    const entries = [...this.tools.entries()];\n    if (entries.length !== 0) {\n      declaration.functionDeclarations = entries.map(([, value]) => value.tool);\n    }\n    if (this.#hasSearch) {\n      declaration.googleSearch = {};\n    }\n    if (Object.keys(declaration).length === 0) return [];\n    return [declaration];\n  }\n}\n",
          "language": "typescript"
        },
        "description": "Manages tools.",
        "runnable": false
      }
    },
    "worker-worker": {
      "code": "/**\n * @fileoverview Performs assigned task. Part of the worker.\n */\nimport invokeBoard from \"@invoke\";\nimport output from \"@output\";\nimport { toText, toLLMContent, ok, err, llm, generateId } from \"./utils\";\nimport { defaultSafetySettings, } from \"./gemini\";\nimport { callGemini } from \"./gemini-client\";\nimport { GeminiPrompt } from \"./gemini-prompt\";\nimport { StructuredResponse } from \"./structured-response\";\nimport { ToolManager } from \"./tool-manager\";\nimport { fanOutContext, toList, listPrompt, listSchema, hasLists, } from \"./lists\";\nexport { invoke as default, describe };\nfunction computeWorkMode(tools, summarize) {\n    if (summarize) {\n        return \"summarize\";\n    }\n    if (tools.length > 0) {\n        return \"call-tools\";\n    }\n    return \"generate\";\n}\nasync function callTools(inputs, model, tools, retries) {\n    inputs.body.tools = tools;\n    inputs.body.toolConfig = {\n        functionCallingConfig: {\n            mode: \"ANY\",\n        },\n    };\n    const response = await callGemini(inputs, model, (response) => {\n        const r = response;\n        if (r.candidates?.at(0)?.content)\n            return;\n        return err(\"No content\");\n    }, retries);\n    if (!ok(response)) {\n        return toLLMContent(\"TODO: Handle Gemini error response\");\n    }\n    const r = response;\n    return r.candidates?.at(0)?.content || toLLMContent(\"No valid response\");\n}\nasync function generate(inputs, model, responseManager, retries) {\n    const response = await callGemini(inputs, model, (response) => {\n        return responseManager.parse(response);\n    }, retries);\n    if (!ok(response)) {\n        return response;\n    }\n    else {\n        return {\n            product: toLLMContent(responseManager.body, \"model\"),\n            response: responseManager.response,\n        };\n    }\n}\nasync function invoke({ id, work: context, description: instruction, model, toolManager, summarize, chat, makeList, }) {\n    // TODO: Make this a parameter.\n    const retries = 5;\n    const tools = toolManager.list();\n    const mode = computeWorkMode(tools, summarize);\n    const responseManager = new StructuredResponse(id, chat);\n    if (mode === \"call-tools\") {\n        const product = await callTools({\n            body: {\n                contents: [...context, prompt(instruction, mode, chat)],\n                systemInstruction: responseManager.instruction(),\n                safetySettings: defaultSafetySettings(),\n            },\n        }, model, tools, retries);\n        return { product };\n    }\n    else {\n        let product = null;\n        if (makeList) {\n            const generating = await new GeminiPrompt({\n                body: {\n                    contents: [...context, listPrompt(prompt(instruction, mode, chat))],\n                    safetySettings: defaultSafetySettings(),\n                    generationConfig: {\n                        responseSchema: listSchema(),\n                        responseMimeType: \"application/json\",\n                    },\n                },\n            }, {\n                toolManager,\n            }).invoke();\n            if (!ok(generating))\n                return generating;\n            const list = toList(generating.last);\n            if (!ok(list))\n                return list;\n            product = list;\n        }\n        else {\n            const result = await generate({\n                body: {\n                    contents: responseManager.addPrompt(context, prompt(instruction, mode, chat)),\n                    systemInstruction: responseManager.instruction(),\n                    safetySettings: defaultSafetySettings(),\n                },\n            }, model, responseManager, retries);\n            if (\"$error\" in result) {\n                return result;\n            }\n            product = result.product;\n        }\n        return { product, epilog: responseManager.epilog };\n    }\n}\nfunction prompt(description, mode, chat) {\n    const preamble = llm `\n${description}\n\n`;\n    const postamble = `\n\nToday is ${new Date().toLocaleString(\"en-US\", {\n        month: \"long\",\n        day: \"numeric\",\n        year: \"numeric\",\n        hour: \"numeric\",\n        minute: \"2-digit\",\n    })}`;\n    switch (mode) {\n        case \"summarize\":\n            return llm ` \n${preamble}\nSummarize the research results to fulfill the specified task.\n${postamble}`.asContent();\n        case \"call-tools\":\n            return llm `\n${preamble}\nGenerate multiple function calls to fulfill the specified task.\n${postamble}`.asContent();\n        case \"generate\":\n            return llm `\n${preamble}\nProvide the response that fulfills the specified task.\n${postamble}`.asContent();\n    }\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                work: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Work\",\n                },\n                description: {\n                    type: \"object\",\n                    behavior: [\"llm-content\"],\n                    title: \"Job Description\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"object\",\n                    behavior: [\"llm-content\"],\n                    title: \"Work Product\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "worker-worker",
        "source": {
          "code": "/**\n * @fileoverview Performs assigned task. Part of the worker.\n */\nimport invokeBoard from \"@invoke\";\nimport output from \"@output\";\n\nimport { toText, toLLMContent, ok, err, llm, generateId } from \"./utils\";\nimport {\n  type GeminiSchema,\n  type GeminiInputs,\n  type GeminiOutputs,\n  type Tool,\n  defaultSafetySettings,\n  type GeminiAPIOutputs,\n} from \"./gemini\";\nimport { callGemini } from \"./gemini-client\";\nimport { GeminiPrompt } from \"./gemini-prompt\";\nimport { StructuredResponse } from \"./structured-response\";\nimport { ToolManager } from \"./tool-manager\";\nimport {\n  fanOutContext,\n  toList,\n  listPrompt,\n  listSchema,\n  hasLists,\n} from \"./lists\";\n\nexport { invoke as default, describe };\n\ntype Inputs = {\n  id: string;\n  work: LLMContent[];\n  description: LLMContent;\n  model: string;\n  toolManager: ToolManager;\n  summarize: boolean;\n  chat: boolean;\n  makeList: boolean;\n};\n\ntype Outputs = {\n  product: LLMContent;\n  response?: LLMContent;\n  epilog?: string;\n};\n\ntype WorkMode = \"generate\" | \"call-tools\" | \"summarize\";\n\nfunction computeWorkMode(tools: Tool[], summarize: boolean): WorkMode {\n  if (summarize) {\n    return \"summarize\";\n  }\n  if (tools.length > 0) {\n    return \"call-tools\";\n  }\n  return \"generate\";\n}\n\nasync function callTools(\n  inputs: Omit<GeminiInputs, \"model\">,\n  model: string,\n  tools: Tool[],\n  retries: number\n): Promise<LLMContent> {\n  inputs.body.tools = tools;\n  inputs.body.toolConfig = {\n    functionCallingConfig: {\n      mode: \"ANY\",\n    },\n  };\n  const response = await callGemini(\n    inputs,\n    model,\n    (response) => {\n      const r = response as GeminiAPIOutputs;\n      if (r.candidates?.at(0)?.content) return;\n      return err(\"No content\");\n    },\n    retries\n  );\n  if (!ok(response)) {\n    return toLLMContent(\"TODO: Handle Gemini error response\");\n  }\n  const r = response as GeminiAPIOutputs;\n  return r.candidates?.at(0)?.content || toLLMContent(\"No valid response\");\n}\n\nasync function generate(\n  inputs: Omit<GeminiInputs, \"model\">,\n  model: string,\n  responseManager: StructuredResponse,\n  retries: number\n): Promise<Outcome<Outputs>> {\n  const response = await callGemini(\n    inputs,\n    model,\n    (response) => {\n      return responseManager.parse(response);\n    },\n    retries\n  );\n  if (!ok(response)) {\n    return response;\n  } else {\n    return {\n      product: toLLMContent(responseManager.body, \"model\"),\n      response: responseManager.response!,\n    };\n  }\n}\n\nasync function invoke({\n  id,\n  work: context,\n  description: instruction,\n  model,\n  toolManager,\n  summarize,\n  chat,\n  makeList,\n}: Inputs): Promise<Outcome<Outputs>> {\n  // TODO: Make this a parameter.\n  const retries = 5;\n  const tools = toolManager.list();\n  const mode = computeWorkMode(tools, summarize);\n  const responseManager = new StructuredResponse(id, chat);\n  if (mode === \"call-tools\") {\n    const product = await callTools(\n      {\n        body: {\n          contents: [...context, prompt(instruction, mode, chat)],\n          systemInstruction: responseManager.instruction(),\n          safetySettings: defaultSafetySettings(),\n        },\n      },\n      model,\n      tools,\n      retries\n    );\n    return { product };\n  } else {\n    let product: LLMContent | null = null;\n    if (makeList) {\n      const generating = await new GeminiPrompt(\n        {\n          body: {\n            contents: [...context, listPrompt(prompt(instruction, mode, chat))],\n            safetySettings: defaultSafetySettings(),\n            generationConfig: {\n              responseSchema: listSchema(),\n              responseMimeType: \"application/json\",\n            },\n          },\n        },\n        {\n          toolManager,\n        }\n      ).invoke();\n      if (!ok(generating)) return generating;\n\n      const list = toList(generating.last);\n      if (!ok(list)) return list;\n\n      product = list;\n    } else {\n      const result = await generate(\n        {\n          body: {\n            contents: responseManager.addPrompt(\n              context,\n              prompt(instruction, mode, chat)\n            ),\n            systemInstruction: responseManager.instruction(),\n            safetySettings: defaultSafetySettings(),\n          },\n        },\n        model,\n        responseManager,\n        retries\n      );\n      if (\"$error\" in result) {\n        return result;\n      }\n      product = result.product;\n    }\n    return { product, epilog: responseManager.epilog };\n  }\n}\n\nfunction prompt(\n  description: LLMContent,\n  mode: WorkMode,\n  chat: boolean\n): LLMContent {\n  const preamble = llm`\n${description}\n\n`;\n  const postamble = `\n\nToday is ${new Date().toLocaleString(\"en-US\", {\n    month: \"long\",\n    day: \"numeric\",\n    year: \"numeric\",\n    hour: \"numeric\",\n    minute: \"2-digit\",\n  })}`;\n\n  switch (mode) {\n    case \"summarize\":\n      return llm` \n${preamble}\nSummarize the research results to fulfill the specified task.\n${postamble}`.asContent();\n\n    case \"call-tools\":\n      return llm`\n${preamble}\nGenerate multiple function calls to fulfill the specified task.\n${postamble}`.asContent();\n\n    case \"generate\":\n      return llm`\n${preamble}\nProvide the response that fulfills the specified task.\n${postamble}`.asContent();\n  }\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        work: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Work\",\n        },\n        description: {\n          type: \"object\",\n          behavior: [\"llm-content\"],\n          title: \"Job Description\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"object\",\n          behavior: [\"llm-content\"],\n          title: \"Work Product\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Performs assigned task. Part of the worker.",
        "runnable": false
      }
    },
    "gemini-client": {
      "code": "import invokeGemini, {} from \"./gemini\";\nimport { ok, err } from \"./utils\";\nexport { callGemini };\nasync function callGemini(inputs, model, validator, retries) {\n    // TODO: Add more nuanced logic around retries\n    for (let i = 0; i < retries; ++i) {\n        const nextStep = i == retries ? \"bailing\" : \"will retry\";\n        const response = await invokeGemini(inputs);\n        if (!ok(response)) {\n            console.error(`Error from model, ${nextStep}`);\n            return response;\n        }\n        else {\n            const validating = validator(response);\n            if (!ok(validating)) {\n                console.error(`Validation error, ${nextStep}`, validating.$error);\n                continue;\n            }\n            return response;\n        }\n    }\n    return err(`Failed to get valid response after ${retries} tries`);\n}\n",
      "metadata": {
        "title": "gemini-client",
        "source": {
          "code": "import invokeGemini, { type GeminiInputs, type GeminiOutputs } from \"./gemini\";\nimport { ok, err } from \"./utils\";\n\nexport type ValidatorFunction = (response: GeminiOutputs) => Outcome<void>;\n\nexport { callGemini };\n\nasync function callGemini(\n  inputs: Omit<GeminiInputs, \"model\">,\n  model: string,\n  validator: ValidatorFunction,\n  retries: number\n): Promise<Outcome<GeminiOutputs>> {\n  // TODO: Add more nuanced logic around retries\n  for (let i = 0; i < retries; ++i) {\n    const nextStep = i == retries ? \"bailing\" : \"will retry\";\n    const response = await invokeGemini(inputs);\n    if (!ok(response)) {\n      console.error(`Error from model, ${nextStep}`);\n      return response;\n    } else {\n      const validating = validator(response);\n      if (!ok(validating)) {\n        console.error(`Validation error, ${nextStep}`, validating.$error);\n        continue;\n      }\n      return response;\n    }\n  }\n  return err(`Failed to get valid response after ${retries} tries`);\n}\n",
          "language": "typescript"
        },
        "description": "",
        "runnable": false
      }
    },
    "agent-main": {
      "code": "/**\n * @fileoverview The main body of the agent\n */\nimport output from \"@output\";\nimport {} from \"./common\";\nimport { ToolManager } from \"./tool-manager\";\nimport workerWorker from \"./worker-worker\";\nimport { report } from \"./output\";\nimport { defaultLLMContent, toText, err, endsWithRole, ok, llm } from \"./utils\";\nimport invokeGraph from \"@invoke\";\nimport { Template } from \"./template\";\nimport { ArgumentNameGenerator } from \"./introducer\";\nimport { ListExpander } from \"./lists\";\nexport { invoke as default, describe };\nfunction toLLMContent(text, role = \"user\") {\n    return { parts: [{ text }], role };\n}\nasync function invoke({ context }) {\n    console.log(\"AGENT MAIN\", context);\n    let { id, description, context: initialContext, model, defaultModel, tools, chat, makeList, work: workContext, params, } = context;\n    if (!description) {\n        const $error = \"No instruction supplied\";\n        await report({\n            actor: \"Text Generator\",\n            name: $error,\n            category: \"Runtime error\",\n            details: `In order to run, I need to have an instruction.`,\n        });\n        return { $error };\n    }\n    const template = new Template(description);\n    const toolManager = new ToolManager(new ArgumentNameGenerator());\n    const substituting = await template.substitute(params, async ({ path: url, instance }) => toolManager.addTool(url, instance));\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    description = substituting;\n    if (!(await toolManager.initialize(tools))) {\n        const $error = `Problem initializing tools. \nThe following errors were encountered: ${toolManager.errors.join(\",\")}`;\n        console.error(\"MAIN ERROR\", $error, toolManager.errors);\n        return { $error };\n    }\n    const { userEndedChat, userInputs, last } = context;\n    if (userEndedChat) {\n        if (!last) {\n            return err(\"Chat ended without any work\");\n        }\n        return {\n            done: [...initialContext, last],\n        };\n    }\n    let epilog;\n    const result = await new ListExpander(description, workContext.length > 0 ? workContext : initialContext).map(async (description, work, isList) => {\n        // Disallow making nested lists\n        const disallowNestedMakeList = makeList && !isList;\n        // 1) Make first attempt to make text\n        const response = await workerWorker({\n            id,\n            description,\n            work,\n            model,\n            toolManager,\n            summarize: false,\n            chat,\n            makeList: disallowNestedMakeList,\n        });\n        if (!ok(response)) {\n            console.error(\"ERROR FROM WORKER\", response.$error);\n            return response;\n        }\n        const workerResponse = response.product;\n        epilog = response.epilog;\n        // 2) Call tools\n        const toolResults = [];\n        // TODO: Convert to GeminiPrompt. Somebody. Please.\n        const errors = [];\n        await toolManager.processResponse(workerResponse, async ($board, args) => {\n            const result = await invokeGraph({\n                $board,\n                ...args,\n            });\n            if (\"$error\" in result) {\n                errors.push(result.$error);\n            }\n            toolResults.push(result);\n        });\n        console.log(\"TOOL RESULTS\", toolResults);\n        if (errors.length > 0) {\n            console.error(\"TOOL ERRORS\", errors);\n            return err(`Tool Errors: ${errors.join(\"\\n\\n\")}`);\n        }\n        // 3) Handle tool results\n        if (toolResults.length > 0) {\n            const summary = await workerWorker({\n                id,\n                description,\n                work: [\n                    ...toolResults.map((toolResult) => toLLMContent(JSON.stringify(toolResult))),\n                ],\n                model,\n                toolManager,\n                summarize: true,\n                chat: false,\n                makeList: disallowNestedMakeList,\n            });\n            if (!ok(summary)) {\n                console.error(\"ERROR FROM SUMMARY\", summary.$error);\n                return summary;\n            }\n            console.log(\"SUMMARY RESPONSE\", summary);\n            const summaryResponse = summary.product;\n            epilog = summary.epilog;\n            return summaryResponse;\n        }\n        return workerResponse;\n    });\n    if (!ok(result))\n        return result;\n    // 4) Handle chat.\n    if (chat) {\n        const last = result.at(-1);\n        epilog ??= \"Please provide feedback on the draft\";\n        await output({\n            schema: {\n                type: \"object\",\n                properties: {\n                    \"a-product\": {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                        title: \"Draft\",\n                    },\n                    \"b-message\": {\n                        type: \"string\",\n                        title: \"\",\n                        format: \"markdown\",\n                    },\n                },\n            },\n            $metadata: {\n                title: \"Writer\",\n                description: \"Asking for feedback on a draft\",\n                icon: \"generative-text\",\n            },\n            \"b-message\": epilog,\n            \"a-product\": last,\n        });\n        const { userInputs } = context;\n        if (!userEndedChat) {\n            const toInput = {\n                type: \"object\",\n                properties: {\n                    request: {\n                        type: \"object\",\n                        title: \"Please provide feedback\",\n                        description: \"Provide feedback or click submit to continue\",\n                        behavior: [\"transient\", \"llm-content\"],\n                        examples: [defaultLLMContent()],\n                    },\n                },\n            };\n            return {\n                toInput,\n                context: {\n                    ...context,\n                    work: result,\n                    last,\n                },\n            };\n        }\n    }\n    // 5) Fall through to default response.\n    return { done: result };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"object\",\n                    title: \"Agent Context\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                toInput: {\n                    type: \"object\",\n                    title: \"Input Schema\",\n                },\n                context: {\n                    type: \"object\",\n                    title: \"Agent Context\",\n                },\n                done: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Done\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "agent-main",
        "source": {
          "code": "/**\n * @fileoverview The main body of the agent\n */\nimport output from \"@output\";\nimport { type AgentContext, type DescriberResult } from \"./common\";\nimport { ToolManager } from \"./tool-manager\";\nimport workerWorker from \"./worker-worker\";\nimport { report } from \"./output\";\nimport { defaultLLMContent, toText, err, endsWithRole, ok, llm } from \"./utils\";\nimport invokeGraph from \"@invoke\";\nimport { Template } from \"./template\";\nimport { ArgumentNameGenerator } from \"./introducer\";\nimport { ListExpander } from \"./lists\";\n\nexport { invoke as default, describe };\n\ntype Inputs = {\n  context: AgentContext;\n};\n\ntype Outputs = {\n  $error?: string;\n  context?: AgentContext;\n  toInput?: Schema;\n  done?: LLMContent[];\n};\n\nfunction toLLMContent(\n  text: string,\n  role: LLMContent[\"role\"] = \"user\"\n): LLMContent {\n  return { parts: [{ text }], role };\n}\n\nasync function invoke({ context }: Inputs): Promise<Outputs> {\n  console.log(\"AGENT MAIN\", context);\n  let {\n    id,\n    description,\n    context: initialContext,\n    model,\n    defaultModel,\n    tools,\n    chat,\n    makeList,\n    work: workContext,\n    params,\n  } = context;\n  if (!description) {\n    const $error = \"No instruction supplied\";\n    await report({\n      actor: \"Text Generator\",\n      name: $error,\n      category: \"Runtime error\",\n      details: `In order to run, I need to have an instruction.`,\n    });\n    return { $error };\n  }\n  const template = new Template(description);\n  const toolManager = new ToolManager(new ArgumentNameGenerator());\n  const substituting = await template.substitute(\n    params,\n    async ({ path: url, instance }) => toolManager.addTool(url, instance)\n  );\n  if (!ok(substituting)) {\n    return substituting;\n  }\n  description = substituting;\n\n  if (!(await toolManager.initialize(tools))) {\n    const $error = `Problem initializing tools. \nThe following errors were encountered: ${toolManager.errors.join(\",\")}`;\n    console.error(\"MAIN ERROR\", $error, toolManager.errors);\n    return { $error };\n  }\n\n  const { userEndedChat, userInputs, last } = context;\n  if (userEndedChat) {\n    if (!last) {\n      return err(\"Chat ended without any work\");\n    }\n    return {\n      done: [...initialContext, last],\n    };\n  }\n  let epilog: string | undefined;\n  const result = await new ListExpander(\n    description,\n    workContext.length > 0 ? workContext : initialContext\n  ).map(async (description, work, isList) => {\n    // Disallow making nested lists\n    const disallowNestedMakeList = makeList && !isList;\n\n    // 1) Make first attempt to make text\n    const response = await workerWorker({\n      id,\n      description,\n      work,\n      model,\n      toolManager,\n      summarize: false,\n      chat,\n      makeList: disallowNestedMakeList,\n    });\n    if (!ok(response)) {\n      console.error(\"ERROR FROM WORKER\", response.$error);\n      return response;\n    }\n    const workerResponse = response.product;\n    epilog = response.epilog;\n\n    // 2) Call tools\n    const toolResults: object[] = [];\n    // TODO: Convert to GeminiPrompt. Somebody. Please.\n    const errors: string[] = [];\n    await toolManager.processResponse(workerResponse, async ($board, args) => {\n      const result = await invokeGraph({\n        $board,\n        ...args,\n      });\n      if (\"$error\" in result) {\n        errors.push(result.$error as string);\n      }\n      toolResults.push(result);\n    });\n    console.log(\"TOOL RESULTS\", toolResults);\n    if (errors.length > 0) {\n      console.error(\"TOOL ERRORS\", errors);\n      return err(`Tool Errors: ${errors.join(\"\\n\\n\")}`);\n    }\n\n    // 3) Handle tool results\n    if (toolResults.length > 0) {\n      const summary = await workerWorker({\n        id,\n        description,\n        work: [\n          ...toolResults.map((toolResult) =>\n            toLLMContent(JSON.stringify(toolResult))\n          ),\n        ],\n        model,\n        toolManager,\n        summarize: true,\n        chat: false,\n        makeList: disallowNestedMakeList,\n      });\n      if (!ok(summary)) {\n        console.error(\"ERROR FROM SUMMARY\", summary.$error);\n        return summary;\n      }\n      console.log(\"SUMMARY RESPONSE\", summary);\n      const summaryResponse = summary.product;\n      epilog = summary.epilog;\n      return summaryResponse;\n    }\n\n    return workerResponse;\n  });\n  if (!ok(result)) return result;\n\n  // 4) Handle chat.\n  if (chat) {\n    const last = result.at(-1)!;\n    epilog ??= \"Please provide feedback on the draft\";\n    await output({\n      schema: {\n        type: \"object\",\n        properties: {\n          \"a-product\": {\n            type: \"object\",\n            behavior: [\"llm-content\"],\n            title: \"Draft\",\n          },\n          \"b-message\": {\n            type: \"string\",\n            title: \"\",\n            format: \"markdown\",\n          },\n        },\n      },\n      $metadata: {\n        title: \"Writer\",\n        description: \"Asking for feedback on a draft\",\n        icon: \"generative-text\",\n      },\n      \"b-message\": epilog,\n      \"a-product\": last,\n    });\n\n    const { userInputs } = context;\n    if (!userEndedChat) {\n      const toInput: Schema = {\n        type: \"object\",\n        properties: {\n          request: {\n            type: \"object\",\n            title: \"Please provide feedback\",\n            description: \"Provide feedback or click submit to continue\",\n            behavior: [\"transient\", \"llm-content\"],\n            examples: [defaultLLMContent()],\n          },\n        },\n      };\n      return {\n        toInput,\n        context: {\n          ...context,\n          work: result,\n          last,\n        },\n      };\n    }\n  }\n\n  // 5) Fall through to default response.\n  return { done: result };\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"object\",\n          title: \"Agent Context\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        toInput: {\n          type: \"object\",\n          title: \"Input Schema\",\n        },\n        context: {\n          type: \"object\",\n          title: \"Agent Context\",\n        },\n        done: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Done\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "The main body of the agent",
        "runnable": true
      }
    },
    "researcher": {
      "code": "/**\n * @fileoverview Searching the Internet according to your plan.\n */\nimport { ToolManager } from \"./tool-manager\";\nimport { Template } from \"./template\";\nimport invokeGraph from \"@invoke\";\nimport invokeGemini, { defaultSafetySettings, } from \"./gemini\";\nimport { ok, err, llm, toLLMContent, toText, addUserTurn } from \"./utils\";\nimport { report } from \"./output\";\nimport {} from \"./common\";\nimport { ArgumentNameGenerator } from \"./introducer\";\nexport { invoke as default, describe };\nconst RESEARCH_TOOLS = [\n    {\n        url: \"embed://a2/tools.bgl.json#module:search-web\",\n        title: \"Search Web\",\n    },\n    {\n        url: \"embed://a2/tools.bgl.json#module:search-wikipedia\",\n        title: \"Search Wikipedia\",\n    },\n    {\n        url: \"embed://a2/tools.bgl.json#module:get-webpage\",\n        title: \"Get Webpage\",\n    },\n    {\n        url: \"embed://a2/tools.bgl.json#module:search-maps\",\n        title: \"Search Maps\",\n    },\n];\nconst RESEARCH_MODEL = \"gemini-2.0-flash\";\nconst MAX_ITERATIONS = 7;\nfunction systemInstruction(first) {\n    const which = first ? \"first\" : \"next\";\n    return `You are a researcher.\n  \nYour job is to use the provided research plan to produce raw research that will be later turned into a detailed research report.\nYou are tasked with finding as much of relevant information as possible.\n\nYou examine the conversation context so far and come up with the ${which} step to produce the report, \nusing the conversation context as the the guide of steps taken so far and the outcomes recorded.\n\nYou do not ask user for feedback. You do not try to have a conversation with the user. \nYou know that the user will only ask you to proceed to next step.\n\nYour next step consists of answering two questions.\n\nFirst, ask yourself \"am I done?\" -- looking back at all that you've researched and the plan, \ndo you have enough to produce the detailed report?\n\nSecond, provide a response. Your response must contain two parts:\nThought: a brief plain text reasoning why this is the right ${which} step and a description of what you will do in plain English.\nAction: invoking the tools are your disposal, more than one if necessary. If you're done, do not invoke any tools.`;\n}\nfunction researcherPrompt(contents, plan, tools, first) {\n    return {\n        model: RESEARCH_MODEL,\n        body: {\n            contents: addUserTurn(llm `\nDo the research according to this plan:\n\n---\n\n${plan}\n\n---\n`.asContent(), contents),\n            tools,\n            systemInstruction: toLLMContent(systemInstruction(first)),\n            safetySettings: defaultSafetySettings(),\n        },\n    };\n}\nfunction reportWriterInstruction() {\n    return `You are a research report writer. \nYour teammates produced a wealth of raw research according to the supplied plan.\n\nYour task is to take the raw research and write a thorough, detailed research report that captures it in a way that follows the plan. Use markdown.\n\nA report must additionally contain references to the source (always cite your sources).`;\n}\nfunction reportWriterPrompt(plan, research) {\n    return {\n        model: RESEARCH_MODEL,\n        body: {\n            contents: [toLLMContent(research.join(\"\\n\\n\"))],\n            systemInstruction: toLLMContent(reportWriterInstruction()),\n            safetySettings: defaultSafetySettings(),\n        },\n    };\n}\nasync function thought(response, iteration) {\n    const first = response.parts?.at(0);\n    if (!first || !(\"text\" in first)) {\n        return;\n    }\n    await report({\n        actor: \"Researcher\",\n        category: `Progress report, iteration ${iteration + 1}`,\n        name: \"Thought\",\n        icon: \"generative\",\n        details: first.text\n            .replace(/^Thought: ?/gm, \"\")\n            .replace(/^Action:.*$/gm, \"\")\n            .trim(),\n    });\n}\nasync function invoke({ context, plan, summarize, ...params }) {\n    const tools = RESEARCH_TOOLS.map((descriptor) => descriptor.url);\n    const toolManager = new ToolManager(new ArgumentNameGenerator());\n    let content = context || [toLLMContent(\"Start the research\")];\n    const template = new Template(plan);\n    const substituting = await template.substitute(params, async ({ path: url, instance }) => toolManager.addTool(url, instance));\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    if (!toolManager.hasTools()) {\n        // If no tools supplied (legacy case, actually), initialize\n        // with a set of default tools.\n        const initializing = await toolManager.initialize(tools);\n        if (!initializing) {\n            return err(\"Unable to initialize tools\");\n        }\n    }\n    plan = substituting;\n    const research = [];\n    for (let i = 0; i <= MAX_ITERATIONS; i++) {\n        const askingGemini = await invokeGemini(researcherPrompt(content, plan, toolManager.list(), i === 0));\n        if (!ok(askingGemini)) {\n            return askingGemini;\n        }\n        if (\"context\" in askingGemini) {\n            return err(`Unexpected \"context\" response`);\n        }\n        const response = askingGemini.candidates.at(0)?.content;\n        if (!response) {\n            return err(\"No actionable response\");\n        }\n        await thought(response, i);\n        const toolResponses = [];\n        await toolManager.processResponse(response, async ($board, args) => {\n            toolResponses.push(JSON.stringify(await invokeGraph({ $board, ...args })));\n        });\n        if (toolResponses.length === 0) {\n            break;\n        }\n        research.push(...toolResponses);\n        content = [...content, response, toLLMContent(toolResponses.join(\"\\n\\n\"))];\n    }\n    if (research.length === 0) {\n        await report({\n            actor: \"Researcher\",\n            category: \"Error\",\n            name: \"Error\",\n            details: \"I was unable to obtain any research results\",\n        });\n        return { context };\n    }\n    if (summarize) {\n        const producingReport = await invokeGemini(reportWriterPrompt(plan, research));\n        if (!ok(producingReport)) {\n            return producingReport;\n        }\n        if (\"context\" in producingReport) {\n            return err(`Unexpected \"context\" response`);\n        }\n        const response = producingReport.candidates.at(0)?.content;\n        if (!response) {\n            return err(\"No actionable response\");\n        }\n        return { context: [...(context || []), response] };\n    }\n    return { context: [...(context || []), toLLMContent(research.join(\"\\n\\n\"))] };\n}\nfunction toOxfordList(items) {\n    if (items.length === 0)\n        return \"\";\n    if (items.length === 1)\n        return items[0];\n    if (items.length === 2)\n        return items.join(\" and \");\n    const lastItem = items.pop();\n    return `${items.join(\", \")}, and ${lastItem}`;\n}\nfunction researchExample() {\n    const type = \"tool\";\n    const tools = RESEARCH_TOOLS.map(({ url: path, title }) => Template.part({ title, path, type }));\n    return [\n        JSON.stringify({\n            plan: toLLMContent(`Research the topic provided using ${toOxfordList(tools)} tools`),\n        }),\n    ];\n}\nasync function describe({ inputs: { plan } }) {\n    const template = new Template(plan);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                plan: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"Research Plan\",\n                    description: \"Provide an outline of what to research, what areas to cover, etc.\",\n                },\n                summarize: {\n                    type: \"boolean\",\n                    behavior: [\"config\", \"hint-preview\"],\n                    icon: \"summarize\",\n                    title: \"Summarize research\",\n                    description: \"If checked, the Researcher will summarize the results of the research and only pass the research summary along.\",\n                },\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n            additionalProperties: false,\n            examples: researchExample(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"main-port\", \"hint-text\"],\n                },\n            },\n            additionalProperties: false,\n        },\n        title: \"Do deep research\",\n        description: \"Do deep research according to your plan\",\n        metadata: {\n            icon: \"generative-search\",\n            tags: [\"quick-access\", \"generative\"],\n            order: 101,\n        },\n    };\n}\n",
      "metadata": {
        "title": "Do deep research",
        "source": {
          "code": "/**\n * @fileoverview Searching the Internet according to your plan.\n */\nimport { ToolManager, type ToolDescriptor } from \"./tool-manager\";\nimport { Template } from \"./template\";\nimport invokeGraph from \"@invoke\";\nimport invokeGemini, {\n  type GeminiInputs,\n  type Tool,\n  defaultSafetySettings,\n} from \"./gemini\";\nimport { ok, err, llm, toLLMContent, toText, addUserTurn } from \"./utils\";\nimport { report } from \"./output\";\nimport { type Params } from \"./common\";\nimport { ArgumentNameGenerator } from \"./introducer\";\n\nexport { invoke as default, describe };\n\nexport type ResearcherInputs = {\n  context?: LLMContent[];\n  plan: LLMContent;\n  summarize: boolean;\n} & Params;\n\nexport type DefaultToolDescriptor = {\n  url: string;\n  title: string;\n};\n\nconst RESEARCH_TOOLS: DefaultToolDescriptor[] = [\n  {\n    url: \"embed://a2/tools.bgl.json#module:search-web\",\n    title: \"Search Web\",\n  },\n  {\n    url: \"embed://a2/tools.bgl.json#module:search-wikipedia\",\n    title: \"Search Wikipedia\",\n  },\n  {\n    url: \"embed://a2/tools.bgl.json#module:get-webpage\",\n    title: \"Get Webpage\",\n  },\n  {\n    url: \"embed://a2/tools.bgl.json#module:search-maps\",\n    title: \"Search Maps\",\n  },\n];\n\nconst RESEARCH_MODEL = \"gemini-2.0-flash\";\n\nconst MAX_ITERATIONS = 7;\n\nfunction systemInstruction(first: boolean): string {\n  const which = first ? \"first\" : \"next\";\n  return `You are a researcher.\n  \nYour job is to use the provided research plan to produce raw research that will be later turned into a detailed research report.\nYou are tasked with finding as much of relevant information as possible.\n\nYou examine the conversation context so far and come up with the ${which} step to produce the report, \nusing the conversation context as the the guide of steps taken so far and the outcomes recorded.\n\nYou do not ask user for feedback. You do not try to have a conversation with the user. \nYou know that the user will only ask you to proceed to next step.\n\nYour next step consists of answering two questions.\n\nFirst, ask yourself \"am I done?\" -- looking back at all that you've researched and the plan, \ndo you have enough to produce the detailed report?\n\nSecond, provide a response. Your response must contain two parts:\nThought: a brief plain text reasoning why this is the right ${which} step and a description of what you will do in plain English.\nAction: invoking the tools are your disposal, more than one if necessary. If you're done, do not invoke any tools.`;\n}\n\nfunction researcherPrompt(\n  contents: LLMContent[],\n  plan: LLMContent,\n  tools: Tool[],\n  first: boolean\n): GeminiInputs {\n  return {\n    model: RESEARCH_MODEL,\n    body: {\n      contents: addUserTurn(\n        llm`\nDo the research according to this plan:\n\n---\n\n${plan}\n\n---\n`.asContent(),\n        contents\n      ),\n      tools,\n      systemInstruction: toLLMContent(systemInstruction(first)),\n      safetySettings: defaultSafetySettings(),\n    },\n  };\n}\n\nfunction reportWriterInstruction() {\n  return `You are a research report writer. \nYour teammates produced a wealth of raw research according to the supplied plan.\n\nYour task is to take the raw research and write a thorough, detailed research report that captures it in a way that follows the plan. Use markdown.\n\nA report must additionally contain references to the source (always cite your sources).`;\n}\n\nfunction reportWriterPrompt(\n  plan: LLMContent,\n  research: string[]\n): GeminiInputs {\n  return {\n    model: RESEARCH_MODEL,\n    body: {\n      contents: [toLLMContent(research.join(\"\\n\\n\"))],\n      systemInstruction: toLLMContent(reportWriterInstruction()),\n      safetySettings: defaultSafetySettings(),\n    },\n  };\n}\n\nasync function thought(response: LLMContent, iteration: number) {\n  const first = response.parts?.at(0);\n  if (!first || !(\"text\" in first)) {\n    return;\n  }\n  await report({\n    actor: \"Researcher\",\n    category: `Progress report, iteration ${iteration + 1}`,\n    name: \"Thought\",\n    icon: \"generative\",\n    details: first.text\n      .replace(/^Thought: ?/gm, \"\")\n      .replace(/^Action:.*$/gm, \"\")\n      .trim(),\n  });\n}\n\nasync function invoke({\n  context,\n  plan,\n  summarize,\n  ...params\n}: ResearcherInputs) {\n  const tools = RESEARCH_TOOLS.map((descriptor) => descriptor.url);\n  const toolManager = new ToolManager(new ArgumentNameGenerator());\n  let content = context || [toLLMContent(\"Start the research\")];\n\n  const template = new Template(plan);\n  const substituting = await template.substitute(\n    params,\n    async ({ path: url, instance }) => toolManager.addTool(url, instance)\n  );\n  if (!ok(substituting)) {\n    return substituting;\n  }\n  if (!toolManager.hasTools()) {\n    // If no tools supplied (legacy case, actually), initialize\n    // with a set of default tools.\n    const initializing = await toolManager.initialize(tools);\n    if (!initializing) {\n      return err(\"Unable to initialize tools\");\n    }\n  }\n  plan = substituting;\n\n  const research: string[] = [];\n  for (let i = 0; i <= MAX_ITERATIONS; i++) {\n    const askingGemini = await invokeGemini(\n      researcherPrompt(content, plan, toolManager.list(), i === 0)\n    );\n\n    if (!ok(askingGemini)) {\n      return askingGemini;\n    }\n    if (\"context\" in askingGemini) {\n      return err(`Unexpected \"context\" response`);\n    }\n    const response = askingGemini.candidates.at(0)?.content;\n    if (!response) {\n      return err(\"No actionable response\");\n    }\n    await thought(response, i);\n\n    const toolResponses: string[] = [];\n    await toolManager.processResponse(response, async ($board, args) => {\n      toolResponses.push(\n        JSON.stringify(await invokeGraph({ $board, ...args }))\n      );\n    });\n    if (toolResponses.length === 0) {\n      break;\n    }\n    research.push(...toolResponses);\n    content = [...content, response, toLLMContent(toolResponses.join(\"\\n\\n\"))];\n  }\n  if (research.length === 0) {\n    await report({\n      actor: \"Researcher\",\n      category: \"Error\",\n      name: \"Error\",\n      details: \"I was unable to obtain any research results\",\n    });\n    return { context };\n  }\n  if (summarize) {\n    const producingReport = await invokeGemini(\n      reportWriterPrompt(plan, research)\n    );\n    if (!ok(producingReport)) {\n      return producingReport;\n    }\n    if (\"context\" in producingReport) {\n      return err(`Unexpected \"context\" response`);\n    }\n    const response = producingReport.candidates.at(0)?.content;\n    if (!response) {\n      return err(\"No actionable response\");\n    }\n    return { context: [...(context || []), response] };\n  }\n  return { context: [...(context || []), toLLMContent(research.join(\"\\n\\n\"))] };\n}\n\ntype DescribeInputs = {\n  inputs: {\n    plan: LLMContent;\n  };\n};\n\nfunction toOxfordList(items: string[]): string {\n  if (items.length === 0) return \"\";\n  if (items.length === 1) return items[0];\n  if (items.length === 2) return items.join(\" and \");\n  const lastItem = items.pop();\n  return `${items.join(\", \")}, and ${lastItem}`;\n}\n\nfunction researchExample(): string[] {\n  const type = \"tool\";\n  const tools = RESEARCH_TOOLS.map(({ url: path, title }) =>\n    Template.part({ title, path, type })\n  );\n  return [\n    JSON.stringify({\n      plan: toLLMContent(\n        `Research the topic provided using ${toOxfordList(tools)} tools`\n      ),\n    }),\n  ];\n}\n\nasync function describe({ inputs: { plan } }: DescribeInputs) {\n  const template = new Template(plan);\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n        plan: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n          title: \"Research Plan\",\n          description:\n            \"Provide an outline of what to research, what areas to cover, etc.\",\n        },\n        summarize: {\n          type: \"boolean\",\n          behavior: [\"config\", \"hint-preview\"],\n          icon: \"summarize\",\n          title: \"Summarize research\",\n          description:\n            \"If checked, the Researcher will summarize the results of the research and only pass the research summary along.\",\n        },\n        ...template.schemas(),\n      },\n      behavior: [\"at-wireable\"],\n      ...template.requireds(),\n      additionalProperties: false,\n      examples: researchExample(),\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n          behavior: [\"main-port\", \"hint-text\"],\n        },\n      },\n      additionalProperties: false,\n    } satisfies Schema,\n    title: \"Do deep research\",\n    description: \"Do deep research according to your plan\",\n    metadata: {\n      icon: \"generative-search\",\n      tags: [\"quick-access\", \"generative\"],\n      order: 101,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Searching the Internet according to your plan.",
        "runnable": true
      }
    },
    "image-generator": {
      "code": "/**\n * @fileoverview Generates an image using supplied context (generation only).\n */\nimport invokeBoard from \"@invoke\";\nimport gemini, { defaultSafetySettings, } from \"./gemini\";\nimport { GeminiPrompt } from \"./gemini-prompt\";\nimport { err, ok, toLLMContent, defaultLLMContent, toText, addUserTurn, toTextConcat, joinContent, llm, extractMediaData, extractTextData, mergeContent, } from \"./utils\";\nimport { callImageGen, promptExpander } from \"./image-utils\";\nimport { Template } from \"./template\";\nimport { ToolManager } from \"./tool-manager\";\nimport {} from \"./common\";\nimport { report } from \"./output\";\nimport { ArgumentNameGenerator } from \"./introducer\";\nimport { ListExpander } from \"./lists\";\nconst MAKE_IMAGE_ICON = \"generative-image\";\nconst ASPECT_RATIOS = [\"1:1\", \"9:16\", \"16:9\", \"4:3\", \"3:4\"];\nexport { invoke as default, describe };\nfunction gatheringRequest(contents, instruction, toolManager) {\n    const promptText = llm `\nAnalyze the instruction below and rather than following it, determine what information needs to be gathered to \ngenerate an accurate prompt for a text-to-image model in the next turn:\n-- begin instruction --\n${instruction}\n-- end instruction --\n\nCall the tools to gather the necessary information that could be used to create an accurate prompt.`;\n    return new GeminiPrompt({\n        body: {\n            contents: addUserTurn(promptText.asContent(), contents),\n            tools: toolManager.list(),\n            systemInstruction: toLLMContent(`\nYou are a researcher whose specialty is to call tools whose output helps gather the necessary information\nto be used to create an accurate prompt for a text-to-image model.\n`),\n        },\n    }, toolManager);\n}\nfunction gracefulExit(notOk) {\n    report({\n        actor: \"Make Image\",\n        category: \"Warning\",\n        name: \"Graceful exit\",\n        details: `I tried a couple of times, but the Gemini API failed to generate the image you requested with the following error:\n\n### ${notOk.$error}\n\nTo keep things moving, I will return a blank result. My apologies!`,\n        icon: MAKE_IMAGE_ICON,\n    });\n    return toLLMContent(\" \");\n}\nconst MAX_RETRIES = 5;\nasync function invoke({ context: incomingContext, instruction, \"p-disable-prompt-rewrite\": disablePromptRewrite, \"p-aspect-ratio\": aspectRatio, ...params }) {\n    incomingContext ??= [];\n    if (!instruction) {\n        instruction = toLLMContent(\"\");\n    }\n    if (!aspectRatio) {\n        aspectRatio = \"1:1\";\n    }\n    let imageContext = extractMediaData(incomingContext);\n    const textContext = extractTextData(incomingContext);\n    // Substitute params in instruction.\n    const toolManager = new ToolManager(new ArgumentNameGenerator());\n    const substituting = await new Template(instruction).substitute(params, async ({ path: url, instance }) => toolManager.addTool(url, instance));\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    const fanningOut = await new ListExpander(substituting, incomingContext).map(async (instruction, context) => {\n        // If there are tools in instruction, add an extra step of preparing\n        // information via tools.\n        if (toolManager.hasTools()) {\n            const gatheringInformation = await gatheringRequest(context, instruction, toolManager).invoke();\n            if (!ok(gatheringInformation))\n                return gatheringInformation;\n            context.push(...gatheringInformation.all);\n        }\n        const refImages = extractMediaData([instruction]);\n        const refText = instruction\n            ? toLLMContent(toTextConcat(extractTextData([instruction])))\n            : toLLMContent(\"\");\n        imageContext = imageContext.concat(refImages);\n        let retryCount = MAX_RETRIES;\n        while (retryCount--) {\n            if (imageContext.length > 0) {\n                return err(`References images are not supported with Imagen. For image editing or style transfer, try Gemini Image Generation.`);\n            }\n            else {\n                console.log(\"Step has text only, using generation API\");\n                let imagePrompt;\n                if (disablePromptRewrite) {\n                    imagePrompt = toLLMContent(toText(addUserTurn(refText, context)));\n                }\n                else {\n                    const generatingPrompt = await promptExpander(context, refText).invoke();\n                    if (!ok(generatingPrompt))\n                        return generatingPrompt;\n                    imagePrompt = generatingPrompt.last;\n                }\n                const iPrompt = toText(imagePrompt).trim();\n                console.log(\"PROMPT\", iPrompt);\n                const generatedImage = await callImageGen(iPrompt, aspectRatio);\n                if (!ok(generatedImage))\n                    return generatedImage;\n                return mergeContent(generatedImage, \"model\");\n            }\n        }\n        return gracefulExit(err(`Failed to generate an image after ${MAX_RETRIES} tries.`));\n    });\n    if (!ok(fanningOut))\n        return fanningOut;\n    return { context: fanningOut };\n}\nasync function describe({ inputs: { instruction } }) {\n    const template = new Template(instruction);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                instruction: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"Instruction\",\n                    description: \"Describe how to generate the image (content, style, etc). Use @ to reference params or outputs from other steps.\",\n                    default: defaultLLMContent(),\n                },\n                \"p-disable-prompt-rewrite\": {\n                    type: \"boolean\",\n                    title: \"Disable prompt expansion\",\n                    behavior: [\"config\"],\n                    description: \"By default, inputs and instructions will be automatically expanded into a high quality image prompt. Check to disable this re-writing behavior.\",\n                },\n                \"p-aspect-ratio\": {\n                    type: \"string\",\n                    behavior: [\"hint-text\", \"config\"],\n                    title: \"Aspect Ratio\",\n                    enum: ASPECT_RATIOS,\n                    description: \"The aspect ratio of the generated image\",\n                    default: \"1:1\",\n                },\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"hint-image\", \"main-port\"],\n                },\n            },\n        },\n        title: \"Make Image\",\n        metadata: {\n            icon: MAKE_IMAGE_ICON,\n            tags: [\"quick-access\", \"generative\"],\n            order: 2,\n        },\n    };\n}\n",
      "metadata": {
        "title": "Make Image",
        "source": {
          "code": "/**\n * @fileoverview Generates an image using supplied context (generation only).\n */\n\nimport invokeBoard from \"@invoke\";\n\nimport gemini, {\n  defaultSafetySettings,\n  type GeminiOutputs,\n  type GeminiInputs,\n  type Tool,\n} from \"./gemini\";\nimport { GeminiPrompt } from \"./gemini-prompt\";\nimport {\n  err,\n  ok,\n  toLLMContent,\n  defaultLLMContent,\n  toText,\n  addUserTurn,\n  toTextConcat,\n  joinContent,\n  llm,\n  extractMediaData,\n  extractTextData,\n  mergeContent,\n} from \"./utils\";\nimport { callImageGen, promptExpander } from \"./image-utils\";\nimport { Template } from \"./template\";\nimport { ToolManager } from \"./tool-manager\";\nimport { type Params, type DescriberResult } from \"./common\";\nimport { report } from \"./output\";\nimport { ArgumentNameGenerator } from \"./introducer\";\nimport { ListExpander } from \"./lists\";\n\nconst MAKE_IMAGE_ICON = \"generative-image\";\nconst ASPECT_RATIOS = [\"1:1\", \"9:16\", \"16:9\", \"4:3\", \"3:4\"];\n\ntype ImageGeneratorInputs = {\n  context: LLMContent[];\n  instruction: LLMContent;\n  \"p-disable-prompt-rewrite\": boolean;\n  \"p-aspect-ratio\": string;\n} & Params;\n\ntype ImageGeneratorOutputs = {\n  context: LLMContent[] | DescriberResult;\n};\n\nexport { invoke as default, describe };\n\nfunction gatheringRequest(\n  contents: LLMContent[] | undefined,\n  instruction: LLMContent,\n  toolManager: ToolManager\n): GeminiPrompt {\n  const promptText = llm`\nAnalyze the instruction below and rather than following it, determine what information needs to be gathered to \ngenerate an accurate prompt for a text-to-image model in the next turn:\n-- begin instruction --\n${instruction}\n-- end instruction --\n\nCall the tools to gather the necessary information that could be used to create an accurate prompt.`;\n  return new GeminiPrompt(\n    {\n      body: {\n        contents: addUserTurn(promptText.asContent(), contents),\n        tools: toolManager.list(),\n        systemInstruction: toLLMContent(`\nYou are a researcher whose specialty is to call tools whose output helps gather the necessary information\nto be used to create an accurate prompt for a text-to-image model.\n`),\n      },\n    },\n    toolManager\n  );\n}\n\nfunction gracefulExit(notOk: { $error: string }): Outcome<LLMContent> {\n  report({\n    actor: \"Make Image\",\n    category: \"Warning\",\n    name: \"Graceful exit\",\n    details: `I tried a couple of times, but the Gemini API failed to generate the image you requested with the following error:\n\n### ${notOk.$error}\n\nTo keep things moving, I will return a blank result. My apologies!`,\n    icon: MAKE_IMAGE_ICON,\n  });\n  return toLLMContent(\" \");\n}\n\nconst MAX_RETRIES = 5;\n\nasync function invoke({\n  context: incomingContext,\n  instruction,\n  \"p-disable-prompt-rewrite\": disablePromptRewrite,\n  \"p-aspect-ratio\": aspectRatio,\n  ...params\n}: ImageGeneratorInputs): Promise<Outcome<ImageGeneratorOutputs>> {\n  incomingContext ??= [];\n  if (!instruction) {\n    instruction = toLLMContent(\"\");\n  }\n  if (!aspectRatio) {\n    aspectRatio = \"1:1\";\n  }\n  let imageContext = extractMediaData(incomingContext);\n  const textContext = extractTextData(incomingContext);\n  // Substitute params in instruction.\n  const toolManager = new ToolManager(new ArgumentNameGenerator());\n  const substituting = await new Template(instruction).substitute(\n    params,\n    async ({ path: url, instance }) => toolManager.addTool(url, instance)\n  );\n  if (!ok(substituting)) {\n    return substituting;\n  }\n\n  const fanningOut = await new ListExpander(substituting, incomingContext).map(\n    async (instruction, context) => {\n      // If there are tools in instruction, add an extra step of preparing\n      // information via tools.\n      if (toolManager.hasTools()) {\n        const gatheringInformation = await gatheringRequest(\n          context,\n          instruction,\n          toolManager\n        ).invoke();\n        if (!ok(gatheringInformation)) return gatheringInformation;\n        context.push(...gatheringInformation.all);\n      }\n\n      const refImages = extractMediaData([instruction]);\n      const refText = instruction\n        ? toLLMContent(toTextConcat(extractTextData([instruction])))\n        : toLLMContent(\"\");\n      imageContext = imageContext.concat(refImages);\n\n      let retryCount = MAX_RETRIES;\n\n      while (retryCount--) {\n        if (imageContext.length > 0) {\n          return err(\n            `References images are not supported with Imagen. For image editing or style transfer, try Gemini Image Generation.`\n          );\n        } else {\n          console.log(\"Step has text only, using generation API\");\n          let imagePrompt: LLMContent;\n          if (disablePromptRewrite) {\n            imagePrompt = toLLMContent(toText(addUserTurn(refText, context)));\n          } else {\n            const generatingPrompt = await promptExpander(\n              context,\n              refText\n            ).invoke();\n            if (!ok(generatingPrompt)) return generatingPrompt;\n            imagePrompt = generatingPrompt.last;\n          }\n          const iPrompt = toText(imagePrompt).trim();\n          console.log(\"PROMPT\", iPrompt);\n          const generatedImage = await callImageGen(iPrompt, aspectRatio);\n          if (!ok(generatedImage)) return generatedImage;\n          return mergeContent(generatedImage, \"model\");\n        }\n      }\n      return gracefulExit(\n        err(`Failed to generate an image after ${MAX_RETRIES} tries.`)\n      );\n    }\n  );\n\n  if (!ok(fanningOut)) return fanningOut;\n  return { context: fanningOut };\n}\n\ntype DescribeInputs = {\n  inputs: {\n    instruction?: LLMContent;\n  };\n};\n\nasync function describe({ inputs: { instruction } }: DescribeInputs) {\n  const template = new Template(instruction);\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n        instruction: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n          title: \"Instruction\",\n          description:\n            \"Describe how to generate the image (content, style, etc). Use @ to reference params or outputs from other steps.\",\n          default: defaultLLMContent(),\n        },\n        \"p-disable-prompt-rewrite\": {\n          type: \"boolean\",\n          title: \"Disable prompt expansion\",\n          behavior: [\"config\"],\n          description:\n            \"By default, inputs and instructions will be automatically expanded into a high quality image prompt. Check to disable this re-writing behavior.\",\n        },\n        \"p-aspect-ratio\": {\n          type: \"string\",\n          behavior: [\"hint-text\", \"config\"],\n          title: \"Aspect Ratio\",\n          enum: ASPECT_RATIOS,\n          description: \"The aspect ratio of the generated image\",\n          default: \"1:1\",\n        },\n        ...template.schemas(),\n      },\n      behavior: [\"at-wireable\"],\n      ...template.requireds(),\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n          behavior: [\"hint-image\", \"main-port\"],\n        },\n      },\n    } satisfies Schema,\n    title: \"Make Image\",\n    metadata: {\n      icon: MAKE_IMAGE_ICON,\n      tags: [\"quick-access\", \"generative\"],\n      order: 2,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Generates an image using supplied context (generation only).",
        "runnable": true
      }
    },
    "structured-response": {
      "code": "import {} from \"./gemini\";\nimport { err, toLLMContent, endsWithRole } from \"./utils\";\nexport { StructuredResponse };\nclass StructuredResponse {\n    id;\n    chat;\n    prolog = \"\";\n    epilog = \"\";\n    body = \"\";\n    response = undefined;\n    constructor(id, chat) {\n        this.id = id;\n        this.chat = chat;\n    }\n    get separator() {\n        return `<sep-${this.id}>`;\n    }\n    addPrompt(c, prompt) {\n        const { parts: p } = prompt;\n        const context = [...c];\n        const parts = [\n            {\n                text: this.instructionText(),\n            },\n            ...p,\n        ];\n        if (endsWithRole(c, \"user\")) {\n            const last = context.pop();\n            context.push({\n                ...last,\n                parts: [...last.parts, ...parts],\n            });\n        }\n        else {\n            context.push({ parts, role: \"user\" });\n        }\n        return context;\n    }\n    instructionText() {\n        const chatOrConclude = this.chat\n            ? `Finally, ask the user to provide feedback on your output as a friendly assistant might.`\n            : `Finally, you briefly summarize what the work product was and how it fulfills the task.`;\n        return `\nConsider the conversation context so far and generate a response.\n\nYour response must consist of three parts, separated by the ${this.separator} tag.\n\n- Briefly describe the work product, why it fulfills the specified task,\nand any notes or comments you might have about it\n- Insert the ${this.separator} tag\n- Provide the work product only, without any additional conversation \nor comments about your output\n- Insert the ${this.separator} tag\n${chatOrConclude}\n`;\n    }\n    instruction() {\n        return toLLMContent(this.instructionText());\n    }\n    parseContent(content) {\n        const part = content.parts?.at(0);\n        if (!part || !(\"text\" in part)) {\n            return err(\"No text in part\");\n        }\n        this.response = content;\n        const structure = part.text.split(this.separator);\n        if (structure.length !== 3) {\n            console.warn(`The output must contain 3 parts, but ${structure.length} were found`);\n            if (structure.length > 1) {\n                // Assume that the prolog and body are here, but the epilog was gone.\n                // This can happen sometimes when we go past the output token window.\n                this.prolog = structure[0];\n                this.body = structure[1].trim();\n                this.epilog = this.chat ? \"Please provide feedback\" : \"\";\n                return;\n            }\n            return err(`No structure response delimiters were found. This is likely an invalid reponse.`);\n        }\n        this.prolog = structure[0];\n        this.body = structure[1].trim();\n        this.epilog = structure[2].trim();\n    }\n    bodyAsContent() {\n        return toLLMContent(this.body, \"model\");\n    }\n    parse(response) {\n        const r = response;\n        const content = r.candidates?.at(0)?.content;\n        if (!content) {\n            return err(\"No content\");\n        }\n        return this.parseContent(content);\n    }\n}\n",
      "metadata": {
        "title": "structured-response",
        "source": {
          "code": "import { type GeminiOutputs, type GeminiAPIOutputs } from \"./gemini\";\nimport { err, toLLMContent, endsWithRole } from \"./utils\";\n\nexport { StructuredResponse };\n\nclass StructuredResponse {\n  public prolog: string = \"\";\n  public epilog: string = \"\";\n  public body: string = \"\";\n  response: LLMContent | undefined = undefined;\n\n  constructor(\n    public readonly id: string,\n    public readonly chat: boolean\n  ) {}\n\n  get separator() {\n    return `<sep-${this.id}>`;\n  }\n\n  addPrompt(c: LLMContent[], prompt: LLMContent): LLMContent[] {\n    const { parts: p } = prompt;\n    const context: LLMContent[] = [...c];\n    const parts = [\n      {\n        text: this.instructionText(),\n      },\n      ...p,\n    ];\n    if (endsWithRole(c, \"user\")) {\n      const last = context.pop()!;\n      context.push({\n        ...last,\n        parts: [...last.parts, ...parts],\n      });\n    } else {\n      context.push({ parts, role: \"user\" });\n    }\n    return context;\n  }\n\n  instructionText(): string {\n    const chatOrConclude = this.chat\n      ? `Finally, ask the user to provide feedback on your output as a friendly assistant might.`\n      : `Finally, you briefly summarize what the work product was and how it fulfills the task.`;\n\n    return `\nConsider the conversation context so far and generate a response.\n\nYour response must consist of three parts, separated by the ${this.separator} tag.\n\n- Briefly describe the work product, why it fulfills the specified task,\nand any notes or comments you might have about it\n- Insert the ${this.separator} tag\n- Provide the work product only, without any additional conversation \nor comments about your output\n- Insert the ${this.separator} tag\n${chatOrConclude}\n`;\n  }\n\n  instruction(): LLMContent {\n    return toLLMContent(this.instructionText());\n  }\n\n  parseContent(content: LLMContent): Outcome<void> {\n    const part = content.parts?.at(0);\n    if (!part || !(\"text\" in part)) {\n      return err(\"No text in part\");\n    }\n    this.response = content;\n    const structure = part.text.split(this.separator);\n    if (structure.length !== 3) {\n      console.warn(\n        `The output must contain 3 parts, but ${structure.length} were found`\n      );\n      if (structure.length > 1) {\n        // Assume that the prolog and body are here, but the epilog was gone.\n        // This can happen sometimes when we go past the output token window.\n        this.prolog = structure[0];\n        this.body = structure[1].trim();\n        this.epilog = this.chat ? \"Please provide feedback\" : \"\";\n        return;\n      }\n      return err(\n        `No structure response delimiters were found. This is likely an invalid reponse.`\n      );\n    }\n    this.prolog = structure[0];\n    this.body = structure[1].trim();\n    this.epilog = structure[2].trim();\n  }\n\n  bodyAsContent(): LLMContent {\n    return toLLMContent(this.body, \"model\");\n  }\n\n  parse(response: GeminiOutputs): Outcome<void> {\n    const r = response as GeminiAPIOutputs;\n    const content = r.candidates?.at(0)?.content;\n    if (!content) {\n      return err(\"No content\");\n    }\n    return this.parseContent(content);\n  }\n}\n",
          "language": "typescript"
        },
        "description": "",
        "runnable": false
      }
    },
    "template": {
      "code": "/**\n * @fileoverview Handles templated content\n */\nexport { invoke as default, describe, Template };\nimport {} from \"./common\";\nimport { ok, err, isLLMContent, isLLMContentArray } from \"./utils\";\nimport { ConnectorManager } from \"./connector-manager\";\nimport readFile from \"@read\";\nfunction unique(params) {\n    return Array.from(new Set(params));\n}\nfunction isTool(param) {\n    return param.type === \"tool\" && !!param.path;\n}\nfunction isIn(param) {\n    return param.type === \"in\" && !!param.path;\n}\nfunction isAsset(param) {\n    return param.type === \"asset\" && !!param.path;\n}\nfunction isParameter(param) {\n    return param.type === \"param\" && !!param.path;\n}\nfunction isParamPart(param) {\n    return isTool(param) || isIn(param) || isAsset(param) || isParameter(param);\n}\nconst PARSING_REGEX = /{(?<json>{(?:.*?)})}/gim;\nclass Template {\n    template;\n    #parts;\n    #role;\n    constructor(template) {\n        this.template = template;\n        if (!template) {\n            this.#role = \"user\";\n            this.#parts = [];\n            return;\n        }\n        this.#parts = this.#splitToTemplateParts(template);\n        this.#role = template.role;\n    }\n    #mergeTextParts(parts) {\n        const merged = [];\n        for (let part of parts) {\n            if (\"text\" in part) {\n                const last = merged[merged.length - 1];\n                if (last && \"text\" in last) {\n                    last.text += part.text;\n                }\n                else {\n                    // We do a copy here otherwise the part is mutated, which\n                    // causes problems if the same part appears in the list twice.\n                    part = JSON.parse(JSON.stringify(part));\n                    merged.push(part);\n                }\n            }\n            else {\n                merged.push(part);\n            }\n        }\n        return merged;\n    }\n    /**\n     * Takes an LLM Content and splits it further into parts where\n     * each {{param}} substitution is a separate part.\n     */\n    #splitToTemplateParts(content) {\n        const parts = [];\n        for (const part of content.parts) {\n            if (!(\"text\" in part)) {\n                parts.push(part);\n                continue;\n            }\n            const matches = part.text.matchAll(PARSING_REGEX);\n            let start = 0;\n            for (const match of matches) {\n                const json = match.groups?.json;\n                const op = match.groups?.op;\n                const arg = match.groups?.arg;\n                const end = match.index;\n                if (end > start) {\n                    parts.push({ text: part.text.slice(start, end) });\n                }\n                if (json) {\n                    let maybeTemplatePart;\n                    try {\n                        maybeTemplatePart = JSON.parse(json);\n                        if (isParamPart(maybeTemplatePart)) {\n                            // Do some extra parsing for connector tools\n                            // if (isTool(maybeTemplatePart)) {\n                            //   const [path, connector] = maybeTemplatePart.path.split(\"|\");\n                            //   if (connector && connector.startsWith(\"connectors/\")) {\n                            //     maybeTemplatePart.path = path;\n                            //     maybeTemplatePart.instance = connector;\n                            //   }\n                            //   console.log(\"TOOL\", maybeTemplatePart);\n                            // }\n                            parts.push(maybeTemplatePart);\n                        }\n                        else {\n                            maybeTemplatePart = null;\n                        }\n                    }\n                    catch (e) {\n                        // do nothing\n                    }\n                    finally {\n                        if (!maybeTemplatePart) {\n                            parts.push({ text: part.text.slice(end, end + match[0].length) });\n                        }\n                    }\n                }\n                start = end + match[0].length;\n            }\n            if (start < part.text.length) {\n                parts.push({ text: part.text.slice(start) });\n            }\n        }\n        return parts;\n    }\n    #getLastNonMetadata(value) {\n        const content = value;\n        for (let i = content.length - 1; i >= 0; i--) {\n            if (content[i].role !== \"$metadata\") {\n                return content[i];\n            }\n        }\n        return null;\n    }\n    async #replaceParam(param, params, whenTool) {\n        if (isIn(param)) {\n            const { type, title: name, path } = param;\n            const paramName = `p-z-${path}`;\n            if (paramName in params) {\n                return params[paramName];\n            }\n            return name;\n        }\n        else if (isAsset(param)) {\n            if (ConnectorManager.isConnector(param)) {\n                return new ConnectorManager(param).materialize();\n            }\n            const path = `/assets/${param.path}`;\n            const reading = await readFile({ path });\n            if (!ok(reading)) {\n                return err(`Unable to find asset \"${param.title}\"`);\n            }\n            return reading.data;\n        }\n        else if (isTool(param)) {\n            const substituted = await whenTool(param);\n            if (!ok(substituted))\n                return substituted;\n            return substituted || param.title;\n        }\n        else if (isParameter(param)) {\n            const path = `/env/parameters/${param.path}`;\n            const reading = await readFile({ path });\n            if (!ok(reading)) {\n                console.error(`Unknown parameter \"${param.title}\"`);\n                return null;\n            }\n            return reading.data;\n        }\n        return null;\n    }\n    async substitute(params, whenTool) {\n        const replaced = [];\n        for (const part of this.#parts) {\n            if (\"type\" in part) {\n                const value = await this.#replaceParam(part, params, whenTool);\n                if (value === null) {\n                    // Ignore if null.\n                    continue;\n                }\n                else if (!ok(value)) {\n                    return value;\n                }\n                else if (typeof value === \"string\") {\n                    replaced.push({ text: value });\n                }\n                else if (isLLMContent(value)) {\n                    replaced.push(...value.parts);\n                }\n                else if (isLLMContentArray(value)) {\n                    const last = this.#getLastNonMetadata(value);\n                    if (last) {\n                        replaced.push(...last.parts);\n                    }\n                }\n                else {\n                    replaced.push({ text: JSON.stringify(value) });\n                }\n            }\n            else {\n                replaced.push(part);\n            }\n        }\n        const parts = this.#mergeTextParts(replaced);\n        return { parts, role: this.#role };\n    }\n    #toId(param) {\n        return `p-z-${param}`;\n    }\n    #toTitle(id) {\n        const spaced = id?.replace(/[_-]/g, \" \");\n        return ((spaced?.at(0)?.toUpperCase() ?? \"\") +\n            (spaced?.slice(1)?.toLowerCase() ?? \"\"));\n    }\n    #forEachParam(handler) {\n        for (const part of this.#parts) {\n            if (\"type\" in part) {\n                handler(part);\n            }\n        }\n    }\n    requireds() {\n        const required = [];\n        let hasValues = false;\n        this.#forEachParam((param) => {\n            if (!isIn(param))\n                return;\n            hasValues = true;\n            required.push(this.#toId(param.title));\n        });\n        return hasValues ? { required } : {};\n    }\n    schemas() {\n        const result = [];\n        this.#forEachParam((param) => {\n            const name = param.title;\n            const id = this.#toId(param.path);\n            if (!isIn(param))\n                return;\n            result.push([\n                id,\n                {\n                    title: this.#toTitle(name),\n                    description: `The value to substitute for the parameter \"${name}\"`,\n                    type: \"object\",\n                    behavior: [\"llm-content\"],\n                },\n            ]);\n        });\n        return Object.fromEntries(result);\n    }\n    static part(part) {\n        return `{${JSON.stringify(part)}}`;\n    }\n    /**\n     * This is roughly the same method as `schemas`, but for connectors.\n     * TODO: UNIFY\n     */\n    async schemaProperties() {\n        let result = {};\n        for (const part of this.#parts) {\n            if (!(\"type\" in part))\n                continue;\n            if (!isAsset(part))\n                continue;\n            if (!ConnectorManager.isConnector(part))\n                continue;\n            const props = await new ConnectorManager(part).schemaProperties();\n            result = { ...result, ...props };\n        }\n        return result;\n    }\n    async save(context, options) {\n        if (!context)\n            return;\n        const errors = [];\n        for (const part of this.#parts) {\n            if (!(\"type\" in part))\n                continue;\n            if (!isAsset(part))\n                continue;\n            if (!ConnectorManager.isConnector(part))\n                continue;\n            const saving = await new ConnectorManager(part).save(context, options || {});\n            if (!ok(saving)) {\n                errors.push(saving.$error);\n            }\n        }\n        if (errors.length > 0) {\n            return err(errors.join(\"\\n\"));\n        }\n    }\n}\n/**\n * API for test harness\n */\nfunction fromTestParams(params) {\n    return Object.fromEntries(Object.entries(params).map(([key, value]) => {\n        return [`p-z-${key}`, value];\n    }));\n}\n/**\n * Only used for testing.\n */\nasync function invoke({ inputs: { content, params }, }) {\n    const template = new Template(content);\n    const result = await template.substitute(fromTestParams(params), async (params) => {\n        return params.path;\n    });\n    if (!ok(result)) {\n        return result;\n    }\n    return { outputs: result };\n}\n/**\n * Only used for testing.\n */\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: { inputs: { type: \"object\", title: \"Test inputs\" } },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: { outputs: { type: \"object\", title: \"Test outputs\" } },\n        },\n    };\n}\n",
      "metadata": {
        "title": "template",
        "source": {
          "code": "/**\n * @fileoverview Handles templated content\n */\n\nexport { invoke as default, describe, Template };\n\nimport { type Params } from \"./common\";\nimport { ok, err, isLLMContent, isLLMContentArray } from \"./utils\";\nimport { ConnectorManager } from \"./connector-manager\";\nimport readFile from \"@read\";\n\ntype LLMContentWithMetadata = LLMContent & {\n  $metadata: unknown;\n};\n\nexport type Requireds = {\n  required?: Schema[\"required\"];\n};\n\ntype Location = {\n  part: LLMContent[\"parts\"][0];\n  parts: LLMContent[\"parts\"];\n};\n\nexport type InParamPart = {\n  type: \"in\";\n  path: string;\n  title: string;\n};\n\nexport type ToolParamPart = {\n  type: \"tool\";\n  path: string;\n  title: string;\n  instance?: string;\n};\n\nexport type AssetParamPart = {\n  type: \"asset\";\n  path: string;\n  title: string;\n};\n\nexport type ParameterParamPart = {\n  type: \"param\";\n  path: string;\n  title: string;\n};\n\nexport type ParamPart =\n  | InParamPart\n  | ToolParamPart\n  | AssetParamPart\n  | ParameterParamPart;\n\nexport type TemplatePart = DataPart | ParamPart;\n\nexport type ToolCallback = (param: ToolParamPart) => Promise<Outcome<string>>;\n\nfunction unique<T>(params: T[]): T[] {\n  return Array.from(new Set(params));\n}\n\nfunction isTool(param: ParamPart): param is ToolParamPart {\n  return param.type === \"tool\" && !!param.path;\n}\n\nfunction isIn(param: ParamPart): param is InParamPart {\n  return param.type === \"in\" && !!param.path;\n}\n\nfunction isAsset(param: ParamPart): param is AssetParamPart {\n  return param.type === \"asset\" && !!param.path;\n}\n\nfunction isParameter(param: ParamPart): param is ParameterParamPart {\n  return param.type === \"param\" && !!param.path;\n}\n\nfunction isParamPart(param: ParamPart): param is ParamPart {\n  return isTool(param) || isIn(param) || isAsset(param) || isParameter(param);\n}\n\nconst PARSING_REGEX = /{(?<json>{(?:.*?)})}/gim;\n\nclass Template {\n  #parts: TemplatePart[];\n  #role: LLMContent[\"role\"];\n\n  constructor(public readonly template: LLMContent | undefined) {\n    if (!template) {\n      this.#role = \"user\";\n      this.#parts = [];\n      return;\n    }\n    this.#parts = this.#splitToTemplateParts(template);\n    this.#role = template.role;\n  }\n\n  #mergeTextParts(parts: TemplatePart[]) {\n    const merged = [];\n    for (let part of parts) {\n      if (\"text\" in part) {\n        const last = merged[merged.length - 1];\n        if (last && \"text\" in last) {\n          last.text += part.text;\n        } else {\n          // We do a copy here otherwise the part is mutated, which\n          // causes problems if the same part appears in the list twice.\n          part = JSON.parse(JSON.stringify(part));\n          merged.push(part);\n        }\n      } else {\n        merged.push(part);\n      }\n    }\n    return merged as DataPart[];\n  }\n\n  /**\n   * Takes an LLM Content and splits it further into parts where\n   * each {{param}} substitution is a separate part.\n   */\n  #splitToTemplateParts(content: LLMContent): TemplatePart[] {\n    const parts: TemplatePart[] = [];\n    for (const part of content.parts) {\n      if (!(\"text\" in part)) {\n        parts.push(part);\n        continue;\n      }\n      const matches = part.text.matchAll(PARSING_REGEX);\n      let start = 0;\n      for (const match of matches) {\n        const json = match.groups?.json;\n        const op = match.groups?.op;\n        const arg = match.groups?.arg;\n        const end = match.index;\n        if (end > start) {\n          parts.push({ text: part.text.slice(start, end) });\n        }\n        if (json) {\n          let maybeTemplatePart;\n          try {\n            maybeTemplatePart = JSON.parse(json);\n            if (isParamPart(maybeTemplatePart)) {\n              // Do some extra parsing for connector tools\n              // if (isTool(maybeTemplatePart)) {\n              //   const [path, connector] = maybeTemplatePart.path.split(\"|\");\n              //   if (connector && connector.startsWith(\"connectors/\")) {\n              //     maybeTemplatePart.path = path;\n              //     maybeTemplatePart.instance = connector;\n              //   }\n              //   console.log(\"TOOL\", maybeTemplatePart);\n              // }\n              parts.push(maybeTemplatePart);\n            } else {\n              maybeTemplatePart = null;\n            }\n          } catch (e) {\n            // do nothing\n          } finally {\n            if (!maybeTemplatePart) {\n              parts.push({ text: part.text.slice(end, end + match[0].length) });\n            }\n          }\n        }\n        start = end + match[0].length;\n      }\n      if (start < part.text.length) {\n        parts.push({ text: part.text.slice(start) });\n      }\n    }\n    return parts;\n  }\n\n  #getLastNonMetadata(value: LLMContent[]): LLMContent | null {\n    const content = value as LLMContentWithMetadata[];\n    for (let i = content.length - 1; i >= 0; i--) {\n      if (content[i].role !== \"$metadata\") {\n        return content[i] as LLMContent;\n      }\n    }\n    return null;\n  }\n\n  async #replaceParam(\n    param: ParamPart,\n    params: Params,\n    whenTool: ToolCallback\n  ): Promise<Outcome<unknown>> {\n    if (isIn(param)) {\n      const { type, title: name, path } = param;\n      const paramName: `p-z-${string}` = `p-z-${path}`;\n      if (paramName in params) {\n        return params[paramName];\n      }\n      return name;\n    } else if (isAsset(param)) {\n      if (ConnectorManager.isConnector(param)) {\n        return new ConnectorManager(param).materialize();\n      }\n      const path: FileSystemPath = `/assets/${param.path}`;\n      const reading = await readFile({ path });\n      if (!ok(reading)) {\n        return err(`Unable to find asset \"${param.title}\"`);\n      }\n      return reading.data;\n    } else if (isTool(param)) {\n      const substituted = await whenTool(param);\n      if (!ok(substituted)) return substituted;\n      return substituted || param.title;\n    } else if (isParameter(param)) {\n      const path: FileSystemPath = `/env/parameters/${param.path}`;\n      const reading = await readFile({ path });\n      if (!ok(reading)) {\n        console.error(`Unknown parameter \"${param.title}\"`);\n        return null;\n      }\n      return reading.data;\n    }\n    return null;\n  }\n\n  async substitute(\n    params: Params,\n    whenTool: ToolCallback\n  ): Promise<Outcome<LLMContent>> {\n    const replaced: DataPart[] = [];\n    for (const part of this.#parts) {\n      if (\"type\" in part) {\n        const value = await this.#replaceParam(part, params, whenTool);\n        if (value === null) {\n          // Ignore if null.\n          continue;\n        } else if (!ok(value)) {\n          return value;\n        } else if (typeof value === \"string\") {\n          replaced.push({ text: value });\n        } else if (isLLMContent(value)) {\n          replaced.push(...value.parts);\n        } else if (isLLMContentArray(value)) {\n          const last = this.#getLastNonMetadata(value);\n          if (last) {\n            replaced.push(...last.parts);\n          }\n        } else {\n          replaced.push({ text: JSON.stringify(value) });\n        }\n      } else {\n        replaced.push(part);\n      }\n    }\n    const parts = this.#mergeTextParts(replaced);\n    return { parts, role: this.#role };\n  }\n\n  #toId(param: string) {\n    return `p-z-${param}`;\n  }\n\n  #toTitle(id: string) {\n    const spaced = id?.replace(/[_-]/g, \" \");\n    return (\n      (spaced?.at(0)?.toUpperCase() ?? \"\") +\n      (spaced?.slice(1)?.toLowerCase() ?? \"\")\n    );\n  }\n\n  #forEachParam(handler: (param: ParamPart) => void) {\n    for (const part of this.#parts) {\n      if (\"type\" in part) {\n        handler(part);\n      }\n    }\n  }\n\n  requireds(): Requireds {\n    const required: string[] = [];\n    let hasValues = false;\n    this.#forEachParam((param) => {\n      if (!isIn(param)) return;\n      hasValues = true;\n      required.push(this.#toId(param.title!));\n    });\n    return hasValues ? { required } : {};\n  }\n\n  schemas(): Record<string, Schema> {\n    const result: [string, Schema][] = [];\n    this.#forEachParam((param) => {\n      const name = param.title!;\n      const id = this.#toId(param.path!);\n      if (!isIn(param)) return;\n      result.push([\n        id,\n        {\n          title: this.#toTitle(name),\n          description: `The value to substitute for the parameter \"${name}\"`,\n          type: \"object\",\n          behavior: [\"llm-content\"],\n        },\n      ]);\n    });\n    return Object.fromEntries(result);\n  }\n\n  static part(part: ParamPart) {\n    return `{${JSON.stringify(part)}}`;\n  }\n\n  /**\n   * This is roughly the same method as `schemas`, but for connectors.\n   * TODO: UNIFY\n   */\n  async schemaProperties(): Promise<Record<string, Schema>> {\n    let result: Record<string, Schema> = {};\n    for (const part of this.#parts) {\n      if (!(\"type\" in part)) continue;\n      if (!isAsset(part)) continue;\n      if (!ConnectorManager.isConnector(part)) continue;\n      const props = await new ConnectorManager(part).schemaProperties();\n      result = { ...result, ...props };\n    }\n    return result;\n  }\n\n  async save(\n    context?: LLMContent[],\n    options?: Record<string, unknown>\n  ): Promise<Outcome<void>> {\n    if (!context) return;\n\n    const errors: string[] = [];\n    for (const part of this.#parts) {\n      if (!(\"type\" in part)) continue;\n      if (!isAsset(part)) continue;\n      if (!ConnectorManager.isConnector(part)) continue;\n      const saving = await new ConnectorManager(part).save(\n        context,\n        options || {}\n      );\n      if (!ok(saving)) {\n        errors.push(saving.$error);\n      }\n    }\n    if (errors.length > 0) {\n      return err(errors.join(\"\\n\"));\n    }\n  }\n}\n\n/**\n * API for test harness\n */\n\nfunction fromTestParams(params: Record<string, string>): Params {\n  return Object.fromEntries(\n    Object.entries(params).map(([key, value]) => {\n      return [`p-z-${key}`, value];\n    })\n  );\n}\n\ntype TestInputs = {\n  inputs: { content: LLMContent; params: Record<string, string> };\n};\n\ntype TestOutputs = {\n  outputs: LLMContent;\n};\n\n/**\n * Only used for testing.\n */\nasync function invoke({\n  inputs: { content, params },\n}: TestInputs): Promise<Outcome<TestOutputs>> {\n  const template = new Template(content);\n  const result = await template.substitute(\n    fromTestParams(params),\n    async (params) => {\n      return params.path;\n    }\n  );\n  if (!ok(result)) {\n    return result;\n  }\n  return { outputs: result };\n}\n\n/**\n * Only used for testing.\n */\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: { inputs: { type: \"object\", title: \"Test inputs\" } },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: { outputs: { type: \"object\", title: \"Test outputs\" } },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Handles templated content",
        "runnable": true
      }
    },
    "audio-generator": {
      "code": "/**\n * @fileoverview Generates audio output using supplied context.\n */\nimport gemini, { defaultSafetySettings, } from \"./gemini\";\nimport { err, ok, llm, toLLMContent, toText } from \"./utils\";\nimport {} from \"./common\";\nexport { invoke as default, describe };\nasync function invoke({ context, }) {\n    // 1) Get last LLMContent from input.\n    const prompt = context && Array.isArray(context) && context.length > 0\n        ? context.at(-1)\n        : undefined;\n    if (!prompt) {\n        return err(\"Must supply context as input\");\n    }\n    prompt.role = \"user\";\n    // 2) Call Gemini to generate audio.\n    const result = await gemini({\n        model: \"gemini-2.0-flash-exp\",\n        body: {\n            contents: [prompt],\n            generationConfig: {\n                responseModalities: [\"AUDIO\"],\n            },\n            safetySettings: defaultSafetySettings(),\n        },\n    });\n    if (!ok(result)) {\n        return result;\n    }\n    if (\"context\" in result) {\n        return err(\"Invalid output from Gemini -- must be candidates\");\n    }\n    const content = result.candidates.at(0)?.content;\n    if (!content) {\n        return err(\"No content\");\n    }\n    return { context: [content] };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n            },\n            additionalProperties: false,\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"hint-audio\"],\n                },\n            },\n            additionalProperties: false,\n        },\n        title: \"Make Audio [Deprecated, Use Make Speech]\",\n        metadata: {\n            icon: \"generative-audio\",\n            tags: [\"quick-access\", \"generative\", \"experimental\"],\n            order: 3,\n        },\n    };\n}\n",
      "metadata": {
        "title": "Audio Generator",
        "source": {
          "code": "/**\n * @fileoverview Generates audio output using supplied context.\n */\n\nimport gemini, {\n  defaultSafetySettings,\n  type GeminiOutputs,\n  type GeminiInputs,\n} from \"./gemini\";\nimport { err, ok, llm, toLLMContent, toText } from \"./utils\";\nimport { type DescriberResult } from \"./common\";\n\ntype AudioGeneratorInputs = {\n  context: LLMContent[];\n};\n\ntype AudioGeneratorOutputs = {\n  context: LLMContent[] | DescriberResult;\n};\n\nexport { invoke as default, describe };\n\nasync function invoke({\n  context,\n}: AudioGeneratorInputs): Promise<Outcome<AudioGeneratorOutputs>> {\n  // 1) Get last LLMContent from input.\n  const prompt =\n    context && Array.isArray(context) && context.length > 0\n      ? context.at(-1)!\n      : undefined;\n  if (!prompt) {\n    return err(\"Must supply context as input\");\n  }\n  prompt.role = \"user\";\n\n  // 2) Call Gemini to generate audio.\n  const result = await gemini({\n    model: \"gemini-2.0-flash-exp\",\n    body: {\n      contents: [prompt],\n      generationConfig: {\n        responseModalities: [\"AUDIO\"],\n      },\n      safetySettings: defaultSafetySettings(),\n    },\n  });\n  if (!ok(result)) {\n    return result;\n  }\n  if (\"context\" in result) {\n    return err(\"Invalid output from Gemini -- must be candidates\");\n  }\n\n  const content = result.candidates.at(0)?.content;\n  if (!content) {\n    return err(\"No content\");\n  }\n\n  return { context: [content] };\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n      },\n      additionalProperties: false,\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n          behavior: [\"hint-audio\"],\n        },\n      },\n      additionalProperties: false,\n    } satisfies Schema,\n    title: \"Make Audio [Deprecated, Use Make Speech]\",\n    metadata: {\n      icon: \"generative-audio\",\n      tags: [\"quick-access\", \"generative\", \"experimental\"],\n      order: 3,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Generates audio output using supplied context.",
        "runnable": true
      }
    },
    "text-entry": {
      "code": "/**\n * @fileoverview Allows asking user for input that could be then used in next steps.\n */\nimport { ok, defaultLLMContent, toText, llm } from \"./utils\";\nimport {} from \"./common\";\nimport { Template } from \"./template\";\nimport { report } from \"./output\";\nexport { invoke as default, describe };\nconst MODALITY = [\n    \"Any\",\n    \"Audio\",\n    \"Image\",\n    \"Video\",\n    \"Upload File\",\n];\nfunction toInput(title, modality) {\n    const toInput = {\n        type: \"object\",\n        properties: {\n            request: {\n                type: \"object\",\n                title,\n                behavior: [\"transient\", \"llm-content\"],\n                examples: [defaultLLMContent()],\n                format: computeIcon(modality),\n            },\n        },\n    };\n    return toInput;\n}\nconst ICONS = {\n    Any: \"multimodal\",\n    Audio: \"audio\",\n    Video: \"video\",\n    Image: \"image\",\n    \"Upload File\": \"file\",\n};\nconst HINTS = {\n    Any: \"hint-multimodal\",\n    Audio: \"hint-audio\",\n    Video: \"hint-image\",\n    Image: \"hint-image\",\n    \"Upload File\": \"hint-text\",\n};\nfunction computeIcon(modality) {\n    return (modality && ICONS[modality]) || \"multimodal\";\n}\nfunction computeHint(modality) {\n    return (modality && HINTS[modality]) || \"hint-multimodal\";\n}\nasync function invoke({ description, \"p-modality\": modality, ...params }) {\n    const template = new Template(description);\n    let details = llm `Please provide input`.asContent();\n    if (description) {\n        const substituting = await template.substitute(params, async () => \"\");\n        if (!ok(substituting)) {\n            return substituting;\n        }\n        details = substituting;\n    }\n    await report({\n        actor: \"Ask User\",\n        category: \"Requesting Input\",\n        name: \"\",\n        details,\n        icon: \"input\",\n    });\n    const title = toText(details);\n    return { context: \"nothing\", toInput: toInput(title, modality) };\n}\nasync function describe({ inputs: { description, [\"p-modality\"]: modality }, }) {\n    const icon = computeIcon(modality);\n    const template = new Template(description);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                description: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"What to ask of user\",\n                    description: \"Provide a request prompt that will be shown to the user.\",\n                },\n                \"p-modality\": {\n                    type: \"string\",\n                    enum: MODALITY,\n                    behavior: [\"config\", \"hint-preview\"],\n                    icon,\n                    title: \"Input type\",\n                    description: \"Set the type of input the user can provide\",\n                },\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            additionalProperties: true,\n            ...template.requireds(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"main-port\", computeHint(modality)],\n                },\n            },\n            additionalProperties: false,\n        },\n        title: \"Ask User\",\n        metadata: {\n            icon: \"ask-user\",\n            tags: [\"quick-access\", \"core\", \"input\"],\n            order: 1,\n        },\n    };\n}\n",
      "metadata": {
        "title": "Ask User",
        "icon": "text",
        "source": {
          "code": "/**\n * @fileoverview Allows asking user for input that could be then used in next steps.\n */\nimport { ok, defaultLLMContent, toText, llm } from \"./utils\";\nimport { type Params } from \"./common\";\nimport { Template } from \"./template\";\nimport { report } from \"./output\";\n\nexport { invoke as default, describe };\n\nconst MODALITY: readonly string[] = [\n  \"Any\",\n  \"Audio\",\n  \"Image\",\n  \"Video\",\n  \"Upload File\",\n] as const;\n\ntype Modality = (typeof MODALITY)[number];\n\ntype TextInputs = {\n  description?: LLMContent;\n  \"p-modality\"?: Modality;\n} & Params;\n\ntype TextOutputs =\n  | {\n      toInput: Schema;\n      context: \"nothing\";\n    }\n  | {\n      toMain: string;\n      context: LLMContent;\n    };\n\nfunction toInput(title: string, modality: Modality | undefined) {\n  const toInput: Schema = {\n    type: \"object\",\n    properties: {\n      request: {\n        type: \"object\",\n        title,\n        behavior: [\"transient\", \"llm-content\"],\n        examples: [defaultLLMContent()],\n        format: computeIcon(modality),\n      },\n    },\n  };\n  return toInput;\n}\n\nconst ICONS: Record<Modality, string> = {\n  Any: \"multimodal\",\n  Audio: \"audio\",\n  Video: \"video\",\n  Image: \"image\",\n  \"Upload File\": \"file\",\n};\n\nconst HINTS: Record<Modality, BehaviorSchema> = {\n  Any: \"hint-multimodal\",\n  Audio: \"hint-audio\",\n  Video: \"hint-image\",\n  Image: \"hint-image\",\n  \"Upload File\": \"hint-text\",\n};\n\nfunction computeIcon(modality?: Modality): string {\n  return (modality && ICONS[modality]) || \"multimodal\";\n}\n\nfunction computeHint(modality?: Modality): BehaviorSchema {\n  return (modality && HINTS[modality]) || \"hint-multimodal\";\n}\n\nasync function invoke({\n  description,\n  \"p-modality\": modality,\n  ...params\n}: TextInputs): Promise<Outcome<TextOutputs>> {\n  const template = new Template(description);\n  let details = llm`Please provide input`.asContent();\n  if (description) {\n    const substituting = await template.substitute(params, async () => \"\");\n    if (!ok(substituting)) {\n      return substituting;\n    }\n    details = substituting;\n  }\n  await report({\n    actor: \"Ask User\",\n    category: \"Requesting Input\",\n    name: \"\",\n    details,\n    icon: \"input\",\n  });\n  const title = toText(details);\n  return { context: \"nothing\", toInput: toInput(title, modality) };\n}\n\ntype DescribeInputs = {\n  inputs: TextInputs;\n};\n\nasync function describe({\n  inputs: { description, [\"p-modality\"]: modality },\n}: DescribeInputs) {\n  const icon = computeIcon(modality);\n  const template = new Template(description);\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        description: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n          title: \"What to ask of user\",\n          description:\n            \"Provide a request prompt that will be shown to the user.\",\n        },\n        \"p-modality\": {\n          type: \"string\",\n          enum: MODALITY as string[],\n          behavior: [\"config\", \"hint-preview\"],\n          icon,\n          title: \"Input type\",\n          description: \"Set the type of input the user can provide\",\n        },\n        ...template.schemas(),\n      },\n      behavior: [\"at-wireable\"],\n      additionalProperties: true,\n      ...template.requireds(),\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n          behavior: [\"main-port\", computeHint(modality)],\n        },\n      },\n      additionalProperties: false,\n    } satisfies Schema,\n    title: \"Ask User\",\n    metadata: {\n      icon: \"ask-user\",\n      tags: [\"quick-access\", \"core\", \"input\"],\n      order: 1,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Allows asking user for input that could be then used in next steps.",
        "runnable": true
      }
    },
    "text-main": {
      "code": "/**\n * @fileoverview Calls Gemini agent loop.\n */\nimport { err } from \"./utils\";\nexport { invoke as default, describe };\nasync function invoke({ context, request, }) {\n    if (context == \"nothing\") {\n        if (!request) {\n            return err(`No text supplied.`);\n        }\n        return { context: [request] };\n    }\n    return { context: [context] };\n}\nasync function describe() {\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"object\",\n                    title: \"Context in\",\n                },\n                request: {\n                    type: \"object\",\n                    title: \"Data From Input\",\n                },\n            },\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                },\n            },\n        },\n    };\n}\n",
      "metadata": {
        "title": "text-main",
        "source": {
          "code": "/**\n * @fileoverview Calls Gemini agent loop.\n */\n\nimport { err } from \"./utils\";\n\nexport { invoke as default, describe };\n\ntype TextMainInputs = {\n  context: LLMContent | \"nothing\";\n  request?: LLMContent;\n};\n\ntype TextMainOutputs = {\n  context: LLMContent[];\n};\n\nasync function invoke({\n  context,\n  request,\n}: TextMainInputs): Promise<Outcome<TextMainOutputs>> {\n  if (context == \"nothing\") {\n    if (!request) {\n      return err(`No text supplied.`);\n    }\n    return { context: [request] };\n  }\n  return { context: [context] };\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"object\",\n          title: \"Context in\",\n        },\n        request: {\n          type: \"object\",\n          title: \"Data From Input\",\n        },\n      },\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n        },\n      },\n    } satisfies Schema,\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Calls Gemini agent loop.",
        "runnable": true
      }
    },
    "combine-outputs": {
      "code": "/**\n * @fileoverview Combines multiple outputs into one.\n */\nimport { Template } from \"./template\";\nimport { ok } from \"./utils\";\nimport { fanOutContext, flattenContext } from \"./lists\";\nexport { invoke as default, describe };\nasync function invoke({ text, \"z-flatten-list\": flatten, ...params }) {\n    const template = new Template(text);\n    const substituting = await template.substitute(params, async () => \"\");\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    let context = await fanOutContext(substituting, undefined, async (instruction) => instruction);\n    if (!ok(context))\n        return context;\n    if (flatten) {\n        context = flattenContext(context);\n    }\n    return { context };\n}\nasync function describe({ inputs: { text } }) {\n    const template = new Template(text);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                text: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"hint-preview\", \"config\"],\n                    title: \"Text\",\n                    description: \"Type the @ character to select the outputs to combine\",\n                },\n                \"z-flatten-list\": {\n                    type: \"boolean\",\n                    behavior: [\"hint-preview\", \"config\"],\n                    icon: \"summarize\",\n                    title: \"Flatten the list\",\n                    description: \"When checked, the step will flatten the incoming list into a single outputs\",\n                },\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                    },\n                    title: \"Context out\",\n                    behavior: [\"main-port\", \"hint-multimodal\"],\n                },\n            },\n        },\n        title: \"Combine Outputs\",\n        metadata: {\n            icon: \"combine-outputs\",\n            tags: [\"quick-access\", \"core\", \"experimental\"],\n            order: 100,\n        },\n    };\n}\n",
      "metadata": {
        "title": "Combine Outputs",
        "source": {
          "code": "/**\n * @fileoverview Combines multiple outputs into one.\n */\n\nimport { Template } from \"./template\";\nimport { ok } from \"./utils\";\nimport { fanOutContext, flattenContext } from \"./lists\";\n\nexport { invoke as default, describe };\n\ntype InvokeInputs = {\n  text?: LLMContent;\n  \"z-flatten-list\": boolean;\n};\n\ntype Outputs = {\n  context: LLMContent[];\n};\n\ntype DescribeInputs = {\n  inputs: {\n    text?: LLMContent;\n  };\n};\n\nasync function invoke({\n  text,\n  \"z-flatten-list\": flatten,\n  ...params\n}: InvokeInputs): Promise<Outcome<Outputs>> {\n  const template = new Template(text);\n  const substituting = await template.substitute(params, async () => \"\");\n  if (!ok(substituting)) {\n    return substituting;\n  }\n  let context = await fanOutContext(\n    substituting,\n    undefined,\n    async (instruction) => instruction\n  );\n  if (!ok(context)) return context;\n  if (flatten) {\n    context = flattenContext(context);\n  }\n  return { context };\n}\n\nasync function describe({ inputs: { text } }: DescribeInputs) {\n  const template = new Template(text);\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        text: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"hint-preview\", \"config\"],\n          title: \"Text\",\n          description: \"Type the @ character to select the outputs to combine\",\n        },\n        \"z-flatten-list\": {\n          type: \"boolean\",\n          behavior: [\"hint-preview\", \"config\"],\n          icon: \"summarize\",\n          title: \"Flatten the list\",\n          description:\n            \"When checked, the step will flatten the incoming list into a single outputs\",\n        },\n        ...template.schemas(),\n      },\n      behavior: [\"at-wireable\"],\n      ...template.requireds(),\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: {\n            type: \"object\",\n            behavior: [\"llm-content\"],\n          },\n          title: \"Context out\",\n          behavior: [\"main-port\", \"hint-multimodal\"],\n        },\n      },\n    } satisfies Schema,\n    title: \"Combine Outputs\",\n    metadata: {\n      icon: \"combine-outputs\",\n      tags: [\"quick-access\", \"core\", \"experimental\"],\n      order: 100,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Combines multiple outputs into one.",
        "runnable": true
      }
    },
    "make-code": {
      "code": "/**\n * @fileoverview Generates code using supplied context.\n */\nimport invokeBoard from \"@invoke\";\nimport gemini, { defaultSafetySettings, } from \"./gemini\";\nimport { err, ok, toLLMContent, toText, addUserTurn } from \"./utils\";\nimport { Template } from \"./template\";\nimport { ToolManager } from \"./tool-manager\";\nimport {} from \"./common\";\nimport { report } from \"./output\";\nimport { ArgumentNameGenerator } from \"./introducer\";\nconst MAKE_CODE_ICON = \"generative-code\";\nexport { invoke as default, describe };\nfunction gatheringRequest(contents, instruction, language, toolManager) {\n    const promptText = `\nAnalyze the instruction below and rather than following it, determine what information needs to be gathered to \ngenerate an accurate prompt for a text-to-${language} model in the next turn:\n-- begin instruction --\n${toText(instruction)}\n-- end instruction --\n\nCall the tools to gather the necessary information that could be used to create an accurate prompt.`;\n    return new GeminiPrompt({\n        model: \"gemini-1.5-flash-latest\",\n        body: {\n            contents: addUserTurn(promptText, contents),\n            tools: toolManager.list(),\n            systemInstruction: toLLMContent(`\nYou are a researcher whose specialty is to call tools whose output helps gather the necessary information\nto be used to create an accurate prompt for a text-to-${language} model.\n`),\n        },\n    }, toolManager);\n}\nfunction promptRequest(contents, instruction, language) {\n    const context = contents?.length\n        ? \"using conversation context and these additional\"\n        : \"with these\";\n    const promptText = `Generate a single text-to-${language} prompt ${context} instructions:\n${toText(instruction)}\n\nTypical output format:\n\n## Setting/background\n\nDetailed description of everything that is understood about the ${language} code that is being requested.\n\n## Primary focus\n\nDetailed description of the primary functionality or the main focal point of the JavaScript code.\n\n## Style\n\nDetailed description of the style and approach of the code (defensive, TDD, creative, etc.). The output should\nalways an invariably be ${language === \"JavaScript\" ? \"EcmaScript JavaScript Modules\" : language} and be fully functional \nwithout any placeholders. \n\nIf you are dealing with JavaScript you may use imports if and only if the instruction indicates, otherwise you must\ncreate the functionality as a standalone piece of EcmaScript JavaScript.\n\nYou output will be fed directly into the text-to-${language} model, so it must be prompt only, no additional chit-chat\n`;\n    return new GeminiPrompt({\n        model: \"gemini-1.5-flash-latest\",\n        body: {\n            contents: addUserTurn(promptText, contents),\n            systemInstruction: toLLMContent(`\nYou are a world-class ${language} developer whose specialty is to write prompts for text-to-${language}Â models that \nalways generate valid outputs.\n\nThe prompt must describe every aspect of the functionality in great detail and describe the problem being solved \nin terms of data structures, algorithms, and style. You must use the instruction to fully understand and replicate\nany reference implementations you've been given and you must not augment or deviate from that style. You should\nensure that the prompt includes enough information to fully replicate that style with examples and maximal clarity.\n\nIf the code pertains to user interface work, you must also maximize the accessibility of the code generated with\nappropriate titles and labels for buttons, controls, inputs, etc, and they should never be empty.\n\nBe sure to export all relevant symbols so that the code can be used outside of the EcmaScript Module. Always do\nthis as named symbols rather than using default exports.\n\nIf writing JavaScript, and where a variable is private, use private fields (#field) rather than an underscore at the start.\n`),\n        },\n    });\n}\nfunction codeRequest(prompt, language) {\n    prompt.role = \"user\";\n    prompt.parts.unshift({\n        text: `Generate ${language} code based on this prompt. Output code only, no chit-chat`,\n    });\n    return new GeminiPrompt({\n        model: \"gemini-2.0-flash-exp\",\n        body: {\n            contents: [prompt],\n            generationConfig: {\n                responseModalities: [\"TEXT\"],\n            },\n            safetySettings: defaultSafetySettings(),\n        },\n    });\n}\nclass GeminiPrompt {\n    inputs;\n    toolManager;\n    constructor(inputs, toolManager) {\n        this.inputs = inputs;\n        this.toolManager = toolManager;\n    }\n    async invoke() {\n        const invoking = await gemini(this.inputs);\n        if (!ok(invoking))\n            return invoking;\n        if (\"context\" in invoking) {\n            return err(\"Invalid output from Gemini -- must be candidates\");\n        }\n        const content = invoking.candidates.at(0)?.content;\n        if (!content) {\n            return err(\"No content from Gemini\");\n        }\n        const results = [];\n        const errors = [];\n        await this.toolManager?.processResponse(content, async ($board, args) => {\n            const callingTool = await invokeBoard({ $board, ...args });\n            if (\"$error\" in callingTool) {\n                errors.push(JSON.stringify(callingTool.$error));\n            }\n            else {\n                results.push(JSON.stringify(callingTool));\n            }\n        });\n        if (errors.length) {\n            return err(`Calling tools generated the following errors: ${errors.join(\",\")}`);\n        }\n        const result = [content];\n        if (results.length) {\n            result.push(toLLMContent(results.join(\"\\n\\n\")));\n        }\n        return { all: result, last: result.at(-1) };\n    }\n}\nfunction gracefulExit(notOk) {\n    report({\n        actor: \"Make Code\",\n        category: \"Warning\",\n        name: \"Graceful exit\",\n        details: `I tried a couple of times, but the Gemini API failed to generate the code you requested with the following error:\n\n### ${notOk.$error}\n\nTo keep things moving, I will return a blank result. My apologies!`,\n        icon: MAKE_CODE_ICON,\n    });\n    return { context: [toLLMContent(\" \")] };\n}\nconst MAX_RETRIES = 5;\nasync function invoke({ context, instruction, language, ...params }) {\n    context ??= [];\n    // 1) Substitute params in instruction.\n    const toolManager = new ToolManager(new ArgumentNameGenerator());\n    const substituting = await new Template(instruction).substitute(params, async ({ path: url, instance }) => toolManager.addTool(url, instance));\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    instruction = substituting;\n    // 2) If there are tools in instruction, add an extra step of preparing\n    // information via tools.\n    if (toolManager.hasTools()) {\n        const gatheringInformation = await gatheringRequest(context, instruction, language, toolManager).invoke();\n        if (!ok(gatheringInformation))\n            return gatheringInformation;\n        context.push(...gatheringInformation.all);\n    }\n    let retryCount = MAX_RETRIES;\n    while (retryCount--) {\n        // 3) Call Gemini to generate prompt.\n        const generatingPrompt = await promptRequest(context, instruction, language).invoke();\n        if (!ok(generatingPrompt))\n            return generatingPrompt;\n        console.log(\"PROMPT\", toText(generatingPrompt.last));\n        // 4) Call Gemini to generate image.\n        const generatingCode = await codeRequest(generatingPrompt.last, language).invoke();\n        if (!ok(generatingCode)) {\n            return generatingCode;\n        }\n        return { context: generatingCode.all };\n    }\n    return gracefulExit(err(`Failed to generate ${language} after ${MAX_RETRIES} tries.`));\n}\nasync function describe({ inputs: { instruction } }) {\n    const template = new Template(instruction);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                instruction: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"Instruction\",\n                    description: \"Describe how to generate the JavaScript based on the input: focus, functionality, aim of the code\",\n                },\n                language: {\n                    type: \"string\",\n                    behavior: [\"hint-text\", \"config\", \"hint-preview\"],\n                    title: \"Language\",\n                    icon: \"frame-source\",\n                    enum: [\"JavaScript\", \"HTML\", \"CSS\"],\n                    description: \"The language you'd like to generate\",\n                    default: \"JavaScript\",\n                },\n                ...template.schemas(),\n            },\n            ...template.requireds(),\n            additionalProperties: false,\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"hint-code\", \"main-port\"],\n                },\n            },\n            additionalProperties: false,\n        },\n        title: \"Make Code\",\n        metadata: {\n            icon: MAKE_CODE_ICON,\n            tags: [\"quick-access\", \"generative\", \"experimental\"],\n            order: 2,\n        },\n    };\n}\n",
      "metadata": {
        "title": "Make Code",
        "source": {
          "code": "/**\n * @fileoverview Generates code using supplied context.\n */\n\nimport invokeBoard from \"@invoke\";\n\nimport gemini, {\n  defaultSafetySettings,\n  type GeminiOutputs,\n  type GeminiInputs,\n  type Tool,\n} from \"./gemini\";\nimport { err, ok, toLLMContent, toText, addUserTurn } from \"./utils\";\nimport { Template } from \"./template\";\nimport { ToolManager } from \"./tool-manager\";\nimport { type Params } from \"./common\";\nimport { report } from \"./output\";\nimport { ArgumentNameGenerator } from \"./introducer\";\n\nconst MAKE_CODE_ICON = \"generative-code\";\n\ntype CodeGeneratorInputs = {\n  context: LLMContent[];\n  instruction: LLMContent;\n  language: string;\n} & Params;\n\ntype CodeGeneratorOutputs = {\n  context: LLMContent[];\n};\n\nexport { invoke as default, describe };\n\nfunction gatheringRequest(\n  contents: LLMContent[] | undefined,\n  instruction: LLMContent,\n  language: string,\n  toolManager: ToolManager\n): GeminiPrompt {\n  const promptText = `\nAnalyze the instruction below and rather than following it, determine what information needs to be gathered to \ngenerate an accurate prompt for a text-to-${language} model in the next turn:\n-- begin instruction --\n${toText(instruction)}\n-- end instruction --\n\nCall the tools to gather the necessary information that could be used to create an accurate prompt.`;\n  return new GeminiPrompt(\n    {\n      model: \"gemini-1.5-flash-latest\",\n      body: {\n        contents: addUserTurn(promptText, contents),\n        tools: toolManager.list(),\n        systemInstruction: toLLMContent(`\nYou are a researcher whose specialty is to call tools whose output helps gather the necessary information\nto be used to create an accurate prompt for a text-to-${language} model.\n`),\n      },\n    },\n    toolManager\n  );\n}\n\nfunction promptRequest(\n  contents: LLMContent[] | undefined,\n  instruction: LLMContent,\n  language: string\n): GeminiPrompt {\n  const context = contents?.length\n    ? \"using conversation context and these additional\"\n    : \"with these\";\n  const promptText = `Generate a single text-to-${language} prompt ${context} instructions:\n${toText(instruction)}\n\nTypical output format:\n\n## Setting/background\n\nDetailed description of everything that is understood about the ${language} code that is being requested.\n\n## Primary focus\n\nDetailed description of the primary functionality or the main focal point of the JavaScript code.\n\n## Style\n\nDetailed description of the style and approach of the code (defensive, TDD, creative, etc.). The output should\nalways an invariably be ${language === \"JavaScript\" ? \"EcmaScript JavaScript Modules\" : language} and be fully functional \nwithout any placeholders. \n\nIf you are dealing with JavaScript you may use imports if and only if the instruction indicates, otherwise you must\ncreate the functionality as a standalone piece of EcmaScript JavaScript.\n\nYou output will be fed directly into the text-to-${language} model, so it must be prompt only, no additional chit-chat\n`;\n  return new GeminiPrompt({\n    model: \"gemini-1.5-flash-latest\",\n    body: {\n      contents: addUserTurn(promptText, contents),\n      systemInstruction: toLLMContent(`\nYou are a world-class ${language} developer whose specialty is to write prompts for text-to-${language}Â models that \nalways generate valid outputs.\n\nThe prompt must describe every aspect of the functionality in great detail and describe the problem being solved \nin terms of data structures, algorithms, and style. You must use the instruction to fully understand and replicate\nany reference implementations you've been given and you must not augment or deviate from that style. You should\nensure that the prompt includes enough information to fully replicate that style with examples and maximal clarity.\n\nIf the code pertains to user interface work, you must also maximize the accessibility of the code generated with\nappropriate titles and labels for buttons, controls, inputs, etc, and they should never be empty.\n\nBe sure to export all relevant symbols so that the code can be used outside of the EcmaScript Module. Always do\nthis as named symbols rather than using default exports.\n\nIf writing JavaScript, and where a variable is private, use private fields (#field) rather than an underscore at the start.\n`),\n    },\n  });\n}\n\nfunction codeRequest(prompt: LLMContent, language: string): GeminiPrompt {\n  prompt.role = \"user\";\n  prompt.parts.unshift({\n    text: `Generate ${language} code based on this prompt. Output code only, no chit-chat`,\n  });\n\n  return new GeminiPrompt({\n    model: \"gemini-2.0-flash-exp\",\n    body: {\n      contents: [prompt],\n      generationConfig: {\n        responseModalities: [\"TEXT\"],\n      },\n      safetySettings: defaultSafetySettings(),\n    },\n  });\n}\n\nexport type GeminiPromptOutput = {\n  last: LLMContent;\n  all: LLMContent[];\n};\n\nclass GeminiPrompt {\n  constructor(\n    public readonly inputs: GeminiInputs,\n    public readonly toolManager?: ToolManager\n  ) {}\n\n  async invoke(): Promise<Outcome<GeminiPromptOutput>> {\n    const invoking = await gemini(this.inputs);\n    if (!ok(invoking)) return invoking;\n    if (\"context\" in invoking) {\n      return err(\"Invalid output from Gemini -- must be candidates\");\n    }\n    const content = invoking.candidates.at(0)?.content;\n    if (!content) {\n      return err(\"No content from Gemini\");\n    }\n    const results: string[] = [];\n    const errors: string[] = [];\n    await this.toolManager?.processResponse(content, async ($board, args) => {\n      const callingTool = await invokeBoard({ $board, ...args });\n      if (\"$error\" in callingTool) {\n        errors.push(JSON.stringify(callingTool.$error));\n      } else {\n        results.push(JSON.stringify(callingTool));\n      }\n    });\n    if (errors.length) {\n      return err(\n        `Calling tools generated the following errors: ${errors.join(\",\")}`\n      );\n    }\n    const result = [content];\n    if (results.length) {\n      result.push(toLLMContent(results.join(\"\\n\\n\")));\n    }\n    return { all: result, last: result.at(-1)! };\n  }\n}\n\nfunction gracefulExit(notOk: {\n  $error: string;\n}): Outcome<CodeGeneratorOutputs> {\n  report({\n    actor: \"Make Code\",\n    category: \"Warning\",\n    name: \"Graceful exit\",\n    details: `I tried a couple of times, but the Gemini API failed to generate the code you requested with the following error:\n\n### ${notOk.$error}\n\nTo keep things moving, I will return a blank result. My apologies!`,\n    icon: MAKE_CODE_ICON,\n  });\n  return { context: [toLLMContent(\" \")] };\n}\n\nconst MAX_RETRIES = 5;\n\nasync function invoke({\n  context,\n  instruction,\n  language,\n  ...params\n}: CodeGeneratorInputs): Promise<Outcome<CodeGeneratorOutputs>> {\n  context ??= [];\n\n  // 1) Substitute params in instruction.\n  const toolManager = new ToolManager(new ArgumentNameGenerator());\n  const substituting = await new Template(instruction).substitute(\n    params,\n    async ({ path: url, instance }) => toolManager.addTool(url, instance)\n  );\n  if (!ok(substituting)) {\n    return substituting;\n  }\n  instruction = substituting;\n\n  // 2) If there are tools in instruction, add an extra step of preparing\n  // information via tools.\n  if (toolManager.hasTools()) {\n    const gatheringInformation = await gatheringRequest(\n      context,\n      instruction,\n      language,\n      toolManager\n    ).invoke();\n    if (!ok(gatheringInformation)) return gatheringInformation;\n    context.push(...gatheringInformation.all);\n  }\n\n  let retryCount = MAX_RETRIES;\n\n  while (retryCount--) {\n    // 3) Call Gemini to generate prompt.\n    const generatingPrompt = await promptRequest(\n      context,\n      instruction,\n      language\n    ).invoke();\n    if (!ok(generatingPrompt)) return generatingPrompt;\n\n    console.log(\"PROMPT\", toText(generatingPrompt.last));\n\n    // 4) Call Gemini to generate image.\n    const generatingCode = await codeRequest(\n      generatingPrompt.last,\n      language\n    ).invoke();\n    if (!ok(generatingCode)) {\n      return generatingCode;\n    }\n\n    return { context: generatingCode.all };\n  }\n  return gracefulExit(\n    err(`Failed to generate ${language} after ${MAX_RETRIES} tries.`)\n  );\n}\n\ntype DescribeInputs = {\n  inputs: {\n    instruction?: LLMContent;\n  };\n};\n\nasync function describe({ inputs: { instruction } }: DescribeInputs) {\n  const template = new Template(instruction);\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n        instruction: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n          title: \"Instruction\",\n          description:\n            \"Describe how to generate the JavaScript based on the input: focus, functionality, aim of the code\",\n        },\n        language: {\n          type: \"string\",\n          behavior: [\"hint-text\", \"config\", \"hint-preview\"],\n          title: \"Language\",\n          icon: \"frame-source\",\n          enum: [\"JavaScript\", \"HTML\", \"CSS\"],\n          description: \"The language you'd like to generate\",\n          default: \"JavaScript\",\n        },\n        ...template.schemas(),\n      },\n      ...template.requireds(),\n      additionalProperties: false,\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n          behavior: [\"hint-code\", \"main-port\"],\n        },\n      },\n      additionalProperties: false,\n    } satisfies Schema,\n    title: \"Make Code\",\n    metadata: {\n      icon: MAKE_CODE_ICON,\n      tags: [\"quick-access\", \"generative\", \"experimental\"],\n      order: 2,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Generates code using supplied context.",
        "runnable": true
      }
    },
    "gemini-prompt": {
      "code": "/**\n * @fileoverview Manages Gemini prompt.\n */\nimport invokeBoard from \"@invoke\";\nimport gemini, {} from \"./gemini\";\nimport { ToolManager } from \"./tool-manager\";\nimport { ok, err, toLLMContent, addUserTurn } from \"./utils\";\nexport { GeminiPrompt };\nfunction textToJson(content) {\n    return {\n        ...content,\n        parts: content.parts.map((part) => {\n            if (\"text\" in part) {\n                try {\n                    return { json: JSON.parse(part.text) };\n                }\n                catch (e) {\n                    // fall through.\n                }\n            }\n            return part;\n        }),\n    };\n}\nfunction mergeLastParts(contexts) {\n    const parts = [];\n    for (const context of contexts) {\n        const last = context.at(-1);\n        if (!last)\n            continue;\n        if (!last.parts)\n            continue;\n        parts.push(...last.parts);\n    }\n    return {\n        parts,\n        role: \"user\",\n    };\n}\nclass GeminiPrompt {\n    inputs;\n    options;\n    calledTools = false;\n    constructor(inputs, options) {\n        this.inputs = inputs;\n        this.options = this.#reconcileOptions(options);\n    }\n    #reconcileOptions(options) {\n        if (!options)\n            return {};\n        if (options instanceof ToolManager) {\n            return { toolManager: options };\n        }\n        return options;\n    }\n    #normalizeArgs(a, passContext) {\n        if (!passContext)\n            return a;\n        const args = a;\n        const context = [...this.inputs.body.contents];\n        const hasContext = \"context\" in args;\n        let contextArg = hasContext\n            ? {}\n            : {\n                context,\n            };\n        return {\n            ...contextArg,\n            ...Object.fromEntries(Object.entries(args).map(([name, value]) => {\n                if (hasContext) {\n                    value = addUserTurn(value, [\n                        ...this.inputs.body.contents,\n                    ]);\n                }\n                return [name, value];\n            })),\n        };\n    }\n    async invoke() {\n        this.calledTools = false;\n        const { allowToolErrors, validator } = this.options;\n        const invoking = await gemini(this.inputs);\n        if (!ok(invoking))\n            return invoking;\n        if (\"context\" in invoking) {\n            return err(\"Invalid output from Gemini -- must be candidates\");\n        }\n        const candidate = invoking.candidates.at(0);\n        const content = candidate?.content;\n        if (!content) {\n            return err(\"No content from Gemini\");\n        }\n        if (!content.parts) {\n            return err(`Gemini failed to generate result due to ${candidate.finishReason}`);\n        }\n        const results = [];\n        const errors = [];\n        if (validator) {\n            const validating = validator(content);\n            if (!ok(validating))\n                return validating;\n        }\n        await this.options.toolManager?.processResponse(content, async ($board, args, passContext) => {\n            console.log(\"CALLING TOOL\", $board, args, passContext);\n            this.calledTools = true;\n            const callingTool = await invokeBoard({\n                $board,\n                ...this.#normalizeArgs(args, passContext),\n            });\n            if (\"$error\" in callingTool) {\n                errors.push(JSON.stringify(callingTool.$error));\n            }\n            else {\n                if (passContext) {\n                    if (!(\"context\" in callingTool)) {\n                        errors.push(`No \"context\" port in outputs of \"${$board}\"`);\n                    }\n                    else {\n                        results.push(callingTool.context);\n                    }\n                }\n                else {\n                    results.push([toLLMContent(JSON.stringify(callingTool))]);\n                }\n            }\n        });\n        console.log(\"ERRORS\", errors);\n        if (errors.length && !allowToolErrors) {\n            return err(`Calling tools generated the following errors: ${errors.join(\",\")}`);\n        }\n        const isJSON = this.inputs.body.generationConfig?.responseMimeType == \"application/json\";\n        const result = isJSON ? [textToJson(content)] : [content];\n        if (results.length) {\n            result.push(mergeLastParts(results));\n        }\n        return { all: result, last: result.at(-1), candidate };\n    }\n}\n",
      "metadata": {
        "title": "gemini-prompt",
        "source": {
          "code": "/**\n * @fileoverview Manages Gemini prompt.\n */\n\nimport invokeBoard from \"@invoke\";\n\nimport gemini, {\n  type GeminiInputs,\n  type GeminiOutputs,\n  type Candidate,\n} from \"./gemini\";\nimport { ToolManager } from \"./tool-manager\";\nimport { ok, err, toLLMContent, addUserTurn } from \"./utils\";\n\nexport { GeminiPrompt };\n\nfunction textToJson(content: LLMContent): LLMContent {\n  return {\n    ...content,\n    parts: content.parts.map((part) => {\n      if (\"text\" in part) {\n        try {\n          return { json: JSON.parse(part.text) };\n        } catch (e) {\n          // fall through.\n        }\n      }\n      return part;\n    }),\n  };\n}\n\nfunction mergeLastParts(contexts: LLMContent[][]): LLMContent {\n  const parts: DataPart[] = [];\n  for (const context of contexts) {\n    const last = context.at(-1);\n    if (!last) continue;\n    if (!last.parts) continue;\n    parts.push(...last.parts);\n  }\n  return {\n    parts,\n    role: \"user\",\n  };\n}\n\nexport type ValidatorFunction = (response: LLMContent) => Outcome<void>;\n\nexport type GeminiPromptOutput = {\n  last: LLMContent;\n  all: LLMContent[];\n  candidate: Candidate;\n};\n\nexport type GeminiPromptInvokeOptions = GeminiPromptOptions;\n\nexport type GeminiPromptOptions = {\n  allowToolErrors?: boolean;\n  validator?: ValidatorFunction;\n  toolManager?: ToolManager;\n};\nclass GeminiPrompt {\n  readonly options: GeminiPromptOptions;\n\n  calledTools: boolean = false;\n\n  constructor(\n    public readonly inputs: GeminiInputs,\n    options?: ToolManager | GeminiPromptOptions\n  ) {\n    this.options = this.#reconcileOptions(options);\n  }\n\n  #reconcileOptions(\n    options?: ToolManager | GeminiPromptOptions\n  ): GeminiPromptOptions {\n    if (!options) return {};\n    if (options instanceof ToolManager) {\n      return { toolManager: options };\n    }\n    return options;\n  }\n\n  #normalizeArgs(a: object, passContext?: boolean) {\n    if (!passContext) return a;\n    const args = a as Record<string, unknown>;\n    const context = [...this.inputs.body.contents];\n    const hasContext = \"context\" in args;\n    let contextArg = hasContext\n      ? {}\n      : {\n          context,\n        };\n    return {\n      ...contextArg,\n      ...Object.fromEntries(\n        Object.entries(args).map(([name, value]) => {\n          if (hasContext) {\n            value = addUserTurn(value as string, [\n              ...this.inputs.body.contents,\n            ]);\n          }\n          return [name, value];\n        })\n      ),\n    };\n  }\n\n  async invoke(): Promise<Outcome<GeminiPromptOutput>> {\n    this.calledTools = false;\n    const { allowToolErrors, validator } = this.options;\n    const invoking = await gemini(this.inputs);\n    if (!ok(invoking)) return invoking;\n    if (\"context\" in invoking) {\n      return err(\"Invalid output from Gemini -- must be candidates\");\n    }\n    const candidate = invoking.candidates.at(0);\n    const content = candidate?.content;\n    if (!content) {\n      return err(\"No content from Gemini\");\n    }\n    if (!content.parts) {\n      return err(\n        `Gemini failed to generate result due to ${candidate.finishReason}`\n      );\n    }\n    const results: LLMContent[][] = [];\n    const errors: string[] = [];\n    if (validator) {\n      const validating = validator(content);\n      if (!ok(validating)) return validating;\n    }\n    await this.options.toolManager?.processResponse(\n      content,\n      async ($board, args, passContext) => {\n        console.log(\"CALLING TOOL\", $board, args, passContext);\n        this.calledTools = true;\n        const callingTool = await invokeBoard({\n          $board,\n          ...this.#normalizeArgs(args, passContext),\n        });\n        if (\"$error\" in callingTool) {\n          errors.push(JSON.stringify(callingTool.$error));\n        } else {\n          if (passContext) {\n            if (!(\"context\" in callingTool)) {\n              errors.push(`No \"context\" port in outputs of \"${$board}\"`);\n            } else {\n              results.push(callingTool.context as LLMContent[]);\n            }\n          } else {\n            results.push([toLLMContent(JSON.stringify(callingTool))]);\n          }\n        }\n      }\n    );\n    console.log(\"ERRORS\", errors);\n    if (errors.length && !allowToolErrors) {\n      return err(\n        `Calling tools generated the following errors: ${errors.join(\",\")}`\n      );\n    }\n    const isJSON =\n      this.inputs.body.generationConfig?.responseMimeType == \"application/json\";\n    const result = isJSON ? [textToJson(content)] : [content];\n    if (results.length) {\n      result.push(mergeLastParts(results));\n    }\n    return { all: result, last: result.at(-1)!, candidate };\n  }\n}\n",
          "language": "typescript"
        },
        "description": "Manages Gemini prompt.",
        "runnable": false
      }
    },
    "step-executor": {
      "code": "/**\n * @fileoverview Utilities to execute tools on the AppCatalyst backend server.\n */\nexport { executeStep, executeTool };\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\nimport { ok, err } from \"./utils\";\nfunction maybeExtractError(e) {\n    try {\n        const parsed = JSON.parse(e);\n        return parsed.error.message;\n    }\n    catch (error) {\n        return e;\n    }\n}\nasync function executeTool(api, params) {\n    const inputParameters = Object.keys(params);\n    const execution_inputs = Object.fromEntries(Object.entries(params).map(([name, value]) => {\n        return [\n            name,\n            {\n                chunks: [{ mimetype: \"text/plan\", data: btoa(value) }],\n            },\n        ];\n    }));\n    const response = await executeStep({\n        planStep: {\n            stepName: api,\n            modelApi: api,\n            output: \"data\",\n            inputParameters,\n            isListOutput: false,\n        },\n        execution_inputs,\n    });\n    if (!ok(response))\n        return response;\n    const data = response?.executionOutputs[\"data\"].chunks.at(0)?.data;\n    if (!data) {\n        return err(`Invalid response from \"${api}\" backend`);\n    }\n    const jsonString = atob(data);\n    try {\n        return JSON.parse(jsonString);\n    }\n    catch (e) {\n        return jsonString;\n    }\n}\nasync function executeStep(body) {\n    // Get an authentication token.\n    const key = \"connection:$sign-in\";\n    const token = (await secrets({ keys: [key] }))[key];\n    // Call the API.\n    const url = \"https://staging-appcatalyst.sandbox.googleapis.com/v1beta1/executeStep\";\n    const fetchResult = await fetch({\n        url: url,\n        method: \"POST\",\n        headers: {\n            \"Content-Type\": \"application/json\",\n            Authorization: `Bearer ${token}`,\n        },\n        body: body,\n    });\n    let $error = \"Unknown error\";\n    if (!ok(fetchResult)) {\n        const { status, $error: errObject } = fetchResult;\n        console.warn(\"Error response\", fetchResult);\n        if (!status) {\n            // This is not an error response, presume fatal error.\n            return { $error };\n        }\n        $error = maybeExtractError(errObject);\n        return { $error };\n    }\n    const response = fetchResult.response;\n    if (response.errorMessage) {\n        $error = response.errorMessage;\n        return { $error };\n    }\n    return response;\n}\n",
      "metadata": {
        "title": "step-executor",
        "source": {
          "code": "/**\n * @fileoverview Utilities to execute tools on the AppCatalyst backend server.\n */\n\nexport { executeStep, executeTool };\n\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\nimport { ok, err } from \"./utils\";\n\ntype FetchErrorResponse = {\n  $error: string;\n  status: number;\n  statusText: string;\n  contentType: string;\n  responseHeaders: Record<string, string>;\n};\n\ntype Chunk = {\n  mimetype: string;\n  data: string;\n};\n\nexport type Content = {\n  chunks: Chunk[];\n};\n\nexport type ContentMap = {\n  [key: string]: Content;\n};\n\nexport type PlanStep = {\n  stepName: string;\n  modelApi: string;\n  inputParameters: string[];\n  systemPrompt?: string;\n  stepIntent?: string;\n  output?: string;\n  isListOutput?: boolean;\n  options?: {\n    disablePromptRewrite?: boolean;\n    renderMode?: string;\n    modelName?: string;\n  };\n};\n\nexport type GcsConfig = {\n  bucket_name: string;\n  folder_path: string;\n  project_name: string;\n};\n\nexport type ExecuteStepRequest = {\n  planStep: PlanStep;\n  execution_inputs: ContentMap;\n  output_gcs_config?: GcsConfig;\n};\n\nexport type ExecuteStepResponse = {\n  executionOutputs: ContentMap;\n  errorMessage?: string;\n};\n\nfunction maybeExtractError(e: string): string {\n  try {\n    const parsed = JSON.parse(e);\n    return parsed.error.message;\n  } catch (error) {\n    return e;\n  }\n}\n\nasync function executeTool<\n  T extends JsonSerializable = Record<string, JsonSerializable>,\n>(api: string, params: Record<string, string>): Promise<Outcome<T | string>> {\n  const inputParameters = Object.keys(params);\n  const execution_inputs = Object.fromEntries(\n    Object.entries(params).map(([name, value]) => {\n      return [\n        name,\n        {\n          chunks: [{ mimetype: \"text/plan\", data: btoa(value) }],\n        },\n      ];\n    })\n  );\n  const response = await executeStep({\n    planStep: {\n      stepName: api,\n      modelApi: api,\n      output: \"data\",\n      inputParameters,\n      isListOutput: false,\n    },\n    execution_inputs,\n  });\n  if (!ok(response)) return response;\n\n  const data = response?.executionOutputs[\"data\"].chunks.at(0)?.data;\n  if (!data) {\n    return err(`Invalid response from \"${api}\" backend`);\n  }\n  const jsonString = atob(data);\n  try {\n    return JSON.parse(jsonString) as T;\n  } catch (e) {\n    return jsonString;\n  }\n}\n\nasync function executeStep(\n  body: ExecuteStepRequest\n): Promise<Outcome<ExecuteStepResponse>> {\n  // Get an authentication token.\n  const key = \"connection:$sign-in\";\n  const token = (await secrets({ keys: [key] }))[key];\n  // Call the API.\n  const url =\n    \"https://staging-appcatalyst.sandbox.googleapis.com/v1beta1/executeStep\";\n  const fetchResult = await fetch({\n    url: url,\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n      Authorization: `Bearer ${token}`,\n    },\n    body: body,\n  });\n  let $error: string = \"Unknown error\";\n  if (!ok(fetchResult)) {\n    const { status, $error: errObject } = fetchResult as FetchErrorResponse;\n    console.warn(\"Error response\", fetchResult);\n    if (!status) {\n      // This is not an error response, presume fatal error.\n      return { $error };\n    }\n    $error = maybeExtractError(errObject);\n    return { $error };\n  }\n  const response = fetchResult.response as ExecuteStepResponse;\n  if (response.errorMessage) {\n    $error = response.errorMessage;\n    return { $error };\n  }\n  return response;\n}\n",
          "language": "typescript"
        },
        "description": "Utilities to execute tools on the AppCatalyst backend server.",
        "runnable": false
      }
    },
    "image-editor": {
      "code": "/**\n * @fileoverview Generates an image using supplied context (generation and editing).\n */\nimport invokeBoard from \"@invoke\";\nimport gemini, { defaultSafetySettings, } from \"./gemini\";\nimport { GeminiPrompt } from \"./gemini-prompt\";\nimport { err, ok, toLLMContent, defaultLLMContent, toText, addUserTurn, toTextConcat, joinContent, llm, extractMediaData, extractTextData, mergeContent, } from \"./utils\";\nimport { callImageGen, callGeminiImage, promptExpander } from \"./image-utils\";\nimport { Template } from \"./template\";\nimport { ToolManager } from \"./tool-manager\";\nimport {} from \"./common\";\nimport { report } from \"./output\";\nimport { ArgumentNameGenerator } from \"./introducer\";\nimport { ListExpander } from \"./lists\";\nconst MAKE_IMAGE_ICON = \"generative-image\";\nconst ASPECT_RATIOS = [\"1:1\", \"9:16\", \"16:9\", \"4:3\", \"3:4\"];\nexport { invoke as default, describe };\nfunction gatheringRequest(contents, instruction, toolManager) {\n    const promptText = llm `\nAnalyze the instruction below and rather than following it, determine what information needs to be gathered to \ngenerate an accurate prompt for a text-to-image model in the next turn:\n-- begin instruction --\n${instruction}\n-- end instruction --\n\nCall the tools to gather the necessary information that could be used to create an accurate prompt.`;\n    return new GeminiPrompt({\n        body: {\n            contents: addUserTurn(promptText.asContent(), contents),\n            tools: toolManager.list(),\n            systemInstruction: toLLMContent(`\nYou are a researcher whose specialty is to call tools whose output helps gather the necessary information\nto be used to create an accurate prompt for a text-to-image model.\n`),\n        },\n    }, toolManager);\n}\nfunction gracefulExit(notOk) {\n    report({\n        actor: \"Make Image\",\n        category: \"Warning\",\n        name: \"Graceful exit\",\n        details: `I tried a couple of times, but the Gemini API failed to generate the image you requested with the following error:\n\n### ${notOk.$error}\n\nTo keep things moving, I will return a blank result. My apologies!`,\n        icon: MAKE_IMAGE_ICON,\n    });\n    return toLLMContent(\" \");\n}\nconst MAX_RETRIES = 5;\nasync function invoke({ context: incomingContext, instruction, \"p-disable-prompt-rewrite\": disablePromptRewrite, \"p-aspect-ratio\": aspectRatio, ...params }) {\n    incomingContext ??= [];\n    if (!instruction) {\n        instruction = toLLMContent(\"\");\n    }\n    if (!aspectRatio) {\n        aspectRatio = \"1:1\";\n    }\n    let imageContext = extractMediaData(incomingContext);\n    const textContext = extractTextData(incomingContext);\n    // Substitute params in instruction.\n    const toolManager = new ToolManager(new ArgumentNameGenerator());\n    const substituting = await new Template(instruction).substitute(params, async ({ path: url, instance }) => toolManager.addTool(url, instance));\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    const fanningOut = await new ListExpander(substituting, incomingContext).map(async (instruction, context) => {\n        // If there are tools in instruction, add an extra step of preparing\n        // information via tools.\n        if (toolManager.hasTools()) {\n            const gatheringInformation = await gatheringRequest(context, instruction, toolManager).invoke();\n            if (!ok(gatheringInformation))\n                return gatheringInformation;\n            context.push(...gatheringInformation.all);\n        }\n        const refImages = extractMediaData([instruction]);\n        const refText = instruction\n            ? toLLMContent(toTextConcat(extractTextData([instruction])))\n            : toLLMContent(\"\");\n        imageContext = imageContext.concat(refImages);\n        let retryCount = MAX_RETRIES;\n        while (retryCount--) {\n            // Image editing case.\n            if (imageContext.length > 0) {\n                console.log(\"Step has reference image, using Gemini Image API: i2i\");\n                const instructionText = refText ? toText(refText) : \"\";\n                const combinedInstruction = toTextConcat(joinContent(instructionText, textContext, false)).trim();\n                if (!combinedInstruction) {\n                    return err(`An image editing instruction must be provided along side the reference image.`);\n                }\n                const finalInstruction = combinedInstruction + \"\\nAspect ratio: \" + aspectRatio;\n                console.log(\"PROMPT: \" + finalInstruction);\n                const generatedImage = await callGeminiImage(finalInstruction, imageContext, disablePromptRewrite, aspectRatio);\n                if (!ok(generatedImage))\n                    return generatedImage;\n                return mergeContent(generatedImage, \"model\");\n            }\n            else {\n                console.log(\"Step has text only, using Gemini Image API: t2i\");\n                let imagePrompt;\n                if (disablePromptRewrite) {\n                    imagePrompt = toLLMContent(toText(addUserTurn(refText, context)));\n                }\n                else {\n                    const generatingPrompt = await promptExpander(context, refText).invoke();\n                    if (!ok(generatingPrompt))\n                        return generatingPrompt;\n                    imagePrompt = generatingPrompt.last;\n                }\n                const iPrompt = toText(imagePrompt).trim();\n                console.log(\"PROMPT\", iPrompt);\n                const generatedImage = await callGeminiImage(iPrompt, [], disablePromptRewrite, aspectRatio);\n                if (!ok(generatedImage))\n                    return generatedImage;\n                return mergeContent(generatedImage, \"model\");\n            }\n        }\n        return gracefulExit(err(`Failed to generate an image after ${MAX_RETRIES} tries.`));\n    });\n    if (!ok(fanningOut))\n        return fanningOut;\n    return { context: fanningOut };\n}\nasync function describe({ inputs: { instruction } }) {\n    const template = new Template(instruction);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context in\",\n                    behavior: [\"main-port\"],\n                },\n                instruction: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n                    title: \"Instruction\",\n                    description: \"Describe how to generate the image (content, style, etc). Use @ to reference params or outputs from other steps.\",\n                    default: defaultLLMContent(),\n                },\n                \"p-disable-prompt-rewrite\": {\n                    type: \"boolean\",\n                    title: \"Disable prompt expansion\",\n                    behavior: [\"config\"],\n                    description: \"By default, inputs and instructions will be automatically expanded into a high quality image prompt. Check to disable this re-writing behavior.\",\n                },\n                \"p-aspect-ratio\": {\n                    type: \"string\",\n                    behavior: [\"hint-text\", \"config\"],\n                    title: \"Aspect Ratio\",\n                    enum: ASPECT_RATIOS,\n                    description: \"The aspect ratio of the generated image\",\n                    default: \"1:1\",\n                },\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: { type: \"object\", behavior: [\"llm-content\"] },\n                    title: \"Context out\",\n                    behavior: [\"hint-image\", \"main-port\"],\n                },\n            },\n        },\n        title: \"Edit Image\",\n        metadata: {\n            icon: MAKE_IMAGE_ICON,\n            tags: [\"quick-access\", \"generative\"],\n            order: 2,\n        },\n    };\n}\n",
      "metadata": {
        "title": "Image Edit",
        "source": {
          "code": "/**\n * @fileoverview Generates an image using supplied context (generation and editing).\n */\n\nimport invokeBoard from \"@invoke\";\n\nimport gemini, {\n  defaultSafetySettings,\n  type GeminiOutputs,\n  type GeminiInputs,\n  type Tool,\n} from \"./gemini\";\nimport { GeminiPrompt } from \"./gemini-prompt\";\nimport {\n  err,\n  ok,\n  toLLMContent,\n  defaultLLMContent,\n  toText,\n  addUserTurn,\n  toTextConcat,\n  joinContent,\n  llm,\n  extractMediaData,\n  extractTextData,\n  mergeContent,\n} from \"./utils\";\nimport { callImageGen, callGeminiImage, promptExpander } from \"./image-utils\";\nimport { Template } from \"./template\";\nimport { ToolManager } from \"./tool-manager\";\nimport { type Params, type DescriberResult } from \"./common\";\nimport { report } from \"./output\";\nimport { ArgumentNameGenerator } from \"./introducer\";\nimport { ListExpander } from \"./lists\";\n\nconst MAKE_IMAGE_ICON = \"generative-image\";\nconst ASPECT_RATIOS = [\"1:1\", \"9:16\", \"16:9\", \"4:3\", \"3:4\"];\n\ntype ImageGeneratorInputs = {\n  context: LLMContent[];\n  instruction: LLMContent;\n  \"p-disable-prompt-rewrite\": boolean;\n  \"p-aspect-ratio\": string;\n} & Params;\n\ntype ImageGeneratorOutputs = {\n  context: LLMContent[] | DescriberResult;\n};\n\nexport { invoke as default, describe };\n\nfunction gatheringRequest(\n  contents: LLMContent[] | undefined,\n  instruction: LLMContent,\n  toolManager: ToolManager\n): GeminiPrompt {\n  const promptText = llm`\nAnalyze the instruction below and rather than following it, determine what information needs to be gathered to \ngenerate an accurate prompt for a text-to-image model in the next turn:\n-- begin instruction --\n${instruction}\n-- end instruction --\n\nCall the tools to gather the necessary information that could be used to create an accurate prompt.`;\n  return new GeminiPrompt(\n    {\n      body: {\n        contents: addUserTurn(promptText.asContent(), contents),\n        tools: toolManager.list(),\n        systemInstruction: toLLMContent(`\nYou are a researcher whose specialty is to call tools whose output helps gather the necessary information\nto be used to create an accurate prompt for a text-to-image model.\n`),\n      },\n    },\n    toolManager\n  );\n}\n\nfunction gracefulExit(notOk: { $error: string }): Outcome<LLMContent> {\n  report({\n    actor: \"Make Image\",\n    category: \"Warning\",\n    name: \"Graceful exit\",\n    details: `I tried a couple of times, but the Gemini API failed to generate the image you requested with the following error:\n\n### ${notOk.$error}\n\nTo keep things moving, I will return a blank result. My apologies!`,\n    icon: MAKE_IMAGE_ICON,\n  });\n  return toLLMContent(\" \");\n}\n\nconst MAX_RETRIES = 5;\n\nasync function invoke({\n  context: incomingContext,\n  instruction,\n  \"p-disable-prompt-rewrite\": disablePromptRewrite,\n  \"p-aspect-ratio\": aspectRatio,\n  ...params\n}: ImageGeneratorInputs): Promise<Outcome<ImageGeneratorOutputs>> {\n  incomingContext ??= [];\n  if (!instruction) {\n    instruction = toLLMContent(\"\");\n  }\n  if (!aspectRatio) {\n    aspectRatio = \"1:1\";\n  }\n  let imageContext = extractMediaData(incomingContext);\n  const textContext = extractTextData(incomingContext);\n  // Substitute params in instruction.\n  const toolManager = new ToolManager(new ArgumentNameGenerator());\n  const substituting = await new Template(instruction).substitute(\n    params,\n    async ({ path: url, instance }) => toolManager.addTool(url, instance)\n  );\n  if (!ok(substituting)) {\n    return substituting;\n  }\n\n  const fanningOut = await new ListExpander(substituting, incomingContext).map(\n    async (instruction, context) => {\n      // If there are tools in instruction, add an extra step of preparing\n      // information via tools.\n      if (toolManager.hasTools()) {\n        const gatheringInformation = await gatheringRequest(\n          context,\n          instruction,\n          toolManager\n        ).invoke();\n        if (!ok(gatheringInformation)) return gatheringInformation;\n        context.push(...gatheringInformation.all);\n      }\n\n      const refImages = extractMediaData([instruction]);\n      const refText = instruction\n        ? toLLMContent(toTextConcat(extractTextData([instruction])))\n        : toLLMContent(\"\");\n      imageContext = imageContext.concat(refImages);\n\n      let retryCount = MAX_RETRIES;\n\n      while (retryCount--) {\n        // Image editing case.\n        if (imageContext.length > 0) {\n          console.log(\"Step has reference image, using Gemini Image API: i2i\");\n          const instructionText = refText ? toText(refText) : \"\";\n          const combinedInstruction = toTextConcat(\n            joinContent(instructionText, textContext, false)\n          ).trim();\n          if (!combinedInstruction) {\n            return err(\n              `An image editing instruction must be provided along side the reference image.`\n            );\n          }\n          const finalInstruction =\n            combinedInstruction + \"\\nAspect ratio: \" + aspectRatio;\n          console.log(\"PROMPT: \" + finalInstruction);\n          const generatedImage = await callGeminiImage(\n            finalInstruction,\n            imageContext,\n            disablePromptRewrite,\n            aspectRatio\n          );\n          if (!ok(generatedImage)) return generatedImage;\n          return mergeContent(generatedImage, \"model\");\n        } else {\n          console.log(\"Step has text only, using Gemini Image API: t2i\");\n          let imagePrompt: LLMContent;\n          if (disablePromptRewrite) {\n            imagePrompt = toLLMContent(toText(addUserTurn(refText, context)));\n          } else {\n            const generatingPrompt = await promptExpander(\n              context,\n              refText\n            ).invoke();\n            if (!ok(generatingPrompt)) return generatingPrompt;\n            imagePrompt = generatingPrompt.last;\n          }\n          const iPrompt = toText(imagePrompt).trim();\n          console.log(\"PROMPT\", iPrompt);\n          const generatedImage = await callGeminiImage(\n            iPrompt,\n            [],\n            disablePromptRewrite,\n            aspectRatio\n          );\n          if (!ok(generatedImage)) return generatedImage;\n          return mergeContent(generatedImage, \"model\");\n        }\n      }\n      return gracefulExit(\n        err(`Failed to generate an image after ${MAX_RETRIES} tries.`)\n      );\n    }\n  );\n\n  if (!ok(fanningOut)) return fanningOut;\n  return { context: fanningOut };\n}\n\ntype DescribeInputs = {\n  inputs: {\n    instruction?: LLMContent;\n  };\n};\n\nasync function describe({ inputs: { instruction } }: DescribeInputs) {\n  const template = new Template(instruction);\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n          behavior: [\"main-port\"],\n        },\n        instruction: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\", \"hint-preview\"],\n          title: \"Instruction\",\n          description:\n            \"Describe how to generate the image (content, style, etc). Use @ to reference params or outputs from other steps.\",\n          default: defaultLLMContent(),\n        },\n        \"p-disable-prompt-rewrite\": {\n          type: \"boolean\",\n          title: \"Disable prompt expansion\",\n          behavior: [\"config\"],\n          description:\n            \"By default, inputs and instructions will be automatically expanded into a high quality image prompt. Check to disable this re-writing behavior.\",\n        },\n        \"p-aspect-ratio\": {\n          type: \"string\",\n          behavior: [\"hint-text\", \"config\"],\n          title: \"Aspect Ratio\",\n          enum: ASPECT_RATIOS,\n          description: \"The aspect ratio of the generated image\",\n          default: \"1:1\",\n        },\n        ...template.schemas(),\n      },\n      behavior: [\"at-wireable\"],\n      ...template.requireds(),\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context out\",\n          behavior: [\"hint-image\", \"main-port\"],\n        },\n      },\n    } satisfies Schema,\n    title: \"Edit Image\",\n    metadata: {\n      icon: MAKE_IMAGE_ICON,\n      tags: [\"quick-access\", \"generative\"],\n      order: 2,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Generates an image using supplied context (generation and editing).",
        "runnable": true
      }
    },
    "introducer": {
      "code": "/**\n * @fileoverview Handles introduction of the step.\n */\nimport { defaultSafetySettings } from \"./gemini\";\nimport { toLLMContent, toText, ok, err, llm } from \"./utils\";\nimport { ToolManager } from \"./tool-manager\";\nimport { GeminiPrompt } from \"./gemini-prompt\";\nimport {} from \"./common\";\nexport { Introducer, ArgumentNameGenerator };\nfunction introductionSchema() {\n    return {\n        type: \"object\",\n        properties: {\n            title: {\n                type: \"string\",\n                description: \"The title of the agent\",\n            },\n            abilities: {\n                type: \"string\",\n                description: \"Verb-first, third-person summary of the agent's abilities\",\n            },\n            argument: {\n                type: \"string\",\n                description: \"The description of the single text argument that the agent takes as input\",\n            },\n        },\n    };\n}\n/**\n * Attempts to adjust the describer result for subgraphs.\n * Accounts for LLMContent[] `context` property\n * and parameters\n */\nclass ArgumentNameGenerator {\n    #containsContext(describerResult) {\n        if (!describerResult.inputSchema?.properties)\n            return true;\n        const context = describerResult.inputSchema.properties[\"context\"];\n        if (!context)\n            return false;\n        if (context.type === \"array\" && context.items) {\n            return !!context.items.behavior?.includes(\"llm-content\");\n        }\n        return false;\n    }\n    async transform(describerResult) {\n        // If there's no `context` property, exit early.\n        if (!this.#containsContext(describerResult)) {\n            return null;\n        }\n        const { title, description } = describerResult;\n        // Fail transform when there's no title or description.\n        // The resulting function declaration will be a dud anyway.\n        if (!title || !description) {\n            return err(`Custom tool must have a title and a description`);\n        }\n        // Add parameters to the describer.\n        const required = [];\n        const params = Object.fromEntries(Object.entries(describerResult.inputSchema?.properties || {})\n            .filter(([name]) => {\n            if (name === \"context\")\n                return false;\n            required.push(name);\n            return true;\n        })\n            .map(([name, value]) => {\n            return [\n                name,\n                {\n                    ...value,\n                    type: \"string\",\n                },\n            ];\n        }));\n        if (required.length > 0) {\n            return {\n                ...describerResult,\n                inputSchema: {\n                    type: \"object\",\n                    properties: params,\n                    required,\n                },\n            };\n        }\n        // When no parameters found, try to discern the parameter name\n        // from description and title.\n        const naming = await new GeminiPrompt({\n            body: {\n                contents: [this.prompt(describerResult)],\n                safetySettings: defaultSafetySettings(),\n                generationConfig: {\n                    responseSchema: this.schema(),\n                    responseMimeType: \"application/json\",\n                },\n            },\n        }).invoke();\n        if (!ok(naming))\n            return naming;\n        const result = naming.last.parts.at(0).json;\n        return {\n            ...describerResult,\n            inputSchema: {\n                type: \"object\",\n                properties: {\n                    context: {\n                        type: \"string\",\n                        description: result.description,\n                    },\n                },\n            },\n        };\n    }\n    schema() {\n        return {\n            type: \"object\",\n            properties: {\n                description: {\n                    type: \"string\",\n                    description: \"One-sentence description of a function argument\",\n                },\n            },\n        };\n    }\n    prompt(describerResult) {\n        return llm `\nYou are amazing at describing things. Today, you will be coming up a one-sentence description \nof a function argument.\n\nThe function's title is: ${describerResult.title}\n\nThe function's description is ${describerResult.description}\n\nIt takes a single argument.\n\nCome up with a one-sentence description of this argument based on the title/description,\nwith the aim of using this description in a JSON Schema.\n`.asContent();\n    }\n}\nclass Introducer {\n    instruction;\n    toolManager;\n    constructor(instruction, toolManager) {\n        this.instruction = instruction;\n        this.toolManager = toolManager;\n    }\n    prompt() {\n        const tools = this.toolManager?.list() || [];\n        let toolInstruction = \"You have no access to tools of any kind.\";\n        if (tools.length > 0) {\n            toolInstruction = `You have access to the following tools:\n${tools.map((tool) => JSON.stringify(tool)).join(\"\\n\\n\")}\nMake sure to mention your ability to call these particular tools in your abilities.\n`;\n        }\n        return toLLMContent(`You are a project manager for team of AI agents.\nRight now, your job is to summarize what one AI agent does.\nGiven an agent prompt, you distill it to a brief summary of abilities.\nThis summary will be used to decide when to invoke this agent.\n# AI Agent Prompt\n\\`\\`\\`\n${toText(this.instruction)}\n${toolInstruction}\n\\`\\`\\`\nReply in JSON using the provided schema.\n`);\n    }\n    async invoke() {\n        const introducing = await new GeminiPrompt({\n            body: {\n                contents: [this.prompt()],\n                safetySettings: defaultSafetySettings(),\n                generationConfig: {\n                    responseSchema: introductionSchema(),\n                    responseMimeType: \"application/json\",\n                },\n            },\n        }).invoke();\n        if (!ok(introducing))\n            return introducing;\n        const intro = introducing.last.parts.at(0)\n            .json;\n        return {\n            title: intro.title,\n            description: intro.abilities,\n            inputSchema: {\n                type: \"object\",\n                properties: {\n                    context: {\n                        type: \"string\",\n                        description: intro.argument,\n                    },\n                },\n            },\n        };\n    }\n}\n",
      "metadata": {
        "title": "introducer",
        "source": {
          "code": "/**\n * @fileoverview Handles introduction of the step.\n */\n\nimport { type Tool, defaultSafetySettings, type GeminiSchema } from \"./gemini\";\nimport { toLLMContent, toText, ok, err, llm } from \"./utils\";\nimport { ToolManager } from \"./tool-manager\";\nimport { GeminiPrompt } from \"./gemini-prompt\";\nimport {\n  type DescriberResult,\n  type DescriberResultTransformer,\n} from \"./common\";\n\nexport { Introducer, ArgumentNameGenerator };\n\nexport type IntroPort = {\n  $intro: boolean;\n};\n\ntype Introduction = {\n  title: string;\n  abilities: string;\n  argument: string;\n};\n\nfunction introductionSchema(): GeminiSchema {\n  return {\n    type: \"object\",\n    properties: {\n      title: {\n        type: \"string\",\n        description: \"The title of the agent\",\n      },\n      abilities: {\n        type: \"string\",\n        description:\n          \"Verb-first, third-person summary of the agent's abilities\",\n      },\n      argument: {\n        type: \"string\",\n        description:\n          \"The description of the single text argument that the agent takes as input\",\n      },\n    },\n  };\n}\n\ntype NamingResult = {\n  description: string;\n};\n\n/**\n * Attempts to adjust the describer result for subgraphs.\n * Accounts for LLMContent[] `context` property\n * and parameters\n */\nclass ArgumentNameGenerator implements DescriberResultTransformer {\n  #containsContext(describerResult: DescriberResult): boolean {\n    if (!describerResult.inputSchema?.properties) return true;\n    const context = describerResult.inputSchema.properties[\"context\"];\n    if (!context) return false;\n    if (context.type === \"array\" && context.items) {\n      return !!(context.items as Schema).behavior?.includes(\"llm-content\");\n    }\n    return false;\n  }\n\n  async transform(\n    describerResult: DescriberResult\n  ): Promise<Outcome<DescriberResult | null>> {\n    // If there's no `context` property, exit early.\n    if (!this.#containsContext(describerResult)) {\n      return null;\n    }\n    const { title, description } = describerResult;\n\n    // Fail transform when there's no title or description.\n    // The resulting function declaration will be a dud anyway.\n    if (!title || !description) {\n      return err(`Custom tool must have a title and a description`);\n    }\n\n    // Add parameters to the describer.\n    const required: string[] = [];\n    const params = Object.fromEntries(\n      Object.entries(describerResult.inputSchema?.properties || {})\n        .filter(([name]) => {\n          if (name === \"context\") return false;\n          required.push(name);\n          return true;\n        })\n        .map(([name, value]) => {\n          return [\n            name,\n            {\n              ...value,\n              type: \"string\",\n            },\n          ];\n        })\n    );\n    if (required.length > 0) {\n      return {\n        ...describerResult,\n        inputSchema: {\n          type: \"object\",\n          properties: params,\n          required,\n        },\n      };\n    }\n\n    // When no parameters found, try to discern the parameter name\n    // from description and title.\n    const naming = await new GeminiPrompt({\n      body: {\n        contents: [this.prompt(describerResult)],\n        safetySettings: defaultSafetySettings(),\n        generationConfig: {\n          responseSchema: this.schema(),\n          responseMimeType: \"application/json\",\n        },\n      },\n    }).invoke();\n    if (!ok(naming)) return naming;\n    const result = (naming.last.parts.at(0) as JSONPart).json as NamingResult;\n\n    return {\n      ...describerResult,\n      inputSchema: {\n        type: \"object\",\n        properties: {\n          context: {\n            type: \"string\",\n            description: result.description,\n          },\n        },\n      },\n    };\n  }\n\n  schema(): GeminiSchema {\n    return {\n      type: \"object\",\n      properties: {\n        description: {\n          type: \"string\",\n          description: \"One-sentence description of a function argument\",\n        },\n      },\n    };\n  }\n\n  prompt(describerResult: DescriberResult): LLMContent {\n    return llm`\nYou are amazing at describing things. Today, you will be coming up a one-sentence description \nof a function argument.\n\nThe function's title is: ${describerResult.title}\n\nThe function's description is ${describerResult.description}\n\nIt takes a single argument.\n\nCome up with a one-sentence description of this argument based on the title/description,\nwith the aim of using this description in a JSON Schema.\n`.asContent();\n  }\n}\n\nclass Introducer {\n  constructor(\n    public readonly instruction: LLMContent,\n    public readonly toolManager?: ToolManager\n  ) {}\n\n  prompt(): LLMContent {\n    const tools = this.toolManager?.list() || [];\n    let toolInstruction = \"You have no access to tools of any kind.\";\n    if (tools.length > 0) {\n      toolInstruction = `You have access to the following tools:\n${tools.map((tool) => JSON.stringify(tool)).join(\"\\n\\n\")}\nMake sure to mention your ability to call these particular tools in your abilities.\n`;\n    }\n    return toLLMContent(`You are a project manager for team of AI agents.\nRight now, your job is to summarize what one AI agent does.\nGiven an agent prompt, you distill it to a brief summary of abilities.\nThis summary will be used to decide when to invoke this agent.\n# AI Agent Prompt\n\\`\\`\\`\n${toText(this.instruction)}\n${toolInstruction}\n\\`\\`\\`\nReply in JSON using the provided schema.\n`);\n  }\n\n  async invoke(): Promise<Outcome<DescriberResult>> {\n    const introducing = await new GeminiPrompt({\n      body: {\n        contents: [this.prompt()],\n        safetySettings: defaultSafetySettings(),\n        generationConfig: {\n          responseSchema: introductionSchema(),\n          responseMimeType: \"application/json\",\n        },\n      },\n    }).invoke();\n    if (!ok(introducing)) return introducing;\n    const intro = (introducing.last.parts.at(0) as JSONPart)\n      .json as Introduction;\n\n    return {\n      title: intro.title,\n      description: intro.abilities,\n      inputSchema: {\n        type: \"object\",\n        properties: {\n          context: {\n            type: \"string\",\n            description: intro.argument,\n          },\n        },\n      },\n    };\n  }\n}\n",
          "language": "typescript"
        },
        "description": "Handles introduction of the step.",
        "runnable": false
      }
    },
    "html-generator": {
      "code": "/**\n * @fileoverview Utility for calling generate_webpage tool.\n */\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\nimport { err, ok, toLLMContent, toLLMContentInline, toText } from \"./utils\";\nimport { executeStep } from \"./step-executor\";\nimport { report } from \"./output\";\nexport { callGenWebpage };\nconst OUTPUT_KEY = \"rendered_outputs\";\nfunction base64DecodeNonAsciiStandard(base64String) {\n    const byteString = atob(base64String);\n    let encodedString = \"\";\n    for (let i = 0; i < byteString.length; i++) {\n        encodedString +=\n            \"%\" + byteString.charCodeAt(i).toString(16).padStart(2, \"0\");\n    }\n    return decodeURIComponent(encodedString);\n}\nasync function callGenWebpage(instruction, content, renderMode, modelName) {\n    const executionInputs = {};\n    const inputParameters = [];\n    let i = 0;\n    for (let val of content) {\n        for (let part of val.parts) {\n            i++;\n            if (\"text\" in part) {\n                const key = `text_${i}`;\n                inputParameters.push(key);\n                const encodedText = btoa(unescape(encodeURIComponent(part.text)));\n                executionInputs[key] = {\n                    chunks: [\n                        {\n                            mimetype: \"text/plain\",\n                            data: encodedText,\n                        },\n                    ],\n                };\n            }\n            else if (\"inlineData\" in part) {\n                const key = `media_${i}`;\n                inputParameters.push(key);\n                executionInputs[key] = {\n                    chunks: [\n                        {\n                            mimetype: part.inlineData.mimeType,\n                            data: part.inlineData.data,\n                        },\n                    ],\n                };\n            }\n            else if (\"storedData\" in part) {\n                const key = `media_${i}`;\n                inputParameters.push(key);\n                executionInputs[key] = {\n                    chunks: [\n                        {\n                            mimetype: \"url/\" + part.storedData.mimeType,\n                            data: btoa(unescape(encodeURIComponent(part.storedData.handle))),\n                        },\n                    ],\n                };\n            }\n            else {\n                console.error(\"Skipping unexpected content part\");\n            }\n        }\n    }\n    const body = {\n        planStep: {\n            stepName: \"GenerateWebpage\",\n            modelApi: \"generate_webpage\",\n            inputParameters: inputParameters,\n            systemPrompt: instruction,\n            stepIntent: instruction,\n            output: OUTPUT_KEY,\n            options: {\n                disablePromptRewrite: true,\n                renderMode: renderMode,\n                modelName: modelName,\n            },\n        },\n        execution_inputs: executionInputs,\n    };\n    // Add the contents\n    // TODO(askerryryan): Remove once functional.\n    console.log(\"request body\");\n    console.log(body);\n    const response = await executeStep(body);\n    // TODO(askerryryan): Remove once functional.\n    console.log(\"response\");\n    console.log(response);\n    if (!ok(response)) {\n        let errorMessage;\n        if (response.$error.includes(\"The service is currently unavailable\")) {\n            errorMessage =\n                \"Request timed out. The model may be experiencing heavy load\";\n        }\n        else {\n            errorMessage = response.$error;\n        }\n        return err(\"Webpage generation failed: \" + errorMessage);\n    }\n    let returnVal;\n    let outputChunk = response.executionOutputs[OUTPUT_KEY];\n    if (!outputChunk) {\n        return err(\"Error: Malformed response. No page generated.\");\n    }\n    const mimetype = outputChunk.chunks[0].mimetype;\n    const base64Data = outputChunk.chunks[0].data;\n    const data = base64DecodeNonAsciiStandard(base64Data);\n    if (mimetype == \"text/html\") {\n        returnVal = toLLMContentInline(mimetype, data);\n    }\n    else {\n        returnVal = toLLMContent(data);\n    }\n    if (!returnVal) {\n        return err(\"Error: No webpage returned from backend\");\n    }\n    return returnVal;\n}\n",
      "metadata": {
        "title": "html-generator",
        "source": {
          "code": "/**\n * @fileoverview Utility for calling generate_webpage tool.\n */\n\nimport fetch from \"@fetch\";\nimport secrets from \"@secrets\";\n\nimport { err, ok, toLLMContent, toLLMContentInline, toText } from \"./utils\";\nimport { executeStep } from \"./step-executor\";\nimport type { ExecuteStepRequest, Content, ContentMap } from \"./step-executor\";\nimport { report } from \"./output\";\n\nexport { callGenWebpage };\n\nconst OUTPUT_KEY = \"rendered_outputs\";\n\nfunction base64DecodeNonAsciiStandard(base64String: string) {\n  const byteString = atob(base64String);\n  let encodedString = \"\";\n  for (let i = 0; i < byteString.length; i++) {\n    encodedString +=\n      \"%\" + byteString.charCodeAt(i).toString(16).padStart(2, \"0\");\n  }\n  return decodeURIComponent(encodedString);\n}\n\nasync function callGenWebpage(\n  instruction: string,\n  content: LLMContent[],\n  renderMode: string,\n  modelName: string\n): Promise<Outcome<LLMContent>> {\n  const executionInputs: ContentMap = {};\n  const inputParameters: string[] = [];\n  let i = 0;\n  for (let val of content) {\n    for (let part of val.parts) {\n      i++;\n      if (\"text\" in part) {\n        const key = `text_${i}`;\n        inputParameters.push(key);\n        const encodedText = btoa(unescape(encodeURIComponent(part.text)));\n        executionInputs[key] = {\n          chunks: [\n            {\n              mimetype: \"text/plain\",\n              data: encodedText,\n            },\n          ],\n        };\n      } else if (\"inlineData\" in part) {\n        const key = `media_${i}`;\n        inputParameters.push(key);\n        executionInputs[key] = {\n          chunks: [\n            {\n              mimetype: part.inlineData.mimeType,\n              data: part.inlineData.data,\n            },\n          ],\n        };\n      } else if (\"storedData\" in part) {\n        const key = `media_${i}`;\n        inputParameters.push(key);\n        executionInputs[key] = {\n          chunks: [\n            {\n              mimetype: \"url/\" + part.storedData.mimeType,\n              data: btoa(unescape(encodeURIComponent(part.storedData.handle))),\n            },\n          ],\n        };\n      } else {\n        console.error(\"Skipping unexpected content part\");\n      }\n    }\n  }\n  const body = {\n    planStep: {\n      stepName: \"GenerateWebpage\",\n      modelApi: \"generate_webpage\",\n      inputParameters: inputParameters,\n      systemPrompt: instruction,\n      stepIntent: instruction,\n      output: OUTPUT_KEY,\n      options: {\n        disablePromptRewrite: true,\n        renderMode: renderMode,\n        modelName: modelName,\n      },\n    },\n    execution_inputs: executionInputs,\n  } satisfies ExecuteStepRequest;\n  // Add the contents\n  // TODO(askerryryan): Remove once functional.\n  console.log(\"request body\");\n  console.log(body);\n  const response = await executeStep(body);\n  // TODO(askerryryan): Remove once functional.\n  console.log(\"response\");\n  console.log(response);\n  if (!ok(response)) {\n    let errorMessage;\n    if (response.$error.includes(\"The service is currently unavailable\")) {\n      errorMessage =\n        \"Request timed out. The model may be experiencing heavy load\";\n    } else {\n      errorMessage = response.$error;\n    }\n    return err(\"Webpage generation failed: \" + errorMessage);\n  }\n\n  let returnVal;\n  let outputChunk = response.executionOutputs[OUTPUT_KEY];\n  if (!outputChunk) {\n    return err(\"Error: Malformed response. No page generated.\");\n  }\n  const mimetype = outputChunk.chunks[0].mimetype;\n  const base64Data = outputChunk.chunks[0].data;\n  const data = base64DecodeNonAsciiStandard(base64Data);\n  if (mimetype == \"text/html\") {\n    returnVal = toLLMContentInline(mimetype, data);\n  } else {\n    returnVal = toLLMContent(data);\n  }\n  if (!returnVal) {\n    return err(\"Error: No webpage returned from backend\");\n  }\n  return returnVal;\n}\n",
          "language": "typescript"
        },
        "description": "Utility for calling generate_webpage tool.",
        "runnable": false
      }
    },
    "render-outputs": {
      "code": "/**\n * @fileoverview Renders multiple outputs into single display.\n */\nimport { Template } from \"./template\";\nimport { ok, err, llm, toText, isEmpty, mergeContent, toLLMContent, } from \"./utils\";\nimport { callGenWebpage } from \"./html-generator\";\nimport { fanOutContext, flattenContext } from \"./lists\";\nimport read from \"@read\";\nexport { invoke as default, describe };\nconst MANUAL_MODE = \"Manual layout\";\nconst AUTO_MODE_LEGACY = \"Webpage with auto-layout\";\nconst FLASH_MODE = \"Webpage with auto-layout by 2.5 Flash\";\nconst PRO_MODE = \"Webpage with auto-layout by 2.5 Pro\";\nfunction defaultSystemInstruction() {\n    return llm `You are a skilled web developer specializing in creating intuitive and visually appealing HTML web pages based on user instructions and data. Your task is to generate a valid HTML webpage that will be rendered in an iframe. The generated code must be valid and functional HTML with JavaScript and CSS embedded inline within <script> and <style> tags respectively. Return only the code, and open the HTML codeblock with the literal string '\\`\\`\\`html'. Render content as a clean, well-structured webpage, paying careful attention to user instructions. Use a responsive or mobile-friendly layout whenever possible and minimize unnecessary padding or margins.`.asContent();\n}\nfunction defaultThemeColors() {\n    return {\n        primaryColor: \"#246db5\",\n        secondaryColor: \"#5cadff\",\n        backgroundColor: \"#ffffff\",\n        textColor: \"#1a1a1a\",\n        primaryTextColor: \"#ffffff\",\n    };\n}\nasync function getThemeColors() {\n    const readingMetadata = await read({ path: \"/env/metadata\" });\n    if (!ok(readingMetadata))\n        return defaultThemeColors();\n    const metadata = readingMetadata.data?.at(0)?.parts?.at(0)\n        ?.json;\n    if (!metadata)\n        return defaultThemeColors();\n    const currentThemeId = metadata?.visual?.presentation?.theme;\n    if (!currentThemeId)\n        return defaultThemeColors();\n    const themeColors = metadata?.visual?.presentation?.themes?.[currentThemeId]?.themeColors;\n    if (!themeColors)\n        return defaultThemeColors();\n    return { ...defaultThemeColors(), ...themeColors };\n}\nfunction themeColorsPrompt(colors) {\n    return `Unless otherwise specified, use the following theme colors:\n\n- primary color: ${colors.primaryColor}\n- secondary color: ${colors.secondaryColor}\n- background color: ${colors.backgroundColor}\n- text color: ${colors.textColor}\n- primary text color: ${colors.primaryTextColor}\n\n`;\n}\nasync function invoke({ text, \"p-render-mode\": renderMode, \"b-system-instruction\": systemInstruction, ...params }) {\n    if (!text) {\n        text = toLLMContent(\"\");\n    }\n    if (!systemInstruction) {\n        systemInstruction = defaultSystemInstruction();\n    }\n    let systemText = toText(systemInstruction);\n    const template = new Template(text);\n    const substituting = await template.substitute(params, async () => \"\");\n    if (!ok(substituting)) {\n        return substituting;\n    }\n    const context = mergeContent(flattenContext([substituting], true, \"\\n\\n\"), \"user\");\n    let modelName = \"\";\n    // TODO(askerryryan): Clean up after backend backwards compatibility window.\n    if (renderMode == MANUAL_MODE) {\n        renderMode = \"Manual\";\n    }\n    else if (renderMode == FLASH_MODE || renderMode == AUTO_MODE_LEGACY) {\n        modelName = \"gemini-2.5-flash-preview-04-17\";\n        renderMode = \"HTML\";\n    }\n    else if (renderMode == PRO_MODE) {\n        modelName = \"gemini-2.5-pro-preview-03-25\";\n        renderMode = \"Interactive\";\n    }\n    else if (!renderMode) {\n        renderMode = \"Manual\";\n    }\n    console.log(\"Rendering mode: \" + renderMode);\n    let out = context;\n    if (renderMode != \"Manual\") {\n        systemText += themeColorsPrompt(await getThemeColors());\n        const webPage = await callGenWebpage(systemText, [context], renderMode, modelName);\n        if (!ok(webPage)) {\n            console.error(\"Failed to generated html output\");\n            return webPage;\n        }\n        else {\n            out = await webPage;\n            console.log(out);\n        }\n    }\n    if (!ok(out))\n        return err(out);\n    return { context: out };\n}\nasync function describe({ inputs: { text } }) {\n    const template = new Template(text);\n    return {\n        inputSchema: {\n            type: \"object\",\n            properties: {\n                text: {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"hint-preview\", \"config\", \"at-wireable\"],\n                    title: \"Outputs to render\",\n                    description: \"Type the @ character to select the outputs to combine. Optionally include style and layout guidlines if using Rendering mode of Markdown or HTML.\",\n                },\n                \"p-render-mode\": {\n                    type: \"string\",\n                    enum: [MANUAL_MODE, FLASH_MODE, PRO_MODE],\n                    title: \"Display format\",\n                    behavior: [\"config\", \"hint-preview\"],\n                    default: MANUAL_MODE,\n                    description: \"Choose how to combine and display the outputs\",\n                },\n                \"b-system-instruction\": {\n                    type: \"object\",\n                    behavior: [\"llm-content\", \"config\", \"hint-advanced\"],\n                    title: \"System Instruction\",\n                    description: \"The system instruction used for auto-layout\",\n                },\n                ...template.schemas(),\n            },\n            behavior: [\"at-wireable\"],\n            ...template.requireds(),\n        },\n        outputSchema: {\n            type: \"object\",\n            properties: {\n                context: {\n                    type: \"array\",\n                    items: {\n                        type: \"object\",\n                        behavior: [\"llm-content\"],\n                    },\n                    title: \"Context out\",\n                    behavior: [\"main-port\", \"hint-multimodal\"],\n                },\n            },\n        },\n        title: \"Display\",\n        metadata: {\n            icon: \"display\",\n            tags: [\"quick-access\", \"core\", \"output\"],\n            order: 100,\n        },\n    };\n}\n",
      "metadata": {
        "title": "render-outputs",
        "source": {
          "code": "/**\n * @fileoverview Renders multiple outputs into single display.\n */\n\nimport { Template } from \"./template\";\nimport {\n  ok,\n  err,\n  llm,\n  toText,\n  isEmpty,\n  mergeContent,\n  toLLMContent,\n} from \"./utils\";\nimport { callGenWebpage } from \"./html-generator\";\nimport { fanOutContext, flattenContext } from \"./lists\";\n\nimport read from \"@read\";\n\nexport { invoke as default, describe };\n\nconst MANUAL_MODE = \"Manual layout\";\nconst AUTO_MODE_LEGACY = \"Webpage with auto-layout\";\nconst FLASH_MODE = \"Webpage with auto-layout by 2.5 Flash\";\nconst PRO_MODE = \"Webpage with auto-layout by 2.5 Pro\";\n\nfunction defaultSystemInstruction(): LLMContent {\n  return llm`You are a skilled web developer specializing in creating intuitive and visually appealing HTML web pages based on user instructions and data. Your task is to generate a valid HTML webpage that will be rendered in an iframe. The generated code must be valid and functional HTML with JavaScript and CSS embedded inline within <script> and <style> tags respectively. Return only the code, and open the HTML codeblock with the literal string '\\`\\`\\`html'. Render content as a clean, well-structured webpage, paying careful attention to user instructions. Use a responsive or mobile-friendly layout whenever possible and minimize unnecessary padding or margins.`.asContent();\n}\n\ntype InvokeInputs = {\n  text?: LLMContent;\n  \"p-render-mode\": string;\n  \"b-system-instruction\"?: LLMContent;\n};\n\ntype DescribeInputs = {\n  inputs: {\n    text?: LLMContent;\n  };\n};\n\ntype GraphMetadata = {\n  title?: string;\n  description?: string;\n  version?: string;\n  url?: string;\n  icon?: string;\n  visual?: {\n    presentation?: Presentation;\n  };\n  userModified?: boolean;\n  tags?: string[];\n  comments: Comment[];\n};\n\ntype Comment = {\n  id: string;\n  text: string;\n  metadata: {\n    title: string;\n    visual: {\n      x: number;\n      y: number;\n      collapsed: \"expanded\";\n      outputHeight: number;\n    };\n  };\n};\n\ntype Presentation = {\n  themes?: Record<string, Theme>;\n  theme?: string;\n};\n\ntype Theme = {\n  themeColors?: ThemeColors;\n  template?: string;\n  splashScreen?: StoredDataCapabilityPart;\n};\n\ntype ThemeColors = {\n  primaryColor?: string;\n  secondaryColor?: string;\n  backgroundColor?: string;\n  textColor?: string;\n  primaryTextColor?: string;\n};\n\nfunction defaultThemeColors(): ThemeColors {\n  return {\n    primaryColor: \"#246db5\",\n    secondaryColor: \"#5cadff\",\n    backgroundColor: \"#ffffff\",\n    textColor: \"#1a1a1a\",\n    primaryTextColor: \"#ffffff\",\n  };\n}\n\nasync function getThemeColors(): Promise<ThemeColors> {\n  const readingMetadata = await read({ path: \"/env/metadata\" });\n  if (!ok(readingMetadata)) return defaultThemeColors();\n  const metadata = (readingMetadata.data?.at(0)?.parts?.at(0) as JSONPart)\n    ?.json as GraphMetadata;\n  if (!metadata) return defaultThemeColors();\n  const currentThemeId = metadata?.visual?.presentation?.theme;\n  if (!currentThemeId) return defaultThemeColors();\n  const themeColors =\n    metadata?.visual?.presentation?.themes?.[currentThemeId]?.themeColors;\n  if (!themeColors) return defaultThemeColors();\n  return { ...defaultThemeColors(), ...themeColors };\n}\n\nfunction themeColorsPrompt(colors: ThemeColors): string {\n  return `Unless otherwise specified, use the following theme colors:\n\n- primary color: ${colors.primaryColor}\n- secondary color: ${colors.secondaryColor}\n- background color: ${colors.backgroundColor}\n- text color: ${colors.textColor}\n- primary text color: ${colors.primaryTextColor}\n\n`;\n}\n\nasync function invoke({\n  text,\n  \"p-render-mode\": renderMode,\n  \"b-system-instruction\": systemInstruction,\n  ...params\n}: InvokeInputs) {\n  if (!text) {\n    text = toLLMContent(\"\");\n  }\n  if (!systemInstruction) {\n    systemInstruction = defaultSystemInstruction();\n  }\n  let systemText = toText(systemInstruction);\n  const template = new Template(text);\n  const substituting = await template.substitute(params, async () => \"\");\n  if (!ok(substituting)) {\n    return substituting;\n  }\n\n  const context = mergeContent(\n    flattenContext([substituting], true, \"\\n\\n\"),\n    \"user\"\n  );\n  let modelName = \"\";\n  // TODO(askerryryan): Clean up after backend backwards compatibility window.\n  if (renderMode == MANUAL_MODE) {\n    renderMode = \"Manual\";\n  } else if (renderMode == FLASH_MODE || renderMode == AUTO_MODE_LEGACY) {\n    modelName = \"gemini-2.5-flash-preview-04-17\";\n    renderMode = \"HTML\";\n  } else if (renderMode == PRO_MODE) {\n    modelName = \"gemini-2.5-pro-preview-03-25\";\n    renderMode = \"Interactive\";\n  } else if (!renderMode) {\n    renderMode = \"Manual\";\n  }\n  console.log(\"Rendering mode: \" + renderMode);\n  let out = context;\n  if (renderMode != \"Manual\") {\n    systemText += themeColorsPrompt(await getThemeColors());\n    const webPage = await callGenWebpage(\n      systemText,\n      [context],\n      renderMode,\n      modelName\n    );\n    if (!ok(webPage)) {\n      console.error(\"Failed to generated html output\");\n      return webPage;\n    } else {\n      out = await webPage;\n      console.log(out);\n    }\n  }\n  if (!ok(out)) return err(out);\n  return { context: out };\n}\n\nasync function describe({ inputs: { text } }: DescribeInputs) {\n  const template = new Template(text);\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        text: {\n          type: \"object\",\n          behavior: [\"llm-content\", \"hint-preview\", \"config\", \"at-wireable\"],\n          title: \"Outputs to render\",\n          description:\n            \"Type the @ character to select the outputs to combine. Optionally include style and layout guidlines if using Rendering mode of Markdown or HTML.\",\n        },\n        \"p-render-mode\": {\n          type: \"string\",\n          enum: [MANUAL_MODE, FLASH_MODE, PRO_MODE],\n          title: \"Display format\",\n          behavior: [\"config\", \"hint-preview\"],\n          default: MANUAL_MODE,\n          description: \"Choose how to combine and display the outputs\",\n        },\n        \"b-system-instruction\": {\n          type: \"object\",\n          behavior: [\"llm-content\", \"config\", \"hint-advanced\"],\n          title: \"System Instruction\",\n          description: \"The system instruction used for auto-layout\",\n        },\n        ...template.schemas(),\n      },\n      behavior: [\"at-wireable\"],\n      ...template.requireds(),\n    } satisfies Schema,\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        context: {\n          type: \"array\",\n          items: {\n            type: \"object\",\n            behavior: [\"llm-content\"],\n          },\n          title: \"Context out\",\n          behavior: [\"main-port\", \"hint-multimodal\"],\n        },\n      },\n    } satisfies Schema,\n    title: \"Display\",\n    metadata: {\n      icon: \"display\",\n      tags: [\"quick-access\", \"core\", \"output\"],\n      order: 100,\n    },\n  };\n}\n",
          "language": "typescript"
        },
        "description": "Renders multiple outputs into single display.",
        "runnable": true
      }
    },
    "lists": {
      "code": "/**\n * @fileoverview Handles lists according to https://github.com/breadboard-ai/breadboard/wiki/Step-Listification\n */\nimport { ok, err, generateId, mergeTextParts, toLLMContent, llm, } from \"./utils\";\nimport {} from \"./gemini\";\nexport { fanOutContext, flattenContext, hasLists, addContent, toList, listPrompt, listSchema, ListExpander, };\nfunction isListPart(o) {\n    return !!o && \"list\" in o;\n}\nfunction emptyContent() {\n    return { parts: [{ text: \"\" }] };\n}\nfunction addContent(context, content) {\n    const last = context.at(-1);\n    const maybeList = last?.parts?.at(0);\n    const remainder = context.slice(0, -1);\n    if (isListPart(maybeList)) {\n        const list = maybeList.list.map((item) => ({\n            ...item,\n            content: [...item.content, content],\n        }));\n        return [...remainder, { ...last, parts: [{ ...maybeList, list }] }];\n    }\n    return [...context, content];\n}\nfunction unzipContent(content) {\n    // 1) Scan content for lists.\n    const info = new Set();\n    const ids = new Set();\n    let maxLength = 0;\n    content.parts.forEach((part, index) => {\n        if (!isListPart(part))\n            return;\n        const length = part.list.length;\n        if (length > maxLength)\n            maxLength = length;\n        info.add(index);\n        ids.add(part.id);\n    });\n    if (ids.size > 1) {\n        console.warn(\"Multiple list sources aren't yet supported in instruction, using the first one\");\n    }\n    const id = [...ids].at(0) || \"\";\n    if (info.size === 0) {\n        return { contents: [content], id };\n    }\n    // 2) Create a list and replace lists with list entries.\n    return {\n        id,\n        contents: new Array(maxLength).fill(0).map((_, entryIndex) => {\n            const parts = mergeTextParts(content.parts.flatMap((part, partIndex) => {\n                if (!info.has(partIndex))\n                    return part;\n                // We know this exists, so we're ok with not checking\n                // for existence.\n                const item = part.list.at(entryIndex);\n                if (!item) {\n                    return [];\n                }\n                const last = item.content.at(-1);\n                return last ? last.parts : [];\n            }));\n            return { ...content, parts };\n        }),\n    };\n}\nfunction hasLists(context) {\n    return isListPart(context.at(-1)?.parts?.at(0));\n}\n// TODO: When back to implementing nested lists, this approach\n// is wrong. Instead of expanding items and then converging them,\n// we need to iterate over existing structure. Because then, when\n// it's time to converge, we don't need to try to reconstruct\n// the original structure.\n// Also, it is unclear what we should do with nested lists\n// in instruction. It feels like a similar story, and now\n// we need to support multiple lists and hierarchies in\n// instruction.\nclass ListExpander {\n    instruction;\n    context;\n    prolog;\n    // Local prolog -- the parts of context that were preceding the list.\n    #prolog = [];\n    #list = [];\n    #originalListItems = [];\n    #id = \"\";\n    #instructions;\n    #expanded = false;\n    constructor(instruction, context, prolog = []) {\n        this.instruction = instruction;\n        this.context = context;\n        this.prolog = prolog;\n    }\n    expand() {\n        if (this.#expanded)\n            return;\n        const instructions = unzipContent(this.instruction);\n        let list = [];\n        let id;\n        const maybeList = this.context.at(-1)?.parts?.at(0);\n        const localProlog = this.context.slice(0, -1);\n        const originalListItems = [];\n        if (isListPart(maybeList)) {\n            id = maybeList.id;\n            // console.log(\"LIST PART FOUND\", id);\n            for (const [index, item] of maybeList.list.entries()) {\n                const innerContext = item.content;\n                const innerInstruction = instructions.contents.at(index) ||\n                    instructions.contents.at(0) ||\n                    emptyContent();\n                const innerExpander = new ListExpander(innerInstruction, innerContext, [\n                    ...this.prolog,\n                    ...localProlog,\n                ]);\n                innerExpander.expand();\n                // console.log(\"INDEX\", index);\n                // console.log(\"ITEM\", JSON.stringify(item.content));\n                // console.log(\"INNER EXPANDER\", innerExpander.list());\n                originalListItems.push(innerContext);\n                list.push(...innerExpander.list());\n            }\n            // console.log(\"END LIST PART FOUND\", id);\n        }\n        else {\n            id = instructions.id;\n            list = instructions.contents.map((instruction) => {\n                return {\n                    prolog: [...this.prolog, ...this.context],\n                    instruction,\n                    context: [],\n                };\n            });\n        }\n        this.#prolog = localProlog;\n        this.#list = list;\n        this.#id = id;\n        this.#originalListItems = originalListItems;\n        this.#expanded = true;\n    }\n    list() {\n        return this.#list;\n    }\n    async map(transformer) {\n        this.expand();\n        const isList = this.#list.length > 1;\n        const results = await Promise.all(this.#list.map(async (item, index) => {\n            return transformer(item.instruction, [...item.prolog, ...item.context], isList);\n        }));\n        const errors = [];\n        const successes = [];\n        results.forEach((result) => {\n            if (!ok(result)) {\n                errors.push(result);\n            }\n            else {\n                successes.push(result);\n            }\n        });\n        if (errors.length > 0) {\n            return err(errors.map((error) => error.$error).join(\"\\n\"));\n        }\n        return this.#toContext(successes);\n    }\n    #toContext(results) {\n        if (results.length > 1) {\n            const newListItem = {\n                parts: [\n                    {\n                        id: this.#id,\n                        list: results.map((item, i) => ({\n                            content: [...(this.#originalListItems[i] || []), item],\n                        })),\n                    },\n                ],\n            };\n            return [...this.#prolog, newListItem];\n        }\n        const newContextItem = results.at(-1);\n        return [...this.context, newContextItem];\n    }\n}\nasync function fanOutContext(instruction, context, transformer, path) {\n    context ??= [];\n    const expander = new ListExpander(instruction, context);\n    return expander.map(transformer);\n}\nfunction flattenContext(context, all = false, separator = \"\") {\n    context ??= []; // Look at the first part of the last context and see if it's a list.\n    const last = context.at(-1);\n    if (!last)\n        return context;\n    if (all) {\n        return context\n            .map((content) => flattenContent(content, all, separator))\n            .flat();\n    }\n    const remainder = context.slice(0, -1);\n    return [...remainder, ...flattenContent(last, all, separator)];\n}\nfunction zipContexts(contexts, separator = \"\") {\n    let maxLength = 0;\n    contexts.forEach((context) => {\n        if (maxLength < context.length)\n            maxLength = context.length;\n    });\n    const result = [];\n    for (let i = 0; i < maxLength; i++) {\n        let role;\n        let zippedParts = [];\n        for (const context of contexts) {\n            const item = context.at(i);\n            if (!item)\n                continue;\n            if (!role)\n                role = item.role;\n            zippedParts.push(item.parts);\n            // Add separator if previous element was text.\n            const lastItem = item.parts.slice(-1)[0];\n            if (separator.length > 0 && lastItem && \"text\" in lastItem) {\n                zippedParts.push({ text: separator });\n            }\n        }\n        const parts = mergeTextParts(zippedParts.flat());\n        role ??= \"user\";\n        result.push({\n            parts,\n            role,\n        });\n    }\n    return result;\n}\nfunction flattenContent(content, all = false, separatator = \"\") {\n    let hadList = false;\n    const flattened = content.parts\n        .map((part) => {\n        if (isListPart(part)) {\n            hadList = true;\n            return zipContexts(part.list.map((item) => item.content), separatator);\n        }\n        return {\n            parts: [part],\n            role: content.role,\n        };\n    })\n        .flat();\n    if (!hadList)\n        return [content];\n    return flattened;\n}\nfunction toList(content) {\n    const jsonPart = content.parts.at(0);\n    if (!jsonPart || !(\"json\" in jsonPart)) {\n        // TODO: Error recovery\n        return err(`Gemini generated invalid list`);\n    }\n    const response = jsonPart.json;\n    return {\n        parts: [\n            {\n                id: generateId(),\n                list: response.list.map((item) => {\n                    return { content: [toLLMContent(item, \"model\")] };\n                }),\n            },\n        ],\n    };\n}\nfunction listSchema() {\n    return {\n        type: \"object\",\n        properties: {\n            list: {\n                type: \"array\",\n                description: \"The list of results\",\n                items: {\n                    type: \"string\",\n                    description: \"Result list item as markdown text\",\n                },\n            },\n        },\n        required: [\"list\"],\n    };\n}\nfunction listPrompt(content) {\n    return llm `\n  ${content}\n\n  Output as a list of items, each item must be markdown text.\n`.asContent();\n}\n",
      "metadata": {
        "title": "lists",
        "source": {
          "code": "/**\n * @fileoverview Handles lists according to https://github.com/breadboard-ai/breadboard/wiki/Step-Listification\n */\nimport {\n  ok,\n  err,\n  generateId,\n  mergeTextParts,\n  toLLMContent,\n  llm,\n} from \"./utils\";\nimport { type GeminiSchema } from \"./gemini\";\n\nexport {\n  fanOutContext,\n  flattenContext,\n  hasLists,\n  addContent,\n  toList,\n  listPrompt,\n  listSchema,\n  ListExpander,\n};\n\ntype ContentTransformer = (\n  instruction: LLMContent,\n  context: LLMContent[],\n  isList: boolean\n) => Promise<Outcome<LLMContent>>;\n\ntype UnzippedResult = {\n  contents: LLMContent[];\n  id: string;\n};\n\nfunction isListPart(o: DataPart | undefined): o is ListPart {\n  return !!o && \"list\" in o;\n}\n\nfunction emptyContent(): LLMContent {\n  return { parts: [{ text: \"\" }] };\n}\n\nfunction addContent(context: LLMContent[], content: LLMContent): LLMContent[] {\n  const last = context.at(-1);\n  const maybeList = last?.parts?.at(0);\n  const remainder = context.slice(0, -1);\n  if (isListPart(maybeList)) {\n    const list = maybeList.list.map((item) => ({\n      ...item,\n      content: [...item.content, content],\n    }));\n    return [...remainder, { ...last, parts: [{ ...maybeList, list }] }];\n  }\n  return [...context, content];\n}\n\nfunction unzipContent(content: LLMContent): UnzippedResult {\n  // 1) Scan content for lists.\n  const info: Set<number> = new Set();\n  const ids: Set<string> = new Set();\n  let maxLength = 0;\n  content.parts.forEach((part, index) => {\n    if (!isListPart(part)) return;\n    const length = part.list.length;\n    if (length > maxLength) maxLength = length;\n    info.add(index);\n    ids.add(part.id);\n  });\n  if (ids.size > 1) {\n    console.warn(\n      \"Multiple list sources aren't yet supported in instruction, using the first one\"\n    );\n  }\n  const id = [...ids].at(0) || \"\";\n  if (info.size === 0) {\n    return { contents: [content], id };\n  }\n  // 2) Create a list and replace lists with list entries.\n  return {\n    id,\n    contents: new Array(maxLength).fill(0).map((_, entryIndex) => {\n      const parts = mergeTextParts(\n        content.parts.flatMap((part, partIndex) => {\n          if (!info.has(partIndex)) return part;\n\n          // We know this exists, so we're ok with not checking\n          // for existence.\n          const item = (part as ListPart).list.at(entryIndex);\n          if (!item) {\n            return [];\n          }\n          const last = item.content.at(-1);\n          return last ? last.parts : [];\n        })\n      );\n      return { ...content, parts };\n    }),\n  };\n}\n\nfunction hasLists(context: LLMContent[]): boolean {\n  return isListPart(context.at(-1)?.parts?.at(0));\n}\n\n/**\n * All expander list items have the same shape:\n * - the prolog is the \"pre-list\" part of the context\n * - the context is the current context that was expanded\n * - the instruction is the current instruction is associated\n *   with the current context.\n */\ntype ExpanderListItem = {\n  prolog: LLMContent[];\n  instruction: LLMContent;\n  context: LLMContent[];\n};\n\n// TODO: When back to implementing nested lists, this approach\n// is wrong. Instead of expanding items and then converging them,\n// we need to iterate over existing structure. Because then, when\n// it's time to converge, we don't need to try to reconstruct\n// the original structure.\n// Also, it is unclear what we should do with nested lists\n// in instruction. It feels like a similar story, and now\n// we need to support multiple lists and hierarchies in\n// instruction.\nclass ListExpander {\n  // Local prolog -- the parts of context that were preceding the list.\n  #prolog: LLMContent[] = [];\n  #list: ExpanderListItem[] = [];\n  #originalListItems: LLMContent[][] = [];\n  #id: string = \"\";\n  #instructions?: UnzippedResult;\n  #expanded = false;\n\n  constructor(\n    private readonly instruction: LLMContent,\n    private readonly context: LLMContent[],\n    private readonly prolog: LLMContent[] = []\n  ) {}\n\n  expand(): void {\n    if (this.#expanded) return;\n    const instructions = unzipContent(this.instruction);\n    let list: ExpanderListItem[] = [];\n    let id: string;\n    const maybeList = this.context.at(-1)?.parts?.at(0);\n    const localProlog = this.context.slice(0, -1);\n    const originalListItems: LLMContent[][] = [];\n    if (isListPart(maybeList)) {\n      id = maybeList.id;\n      // console.log(\"LIST PART FOUND\", id);\n      for (const [index, item] of maybeList.list.entries()) {\n        const innerContext = item.content;\n        const innerInstruction =\n          instructions.contents.at(index) ||\n          instructions.contents.at(0) ||\n          emptyContent();\n        const innerExpander = new ListExpander(innerInstruction, innerContext, [\n          ...this.prolog,\n          ...localProlog,\n        ]);\n        innerExpander.expand();\n\n        // console.log(\"INDEX\", index);\n        // console.log(\"ITEM\", JSON.stringify(item.content));\n        // console.log(\"INNER EXPANDER\", innerExpander.list());\n        originalListItems.push(innerContext);\n        list.push(...innerExpander.list());\n      }\n      // console.log(\"END LIST PART FOUND\", id);\n    } else {\n      id = instructions.id;\n      list = instructions.contents.map((instruction) => {\n        return {\n          prolog: [...this.prolog, ...this.context],\n          instruction,\n          context: [],\n        };\n      });\n    }\n    this.#prolog = localProlog;\n    this.#list = list;\n    this.#id = id;\n    this.#originalListItems = originalListItems;\n    this.#expanded = true;\n  }\n\n  list() {\n    return this.#list;\n  }\n\n  async map(transformer: ContentTransformer): Promise<Outcome<LLMContent[]>> {\n    this.expand();\n    const isList = this.#list.length > 1;\n    const results = await Promise.all(\n      this.#list.map(async (item, index) => {\n        return transformer(\n          item.instruction,\n          [...item.prolog, ...item.context],\n          isList\n        );\n      })\n    );\n    const errors: { $error: string }[] = [];\n    const successes: LLMContent[] = [];\n    results.forEach((result) => {\n      if (!ok(result)) {\n        errors.push(result);\n      } else {\n        successes.push(result);\n      }\n    });\n    if (errors.length > 0) {\n      return err(errors.map((error) => error.$error).join(\"\\n\"));\n    }\n    return this.#toContext(successes);\n  }\n\n  #toContext(results: LLMContent[]): LLMContent[] {\n    if (results.length > 1) {\n      const newListItem = {\n        parts: [\n          {\n            id: this.#id,\n            list: (results as LLMContent[]).map((item, i) => ({\n              content: [...(this.#originalListItems[i] || []), item],\n            })),\n          },\n        ],\n      };\n      return [...this.#prolog, newListItem];\n    }\n    const newContextItem = results.at(-1)! as LLMContent;\n    return [...this.context, newContextItem];\n  }\n}\n\nasync function fanOutContext(\n  instruction: LLMContent,\n  context: LLMContent[] | undefined,\n  transformer: ContentTransformer,\n  path?: number[]\n): Promise<Outcome<LLMContent[]>> {\n  context ??= [];\n  const expander = new ListExpander(instruction, context);\n  return expander.map(transformer);\n}\n\nfunction flattenContext(\n  context: LLMContent[] | undefined,\n  all = false,\n  separator = \"\"\n): LLMContent[] {\n  context ??= []; // Look at the first part of the last context and see if it's a list.\n  const last = context.at(-1);\n  if (!last) return context;\n  if (all) {\n    return context\n      .map((content) => flattenContent(content, all, separator))\n      .flat();\n  }\n  const remainder = context.slice(0, -1);\n  return [...remainder, ...flattenContent(last, all, separator)];\n}\n\nfunction zipContexts(\n  contexts: LLMContent[][],\n  separator: string = \"\"\n): LLMContent[] {\n  let maxLength = 0;\n  contexts.forEach((context) => {\n    if (maxLength < context.length) maxLength = context.length;\n  });\n  const result: LLMContent[] = [];\n  for (let i = 0; i < maxLength; i++) {\n    let role: string | undefined;\n    let zippedParts = [];\n    for (const context of contexts) {\n      const item = context.at(i);\n      if (!item) continue;\n      if (!role) role = item.role;\n      zippedParts.push(item.parts);\n      // Add separator if previous element was text.\n      const lastItem = item.parts.slice(-1)[0];\n      if (separator.length > 0 && lastItem && \"text\" in lastItem) {\n        zippedParts.push({ text: separator });\n      }\n    }\n    const parts = mergeTextParts(zippedParts.flat());\n    role ??= \"user\";\n    result.push({\n      parts,\n      role,\n    });\n  }\n  return result;\n}\n\nfunction flattenContent(\n  content: LLMContent,\n  all = false,\n  separatator = \"\"\n): LLMContent[] {\n  let hadList = false;\n  const flattened = content.parts\n    .map((part) => {\n      if (isListPart(part)) {\n        hadList = true;\n        return zipContexts(\n          part.list.map((item) => item.content),\n          separatator\n        );\n      }\n      return {\n        parts: [part],\n        role: content.role,\n      } as LLMContent;\n    })\n    .flat();\n  if (!hadList) return [content];\n  return flattened;\n}\n\ntype ListResponse = {\n  list: string[];\n};\n\nfunction toList(content: LLMContent): Outcome<LLMContent> {\n  const jsonPart = content.parts.at(0);\n  if (!jsonPart || !(\"json\" in jsonPart)) {\n    // TODO: Error recovery\n    return err(`Gemini generated invalid list`);\n  }\n  const response = jsonPart.json as ListResponse;\n  return {\n    parts: [\n      {\n        id: generateId(),\n        list: response.list.map((item) => {\n          return { content: [toLLMContent(item, \"model\")] };\n        }),\n      },\n    ],\n  };\n}\n\nfunction listSchema(): GeminiSchema {\n  return {\n    type: \"object\",\n    properties: {\n      list: {\n        type: \"array\",\n        description: \"The list of results\",\n        items: {\n          type: \"string\",\n          description: \"Result list item as markdown text\",\n        },\n      },\n    },\n    required: [\"list\"],\n  };\n}\n\nfunction listPrompt(content: LLMContent): LLMContent {\n  return llm`\n  ${content}\n\n  Output as a list of items, each item must be markdown text.\n`.asContent();\n}\n",
          "language": "typescript"
        },
        "description": "Handles lists according to https://github.com/breadboard-ai/breadboard/wiki/Step-Listification",
        "runnable": false
      }
    },
    "connector-manager": {
      "code": "/**\n * @fileoverview Manages connectors.\n */\nimport { err, ok, isLLMContentArray } from \"./utils\";\nimport read from \"@read\";\nimport describeConnector, {} from \"@describe\";\nimport invokeConnector from \"@invoke\";\nexport { ConnectorManager, createConfigurator, createTools };\nfunction cx(json) {\n    return { context: [{ parts: [{ json }] }] };\n}\nfunction createTools(handler) {\n    return {\n        invoke: async function (inputs) {\n            const { method, id, info } = inputs;\n            if (method === \"list\") {\n                return handler.list(id, info);\n            }\n            else if (method === \"invoke\") {\n                const { name, args } = inputs;\n                return handler.invoke(id, info, name, args);\n            }\n            return err(`Unknown method: \"${method}\"\"`);\n        },\n        describe: async function () {\n            const { title } = handler;\n            return {\n                title,\n                metadata: {\n                    tags: [\"connector-tools\"],\n                },\n                inputSchema: {\n                    type: \"object\",\n                },\n                outputSchema: {\n                    type: \"object\",\n                },\n            };\n        },\n    };\n}\nfunction createConfigurator(configurator) {\n    return {\n        invoke: createConfiguratorInvoke(configurator),\n        describe: createConfiguratorDescribe(configurator),\n    };\n}\nfunction createConfiguratorDescribe(configurator) {\n    const { title } = configurator;\n    return async function () {\n        return {\n            title: title,\n            description: \"\",\n            metadata: {\n                tags: [\"connector-configure\"],\n            },\n            inputSchema: {\n                type: \"object\",\n            },\n            outputSchema: {\n                type: \"object\",\n                properties: {\n                    context: {\n                        type: \"array\",\n                        items: { type: \"object\", behavior: [\"llm-content\"] },\n                        title: \"Context out\",\n                    },\n                },\n            },\n        };\n    };\n}\nfunction createConfiguratorInvoke(configurator) {\n    return async function ({ context, }) {\n        const inputs = context?.at(-1)?.parts?.at(0)?.json;\n        if (!inputs || !(\"stage\" in inputs)) {\n            return err(`Can't configure ${configurator.title || \"\"} connector: invalid input structure`);\n        }\n        if (inputs.stage === \"initialize\") {\n            const initializing = await configurator.initialize(inputs);\n            if (!ok(initializing))\n                return initializing;\n            return cx(initializing);\n        }\n        else if (inputs.stage === \"read\") {\n            const reading = await configurator.read?.(inputs);\n            if (!reading) {\n                return cx({\n                    schema: {},\n                    values: {},\n                });\n            }\n            if (!ok(reading))\n                return reading;\n            return cx(reading);\n        }\n        else if (inputs.stage === \"preview\") {\n            const previewing = await configurator.preview?.(inputs);\n            if (!previewing || !ok(previewing)) {\n                return { context: [{ parts: [{ text: \"\" }] }] };\n            }\n            return { context: previewing };\n        }\n        else if (inputs.stage === \"write\") {\n            const writing = await configurator.write?.(inputs);\n            if (!writing)\n                return cx({});\n            if (!ok(writing))\n                return writing;\n            return cx(writing);\n        }\n        return err(`Unknown stage: ${inputs[\"stage\"]}`);\n    };\n}\nclass ConnectorManager {\n    part;\n    constructor(part) {\n        this.part = part;\n    }\n    #state = null;\n    async title() {\n        const state = await this.#getState();\n        if (!ok(state))\n            return state;\n        return state.info.url;\n    }\n    async #getState() {\n        const path = `/assets/${this.part.path}`;\n        if (this.#state)\n            return this.#state;\n        const reading = await read({ path });\n        if (!ok(reading))\n            return reading;\n        const info = getConnectorInfo(reading.data);\n        if (!ok(info))\n            return info;\n        const describing = await describeConnector({ url: info.url });\n        if (!ok(describing))\n            return describing;\n        this.#state = { info, describeOutputs: describing };\n        return this.#state;\n    }\n    async #getInvocationArgs(tag) {\n        const state = await this.#getState();\n        if (!ok(state))\n            return state;\n        const url = getExportUrl(tag, state.describeOutputs);\n        if (!ok(url))\n            return url;\n        const id = getConnectorId(this.part);\n        if (!ok(id))\n            return id;\n        return { $board: url, id, info: state.info };\n    }\n    async listTools() {\n        const args = await this.#getInvocationArgs(\"connector-tools\");\n        if (!ok(args))\n            return args;\n        const invoking = await invokeConnector({ method: \"list\", ...args });\n        if (!ok(invoking))\n            return invoking;\n        const output = invoking;\n        console.log(\"LIST TOOLS OUTPUT\", output);\n        return output.list;\n    }\n    async invokeTool(name, args, callTool) {\n        const invocationArgs = await this.#getInvocationArgs(\"connector-tools\");\n        if (!ok(invocationArgs))\n            return invocationArgs;\n        const { id, info } = invocationArgs;\n        await callTool(invocationArgs.$board, {\n            method: \"invoke\",\n            id,\n            info,\n            name,\n            args,\n        });\n    }\n    async materialize() {\n        const args = await this.#getInvocationArgs(\"connector-load\");\n        if (!ok(args))\n            return args;\n        const invoking = await invokeConnector(args);\n        if (!ok(invoking))\n            return invoking;\n        const output = invoking;\n        if (output && output.context && isLLMContentArray(output.context)) {\n            return output.context;\n        }\n        return err(`Invalid return value from connector load`);\n    }\n    async schemaProperties() {\n        const args = await this.#getInvocationArgs(\"connector-save\");\n        if (!ok(args))\n            return {};\n        const describing = await describeConnector({ url: args.$board });\n        if (!ok(describing))\n            return {};\n        const props = describing.inputSchema.properties;\n        if (!props || Object.keys(props).length === 0)\n            return {};\n        delete props.context;\n        delete props.id;\n        delete props.info;\n        return props;\n    }\n    async canSave() {\n        const args = await this.#getInvocationArgs(\"connector-save\");\n        if (!ok(args))\n            return false;\n        const invoking = await invokeConnector({\n            ...args,\n            method: \"canSave\",\n        });\n        if (!ok(invoking))\n            return false;\n        return !!invoking.canSave;\n    }\n    async save(context, options) {\n        const args = await this.#getInvocationArgs(\"connector-save\");\n        if (!ok(args))\n            return args;\n        const invoking = await invokeConnector({\n            ...args,\n            context,\n            ...options,\n            method: \"save\",\n        });\n        if (!ok(invoking))\n            return invoking;\n    }\n    static isConnector(part) {\n        return part.path.startsWith(\"connectors/\");\n    }\n}\nfunction getConnectorId(part) {\n    const id = part.path.split(\"/\").at(-1);\n    if (!id)\n        return err(`Invalid connector path: ${part.path}`);\n    return id;\n}\nfunction getExportUrl(tag, result) {\n    const exports = result.exports;\n    if (!exports)\n        return err(`Invalid connector structure: must have exports`);\n    const assetExport = Object.entries(exports).find(([url, e]) => e.metadata?.tags?.includes(tag));\n    if (!assetExport)\n        return err(`Invalid connector structure: must have export tagged as \"${tag}\"`);\n    return assetExport[0];\n}\nfunction getConnectorInfo(data) {\n    const part = data?.at(-1)?.parts.at(0);\n    if (!part)\n        return err(`Invalid asset structure`);\n    if (!(\"json\" in part))\n        return err(`Invalid connector info structure`);\n    return part.json;\n}\n",
      "metadata": {
        "title": "connector-manager",
        "source": {
          "code": "/**\n * @fileoverview Manages connectors.\n */\n\nimport { err, ok, isLLMContentArray } from \"./utils\";\nimport read from \"@read\";\nimport describeConnector, { type DescribeOutputs } from \"@describe\";\nimport invokeConnector from \"@invoke\";\nimport type { ExportDescriberResult, CallToolCallback } from \"./common\";\n\nexport { ConnectorManager, createConfigurator, createTools };\n\ntype ToolsListInput<C extends Record<string, JsonSerializable>> = {\n  method: \"list\";\n  id: string;\n  info: ConnectorInfo<C>;\n};\n\ntype ToolsInvokeInput<\n  C extends Record<string, JsonSerializable>,\n  A extends Record<string, JsonSerializable> = Record<string, JsonSerializable>,\n> = {\n  method: \"invoke\";\n  id: string;\n  info: ConnectorInfo<C>;\n  name: string;\n  args: A;\n};\n\ntype ToolsInput<\n  C extends Record<string, JsonSerializable>,\n  A extends Record<string, JsonSerializable>,\n> = ToolsListInput<C> | ToolsInvokeInput<C, A>;\n\ntype ToolsOutput = ListMethodOutput | InvokeMethodOutput;\n\ntype InitializeInput = {\n  stage: \"initialize\";\n  id: string;\n};\n\ntype InitializeOutput<C extends Record<string, unknown>> = {\n  title: string;\n  configuration: C;\n};\n\ntype ReadInput<C extends Record<string, unknown>> = {\n  stage: \"read\";\n  id: string;\n  configuration: C;\n};\n\ntype ReadOutput<V extends Record<string, unknown>> = {\n  schema: Schema;\n  values: V;\n};\n\ntype PreviewInput<C extends Record<string, unknown>> = {\n  stage: \"preview\";\n  id: string;\n  configuration: C;\n};\n\ntype PreviewOutput = LLMContent[];\n\ntype WriteInput<V extends Record<string, unknown>> = {\n  stage: \"write\";\n  id: string;\n  values: V;\n};\n\ntype WriteOutput = {};\n\ntype Inputs<\n  C extends Record<string, unknown>,\n  V extends Record<string, unknown>,\n> = {\n  context?: {\n    parts?: {\n      json?: InitializeInput | ReadInput<C> | PreviewInput<C> | WriteInput<V>;\n    }[];\n  }[];\n};\n\ntype Outputs<\n  C extends Record<string, unknown>,\n  V extends Record<string, unknown>,\n> =\n  | {\n      context: {\n        parts: { json: InitializeOutput<C> | ReadOutput<V> | WriteOutput }[];\n      }[];\n    }\n  | {\n      context: PreviewOutput;\n    };\n\nfunction cx<\n  C extends Record<string, unknown>,\n  V extends Record<string, unknown>,\n>(json: InitializeOutput<C> | ReadOutput<V> | WriteOutput): Outputs<C, V> {\n  return { context: [{ parts: [{ json }] }] };\n}\n\nexport type Configurator<\n  C extends Record<string, unknown>,\n  V extends Record<string, unknown>,\n> = {\n  title: string;\n  initialize: (input: InitializeInput) => Promise<Outcome<InitializeOutput<C>>>;\n  read?: (input: ReadInput<C>) => Promise<Outcome<ReadOutput<V>>>;\n  write?: (input: WriteInput<V>) => Promise<Outcome<WriteOutput>>;\n  preview?: (input: PreviewInput<C>) => Promise<Outcome<PreviewOutput>>;\n};\n\nexport type ToolHandler<\n  C extends Record<string, JsonSerializable>,\n  A extends Record<string, JsonSerializable> = Record<string, JsonSerializable>,\n> = {\n  title: string;\n  list(id: string, info: ConnectorInfo<C>): Promise<Outcome<ListMethodOutput>>;\n  invoke(\n    id: string,\n    info: ConnectorInfo<C>,\n    name: string,\n    args: A\n  ): Promise<Outcome<InvokeMethodOutput>>;\n};\n\nfunction createTools<\n  C extends Record<string, JsonSerializable>,\n  A extends Record<string, JsonSerializable> = Record<string, JsonSerializable>,\n>(handler: ToolHandler<C, A>) {\n  return {\n    invoke: async function (\n      inputs: ToolsInput<C, A>\n    ): Promise<Outcome<ToolsOutput>> {\n      const { method, id, info } = inputs;\n      if (method === \"list\") {\n        return handler.list(id, info);\n      } else if (method === \"invoke\") {\n        const { name, args } = inputs;\n        return handler.invoke(id, info, name, args);\n      }\n      return err(`Unknown method: \"${method}\"\"`);\n    },\n    describe: async function () {\n      const { title } = handler;\n      return {\n        title,\n        metadata: {\n          tags: [\"connector-tools\"],\n        },\n        inputSchema: {\n          type: \"object\",\n        } satisfies Schema,\n        outputSchema: {\n          type: \"object\",\n        } satisfies Schema,\n      };\n    },\n  };\n}\n\nfunction createConfigurator<\n  C extends Record<string, unknown> = Record<string, unknown>,\n  V extends Record<string, unknown> = Record<string, unknown>,\n>(configurator: Configurator<C, V>) {\n  return {\n    invoke: createConfiguratorInvoke(configurator),\n    describe: createConfiguratorDescribe(configurator),\n  };\n}\n\nfunction createConfiguratorDescribe<\n  C extends Record<string, unknown> = Record<string, unknown>,\n  V extends Record<string, unknown> = Record<string, unknown>,\n>(configurator: Configurator<C, V>) {\n  const { title } = configurator;\n  return async function () {\n    return {\n      title: title,\n      description: \"\",\n      metadata: {\n        tags: [\"connector-configure\"],\n      },\n      inputSchema: {\n        type: \"object\",\n      } satisfies Schema,\n      outputSchema: {\n        type: \"object\",\n        properties: {\n          context: {\n            type: \"array\",\n            items: { type: \"object\", behavior: [\"llm-content\"] },\n            title: \"Context out\",\n          },\n        },\n      } satisfies Schema,\n    };\n  };\n}\n\nfunction createConfiguratorInvoke<\n  C extends Record<string, unknown> = Record<string, unknown>,\n  V extends Record<string, unknown> = Record<string, unknown>,\n>(configurator: Configurator<C, V>) {\n  return async function ({\n    context,\n  }: Inputs<C, V>): Promise<Outcome<Outputs<C, V>>> {\n    const inputs = context?.at(-1)?.parts?.at(0)?.json;\n    if (!inputs || !(\"stage\" in inputs)) {\n      return err(\n        `Can't configure ${configurator.title || \"\"} connector: invalid input structure`\n      );\n    }\n\n    if (inputs.stage === \"initialize\") {\n      const initializing = await configurator.initialize(inputs);\n      if (!ok(initializing)) return initializing;\n      return cx(initializing);\n    } else if (inputs.stage === \"read\") {\n      const reading = await configurator.read?.(inputs);\n      if (!reading) {\n        return cx({\n          schema: {},\n          values: {},\n        });\n      }\n      if (!ok(reading)) return reading;\n      return cx(reading);\n    } else if (inputs.stage === \"preview\") {\n      const previewing = await configurator.preview?.(inputs);\n      if (!previewing || !ok(previewing)) {\n        return { context: [{ parts: [{ text: \"\" }] }] };\n      }\n      return { context: previewing };\n    } else if (inputs.stage === \"write\") {\n      const writing = await configurator.write?.(inputs);\n      if (!writing) return cx({});\n      if (!ok(writing)) return writing;\n      return cx(writing);\n    }\n    return err(`Unknown stage: ${inputs[\"stage\"]}`);\n  };\n}\n\ntype ConnectorConfig = {\n  path: string;\n  instance?: string;\n};\n\nexport type ConnectorInfo<\n  C extends Record<string, JsonSerializable> = Record<string, JsonSerializable>,\n> = {\n  /**\n   * The URL of the connector\n   */\n  url: string;\n  /**\n   * The configuration of the connector\n   */\n  configuration: C;\n};\n\ntype InvocationArgs = {\n  $board: string;\n  id: string;\n  info: ConnectorInfo;\n};\n\ntype LoadOutput = {\n  context?: LLMContent[];\n};\n\nexport type ListMethodOutput = {\n  list: ListToolResult[];\n};\n\nexport type ListToolResult = {\n  url: string;\n  description: ExportDescriberResult;\n  passContext: boolean;\n};\n\nexport type InvokeMethodOutput = {\n  result: string;\n};\n\nexport type CanSaveMethodOutput = {\n  canSave: boolean;\n};\n\ntype ConnectorManagerState = {\n  info: ConnectorInfo;\n  describeOutputs: DescribeOutputs;\n};\n\nclass ConnectorManager {\n  constructor(public readonly part: ConnectorConfig) {}\n\n  #state: ConnectorManagerState | null = null;\n\n  async title(): Promise<Outcome<string>> {\n    const state = await this.#getState();\n    if (!ok(state)) return state;\n\n    return state.info.url;\n  }\n\n  async #getState(): Promise<Outcome<ConnectorManagerState>> {\n    const path: FileSystemPath = `/assets/${this.part.path}`;\n\n    if (this.#state) return this.#state;\n\n    const reading = await read({ path });\n    if (!ok(reading)) return reading;\n\n    const info = getConnectorInfo(reading.data);\n    if (!ok(info)) return info;\n\n    const describing = await describeConnector({ url: info.url });\n    if (!ok(describing)) return describing;\n    this.#state = { info, describeOutputs: describing };\n    return this.#state;\n  }\n\n  async #getInvocationArgs(tag: string): Promise<Outcome<InvocationArgs>> {\n    const state = await this.#getState();\n    if (!ok(state)) return state;\n\n    const url = getExportUrl(tag, state.describeOutputs);\n    if (!ok(url)) return url;\n\n    const id = getConnectorId(this.part);\n    if (!ok(id)) return id;\n\n    return { $board: url, id, info: state.info };\n  }\n\n  async listTools(): Promise<Outcome<ListToolResult[]>> {\n    const args = await this.#getInvocationArgs(\"connector-tools\");\n    if (!ok(args)) return args;\n\n    const invoking = await invokeConnector({ method: \"list\", ...args });\n    if (!ok(invoking)) return invoking;\n\n    const output = invoking as ListMethodOutput;\n    console.log(\"LIST TOOLS OUTPUT\", output);\n    return output.list;\n  }\n\n  async invokeTool(\n    name: string,\n    args: Record<string, unknown>,\n    callTool: CallToolCallback\n  ): Promise<Outcome<unknown>> {\n    const invocationArgs = await this.#getInvocationArgs(\"connector-tools\");\n    if (!ok(invocationArgs)) return invocationArgs;\n\n    const { id, info } = invocationArgs;\n    await callTool(invocationArgs.$board, {\n      method: \"invoke\",\n      id,\n      info,\n      name,\n      args,\n    });\n  }\n\n  async materialize(): Promise<Outcome<unknown>> {\n    const args = await this.#getInvocationArgs(\"connector-load\");\n    if (!ok(args)) return args;\n\n    const invoking = await invokeConnector(args);\n    if (!ok(invoking)) return invoking;\n\n    const output = invoking as LoadOutput;\n    if (output && output.context && isLLMContentArray(output.context)) {\n      return output.context;\n    }\n    return err(`Invalid return value from connector load`);\n  }\n\n  async schemaProperties(): Promise<Record<string, Schema>> {\n    const args = await this.#getInvocationArgs(\"connector-save\");\n\n    if (!ok(args)) return {};\n\n    const describing = await describeConnector({ url: args.$board });\n    if (!ok(describing)) return {};\n\n    const props = describing.inputSchema.properties;\n    if (!props || Object.keys(props).length === 0) return {};\n\n    delete props.context;\n    delete props.id;\n    delete props.info;\n\n    return props;\n  }\n\n  async canSave(): Promise<Outcome<boolean>> {\n    const args = await this.#getInvocationArgs(\"connector-save\");\n    if (!ok(args)) return false;\n\n    const invoking = await invokeConnector({\n      ...args,\n      method: \"canSave\",\n    });\n    if (!ok(invoking)) return false;\n    return !!(invoking as CanSaveMethodOutput).canSave;\n  }\n\n  async save(\n    context: LLMContent[],\n    options: Record<string, unknown>\n  ): Promise<Outcome<void>> {\n    const args = await this.#getInvocationArgs(\"connector-save\");\n    if (!ok(args)) return args;\n\n    const invoking = await invokeConnector({\n      ...args,\n      context,\n      ...options,\n      method: \"save\",\n    });\n    if (!ok(invoking)) return invoking;\n  }\n\n  static isConnector(part: ConnectorConfig) {\n    return part.path.startsWith(\"connectors/\");\n  }\n}\n\nfunction getConnectorId(part: ConnectorConfig): Outcome<string> {\n  const id = part.path.split(\"/\").at(-1);\n  if (!id) return err(`Invalid connector path: ${part.path}`);\n  return id;\n}\n\nfunction getExportUrl(tag: string, result: DescribeOutputs): Outcome<string> {\n  const exports = result.exports;\n  if (!exports) return err(`Invalid connector structure: must have exports`);\n  const assetExport = Object.entries(exports).find(([url, e]) =>\n    e.metadata?.tags?.includes(tag)\n  );\n  if (!assetExport)\n    return err(\n      `Invalid connector structure: must have export tagged as \"${tag}\"`\n    );\n  return assetExport[0];\n}\n\nfunction getConnectorInfo(\n  data: LLMContent[] | undefined\n): Outcome<ConnectorInfo> {\n  const part = data?.at(-1)?.parts.at(0);\n  if (!part) return err(`Invalid asset structure`);\n  if (!(\"json\" in part)) return err(`Invalid connector info structure`);\n  return part.json as ConnectorInfo;\n}\n",
          "language": "typescript"
        },
        "description": "Manages connectors.",
        "runnable": false
      }
    },
    "image-utils": {
      "code": "/**\n * @fileoverview Utilities for generating images.\n */\nimport { err, ok, isStoredData, toLLMContent, toLLMContentInline, toLLMContentStored, toInlineReference, toInlineData, toText, addUserTurn, llm, } from \"./utils\";\nimport { executeStep, } from \"./step-executor\";\nimport { GeminiPrompt } from \"./gemini-prompt\";\nexport { callImageGen, callGeminiImage, promptExpander };\nconst STEP_NAME = \"AI Image Tool\";\nconst OUTPUT_NAME = \"generated_image\";\nconst API_NAME = \"ai_image_tool\";\nasync function callGeminiImage(instruction, imageContent, disablePromptRewrite, aspectRatio = \"1:1\") {\n    const imageChunks = [];\n    for (const element of imageContent) {\n        let inlineChunk;\n        if (isStoredData(element)) {\n            inlineChunk = toInlineReference(element);\n        }\n        else {\n            inlineChunk = toInlineData(element);\n        }\n        if (inlineChunk && inlineChunk != null && typeof inlineChunk != \"string\") {\n            imageChunks.push({\n                mimetype: inlineChunk.mimeType,\n                data: inlineChunk.data,\n            });\n        }\n    }\n    const input_parameters = [\"input_instruction\"];\n    if (imageChunks.length > 0) {\n        input_parameters.push(\"input_image\");\n    }\n    console.log(\"Number of input images: \" + String(imageChunks.length));\n    const encodedInstruction = btoa(unescape(encodeURIComponent(instruction)));\n    const executionInputs = {\n        input_instruction: {\n            chunks: [\n                {\n                    mimetype: \"text/plain\",\n                    data: encodedInstruction,\n                },\n            ],\n        },\n        aspect_ratio_key: {\n            chunks: [\n                {\n                    mimetype: \"text/plain\",\n                    data: btoa(aspectRatio),\n                },\n            ],\n        },\n    };\n    if (imageChunks.length > 0) {\n        executionInputs[\"input_image\"] = {\n            chunks: imageChunks,\n        };\n    }\n    const body = {\n        planStep: {\n            stepName: STEP_NAME,\n            modelApi: API_NAME,\n            inputParameters: input_parameters,\n            systemPrompt: \"\",\n            options: {\n                disablePromptRewrite: disablePromptRewrite,\n            },\n            output: OUTPUT_NAME,\n        },\n        execution_inputs: executionInputs,\n    };\n    // TODO(askerryryan): Remove once functional.\n    console.log(\"request body\");\n    console.log(body);\n    const response = await executeStep(body);\n    // TODO(askerryryan): Remove once functional.\n    console.log(\"response\");\n    console.log(response);\n    if (!ok(response)) {\n        return err(\"Image generation failed. \" +\n            response.$error +\n            \" Check your prompt to ensure it is a valid and compliant image prompt.\");\n    }\n    const outContent = response.executionOutputs && response.executionOutputs[OUTPUT_NAME];\n    if (!outContent) {\n        return err(\"Error: No image returned from backend\");\n    }\n    return outContent.chunks.map((c) => {\n        if (c.mimetype.endsWith(\"/storedData\")) {\n            return toLLMContentStored(c.mimetype.replace(\"/storedData\", \"\"), c.data);\n        }\n        return toLLMContentInline(c.mimetype, c.data);\n    });\n}\nasync function callImageGen(imageInstruction, aspectRatio = \"1:1\") {\n    const executionInputs = {};\n    const encodedInstruction = btoa(unescape(encodeURIComponent(imageInstruction)));\n    executionInputs[\"image_prompt\"] = {\n        chunks: [\n            {\n                mimetype: \"text/plain\",\n                data: encodedInstruction,\n            },\n        ],\n    };\n    executionInputs[\"aspect_ratio_key\"] = {\n        chunks: [\n            {\n                mimetype: \"text/plain\",\n                data: btoa(aspectRatio),\n            },\n        ],\n    };\n    const inputParameters = [\"image_prompt\"];\n    const body = {\n        planStep: {\n            stepName: \"GenerateImage\",\n            modelApi: \"image_generation\",\n            inputParameters: inputParameters,\n            systemPrompt: \"\",\n            output: OUTPUT_NAME,\n        },\n        execution_inputs: executionInputs,\n    };\n    const response = await executeStep(body);\n    console.log(response);\n    if (!ok(response)) {\n        return err(\"Image generation failed. \" + response.$error);\n    }\n    const outContent = response.executionOutputs && response.executionOutputs[OUTPUT_NAME];\n    if (!outContent) {\n        return err(\"Error: No image returned from backend\");\n    }\n    return outContent.chunks.map((c) => {\n        if (c.mimetype.endsWith(\"/storedData\")) {\n            return toLLMContentStored(c.mimetype.replace(\"/storedData\", \"\"), c.data);\n        }\n        return toLLMContentInline(c.mimetype, c.data);\n    });\n}\nfunction promptExpander(contents, instruction) {\n    const context = contents?.length\n        ? \"using conversation context and these additional\"\n        : \"with these\";\n    const promptText = llm `Generate a single text-to-image prompt ${context} instructions:\n${instruction}\n\nTypical output format:\n\"\"\"\nCreate the following image:\n\n## Setting/background\n\n<Detailed description of everything that is in the background of the image.>\n\n## Foreground/focus\n\n<Detailed description of object and/or shapes that are in the foreground and are the main focal point of the image>\n\n## Style\n\n<Detailed description of the style, color scheme, vibe, kind of drawing (illustration, photorealistic, etc.)>\n\nYou output will be fed directly into the text-to-image model, so it must be prompt only, no additional chit-chat\n\"\"\"\n`;\n    return new GeminiPrompt({\n        body: {\n            contents: addUserTurn(promptText.asContent(), contents),\n            systemInstruction: toLLMContent(`\nYou are a creative writer whose specialty is to write prompts for text-to-imageÂ models.\n\nThe prompt must describe every object in the image in great detail and describe the style\nin terms of color scheme and vibe.\n`),\n        },\n    });\n}\n",
      "metadata": {
        "title": "image-utils",
        "source": {
          "code": "/**\n * @fileoverview Utilities for generating images.\n */\n\nimport {\n  err,\n  ok,\n  isStoredData,\n  toLLMContent,\n  toLLMContentInline,\n  toLLMContentStored,\n  toInlineReference,\n  toInlineData,\n  toText,\n  addUserTurn,\n  llm,\n} from \"./utils\";\nimport {\n  type ContentMap,\n  type ExecuteStepRequest,\n  executeStep,\n} from \"./step-executor\";\nimport { GeminiPrompt } from \"./gemini-prompt\";\n\nexport { callImageGen, callGeminiImage, promptExpander };\n\nconst STEP_NAME = \"AI Image Tool\";\nconst OUTPUT_NAME = \"generated_image\";\nconst API_NAME = \"ai_image_tool\";\n\nasync function callGeminiImage(\n  instruction: string,\n  imageContent: LLMContent[],\n  disablePromptRewrite: boolean,\n  aspectRatio: string = \"1:1\"\n): Promise<Outcome<LLMContent[]>> {\n  const imageChunks = [];\n  for (const element of imageContent) {\n    let inlineChunk;\n    if (isStoredData(element)) {\n      inlineChunk = toInlineReference(element);\n    } else {\n      inlineChunk = toInlineData(element);\n    }\n    if (inlineChunk && inlineChunk != null && typeof inlineChunk != \"string\") {\n      imageChunks.push({\n        mimetype: inlineChunk.mimeType,\n        data: inlineChunk.data,\n      });\n    }\n  }\n  const input_parameters = [\"input_instruction\"];\n  if (imageChunks.length > 0) {\n    input_parameters.push(\"input_image\");\n  }\n  console.log(\"Number of input images: \" + String(imageChunks.length));\n  const encodedInstruction = btoa(unescape(encodeURIComponent(instruction)));\n  const executionInputs: ContentMap = {\n    input_instruction: {\n      chunks: [\n        {\n          mimetype: \"text/plain\",\n          data: encodedInstruction,\n        },\n      ],\n    },\n    aspect_ratio_key: {\n      chunks: [\n        {\n          mimetype: \"text/plain\",\n          data: btoa(aspectRatio),\n        },\n      ],\n    },\n  };\n  if (imageChunks.length > 0) {\n    executionInputs[\"input_image\"] = {\n      chunks: imageChunks,\n    };\n  }\n  const body = {\n    planStep: {\n      stepName: STEP_NAME,\n      modelApi: API_NAME,\n      inputParameters: input_parameters,\n      systemPrompt: \"\",\n      options: {\n        disablePromptRewrite: disablePromptRewrite,\n      },\n      output: OUTPUT_NAME,\n    },\n    execution_inputs: executionInputs,\n  } satisfies ExecuteStepRequest;\n  // TODO(askerryryan): Remove once functional.\n  console.log(\"request body\");\n  console.log(body);\n  const response = await executeStep(body);\n  // TODO(askerryryan): Remove once functional.\n  console.log(\"response\");\n  console.log(response);\n  if (!ok(response)) {\n    return err(\n      \"Image generation failed. \" +\n        response.$error +\n        \" Check your prompt to ensure it is a valid and compliant image prompt.\"\n    );\n  }\n\n  const outContent =\n    response.executionOutputs && response.executionOutputs[OUTPUT_NAME];\n  if (!outContent) {\n    return err(\"Error: No image returned from backend\");\n  }\n  return outContent.chunks.map((c) => {\n    if (c.mimetype.endsWith(\"/storedData\")) {\n      return toLLMContentStored(c.mimetype.replace(\"/storedData\", \"\"), c.data);\n    }\n    return toLLMContentInline(c.mimetype, c.data);\n  });\n}\n\nasync function callImageGen(\n  imageInstruction: string,\n  aspectRatio: string = \"1:1\"\n): Promise<Outcome<LLMContent[]>> {\n  const executionInputs: ContentMap = {};\n  const encodedInstruction = btoa(\n    unescape(encodeURIComponent(imageInstruction))\n  );\n  executionInputs[\"image_prompt\"] = {\n    chunks: [\n      {\n        mimetype: \"text/plain\",\n        data: encodedInstruction,\n      },\n    ],\n  };\n  executionInputs[\"aspect_ratio_key\"] = {\n    chunks: [\n      {\n        mimetype: \"text/plain\",\n        data: btoa(aspectRatio),\n      },\n    ],\n  };\n  const inputParameters: string[] = [\"image_prompt\"];\n  const body = {\n    planStep: {\n      stepName: \"GenerateImage\",\n      modelApi: \"image_generation\",\n      inputParameters: inputParameters,\n      systemPrompt: \"\",\n      output: OUTPUT_NAME,\n    },\n    execution_inputs: executionInputs,\n  } satisfies ExecuteStepRequest;\n  const response = await executeStep(body);\n  console.log(response);\n  if (!ok(response)) {\n    return err(\"Image generation failed. \" + response.$error);\n  }\n\n  const outContent =\n    response.executionOutputs && response.executionOutputs[OUTPUT_NAME];\n  if (!outContent) {\n    return err(\"Error: No image returned from backend\");\n  }\n  return outContent.chunks.map((c) => {\n    if (c.mimetype.endsWith(\"/storedData\")) {\n      return toLLMContentStored(c.mimetype.replace(\"/storedData\", \"\"), c.data);\n    }\n    return toLLMContentInline(c.mimetype, c.data);\n  });\n}\n\nfunction promptExpander(\n  contents: LLMContent[] | undefined,\n  instruction: LLMContent\n): GeminiPrompt {\n  const context = contents?.length\n    ? \"using conversation context and these additional\"\n    : \"with these\";\n  const promptText = llm`Generate a single text-to-image prompt ${context} instructions:\n${instruction}\n\nTypical output format:\n\"\"\"\nCreate the following image:\n\n## Setting/background\n\n<Detailed description of everything that is in the background of the image.>\n\n## Foreground/focus\n\n<Detailed description of object and/or shapes that are in the foreground and are the main focal point of the image>\n\n## Style\n\n<Detailed description of the style, color scheme, vibe, kind of drawing (illustration, photorealistic, etc.)>\n\nYou output will be fed directly into the text-to-image model, so it must be prompt only, no additional chit-chat\n\"\"\"\n`;\n  return new GeminiPrompt({\n    body: {\n      contents: addUserTurn(promptText.asContent(), contents),\n      systemInstruction: toLLMContent(`\nYou are a creative writer whose specialty is to write prompts for text-to-imageÂ models.\n\nThe prompt must describe every object in the image in great detail and describe the style\nin terms of color scheme and vibe.\n`),\n    },\n  });\n}\n",
          "language": "typescript"
        },
        "description": "Utilities for generating images.",
        "runnable": false
      }
    },
    "settings": {
      "code": "/**\n * @fileoverview A helper to retrieve current settings\n */\nimport { ok } from \"./utils\";\nimport read from \"@read\";\nexport { readSettings };\nasync function readSettings() {\n    const reading = await read({ path: \"/env/settings/general\" });\n    if (!ok(reading))\n        return reading;\n    const json = (reading.data?.at(0)?.parts?.at(0)).json;\n    if (!json)\n        return {};\n    return json;\n}\n",
      "metadata": {
        "title": "settings",
        "source": {
          "code": "/**\n * @fileoverview A helper to retrieve current settings\n */\n\nimport { ok } from \"./utils\";\n\nimport read from \"@read\";\n\nexport { readSettings };\n\nasync function readSettings(): Promise<Outcome<Record<string, boolean>>> {\n  const reading = await read({ path: \"/env/settings/general\" });\n  if (!ok(reading)) return reading;\n\n  const json = (reading.data?.at(0)?.parts?.at(0) as JSONPart).json;\n  if (!json) return {};\n\n  return json as Record<string, boolean>;\n}\n",
          "language": "typescript"
        },
        "description": "A helper to retrieve current settings",
        "runnable": false
      }
    }
  },
  "exports": [
    "#daf082ca-c1aa-4aff-b2c8-abeb984ab66c",
    "#module:researcher",
    "#module:image-generator",
    "#module:image-editor",
    "#module:render-outputs",
    "#module:audio-generator",
    "#21ee02e7-83fa-49d0-964c-0cab10eafc2c",
    "#module:combine-outputs",
    "#module:make-code"
  ],
  "graphs": {
    "daf082ca-c1aa-4aff-b2c8-abeb984ab66c": {
      "title": "Make Text",
      "description": "Generates text and so much more.",
      "version": "0.0.1",
      "describer": "module:entry",
      "nodes": [
        {
          "type": "output",
          "id": "output",
          "configuration": {
            "schema": {
              "properties": {
                "context": {
                  "type": "array",
                  "title": "Context",
                  "items": {
                    "type": "object",
                    "behavior": [
                      "llm-content"
                    ]
                  },
                  "default": "null"
                }
              },
              "type": "object",
              "required": []
            }
          },
          "metadata": {
            "visual": {
              "x": 720,
              "y": 0,
              "collapsed": "expanded",
              "outputHeight": 44
            }
          }
        },
        {
          "id": "board-f138aa03",
          "type": "#module:entry",
          "metadata": {
            "visual": {
              "x": -46.99999999999966,
              "y": -71.99999999999898,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "entry"
          }
        },
        {
          "id": "board-d340ad8f",
          "type": "#module:agent-main",
          "metadata": {
            "visual": {
              "x": 340,
              "y": 0,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "Generating draft",
            "logLevel": "info"
          },
          "configuration": {}
        },
        {
          "id": "board-1946064a",
          "type": "#module:join",
          "metadata": {
            "visual": {
              "x": 1059.9999999999986,
              "y": -159.99999999999886,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "join"
          }
        },
        {
          "type": "input",
          "id": "input",
          "metadata": {
            "visual": {
              "x": 720.0000000000005,
              "y": 160.00000000000114,
              "collapsed": "advanced",
              "outputHeight": 44
            },
            "title": "Waiting for user feedback",
            "logLevel": "info"
          },
          "configuration": {}
        }
      ],
      "edges": [
        {
          "from": "board-f138aa03",
          "to": "board-d340ad8f",
          "out": "context",
          "in": "context"
        },
        {
          "from": "board-d340ad8f",
          "to": "output",
          "out": "done",
          "in": "context"
        },
        {
          "from": "input",
          "to": "board-1946064a",
          "out": "request",
          "in": "request"
        },
        {
          "from": "board-d340ad8f",
          "to": "input",
          "out": "toInput",
          "in": "schema"
        },
        {
          "from": "board-d340ad8f",
          "to": "board-1946064a",
          "out": "context",
          "in": "context"
        },
        {
          "from": "board-1946064a",
          "to": "board-d340ad8f",
          "out": "context",
          "in": "context"
        }
      ],
      "metadata": {
        "visual": {
          "minimized": false
        },
        "describer": "module:entry",
        "tags": []
      }
    },
    "21ee02e7-83fa-49d0-964c-0cab10eafc2c": {
      "title": "Ask User",
      "description": "A block of text as input or output",
      "version": "0.0.1",
      "nodes": [
        {
          "type": "input",
          "id": "input",
          "metadata": {
            "visual": {
              "x": 580.0000000000005,
              "y": -539.9999999999994,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "Waiting for user input",
            "logLevel": "info"
          },
          "configuration": {}
        },
        {
          "type": "output",
          "id": "output",
          "configuration": {
            "schema": {
              "properties": {
                "context": {
                  "type": "array",
                  "title": "Context",
                  "items": {
                    "type": "object",
                    "behavior": [
                      "llm-content"
                    ]
                  },
                  "default": "null"
                }
              },
              "type": "object",
              "required": []
            }
          },
          "metadata": {
            "visual": {
              "x": 1240.0000000000005,
              "y": -399.99999999999943,
              "collapsed": "expanded",
              "outputHeight": 44
            }
          }
        },
        {
          "id": "board-64b2c3a8",
          "type": "#module:text-entry",
          "metadata": {
            "visual": {
              "x": 225.9030760391795,
              "y": -646.8568148490385,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "text-entry"
          }
        },
        {
          "id": "board-95a57400",
          "type": "#module:text-main",
          "metadata": {
            "visual": {
              "x": 900,
              "y": -459.99999999999943,
              "collapsed": "expanded",
              "outputHeight": 44
            },
            "title": "text-main"
          }
        }
      ],
      "edges": [
        {
          "from": "board-64b2c3a8",
          "out": "toInput",
          "to": "input",
          "in": "schema"
        },
        {
          "from": "board-64b2c3a8",
          "out": "toMain",
          "to": "board-95a57400",
          "in": "request"
        },
        {
          "from": "board-95a57400",
          "to": "output",
          "out": "context",
          "in": "context"
        },
        {
          "from": "board-64b2c3a8",
          "to": "board-95a57400",
          "out": "context",
          "in": "context"
        },
        {
          "from": "input",
          "to": "board-95a57400",
          "out": "request",
          "in": "request"
        }
      ],
      "metadata": {
        "visual": {
          "minimized": false
        },
        "tags": [],
        "describer": "module:text-entry",
        "icon": "text"
      }
    }
  },
  "assets": {
    "@@thumbnail": {
      "metadata": {
        "title": "Thumbnail",
        "type": "file"
      },
      "data": "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjUwIiBoZWlnaHQ9IjIwMCIgdmlld0JveD0iMCAwIDI1MCAyMDAiIGZpbGw9Im5vbmUiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+CiAgICAKICAgICAgPHJlY3QgeD0iMTI0LjAzIgogICAgICAgICAgICAgICAgICAgIHk9IjEyNS40OSIKICAgICAgICAgICAgICAgICAgICB3aWR0aD0iMzguNjYiCiAgICAgICAgICAgICAgICAgICAgaGVpZ2h0PSIyMS40MSIKICAgICAgICAgICAgICAgICAgICByeD0iMy41IgogICAgICAgICAgICAgICAgICAgIGZpbGw9IndoaXRlIgogICAgICAgICAgICAgICAgICAgIHN0cm9rZT0iIzIwYTIwMiIgLz4KPHJlY3QgeD0iMTAuMDAiCiAgICAgICAgICAgICAgICAgICAgeT0iMTE0Ljc4IgogICAgICAgICAgICAgICAgICAgIHdpZHRoPSIzOC42NiIKICAgICAgICAgICAgICAgICAgICBoZWlnaHQ9IjIxLjQxIgogICAgICAgICAgICAgICAgICAgIHJ4PSIzLjUiCiAgICAgICAgICAgICAgICAgICAgZmlsbD0id2hpdGUiCiAgICAgICAgICAgICAgICAgICAgc3Ryb2tlPSIjNzc1N2Q5IiAvPgo8cmVjdCB4PSI2Ny41NCIKICAgICAgICAgICAgICAgICAgICB5PSIxMjUuNDkiCiAgICAgICAgICAgICAgICAgICAgd2lkdGg9IjM4LjY2IgogICAgICAgICAgICAgICAgICAgIGhlaWdodD0iMjEuNDEiCiAgICAgICAgICAgICAgICAgICAgcng9IjMuNSIKICAgICAgICAgICAgICAgICAgICBmaWxsPSJ3aGl0ZSIKICAgICAgICAgICAgICAgICAgICBzdHJva2U9IiMyZThiZTgiIC8+CjxyZWN0IHg9IjE3NC41OCIKICAgICAgICAgICAgICAgICAgICB5PSIxMDEuNzAiCiAgICAgICAgICAgICAgICAgICAgd2lkdGg9IjM4LjY2IgogICAgICAgICAgICAgICAgICAgIGhlaWdodD0iMjEuNDEiCiAgICAgICAgICAgICAgICAgICAgcng9IjMuNSIKICAgICAgICAgICAgICAgICAgICBmaWxsPSJ3aGl0ZSIKICAgICAgICAgICAgICAgICAgICBzdHJva2U9IiMyZThiZTgiIC8+CjxyZWN0IHg9IjEyNC4wMyIKICAgICAgICAgICAgICAgICAgICB5PSIxNDkuMjgiCiAgICAgICAgICAgICAgICAgICAgd2lkdGg9IjM4LjY2IgogICAgICAgICAgICAgICAgICAgIGhlaWdodD0iMjEuNDEiCiAgICAgICAgICAgICAgICAgICAgcng9IjMuNSIKICAgICAgICAgICAgICAgICAgICBmaWxsPSJ3aGl0ZSIKICAgICAgICAgICAgICAgICAgICBzdHJva2U9IiMyMGEyMDIiIC8+CjxyZWN0IHg9IjEwMy4yMiIKICAgICAgICAgICAgICAgICAgICB5PSI0NS4yMCIKICAgICAgICAgICAgICAgICAgICB3aWR0aD0iMzguNjYiCiAgICAgICAgICAgICAgICAgICAgaGVpZ2h0PSIyMS40MSIKICAgICAgICAgICAgICAgICAgICByeD0iMy41IgogICAgICAgICAgICAgICAgICAgIGZpbGw9IndoaXRlIgogICAgICAgICAgICAgICAgICAgIHN0cm9rZT0iIzIwYTIwMiIgLz4KPHJlY3QgeD0iMjAxLjM0IgogICAgICAgICAgICAgICAgICAgIHk9IjY2LjAyIgogICAgICAgICAgICAgICAgICAgIHdpZHRoPSIzOC42NiIKICAgICAgICAgICAgICAgICAgICBoZWlnaHQ9IjIxLjQxIgogICAgICAgICAgICAgICAgICAgIHJ4PSIzLjUiCiAgICAgICAgICAgICAgICAgICAgZmlsbD0id2hpdGUiCiAgICAgICAgICAgICAgICAgICAgc3Ryb2tlPSIjMjBhMjAyIiAvPgo8cmVjdCB4PSI1MC41NyIKICAgICAgICAgICAgICAgICAgICB5PSIyOS4zMiIKICAgICAgICAgICAgICAgICAgICB3aWR0aD0iMzguNjYiCiAgICAgICAgICAgICAgICAgICAgaGVpZ2h0PSIyMS40MSIKICAgICAgICAgICAgICAgICAgICByeD0iMy41IgogICAgICAgICAgICAgICAgICAgIGZpbGw9IndoaXRlIgogICAgICAgICAgICAgICAgICAgIHN0cm9rZT0iIzJlOGJlOCIgLz4KPHJlY3QgeD0iMTUwLjgwIgogICAgICAgICAgICAgICAgICAgIHk9IjU3LjEwIgogICAgICAgICAgICAgICAgICAgIHdpZHRoPSIzOC42NiIKICAgICAgICAgICAgICAgICAgICBoZWlnaHQ9IjIxLjQxIgogICAgICAgICAgICAgICAgICAgIHJ4PSIzLjUiCiAgICAgICAgICAgICAgICAgICAgZmlsbD0id2hpdGUiCiAgICAgICAgICAgICAgICAgICAgc3Ryb2tlPSIjMmU4YmU4IiAvPgogICAgPC9zdmc+"
    }
  }
}