{
  "title": "Append To Doc",
  "description": "Appends conversation context to  a Google Document, converting Markdown to proper formatting.",
  "version": "0.1.0",
  "nodes": [
    {
      "type": "input",
      "id": "input",
      "configuration": {
        "schema": {
          "properties": {
            "context": {
              "type": "array",
              "title": "Context in",
              "items": {
                "type": "object",
                "behavior": [
                  "llm-content"
                ]
              },
              "default": "[{\"role\":\"user\",\"parts\":[]}]",
              "description": "Incoming conversation context to append to Google Document."
            },
            "title": {
              "type": "string",
              "title": "Document Title",
              "description": "The title of the Google Document to which the incoming context will be appended. The document will be created if does not already exist.",
              "default": "Untitled Document (Created with Breadboard)",
              "behavior": [
                "config"
              ]
            }
          },
          "type": "object",
          "required": []
        }
      },
      "metadata": {
        "visual": {
          "x": -298,
          "y": -470,
          "collapsed": "expanded"
        },
        "logLevel": "debug"
      }
    },
    {
      "type": "output",
      "id": "output",
      "configuration": {
        "schema": {
          "properties": {
            "id": {
              "type": "string",
              "title": "Document ID",
              "format": "markdown",
              "description": "The Google Drive ID of the Google Document to which the conversation context was appended.",
              "default": ""
            }
          },
          "type": "object",
          "required": []
        }
      },
      "metadata": {
        "visual": {
          "x": 421,
          "y": -394.99999999999994,
          "collapsed": "expanded"
        },
        "logLevel": "debug"
      }
    },
    {
      "id": "runModule-7f9c8340",
      "type": "runModule",
      "metadata": {
        "visual": {
          "x": 8,
          "y": -469,
          "collapsed": "expanded"
        },
        "title": "Run Module",
        "logLevel": "debug"
      },
      "configuration": {
        "$module": "append-to-doc"
      }
    }
  ],
  "edges": [
    {
      "from": "input",
      "to": "runModule-7f9c8340",
      "out": "context",
      "in": "context"
    },
    {
      "from": "input",
      "to": "runModule-7f9c8340",
      "out": "title",
      "in": "title"
    },
    {
      "from": "runModule-7f9c8340",
      "to": "output",
      "out": "id",
      "in": "id"
    }
  ],
  "modules": {
    "api": {
      "code": "import fetch from \"@fetch\";\nimport secrets from \"@secrets\";\n\nconst connectionId = \"connection:google-drive-limited\";\n\nexport {\n  connect,\n  get,\n  create,\n  del,\n  query,\n  createMultipart,\n  getDoc,\n  updateDoc,\n  unwrap,\n};\n\nasync function get(token, id, metadata) {\n  if (!token) {\n    return error(\"Authentication token is required.\");\n  }\n  if (!id) {\n    return error(\"Please supply file id.\");\n  }\n  return api(\n    metadata,\n    token,\n    `https://www.googleapis.com/drive/v3/files/${id}`,\n    \"GET\"\n  );\n}\n\nasync function create(token, body, metadata) {\n  if (!token) {\n    return error(\"Authentication token is required.\");\n  }\n  if (!body) {\n    return error(\"Please supply the body of the file to create.\");\n  }\n\n  return api(\n    metadata,\n    token,\n    \"https://www.googleapis.com/drive/v3/files\",\n    \"POST\",\n    body\n  );\n}\n\nasync function query(token, query, metadata) {\n  if (!token) {\n    return error(\"Authentication token is required.\");\n  }\n  if (!query) {\n    return error(\"Please supply the query.\");\n  }\n\n  return api(\n    metadata,\n    token,\n    `https://www.googleapis.com/drive/v3/files?q=${encodeURIComponent(query)}`,\n    \"GET\"\n  );\n}\n\nasync function del(token, id, metadata) {\n  if (!token) {\n    return error(\"Authentication token is required.\");\n  }\n  if (!id) {\n    return error(\"Please supply the id of the file to delete\");\n  }\n\n  return api(\n    metadata,\n    token,\n    `https://www.googleapis.com/drive/v3/files/${id}`,\n    \"DELETE\"\n  );\n}\n\nasync function getDoc(token, id, metadata) {\n  if (!token) {\n    return error(\"Authentication token is required.\");\n  }\n  if (!id) {\n    return error(\"Please supply the doc id to get.\");\n  }\n  return api(\n    metadata,\n    token,\n    `https://docs.googleapis.com/v1/documents/${id}`,\n    \"GET\"\n  );\n}\n\nasync function updateDoc(token, id, body, metadata) {\n  if (!token) {\n    return error(\"Authentication token is required.\");\n  }\n  if (!id) {\n    return error(\"Please supply the id of the doc to update.\");\n  }\n  if (!body) {\n    return error(\"Please supply the body of the doc update request.\");\n  }\n  return api(\n    metadata,\n    token,\n    `https://docs.googleapis.com/v1/documents/${id}:batchUpdate`,\n    \"POST\",\n    body\n  );\n}\n\nasync function connect(metadata) {\n  const { [connectionId]: token } = await secrets({\n    ...meta(metadata),\n    keys: [connectionId],\n  });\n  return token;\n}\n\nasync function createMultipart(token, metadata, body, mimeType, $metadata) {\n  const boundary = \"BB-BB-BB-BB-BB-BB\";\n  const url = `https://www.googleapis.com/upload/drive/v3/files?uploadType=multipart`;\n  const request = {\n    ...meta($metadata),\n    url,\n    method: \"POST\",\n    headers: {\n      Authorization: `Bearer ${token}`,\n      [\"Content-Type\"]: `multipart/related; boundary=${boundary}`,\n    },\n    body: `--${boundary}\nContent-Type: application/json; charset=UTF-8\n\n${JSON.stringify(metadata, null, 2)}\n--${boundary}\nContent-Type: ${mimeType}; charset=UTF-8\n\n${body}\n--${boundary}--`,\n  };\n  const { response, $error } = await fetch(request);\n  if ($error) {\n    return { success: false, error: $error };\n  }\n  return { success: true, info: response };\n}\n\nasync function api(metadata, token, url, method, body = null) {\n  const request = {\n    ...meta(metadata),\n    url,\n    method,\n    headers: {\n      Authorization: `Bearer ${token}`,\n    },\n  };\n  if (body) {\n    request.body = body;\n  }\n  const { response, $error } = await fetch(request);\n  if ($error) {\n    return { success: false, error: $error };\n  }\n  return { success: true, info: response };\n}\n\nfunction unwrap(result, message = \"Error\") {\n  if (result.error) {\n    throw new Error(`${message}:\\n${JSON.stringify(result.error)}`);\n  }\n  return result.info;\n}\n\nfunction error(message) {\n  return {\n    success: false,\n    error: message,\n  };\n}\n\nfunction meta({ title, description } = {}) {\n  if (!(title || description)) return {};\n  const $metadata = {};\n  if (title) {\n    $metadata.title = title;\n  }\n  if (description) {\n    $metadata.description = description;\n  }\n  return { $metadata };\n}\n",
      "metadata": {
        "description": "",
        "url": "api.js",
        "runnable": false
      }
    },
    "append-to-doc": {
      "code": "import { connect, create, getDoc, query, unwrap, updateDoc } from \"./api\";\nimport { markdownToRequests } from \"./markdown-to-requests\";\n\nexport { invoke as default, describe };\n\nasync function invoke({ context, title }) {\n  const token = await connect({ title: \"Get API token\" });\n  const { id, end } = await getCollectorId(token, title);\n  const markdown = contextToMarkdown(context);\n  if (markdown) {\n    const requests = markdownToRequests(markdown, end);\n    // console.log(\"REQUESTS\", requests);\n    unwrap(\n      await updateDoc(token, id, { requests }, { title: \"Append to doc\" }),\n      \"Failed to update the doc.\"\n    );\n  }\n  return { id };\n}\n\n/**\n * Gets or creates the Google Doc id that serves as the collector: the\n * doc to which context is appended.\n */\nasync function getCollectorId(token, title) {\n  const findFile = await query(\n    token,\n    `appProperties has { key = 'appendToDoc' and value = '${title.replace(\"'\", \"\\\\'\")}' } and trashed = false`,\n    { title: \"Find the doc to which to append\" }\n  );\n  const file = unwrap(\n    findFile,\n    \"Failed to call Drive API to find the file to append\"\n  ).files.at(0);\n  if (!file) {\n    const createdFile = await create(\n      token,\n      {\n        name: title,\n        mimeType: \"application/vnd.google-apps.document\",\n        appProperties: {\n          appendToDoc: title,\n        },\n      },\n      { title: \"Create new doc to which to append\" }\n    );\n\n    return {\n      id: unwrap(createdFile, \"Failed to call Drive API to create a new file\")\n        .id,\n      end: 1,\n    };\n  }\n  const id = file.id;\n  const end =\n    unwrap(\n      await getDoc(token, id, { title: \"Get current doc contents\" }),\n      \"Failed to get the Doc to append to\"\n    ).body.content.reduce(\n      (acc, element) => Math.max(acc, element.endIndex || 0),\n      1\n    ) - 1;\n  return { id, end };\n}\n\nfunction contextToMarkdown(context) {\n  if (!Array.isArray(context)) {\n    if (typeof context === \"string\") {\n      return context;\n    }\n    if (context === undefined || context === null) {\n      return \"\";\n    }\n    return JSON.stringify(context);\n  }\n  // For now, take the last item in context.\n  context = [context.at(-1)];\n  return context\n    .flatMap((item) => {\n      if (\"parts\" in item) return item.parts.map((part) => part.text);\n      return null;\n    })\n    .filter((item) => !!item)\n    .join(\"\\n\\n\");\n}\n\nasync function describe() {\n  return {\n    inputSchema: {\n      type: \"object\",\n      properties: {\n        title: { type: \"string\", title: \"Title\" },\n        context: {\n          type: \"array\",\n          items: { type: \"object\", behavior: [\"llm-content\"] },\n          title: \"Context in\",\n        },\n      },\n    },\n    outputSchema: {\n      type: \"object\",\n      properties: {\n        id: { type: \"string\", title: \"Document ID\" },\n      },\n    },\n  };\n}\n",
      "metadata": {
        "description": "",
        "url": "append-to-doc.js",
        "runnable": true
      }
    },
    "markdown-to-requests": {
      "code": "import { marked } from \"./marked\";\nimport { unescape } from \"./unescape\";\n\nexport { markdownToRequests };\n\nfunction markdownToRequests(markdown, startIndex) {\n  const tokens = marked.lexer(markdown);\n  return tokensToRequests(tokens, startIndex);\n}\n\n/**\n * Converts markdown tokens to Google Doc Request array for the\n * `batchUpdate` call.\n */\nfunction tokensToRequests(tokens, startIndex) {\n  let current = startIndex;\n  // console.log(\"TOKENS\", tokens, startIndex);\n  return tokens.flatMap((token) => {\n    switch (token.type) {\n      case \"paragraph\":\n        return insertFormattedText(token, \"NORMAL_TEXT\");\n      case \"space\":\n        return insertSpace(token);\n      case \"code\":\n        return insertFormattedText(token, \"NORMAL_TEXT\");\n      case \"heading\":\n        return insertFormattedText(token, `HEADING_${token.depth}`);\n      case \"blockquote\":\n        return insertFormattedText(token, \"NORMAL_TEXT\");\n      case \"list\":\n        return insertList(token.items, token.ordered, 0);\n    }\n    return [];\n  });\n\n  function insertFormattedText(token, namedStyleType) {\n    const { requests, text: withoutBreak } = new TextStyles(\n      current,\n      token\n    ).parse();\n    const text = `${withoutBreak}\\n`;\n    if (namedStyleType) {\n      requests.unshift({\n        updateParagraphStyle: {\n          range: range(text.length),\n          paragraphStyle: { namedStyleType },\n          fields: \"namedStyleType\",\n        },\n      });\n    }\n    requests.unshift({ insertText: { text, location: location() } });\n    current += text.length;\n    return requests;\n  }\n\n  function insertSpace(token) {\n    const text = token.raw.startsWith(\"\\n\") ? token.raw.slice(1) : token.raw;\n    const result = [\n      {\n        insertText: { text, location: location() },\n      },\n    ];\n    return advance(result, text.length);\n  }\n\n  function range(length) {\n    return { startIndex: current, endIndex: current + length };\n  }\n\n  function location() {\n    return { index: current };\n  }\n\n  function insertList(items, ordered, depth) {\n    const start = current;\n    // This is necessary to counteract a gnarly side-effect of creating a\n    // bullet list: the indent markers are being removed during that change,\n    // and change all of the ranges. So we have to make sure that the next\n    // request accounts for that.\n    let depthToRemove = 0;\n    const list = descendIntoList(items, ordered, depth).flat();\n    list.push({\n      createParagraphBullets: {\n        range: { startIndex: start, endIndex: current },\n        bulletPreset: \"BULLET_DISC_CIRCLE_SQUARE\",\n      },\n    });\n    current -= depthToRemove;\n    return list;\n\n    function descendIntoList(items, ordered, depth) {\n      const list = items.flatMap((item) => {\n        // For item, item.type === \"list_item\".\n        const indent = \"\\t\".repeat(depth);\n        depthToRemove += depth;\n        const result = [];\n        const subList = item.tokens?.find((token) => token.type === \"list\");\n        if (subList) {\n          // assume that the first token is actually the text token.\n          result.push(insertItemText(indent, item.tokens.at(0)));\n          result.push(...descendIntoList(subList.items, ordered, depth + 1));\n        } else {\n          result.push(insertItemText(indent, item.tokens.at(0)));\n        }\n        return result;\n      });\n      return list;\n    }\n\n    function insertItemText(indent, token) {\n      const offset = current + indent.length;\n      const { requests, text: withoutIndent } = new TextStyles(\n        offset,\n        token\n      ).parse();\n      const text = `${indent}${withoutIndent}\\n`;\n      requests.unshift({\n        updateParagraphStyle: {\n          range: range(text.length),\n          paragraphStyle: { namedStyleType: \"NORMAL_TEXT\" },\n          fields: \"namedStyleType\",\n        },\n      });\n      requests.unshift({ insertText: { text, location: location() } });\n      current += text.length;\n      return requests;\n    }\n  }\n\n  function advance(result, length) {\n    current += length;\n    return result;\n  }\n}\n\nclass TextStyles {\n  #offset;\n  #tokens;\n  #styles = [];\n\n  constructor(offset, token) {\n    this.#offset = offset;\n    this.#tokens = token.tokens || [];\n  }\n\n  range(startIndex, length) {\n    return { startIndex, endIndex: startIndex + length };\n  }\n\n  style(startIndex, length, textStyle) {\n    this.#styles.push({\n      updateTextStyle: {\n        range: this.range(startIndex, length),\n        textStyle,\n        fields: Object.keys(textStyle).join(\",\"),\n      },\n    });\n  }\n\n  parse() {\n    let current = this.#offset;\n    let text = \"\";\n    for (let token of this.#tokens) {\n      const length = token.text.length;\n      text += unescape(token.text);\n      switch (token.type) {\n        case \"strong\":\n          this.style(current, length, { bold: true });\n          break;\n        case \"em\":\n          this.style(current, length, { italic: true });\n          break;\n        case \"codespan\":\n          this.style(current, length, {\n            weightedFontFamily: {\n              fontFamily: \"Fira Code\",\n            },\n          });\n          break;\n        case \"del\":\n          this.style(current, length, { strikethrough: true });\n          break;\n        case \"link\":\n          this.style(current, length, { link: { url: token.href } });\n          break;\n        case \"escape\":\n          console.log(\"ESCAPE\", token);\n          break;\n      }\n      current += length;\n    }\n    return {\n      requests: this.#styles,\n      text,\n    };\n  }\n}\n",
      "metadata": {
        "description": "",
        "url": "markdown-to-requests.js",
        "runnable": false
      }
    },
    "marked": {
      "code": "/**\n * marked v14.1.3 - a markdown parser\n * Copyright (c) 2011-2024, Christopher Jeffrey. (MIT Licensed)\n * https://github.com/markedjs/marked\n */\n\n/**\n * DO NOT EDIT THIS FILE\n * The code in this file is generated from files in ./src/\n */\n\n/**\n * Gets the original marked default options.\n */\nfunction _getDefaults() {\n  return {\n    async: false,\n    breaks: false,\n    extensions: null,\n    gfm: true,\n    hooks: null,\n    pedantic: false,\n    renderer: null,\n    silent: false,\n    tokenizer: null,\n    walkTokens: null,\n  };\n}\nlet _defaults = _getDefaults();\nfunction changeDefaults(newDefaults) {\n  _defaults = newDefaults;\n}\n\n/**\n * Helpers\n */\nconst escapeTest = /[&<>\"']/;\nconst escapeReplace = new RegExp(escapeTest.source, \"g\");\nconst escapeTestNoEncode = /[<>\"']|&(?!(#\\d{1,7}|#[Xx][a-fA-F0-9]{1,6}|\\w+);)/;\nconst escapeReplaceNoEncode = new RegExp(escapeTestNoEncode.source, \"g\");\nconst escapeReplacements = {\n  \"&\": \"&amp;\",\n  \"<\": \"&lt;\",\n  \">\": \"&gt;\",\n  '\"': \"&quot;\",\n  \"'\": \"&#39;\",\n};\nconst getEscapeReplacement = (ch) => escapeReplacements[ch];\nfunction escape$1(html, encode) {\n  if (encode) {\n    if (escapeTest.test(html)) {\n      return html.replace(escapeReplace, getEscapeReplacement);\n    }\n  } else {\n    if (escapeTestNoEncode.test(html)) {\n      return html.replace(escapeReplaceNoEncode, getEscapeReplacement);\n    }\n  }\n  return html;\n}\nconst caret = /(^|[^\\[])\\^/g;\nfunction edit(regex, opt) {\n  let source = typeof regex === \"string\" ? regex : regex.source;\n  opt = opt || \"\";\n  const obj = {\n    replace: (name, val) => {\n      let valSource = typeof val === \"string\" ? val : val.source;\n      valSource = valSource.replace(caret, \"$1\");\n      source = source.replace(name, valSource);\n      return obj;\n    },\n    getRegex: () => {\n      return new RegExp(source, opt);\n    },\n  };\n  return obj;\n}\nfunction cleanUrl(href) {\n  try {\n    href = encodeURI(href).replace(/%25/g, \"%\");\n  } catch {\n    return null;\n  }\n  return href;\n}\nconst noopTest = { exec: () => null };\nfunction splitCells(tableRow, count) {\n  // ensure that every cell-delimiting pipe has a space\n  // before it to distinguish it from an escaped pipe\n  const row = tableRow.replace(/\\|/g, (match, offset, str) => {\n      let escaped = false;\n      let curr = offset;\n      while (--curr >= 0 && str[curr] === \"\\\\\") escaped = !escaped;\n      if (escaped) {\n        // odd number of slashes means | is escaped\n        // so we leave it alone\n        return \"|\";\n      } else {\n        // add space before unescaped |\n        return \" |\";\n      }\n    }),\n    cells = row.split(/ \\|/);\n  let i = 0;\n  // First/last cell in a row cannot be empty if it has no leading/trailing pipe\n  if (!cells[0].trim()) {\n    cells.shift();\n  }\n  if (cells.length > 0 && !cells[cells.length - 1].trim()) {\n    cells.pop();\n  }\n  if (count) {\n    if (cells.length > count) {\n      cells.splice(count);\n    } else {\n      while (cells.length < count) cells.push(\"\");\n    }\n  }\n  for (; i < cells.length; i++) {\n    // leading or trailing whitespace is ignored per the gfm spec\n    cells[i] = cells[i].trim().replace(/\\\\\\|/g, \"|\");\n  }\n  return cells;\n}\n/**\n * Remove trailing 'c's. Equivalent to str.replace(/c*$/, '').\n * /c*$/ is vulnerable to REDOS.\n *\n * @param str\n * @param c\n * @param invert Remove suffix of non-c chars instead. Default falsey.\n */\nfunction rtrim(str, c, invert) {\n  const l = str.length;\n  if (l === 0) {\n    return \"\";\n  }\n  // Length of suffix matching the invert condition.\n  let suffLen = 0;\n  // Step left until we fail to match the invert condition.\n  while (suffLen < l) {\n    const currChar = str.charAt(l - suffLen - 1);\n    if (currChar === c && !invert) {\n      suffLen++;\n    } else if (currChar !== c && invert) {\n      suffLen++;\n    } else {\n      break;\n    }\n  }\n  return str.slice(0, l - suffLen);\n}\nfunction findClosingBracket(str, b) {\n  if (str.indexOf(b[1]) === -1) {\n    return -1;\n  }\n  let level = 0;\n  for (let i = 0; i < str.length; i++) {\n    if (str[i] === \"\\\\\") {\n      i++;\n    } else if (str[i] === b[0]) {\n      level++;\n    } else if (str[i] === b[1]) {\n      level--;\n      if (level < 0) {\n        return i;\n      }\n    }\n  }\n  return -1;\n}\n\nfunction outputLink(cap, link, raw, lexer) {\n  const href = link.href;\n  const title = link.title ? escape$1(link.title) : null;\n  const text = cap[1].replace(/\\\\([\\[\\]])/g, \"$1\");\n  if (cap[0].charAt(0) !== \"!\") {\n    lexer.state.inLink = true;\n    const token = {\n      type: \"link\",\n      raw,\n      href,\n      title,\n      text,\n      tokens: lexer.inlineTokens(text),\n    };\n    lexer.state.inLink = false;\n    return token;\n  }\n  return {\n    type: \"image\",\n    raw,\n    href,\n    title,\n    text: escape$1(text),\n  };\n}\nfunction indentCodeCompensation(raw, text) {\n  const matchIndentToCode = raw.match(/^(\\s+)(?:```)/);\n  if (matchIndentToCode === null) {\n    return text;\n  }\n  const indentToCode = matchIndentToCode[1];\n  return text\n    .split(\"\\n\")\n    .map((node) => {\n      const matchIndentInNode = node.match(/^\\s+/);\n      if (matchIndentInNode === null) {\n        return node;\n      }\n      const [indentInNode] = matchIndentInNode;\n      if (indentInNode.length >= indentToCode.length) {\n        return node.slice(indentToCode.length);\n      }\n      return node;\n    })\n    .join(\"\\n\");\n}\n/**\n * Tokenizer\n */\nclass _Tokenizer {\n  options;\n  rules; // set by the lexer\n  lexer; // set by the lexer\n  constructor(options) {\n    this.options = options || _defaults;\n  }\n  space(src) {\n    const cap = this.rules.block.newline.exec(src);\n    if (cap && cap[0].length > 0) {\n      return {\n        type: \"space\",\n        raw: cap[0],\n      };\n    }\n  }\n  code(src) {\n    const cap = this.rules.block.code.exec(src);\n    if (cap) {\n      const text = cap[0].replace(/^(?: {1,4}| {0,3}\\t)/gm, \"\");\n      return {\n        type: \"code\",\n        raw: cap[0],\n        codeBlockStyle: \"indented\",\n        text: !this.options.pedantic ? rtrim(text, \"\\n\") : text,\n      };\n    }\n  }\n  fences(src) {\n    const cap = this.rules.block.fences.exec(src);\n    if (cap) {\n      const raw = cap[0];\n      const text = indentCodeCompensation(raw, cap[3] || \"\");\n      return {\n        type: \"code\",\n        raw,\n        lang: cap[2]\n          ? cap[2].trim().replace(this.rules.inline.anyPunctuation, \"$1\")\n          : cap[2],\n        text,\n      };\n    }\n  }\n  heading(src) {\n    const cap = this.rules.block.heading.exec(src);\n    if (cap) {\n      let text = cap[2].trim();\n      // remove trailing #s\n      if (/#$/.test(text)) {\n        const trimmed = rtrim(text, \"#\");\n        if (this.options.pedantic) {\n          text = trimmed.trim();\n        } else if (!trimmed || / $/.test(trimmed)) {\n          // CommonMark requires space before trailing #s\n          text = trimmed.trim();\n        }\n      }\n      return {\n        type: \"heading\",\n        raw: cap[0],\n        depth: cap[1].length,\n        text,\n        tokens: this.lexer.inline(text),\n      };\n    }\n  }\n  hr(src) {\n    const cap = this.rules.block.hr.exec(src);\n    if (cap) {\n      return {\n        type: \"hr\",\n        raw: rtrim(cap[0], \"\\n\"),\n      };\n    }\n  }\n  blockquote(src) {\n    const cap = this.rules.block.blockquote.exec(src);\n    if (cap) {\n      let lines = rtrim(cap[0], \"\\n\").split(\"\\n\");\n      let raw = \"\";\n      let text = \"\";\n      const tokens = [];\n      while (lines.length > 0) {\n        let inBlockquote = false;\n        const currentLines = [];\n        let i;\n        for (i = 0; i < lines.length; i++) {\n          // get lines up to a continuation\n          if (/^ {0,3}>/.test(lines[i])) {\n            currentLines.push(lines[i]);\n            inBlockquote = true;\n          } else if (!inBlockquote) {\n            currentLines.push(lines[i]);\n          } else {\n            break;\n          }\n        }\n        lines = lines.slice(i);\n        const currentRaw = currentLines.join(\"\\n\");\n        const currentText = currentRaw\n          // precede setext continuation with 4 spaces so it isn't a setext\n          .replace(/\\n {0,3}((?:=+|-+) *)(?=\\n|$)/g, \"\\n    $1\")\n          .replace(/^ {0,3}>[ \\t]?/gm, \"\");\n        raw = raw ? `${raw}\\n${currentRaw}` : currentRaw;\n        text = text ? `${text}\\n${currentText}` : currentText;\n        // parse blockquote lines as top level tokens\n        // merge paragraphs if this is a continuation\n        const top = this.lexer.state.top;\n        this.lexer.state.top = true;\n        this.lexer.blockTokens(currentText, tokens, true);\n        this.lexer.state.top = top;\n        // if there is no continuation then we are done\n        if (lines.length === 0) {\n          break;\n        }\n        const lastToken = tokens[tokens.length - 1];\n        if (lastToken?.type === \"code\") {\n          // blockquote continuation cannot be preceded by a code block\n          break;\n        } else if (lastToken?.type === \"blockquote\") {\n          // include continuation in nested blockquote\n          const oldToken = lastToken;\n          const newText = oldToken.raw + \"\\n\" + lines.join(\"\\n\");\n          const newToken = this.blockquote(newText);\n          tokens[tokens.length - 1] = newToken;\n          raw =\n            raw.substring(0, raw.length - oldToken.raw.length) + newToken.raw;\n          text =\n            text.substring(0, text.length - oldToken.text.length) +\n            newToken.text;\n          break;\n        } else if (lastToken?.type === \"list\") {\n          // include continuation in nested list\n          const oldToken = lastToken;\n          const newText = oldToken.raw + \"\\n\" + lines.join(\"\\n\");\n          const newToken = this.list(newText);\n          tokens[tokens.length - 1] = newToken;\n          raw =\n            raw.substring(0, raw.length - lastToken.raw.length) + newToken.raw;\n          text =\n            text.substring(0, text.length - oldToken.raw.length) + newToken.raw;\n          lines = newText\n            .substring(tokens[tokens.length - 1].raw.length)\n            .split(\"\\n\");\n          continue;\n        }\n      }\n      return {\n        type: \"blockquote\",\n        raw,\n        tokens,\n        text,\n      };\n    }\n  }\n  list(src) {\n    let cap = this.rules.block.list.exec(src);\n    if (cap) {\n      let bull = cap[1].trim();\n      const isordered = bull.length > 1;\n      const list = {\n        type: \"list\",\n        raw: \"\",\n        ordered: isordered,\n        start: isordered ? +bull.slice(0, -1) : \"\",\n        loose: false,\n        items: [],\n      };\n      bull = isordered ? `\\\\d{1,9}\\\\${bull.slice(-1)}` : `\\\\${bull}`;\n      if (this.options.pedantic) {\n        bull = isordered ? bull : \"[*+-]\";\n      }\n      // Get next list item\n      const itemRegex = new RegExp(\n        `^( {0,3}${bull})((?:[\\t ][^\\\\n]*)?(?:\\\\n|$))`\n      );\n      let endsWithBlankLine = false;\n      // Check if current bullet point can start a new List Item\n      while (src) {\n        let endEarly = false;\n        let raw = \"\";\n        let itemContents = \"\";\n        if (!(cap = itemRegex.exec(src))) {\n          break;\n        }\n        if (this.rules.block.hr.test(src)) {\n          // End list if bullet was actually HR (possibly move into itemRegex?)\n          break;\n        }\n        raw = cap[0];\n        src = src.substring(raw.length);\n        let line = cap[2]\n          .split(\"\\n\", 1)[0]\n          .replace(/^\\t+/, (t) => \" \".repeat(3 * t.length));\n        let nextLine = src.split(\"\\n\", 1)[0];\n        let blankLine = !line.trim();\n        let indent = 0;\n        if (this.options.pedantic) {\n          indent = 2;\n          itemContents = line.trimStart();\n        } else if (blankLine) {\n          indent = cap[1].length + 1;\n        } else {\n          indent = cap[2].search(/[^ ]/); // Find first non-space char\n          indent = indent > 4 ? 1 : indent; // Treat indented code blocks (> 4 spaces) as having only 1 indent\n          itemContents = line.slice(indent);\n          indent += cap[1].length;\n        }\n        if (blankLine && /^[ \\t]*$/.test(nextLine)) {\n          // Items begin with at most one blank line\n          raw += nextLine + \"\\n\";\n          src = src.substring(nextLine.length + 1);\n          endEarly = true;\n        }\n        if (!endEarly) {\n          const nextBulletRegex = new RegExp(\n            `^ {0,${Math.min(3, indent - 1)}}(?:[*+-]|\\\\d{1,9}[.)])((?:[ \\t][^\\\\n]*)?(?:\\\\n|$))`\n          );\n          const hrRegex = new RegExp(\n            `^ {0,${Math.min(3, indent - 1)}}((?:- *){3,}|(?:_ *){3,}|(?:\\\\* *){3,})(?:\\\\n+|$)`\n          );\n          const fencesBeginRegex = new RegExp(\n            `^ {0,${Math.min(3, indent - 1)}}(?:\\`\\`\\`|~~~)`\n          );\n          const headingBeginRegex = new RegExp(\n            `^ {0,${Math.min(3, indent - 1)}}#`\n          );\n          const htmlBeginRegex = new RegExp(\n            `^ {0,${Math.min(3, indent - 1)}}<[a-z].*>`,\n            \"i\"\n          );\n          // Check if following lines should be included in List Item\n          while (src) {\n            const rawLine = src.split(\"\\n\", 1)[0];\n            let nextLineWithoutTabs;\n            nextLine = rawLine;\n            // Re-align to follow commonmark nesting rules\n            if (this.options.pedantic) {\n              nextLine = nextLine.replace(/^ {1,4}(?=( {4})*[^ ])/g, \"  \");\n              nextLineWithoutTabs = nextLine;\n            } else {\n              nextLineWithoutTabs = nextLine.replace(/\\t/g, \"    \");\n            }\n            // End list item if found code fences\n            if (fencesBeginRegex.test(nextLine)) {\n              break;\n            }\n            // End list item if found start of new heading\n            if (headingBeginRegex.test(nextLine)) {\n              break;\n            }\n            // End list item if found start of html block\n            if (htmlBeginRegex.test(nextLine)) {\n              break;\n            }\n            // End list item if found start of new bullet\n            if (nextBulletRegex.test(nextLine)) {\n              break;\n            }\n            // Horizontal rule found\n            if (hrRegex.test(nextLine)) {\n              break;\n            }\n            if (\n              nextLineWithoutTabs.search(/[^ ]/) >= indent ||\n              !nextLine.trim()\n            ) {\n              // Dedent if possible\n              itemContents += \"\\n\" + nextLineWithoutTabs.slice(indent);\n            } else {\n              // not enough indentation\n              if (blankLine) {\n                break;\n              }\n              // paragraph continuation unless last line was a different block level element\n              if (line.replace(/\\t/g, \"    \").search(/[^ ]/) >= 4) {\n                // indented code block\n                break;\n              }\n              if (fencesBeginRegex.test(line)) {\n                break;\n              }\n              if (headingBeginRegex.test(line)) {\n                break;\n              }\n              if (hrRegex.test(line)) {\n                break;\n              }\n              itemContents += \"\\n\" + nextLine;\n            }\n            if (!blankLine && !nextLine.trim()) {\n              // Check if current line is blank\n              blankLine = true;\n            }\n            raw += rawLine + \"\\n\";\n            src = src.substring(rawLine.length + 1);\n            line = nextLineWithoutTabs.slice(indent);\n          }\n        }\n        if (!list.loose) {\n          // If the previous item ended with a blank line, the list is loose\n          if (endsWithBlankLine) {\n            list.loose = true;\n          } else if (/\\n[ \\t]*\\n[ \\t]*$/.test(raw)) {\n            endsWithBlankLine = true;\n          }\n        }\n        let istask = null;\n        let ischecked;\n        // Check for task list items\n        if (this.options.gfm) {\n          istask = /^\\[[ xX]\\] /.exec(itemContents);\n          if (istask) {\n            ischecked = istask[0] !== \"[ ] \";\n            itemContents = itemContents.replace(/^\\[[ xX]\\] +/, \"\");\n          }\n        }\n        list.items.push({\n          type: \"list_item\",\n          raw,\n          task: !!istask,\n          checked: ischecked,\n          loose: false,\n          text: itemContents,\n          tokens: [],\n        });\n        list.raw += raw;\n      }\n      // Do not consume newlines at end of final item. Alternatively, make itemRegex *start* with any newlines to simplify/speed up endsWithBlankLine logic\n      list.items[list.items.length - 1].raw =\n        list.items[list.items.length - 1].raw.trimEnd();\n      list.items[list.items.length - 1].text =\n        list.items[list.items.length - 1].text.trimEnd();\n      list.raw = list.raw.trimEnd();\n      // Item child tokens handled here at end because we needed to have the final item to trim it first\n      for (let i = 0; i < list.items.length; i++) {\n        this.lexer.state.top = false;\n        list.items[i].tokens = this.lexer.blockTokens(list.items[i].text, []);\n        if (!list.loose) {\n          // Check if list should be loose\n          const spacers = list.items[i].tokens.filter(\n            (t) => t.type === \"space\"\n          );\n          const hasMultipleLineBreaks =\n            spacers.length > 0 && spacers.some((t) => /\\n.*\\n/.test(t.raw));\n          list.loose = hasMultipleLineBreaks;\n        }\n      }\n      // Set all items to loose if list is loose\n      if (list.loose) {\n        for (let i = 0; i < list.items.length; i++) {\n          list.items[i].loose = true;\n        }\n      }\n      return list;\n    }\n  }\n  html(src) {\n    const cap = this.rules.block.html.exec(src);\n    if (cap) {\n      const token = {\n        type: \"html\",\n        block: true,\n        raw: cap[0],\n        pre: cap[1] === \"pre\" || cap[1] === \"script\" || cap[1] === \"style\",\n        text: cap[0],\n      };\n      return token;\n    }\n  }\n  def(src) {\n    const cap = this.rules.block.def.exec(src);\n    if (cap) {\n      const tag = cap[1].toLowerCase().replace(/\\s+/g, \" \");\n      const href = cap[2]\n        ? cap[2]\n            .replace(/^<(.*)>$/, \"$1\")\n            .replace(this.rules.inline.anyPunctuation, \"$1\")\n        : \"\";\n      const title = cap[3]\n        ? cap[3]\n            .substring(1, cap[3].length - 1)\n            .replace(this.rules.inline.anyPunctuation, \"$1\")\n        : cap[3];\n      return {\n        type: \"def\",\n        tag,\n        raw: cap[0],\n        href,\n        title,\n      };\n    }\n  }\n  table(src) {\n    const cap = this.rules.block.table.exec(src);\n    if (!cap) {\n      return;\n    }\n    if (!/[:|]/.test(cap[2])) {\n      // delimiter row must have a pipe (|) or colon (:) otherwise it is a setext heading\n      return;\n    }\n    const headers = splitCells(cap[1]);\n    const aligns = cap[2].replace(/^\\||\\| *$/g, \"\").split(\"|\");\n    const rows =\n      cap[3] && cap[3].trim()\n        ? cap[3].replace(/\\n[ \\t]*$/, \"\").split(\"\\n\")\n        : [];\n    const item = {\n      type: \"table\",\n      raw: cap[0],\n      header: [],\n      align: [],\n      rows: [],\n    };\n    if (headers.length !== aligns.length) {\n      // header and align columns must be equal, rows can be different.\n      return;\n    }\n    for (const align of aligns) {\n      if (/^ *-+: *$/.test(align)) {\n        item.align.push(\"right\");\n      } else if (/^ *:-+: *$/.test(align)) {\n        item.align.push(\"center\");\n      } else if (/^ *:-+ *$/.test(align)) {\n        item.align.push(\"left\");\n      } else {\n        item.align.push(null);\n      }\n    }\n    for (let i = 0; i < headers.length; i++) {\n      item.header.push({\n        text: headers[i],\n        tokens: this.lexer.inline(headers[i]),\n        header: true,\n        align: item.align[i],\n      });\n    }\n    for (const row of rows) {\n      item.rows.push(\n        splitCells(row, item.header.length).map((cell, i) => {\n          return {\n            text: cell,\n            tokens: this.lexer.inline(cell),\n            header: false,\n            align: item.align[i],\n          };\n        })\n      );\n    }\n    return item;\n  }\n  lheading(src) {\n    const cap = this.rules.block.lheading.exec(src);\n    if (cap) {\n      return {\n        type: \"heading\",\n        raw: cap[0],\n        depth: cap[2].charAt(0) === \"=\" ? 1 : 2,\n        text: cap[1],\n        tokens: this.lexer.inline(cap[1]),\n      };\n    }\n  }\n  paragraph(src) {\n    const cap = this.rules.block.paragraph.exec(src);\n    if (cap) {\n      const text =\n        cap[1].charAt(cap[1].length - 1) === \"\\n\"\n          ? cap[1].slice(0, -1)\n          : cap[1];\n      return {\n        type: \"paragraph\",\n        raw: cap[0],\n        text,\n        tokens: this.lexer.inline(text),\n      };\n    }\n  }\n  text(src) {\n    const cap = this.rules.block.text.exec(src);\n    if (cap) {\n      return {\n        type: \"text\",\n        raw: cap[0],\n        text: cap[0],\n        tokens: this.lexer.inline(cap[0]),\n      };\n    }\n  }\n  escape(src) {\n    const cap = this.rules.inline.escape.exec(src);\n    if (cap) {\n      return {\n        type: \"escape\",\n        raw: cap[0],\n        text: escape$1(cap[1]),\n      };\n    }\n  }\n  tag(src) {\n    const cap = this.rules.inline.tag.exec(src);\n    if (cap) {\n      if (!this.lexer.state.inLink && /^<a /i.test(cap[0])) {\n        this.lexer.state.inLink = true;\n      } else if (this.lexer.state.inLink && /^<\\/a>/i.test(cap[0])) {\n        this.lexer.state.inLink = false;\n      }\n      if (\n        !this.lexer.state.inRawBlock &&\n        /^<(pre|code|kbd|script)(\\s|>)/i.test(cap[0])\n      ) {\n        this.lexer.state.inRawBlock = true;\n      } else if (\n        this.lexer.state.inRawBlock &&\n        /^<\\/(pre|code|kbd|script)(\\s|>)/i.test(cap[0])\n      ) {\n        this.lexer.state.inRawBlock = false;\n      }\n      return {\n        type: \"html\",\n        raw: cap[0],\n        inLink: this.lexer.state.inLink,\n        inRawBlock: this.lexer.state.inRawBlock,\n        block: false,\n        text: cap[0],\n      };\n    }\n  }\n  link(src) {\n    const cap = this.rules.inline.link.exec(src);\n    if (cap) {\n      const trimmedUrl = cap[2].trim();\n      if (!this.options.pedantic && /^</.test(trimmedUrl)) {\n        // commonmark requires matching angle brackets\n        if (!/>$/.test(trimmedUrl)) {\n          return;\n        }\n        // ending angle bracket cannot be escaped\n        const rtrimSlash = rtrim(trimmedUrl.slice(0, -1), \"\\\\\");\n        if ((trimmedUrl.length - rtrimSlash.length) % 2 === 0) {\n          return;\n        }\n      } else {\n        // find closing parenthesis\n        const lastParenIndex = findClosingBracket(cap[2], \"()\");\n        if (lastParenIndex > -1) {\n          const start = cap[0].indexOf(\"!\") === 0 ? 5 : 4;\n          const linkLen = start + cap[1].length + lastParenIndex;\n          cap[2] = cap[2].substring(0, lastParenIndex);\n          cap[0] = cap[0].substring(0, linkLen).trim();\n          cap[3] = \"\";\n        }\n      }\n      let href = cap[2];\n      let title = \"\";\n      if (this.options.pedantic) {\n        // split pedantic href and title\n        const link = /^([^'\"]*[^\\s])\\s+(['\"])(.*)\\2/.exec(href);\n        if (link) {\n          href = link[1];\n          title = link[3];\n        }\n      } else {\n        title = cap[3] ? cap[3].slice(1, -1) : \"\";\n      }\n      href = href.trim();\n      if (/^</.test(href)) {\n        if (this.options.pedantic && !/>$/.test(trimmedUrl)) {\n          // pedantic allows starting angle bracket without ending angle bracket\n          href = href.slice(1);\n        } else {\n          href = href.slice(1, -1);\n        }\n      }\n      return outputLink(\n        cap,\n        {\n          href: href\n            ? href.replace(this.rules.inline.anyPunctuation, \"$1\")\n            : href,\n          title: title\n            ? title.replace(this.rules.inline.anyPunctuation, \"$1\")\n            : title,\n        },\n        cap[0],\n        this.lexer\n      );\n    }\n  }\n  reflink(src, links) {\n    let cap;\n    if (\n      (cap = this.rules.inline.reflink.exec(src)) ||\n      (cap = this.rules.inline.nolink.exec(src))\n    ) {\n      const linkString = (cap[2] || cap[1]).replace(/\\s+/g, \" \");\n      const link = links[linkString.toLowerCase()];\n      if (!link) {\n        const text = cap[0].charAt(0);\n        return {\n          type: \"text\",\n          raw: text,\n          text,\n        };\n      }\n      return outputLink(cap, link, cap[0], this.lexer);\n    }\n  }\n  emStrong(src, maskedSrc, prevChar = \"\") {\n    let match = this.rules.inline.emStrongLDelim.exec(src);\n    if (!match) return;\n    // _ can't be between two alphanumerics. \\p{L}\\p{N} includes non-english alphabet/numbers as well\n    if (match[3] && prevChar.match(/[\\p{L}\\p{N}]/u)) return;\n    const nextChar = match[1] || match[2] || \"\";\n    if (\n      !nextChar ||\n      !prevChar ||\n      this.rules.inline.punctuation.exec(prevChar)\n    ) {\n      // unicode Regex counts emoji as 1 char; spread into array for proper count (used multiple times below)\n      const lLength = [...match[0]].length - 1;\n      let rDelim,\n        rLength,\n        delimTotal = lLength,\n        midDelimTotal = 0;\n      const endReg =\n        match[0][0] === \"*\"\n          ? this.rules.inline.emStrongRDelimAst\n          : this.rules.inline.emStrongRDelimUnd;\n      endReg.lastIndex = 0;\n      // Clip maskedSrc to same section of string as src (move to lexer?)\n      maskedSrc = maskedSrc.slice(-1 * src.length + lLength);\n      while ((match = endReg.exec(maskedSrc)) != null) {\n        rDelim =\n          match[1] || match[2] || match[3] || match[4] || match[5] || match[6];\n        if (!rDelim) continue; // skip single * in __abc*abc__\n        rLength = [...rDelim].length;\n        if (match[3] || match[4]) {\n          // found another Left Delim\n          delimTotal += rLength;\n          continue;\n        } else if (match[5] || match[6]) {\n          // either Left or Right Delim\n          if (lLength % 3 && !((lLength + rLength) % 3)) {\n            midDelimTotal += rLength;\n            continue; // CommonMark Emphasis Rules 9-10\n          }\n        }\n        delimTotal -= rLength;\n        if (delimTotal > 0) continue; // Haven't found enough closing delimiters\n        // Remove extra characters. *a*** -> *a*\n        rLength = Math.min(rLength, rLength + delimTotal + midDelimTotal);\n        // char length can be >1 for unicode characters;\n        const lastCharLength = [...match[0]][0].length;\n        const raw = src.slice(\n          0,\n          lLength + match.index + lastCharLength + rLength\n        );\n        // Create `em` if smallest delimiter has odd char count. *a***\n        if (Math.min(lLength, rLength) % 2) {\n          const text = raw.slice(1, -1);\n          return {\n            type: \"em\",\n            raw,\n            text,\n            tokens: this.lexer.inlineTokens(text),\n          };\n        }\n        // Create 'strong' if smallest delimiter has even char count. **a***\n        const text = raw.slice(2, -2);\n        return {\n          type: \"strong\",\n          raw,\n          text,\n          tokens: this.lexer.inlineTokens(text),\n        };\n      }\n    }\n  }\n  codespan(src) {\n    const cap = this.rules.inline.code.exec(src);\n    if (cap) {\n      let text = cap[2].replace(/\\n/g, \" \");\n      const hasNonSpaceChars = /[^ ]/.test(text);\n      const hasSpaceCharsOnBothEnds = /^ /.test(text) && / $/.test(text);\n      if (hasNonSpaceChars && hasSpaceCharsOnBothEnds) {\n        text = text.substring(1, text.length - 1);\n      }\n      text = escape$1(text, true);\n      return {\n        type: \"codespan\",\n        raw: cap[0],\n        text,\n      };\n    }\n  }\n  br(src) {\n    const cap = this.rules.inline.br.exec(src);\n    if (cap) {\n      return {\n        type: \"br\",\n        raw: cap[0],\n      };\n    }\n  }\n  del(src) {\n    const cap = this.rules.inline.del.exec(src);\n    if (cap) {\n      return {\n        type: \"del\",\n        raw: cap[0],\n        text: cap[2],\n        tokens: this.lexer.inlineTokens(cap[2]),\n      };\n    }\n  }\n  autolink(src) {\n    const cap = this.rules.inline.autolink.exec(src);\n    if (cap) {\n      let text, href;\n      if (cap[2] === \"@\") {\n        text = escape$1(cap[1]);\n        href = \"mailto:\" + text;\n      } else {\n        text = escape$1(cap[1]);\n        href = text;\n      }\n      return {\n        type: \"link\",\n        raw: cap[0],\n        text,\n        href,\n        tokens: [\n          {\n            type: \"text\",\n            raw: text,\n            text,\n          },\n        ],\n      };\n    }\n  }\n  url(src) {\n    let cap;\n    if ((cap = this.rules.inline.url.exec(src))) {\n      let text, href;\n      if (cap[2] === \"@\") {\n        text = escape$1(cap[0]);\n        href = \"mailto:\" + text;\n      } else {\n        // do extended autolink path validation\n        let prevCapZero;\n        do {\n          prevCapZero = cap[0];\n          cap[0] = this.rules.inline._backpedal.exec(cap[0])?.[0] ?? \"\";\n        } while (prevCapZero !== cap[0]);\n        text = escape$1(cap[0]);\n        if (cap[1] === \"www.\") {\n          href = \"http://\" + cap[0];\n        } else {\n          href = cap[0];\n        }\n      }\n      return {\n        type: \"link\",\n        raw: cap[0],\n        text,\n        href,\n        tokens: [\n          {\n            type: \"text\",\n            raw: text,\n            text,\n          },\n        ],\n      };\n    }\n  }\n  inlineText(src) {\n    const cap = this.rules.inline.text.exec(src);\n    if (cap) {\n      let text;\n      if (this.lexer.state.inRawBlock) {\n        text = cap[0];\n      } else {\n        text = escape$1(cap[0]);\n      }\n      return {\n        type: \"text\",\n        raw: cap[0],\n        text,\n      };\n    }\n  }\n}\n\n/**\n * Block-Level Grammar\n */\nconst newline = /^(?:[ \\t]*(?:\\n|$))+/;\nconst blockCode = /^((?: {4}| {0,3}\\t)[^\\n]+(?:\\n(?:[ \\t]*(?:\\n|$))*)?)+/;\nconst fences =\n  /^ {0,3}(`{3,}(?=[^`\\n]*(?:\\n|$))|~{3,})([^\\n]*)(?:\\n|$)(?:|([\\s\\S]*?)(?:\\n|$))(?: {0,3}\\1[~`]* *(?=\\n|$)|$)/;\nconst hr = /^ {0,3}((?:-[\\t ]*){3,}|(?:_[ \\t]*){3,}|(?:\\*[ \\t]*){3,})(?:\\n+|$)/;\nconst heading = /^ {0,3}(#{1,6})(?=\\s|$)(.*)(?:\\n+|$)/;\nconst bullet = /(?:[*+-]|\\d{1,9}[.)])/;\nconst lheading = edit(\n  /^(?!bull |blockCode|fences|blockquote|heading|html)((?:.|\\n(?!\\s*?\\n|bull |blockCode|fences|blockquote|heading|html))+?)\\n {0,3}(=+|-+) *(?:\\n+|$)/\n)\n  .replace(/bull/g, bullet) // lists can interrupt\n  .replace(/blockCode/g, /(?: {4}| {0,3}\\t)/) // indented code blocks can interrupt\n  .replace(/fences/g, / {0,3}(?:`{3,}|~{3,})/) // fenced code blocks can interrupt\n  .replace(/blockquote/g, / {0,3}>/) // blockquote can interrupt\n  .replace(/heading/g, / {0,3}#{1,6}/) // ATX heading can interrupt\n  .replace(/html/g, / {0,3}<[^\\n>]+>\\n/) // block html can interrupt\n  .getRegex();\nconst _paragraph =\n  /^([^\\n]+(?:\\n(?!hr|heading|lheading|blockquote|fences|list|html|table| +\\n)[^\\n]+)*)/;\nconst blockText = /^[^\\n]+/;\nconst _blockLabel = /(?!\\s*\\])(?:\\\\.|[^\\[\\]\\\\])+/;\nconst def = edit(\n  /^ {0,3}\\[(label)\\]: *(?:\\n[ \\t]*)?([^<\\s][^\\s]*|<.*?>)(?:(?: +(?:\\n[ \\t]*)?| *\\n[ \\t]*)(title))? *(?:\\n+|$)/\n)\n  .replace(\"label\", _blockLabel)\n  .replace(\n    \"title\",\n    /(?:\"(?:\\\\\"?|[^\"\\\\])*\"|'[^'\\n]*(?:\\n[^'\\n]+)*\\n?'|\\([^()]*\\))/\n  )\n  .getRegex();\nconst list = edit(/^( {0,3}bull)([ \\t][^\\n]+?)?(?:\\n|$)/)\n  .replace(/bull/g, bullet)\n  .getRegex();\nconst _tag =\n  \"address|article|aside|base|basefont|blockquote|body|caption\" +\n  \"|center|col|colgroup|dd|details|dialog|dir|div|dl|dt|fieldset|figcaption\" +\n  \"|figure|footer|form|frame|frameset|h[1-6]|head|header|hr|html|iframe\" +\n  \"|legend|li|link|main|menu|menuitem|meta|nav|noframes|ol|optgroup|option\" +\n  \"|p|param|search|section|summary|table|tbody|td|tfoot|th|thead|title\" +\n  \"|tr|track|ul\";\nconst _comment = /<!--(?:-?>|[\\s\\S]*?(?:-->|$))/;\nconst html = edit(\n  \"^ {0,3}(?:\" + // optional indentation\n    \"<(script|pre|style|textarea)[\\\\s>][\\\\s\\\\S]*?(?:</\\\\1>[^\\\\n]*\\\\n+|$)\" + // (1)\n    \"|comment[^\\\\n]*(\\\\n+|$)\" + // (2)\n    \"|<\\\\?[\\\\s\\\\S]*?(?:\\\\?>\\\\n*|$)\" + // (3)\n    \"|<![A-Z][\\\\s\\\\S]*?(?:>\\\\n*|$)\" + // (4)\n    \"|<!\\\\[CDATA\\\\[[\\\\s\\\\S]*?(?:\\\\]\\\\]>\\\\n*|$)\" + // (5)\n    \"|</?(tag)(?: +|\\\\n|/?>)[\\\\s\\\\S]*?(?:(?:\\\\n[ \\t]*)+\\\\n|$)\" + // (6)\n    \"|<(?!script|pre|style|textarea)([a-z][\\\\w-]*)(?:attribute)*? */?>(?=[ \\\\t]*(?:\\\\n|$))[\\\\s\\\\S]*?(?:(?:\\\\n[ \\t]*)+\\\\n|$)\" + // (7) open tag\n    \"|</(?!script|pre|style|textarea)[a-z][\\\\w-]*\\\\s*>(?=[ \\\\t]*(?:\\\\n|$))[\\\\s\\\\S]*?(?:(?:\\\\n[ \\t]*)+\\\\n|$)\" + // (7) closing tag\n    \")\",\n  \"i\"\n)\n  .replace(\"comment\", _comment)\n  .replace(\"tag\", _tag)\n  .replace(\n    \"attribute\",\n    / +[a-zA-Z:_][\\w.:-]*(?: *= *\"[^\"\\n]*\"| *= *'[^'\\n]*'| *= *[^\\s\"'=<>`]+)?/\n  )\n  .getRegex();\nconst paragraph = edit(_paragraph)\n  .replace(\"hr\", hr)\n  .replace(\"heading\", \" {0,3}#{1,6}(?:\\\\s|$)\")\n  .replace(\"|lheading\", \"\") // setext headings don't interrupt commonmark paragraphs\n  .replace(\"|table\", \"\")\n  .replace(\"blockquote\", \" {0,3}>\")\n  .replace(\"fences\", \" {0,3}(?:`{3,}(?=[^`\\\\n]*\\\\n)|~{3,})[^\\\\n]*\\\\n\")\n  .replace(\"list\", \" {0,3}(?:[*+-]|1[.)]) \") // only lists starting from 1 can interrupt\n  .replace(\n    \"html\",\n    \"</?(?:tag)(?: +|\\\\n|/?>)|<(?:script|pre|style|textarea|!--)\"\n  )\n  .replace(\"tag\", _tag) // pars can be interrupted by type (6) html blocks\n  .getRegex();\nconst blockquote = edit(/^( {0,3}> ?(paragraph|[^\\n]*)(?:\\n|$))+/)\n  .replace(\"paragraph\", paragraph)\n  .getRegex();\n/**\n * Normal Block Grammar\n */\nconst blockNormal = {\n  blockquote,\n  code: blockCode,\n  def,\n  fences,\n  heading,\n  hr,\n  html,\n  lheading,\n  list,\n  newline,\n  paragraph,\n  table: noopTest,\n  text: blockText,\n};\n/**\n * GFM Block Grammar\n */\nconst gfmTable = edit(\n  \"^ *([^\\\\n ].*)\\\\n\" + // Header\n    \" {0,3}((?:\\\\| *)?:?-+:? *(?:\\\\| *:?-+:? *)*(?:\\\\| *)?)\" + // Align\n    \"(?:\\\\n((?:(?! *\\\\n|hr|heading|blockquote|code|fences|list|html).*(?:\\\\n|$))*)\\\\n*|$)\"\n) // Cells\n  .replace(\"hr\", hr)\n  .replace(\"heading\", \" {0,3}#{1,6}(?:\\\\s|$)\")\n  .replace(\"blockquote\", \" {0,3}>\")\n  .replace(\"code\", \"(?: {4}| {0,3}\\t)[^\\\\n]\")\n  .replace(\"fences\", \" {0,3}(?:`{3,}(?=[^`\\\\n]*\\\\n)|~{3,})[^\\\\n]*\\\\n\")\n  .replace(\"list\", \" {0,3}(?:[*+-]|1[.)]) \") // only lists starting from 1 can interrupt\n  .replace(\n    \"html\",\n    \"</?(?:tag)(?: +|\\\\n|/?>)|<(?:script|pre|style|textarea|!--)\"\n  )\n  .replace(\"tag\", _tag) // tables can be interrupted by type (6) html blocks\n  .getRegex();\nconst blockGfm = {\n  ...blockNormal,\n  table: gfmTable,\n  paragraph: edit(_paragraph)\n    .replace(\"hr\", hr)\n    .replace(\"heading\", \" {0,3}#{1,6}(?:\\\\s|$)\")\n    .replace(\"|lheading\", \"\") // setext headings don't interrupt commonmark paragraphs\n    .replace(\"table\", gfmTable) // interrupt paragraphs with table\n    .replace(\"blockquote\", \" {0,3}>\")\n    .replace(\"fences\", \" {0,3}(?:`{3,}(?=[^`\\\\n]*\\\\n)|~{3,})[^\\\\n]*\\\\n\")\n    .replace(\"list\", \" {0,3}(?:[*+-]|1[.)]) \") // only lists starting from 1 can interrupt\n    .replace(\n      \"html\",\n      \"</?(?:tag)(?: +|\\\\n|/?>)|<(?:script|pre|style|textarea|!--)\"\n    )\n    .replace(\"tag\", _tag) // pars can be interrupted by type (6) html blocks\n    .getRegex(),\n};\n/**\n * Pedantic grammar (original John Gruber's loose markdown specification)\n */\nconst blockPedantic = {\n  ...blockNormal,\n  html: edit(\n    \"^ *(?:comment *(?:\\\\n|\\\\s*$)\" +\n      \"|<(tag)[\\\\s\\\\S]+?</\\\\1> *(?:\\\\n{2,}|\\\\s*$)\" + // closed tag\n      \"|<tag(?:\\\"[^\\\"]*\\\"|'[^']*'|\\\\s[^'\\\"/>\\\\s]*)*?/?> *(?:\\\\n{2,}|\\\\s*$))\"\n  )\n    .replace(\"comment\", _comment)\n    .replace(\n      /tag/g,\n      \"(?!(?:\" +\n        \"a|em|strong|small|s|cite|q|dfn|abbr|data|time|code|var|samp|kbd|sub\" +\n        \"|sup|i|b|u|mark|ruby|rt|rp|bdi|bdo|span|br|wbr|ins|del|img)\" +\n        \"\\\\b)\\\\w+(?!:|[^\\\\w\\\\s@]*@)\\\\b\"\n    )\n    .getRegex(),\n  def: /^ *\\[([^\\]]+)\\]: *<?([^\\s>]+)>?(?: +([\"(][^\\n]+[\")]))? *(?:\\n+|$)/,\n  heading: /^(#{1,6})(.*)(?:\\n+|$)/,\n  fences: noopTest, // fences not supported\n  lheading: /^(.+?)\\n {0,3}(=+|-+) *(?:\\n+|$)/,\n  paragraph: edit(_paragraph)\n    .replace(\"hr\", hr)\n    .replace(\"heading\", \" *#{1,6} *[^\\n]\")\n    .replace(\"lheading\", lheading)\n    .replace(\"|table\", \"\")\n    .replace(\"blockquote\", \" {0,3}>\")\n    .replace(\"|fences\", \"\")\n    .replace(\"|list\", \"\")\n    .replace(\"|html\", \"\")\n    .replace(\"|tag\", \"\")\n    .getRegex(),\n};\n/**\n * Inline-Level Grammar\n */\nconst escape = /^\\\\([!\"#$%&'()*+,\\-./:;<=>?@\\[\\]\\\\^_`{|}~])/;\nconst inlineCode = /^(`+)([^`]|[^`][\\s\\S]*?[^`])\\1(?!`)/;\nconst br = /^( {2,}|\\\\)\\n(?!\\s*$)/;\nconst inlineText =\n  /^(`+|[^`])(?:(?= {2,}\\n)|[\\s\\S]*?(?:(?=[\\\\<!\\[`*_]|\\b_|$)|[^ ](?= {2,}\\n)))/;\n// list of unicode punctuation marks, plus any missing characters from CommonMark spec\nconst _punctuation = \"\\\\p{P}\\\\p{S}\";\nconst punctuation = edit(/^((?![*_])[\\spunctuation])/, \"u\")\n  .replace(/punctuation/g, _punctuation)\n  .getRegex();\n// sequences em should skip over [title](link), `code`, <html>\nconst blockSkip =\n  /\\[[^[\\]]*?\\]\\((?:\\\\.|[^\\\\\\(\\)]|\\((?:\\\\.|[^\\\\\\(\\)])*\\))*\\)|`[^`]*?`|<[^<>]*?>/g;\nconst emStrongLDelim = edit(\n  /^(?:\\*+(?:((?!\\*)[punct])|[^\\s*]))|^_+(?:((?!_)[punct])|([^\\s_]))/,\n  \"u\"\n)\n  .replace(/punct/g, _punctuation)\n  .getRegex();\nconst emStrongRDelimAst = edit(\n  \"^[^_*]*?__[^_*]*?\\\\*[^_*]*?(?=__)\" + // Skip orphan inside strong\n    \"|[^*]+(?=[^*])\" + // Consume to delim\n    \"|(?!\\\\*)[punct](\\\\*+)(?=[\\\\s]|$)\" + // (1) #*** can only be a Right Delimiter\n    \"|[^punct\\\\s](\\\\*+)(?!\\\\*)(?=[punct\\\\s]|$)\" + // (2) a***#, a*** can only be a Right Delimiter\n    \"|(?!\\\\*)[punct\\\\s](\\\\*+)(?=[^punct\\\\s])\" + // (3) #***a, ***a can only be Left Delimiter\n    \"|[\\\\s](\\\\*+)(?!\\\\*)(?=[punct])\" + // (4) ***# can only be Left Delimiter\n    \"|(?!\\\\*)[punct](\\\\*+)(?!\\\\*)(?=[punct])\" + // (5) #***# can be either Left or Right Delimiter\n    \"|[^punct\\\\s](\\\\*+)(?=[^punct\\\\s])\",\n  \"gu\"\n) // (6) a***a can be either Left or Right Delimiter\n  .replace(/punct/g, _punctuation)\n  .getRegex();\n// (6) Not allowed for _\nconst emStrongRDelimUnd = edit(\n  \"^[^_*]*?\\\\*\\\\*[^_*]*?_[^_*]*?(?=\\\\*\\\\*)\" + // Skip orphan inside strong\n    \"|[^_]+(?=[^_])\" + // Consume to delim\n    \"|(?!_)[punct](_+)(?=[\\\\s]|$)\" + // (1) #___ can only be a Right Delimiter\n    \"|[^punct\\\\s](_+)(?!_)(?=[punct\\\\s]|$)\" + // (2) a___#, a___ can only be a Right Delimiter\n    \"|(?!_)[punct\\\\s](_+)(?=[^punct\\\\s])\" + // (3) #___a, ___a can only be Left Delimiter\n    \"|[\\\\s](_+)(?!_)(?=[punct])\" + // (4) ___# can only be Left Delimiter\n    \"|(?!_)[punct](_+)(?!_)(?=[punct])\",\n  \"gu\"\n) // (5) #___# can be either Left or Right Delimiter\n  .replace(/punct/g, _punctuation)\n  .getRegex();\nconst anyPunctuation = edit(/\\\\([punct])/, \"gu\")\n  .replace(/punct/g, _punctuation)\n  .getRegex();\nconst autolink = edit(/^<(scheme:[^\\s\\x00-\\x1f<>]*|email)>/)\n  .replace(\"scheme\", /[a-zA-Z][a-zA-Z0-9+.-]{1,31}/)\n  .replace(\n    \"email\",\n    /[a-zA-Z0-9.!#$%&'*+/=?^_`{|}~-]+(@)[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)+(?![-_])/\n  )\n  .getRegex();\nconst _inlineComment = edit(_comment).replace(\"(?:-->|$)\", \"-->\").getRegex();\nconst tag = edit(\n  \"^comment\" +\n    \"|^</[a-zA-Z][\\\\w:-]*\\\\s*>\" + // self-closing tag\n    \"|^<[a-zA-Z][\\\\w-]*(?:attribute)*?\\\\s*/?>\" + // open tag\n    \"|^<\\\\?[\\\\s\\\\S]*?\\\\?>\" + // processing instruction, e.g. <?php ?>\n    \"|^<![a-zA-Z]+\\\\s[\\\\s\\\\S]*?>\" + // declaration, e.g. <!DOCTYPE html>\n    \"|^<!\\\\[CDATA\\\\[[\\\\s\\\\S]*?\\\\]\\\\]>\"\n) // CDATA section\n  .replace(\"comment\", _inlineComment)\n  .replace(\n    \"attribute\",\n    /\\s+[a-zA-Z:_][\\w.:-]*(?:\\s*=\\s*\"[^\"]*\"|\\s*=\\s*'[^']*'|\\s*=\\s*[^\\s\"'=<>`]+)?/\n  )\n  .getRegex();\nconst _inlineLabel = /(?:\\[(?:\\\\.|[^\\[\\]\\\\])*\\]|\\\\.|`[^`]*`|[^\\[\\]\\\\`])*?/;\nconst link = edit(/^!?\\[(label)\\]\\(\\s*(href)(?:\\s+(title))?\\s*\\)/)\n  .replace(\"label\", _inlineLabel)\n  .replace(\"href\", /<(?:\\\\.|[^\\n<>\\\\])+>|[^\\s\\x00-\\x1f]*/)\n  .replace(\n    \"title\",\n    /\"(?:\\\\\"?|[^\"\\\\])*\"|'(?:\\\\'?|[^'\\\\])*'|\\((?:\\\\\\)?|[^)\\\\])*\\)/\n  )\n  .getRegex();\nconst reflink = edit(/^!?\\[(label)\\]\\[(ref)\\]/)\n  .replace(\"label\", _inlineLabel)\n  .replace(\"ref\", _blockLabel)\n  .getRegex();\nconst nolink = edit(/^!?\\[(ref)\\](?:\\[\\])?/)\n  .replace(\"ref\", _blockLabel)\n  .getRegex();\nconst reflinkSearch = edit(\"reflink|nolink(?!\\\\()\", \"g\")\n  .replace(\"reflink\", reflink)\n  .replace(\"nolink\", nolink)\n  .getRegex();\n/**\n * Normal Inline Grammar\n */\nconst inlineNormal = {\n  _backpedal: noopTest, // only used for GFM url\n  anyPunctuation,\n  autolink,\n  blockSkip,\n  br,\n  code: inlineCode,\n  del: noopTest,\n  emStrongLDelim,\n  emStrongRDelimAst,\n  emStrongRDelimUnd,\n  escape,\n  link,\n  nolink,\n  punctuation,\n  reflink,\n  reflinkSearch,\n  tag,\n  text: inlineText,\n  url: noopTest,\n};\n/**\n * Pedantic Inline Grammar\n */\nconst inlinePedantic = {\n  ...inlineNormal,\n  link: edit(/^!?\\[(label)\\]\\((.*?)\\)/)\n    .replace(\"label\", _inlineLabel)\n    .getRegex(),\n  reflink: edit(/^!?\\[(label)\\]\\s*\\[([^\\]]*)\\]/)\n    .replace(\"label\", _inlineLabel)\n    .getRegex(),\n};\n/**\n * GFM Inline Grammar\n */\nconst inlineGfm = {\n  ...inlineNormal,\n  escape: edit(escape).replace(\"])\", \"~|])\").getRegex(),\n  url: edit(\n    /^((?:ftp|https?):\\/\\/|www\\.)(?:[a-zA-Z0-9\\-]+\\.?)+[^\\s<]*|^email/,\n    \"i\"\n  )\n    .replace(\n      \"email\",\n      /[A-Za-z0-9._+-]+(@)[a-zA-Z0-9-_]+(?:\\.[a-zA-Z0-9-_]*[a-zA-Z0-9])+(?![-_])/\n    )\n    .getRegex(),\n  _backpedal:\n    /(?:[^?!.,:;*_'\"~()&]+|\\([^)]*\\)|&(?![a-zA-Z0-9]+;$)|[?!.,:;*_'\"~)]+(?!$))+/,\n  del: /^(~~?)(?=[^\\s~])([\\s\\S]*?[^\\s~])\\1(?=[^~]|$)/,\n  text: /^([`~]+|[^`~])(?:(?= {2,}\\n)|(?=[a-zA-Z0-9.!#$%&'*+\\/=?_`{\\|}~-]+@)|[\\s\\S]*?(?:(?=[\\\\<!\\[`*~_]|\\b_|https?:\\/\\/|ftp:\\/\\/|www\\.|$)|[^ ](?= {2,}\\n)|[^a-zA-Z0-9.!#$%&'*+\\/=?_`{\\|}~-](?=[a-zA-Z0-9.!#$%&'*+\\/=?_`{\\|}~-]+@)))/,\n};\n/**\n * GFM + Line Breaks Inline Grammar\n */\nconst inlineBreaks = {\n  ...inlineGfm,\n  br: edit(br).replace(\"{2,}\", \"*\").getRegex(),\n  text: edit(inlineGfm.text)\n    .replace(\"\\\\b_\", \"\\\\b_| {2,}\\\\n\")\n    .replace(/\\{2,\\}/g, \"*\")\n    .getRegex(),\n};\n/**\n * exports\n */\nconst block = {\n  normal: blockNormal,\n  gfm: blockGfm,\n  pedantic: blockPedantic,\n};\nconst inline = {\n  normal: inlineNormal,\n  gfm: inlineGfm,\n  breaks: inlineBreaks,\n  pedantic: inlinePedantic,\n};\n\n/**\n * Block Lexer\n */\nclass _Lexer {\n  tokens;\n  options;\n  state;\n  tokenizer;\n  inlineQueue;\n  constructor(options) {\n    // TokenList cannot be created in one go\n    this.tokens = [];\n    this.tokens.links = Object.create(null);\n    this.options = options || _defaults;\n    this.options.tokenizer = this.options.tokenizer || new _Tokenizer();\n    this.tokenizer = this.options.tokenizer;\n    this.tokenizer.options = this.options;\n    this.tokenizer.lexer = this;\n    this.inlineQueue = [];\n    this.state = {\n      inLink: false,\n      inRawBlock: false,\n      top: true,\n    };\n    const rules = {\n      block: block.normal,\n      inline: inline.normal,\n    };\n    if (this.options.pedantic) {\n      rules.block = block.pedantic;\n      rules.inline = inline.pedantic;\n    } else if (this.options.gfm) {\n      rules.block = block.gfm;\n      if (this.options.breaks) {\n        rules.inline = inline.breaks;\n      } else {\n        rules.inline = inline.gfm;\n      }\n    }\n    this.tokenizer.rules = rules;\n  }\n  /**\n   * Expose Rules\n   */\n  static get rules() {\n    return {\n      block,\n      inline,\n    };\n  }\n  /**\n   * Static Lex Method\n   */\n  static lex(src, options) {\n    const lexer = new _Lexer(options);\n    return lexer.lex(src);\n  }\n  /**\n   * Static Lex Inline Method\n   */\n  static lexInline(src, options) {\n    const lexer = new _Lexer(options);\n    return lexer.inlineTokens(src);\n  }\n  /**\n   * Preprocessing\n   */\n  lex(src) {\n    src = src.replace(/\\r\\n|\\r/g, \"\\n\");\n    this.blockTokens(src, this.tokens);\n    for (let i = 0; i < this.inlineQueue.length; i++) {\n      const next = this.inlineQueue[i];\n      this.inlineTokens(next.src, next.tokens);\n    }\n    this.inlineQueue = [];\n    return this.tokens;\n  }\n  blockTokens(src, tokens = [], lastParagraphClipped = false) {\n    if (this.options.pedantic) {\n      src = src.replace(/\\t/g, \"    \").replace(/^ +$/gm, \"\");\n    }\n    let token;\n    let lastToken;\n    let cutSrc;\n    while (src) {\n      if (\n        this.options.extensions &&\n        this.options.extensions.block &&\n        this.options.extensions.block.some((extTokenizer) => {\n          if ((token = extTokenizer.call({ lexer: this }, src, tokens))) {\n            src = src.substring(token.raw.length);\n            tokens.push(token);\n            return true;\n          }\n          return false;\n        })\n      ) {\n        continue;\n      }\n      // newline\n      if ((token = this.tokenizer.space(src))) {\n        src = src.substring(token.raw.length);\n        if (token.raw.length === 1 && tokens.length > 0) {\n          // if there's a single \\n as a spacer, it's terminating the last line,\n          // so move it there so that we don't get unnecessary paragraph tags\n          tokens[tokens.length - 1].raw += \"\\n\";\n        } else {\n          tokens.push(token);\n        }\n        continue;\n      }\n      // code\n      if ((token = this.tokenizer.code(src))) {\n        src = src.substring(token.raw.length);\n        lastToken = tokens[tokens.length - 1];\n        // An indented code block cannot interrupt a paragraph.\n        if (\n          lastToken &&\n          (lastToken.type === \"paragraph\" || lastToken.type === \"text\")\n        ) {\n          lastToken.raw += \"\\n\" + token.raw;\n          lastToken.text += \"\\n\" + token.text;\n          this.inlineQueue[this.inlineQueue.length - 1].src = lastToken.text;\n        } else {\n          tokens.push(token);\n        }\n        continue;\n      }\n      // fences\n      if ((token = this.tokenizer.fences(src))) {\n        src = src.substring(token.raw.length);\n        tokens.push(token);\n        continue;\n      }\n      // heading\n      if ((token = this.tokenizer.heading(src))) {\n        src = src.substring(token.raw.length);\n        tokens.push(token);\n        continue;\n      }\n      // hr\n      if ((token = this.tokenizer.hr(src))) {\n        src = src.substring(token.raw.length);\n        tokens.push(token);\n        continue;\n      }\n      // blockquote\n      if ((token = this.tokenizer.blockquote(src))) {\n        src = src.substring(token.raw.length);\n        tokens.push(token);\n        continue;\n      }\n      // list\n      if ((token = this.tokenizer.list(src))) {\n        src = src.substring(token.raw.length);\n        tokens.push(token);\n        continue;\n      }\n      // html\n      if ((token = this.tokenizer.html(src))) {\n        src = src.substring(token.raw.length);\n        tokens.push(token);\n        continue;\n      }\n      // def\n      if ((token = this.tokenizer.def(src))) {\n        src = src.substring(token.raw.length);\n        lastToken = tokens[tokens.length - 1];\n        if (\n          lastToken &&\n          (lastToken.type === \"paragraph\" || lastToken.type === \"text\")\n        ) {\n          lastToken.raw += \"\\n\" + token.raw;\n          lastToken.text += \"\\n\" + token.raw;\n          this.inlineQueue[this.inlineQueue.length - 1].src = lastToken.text;\n        } else if (!this.tokens.links[token.tag]) {\n          this.tokens.links[token.tag] = {\n            href: token.href,\n            title: token.title,\n          };\n        }\n        continue;\n      }\n      // table (gfm)\n      if ((token = this.tokenizer.table(src))) {\n        src = src.substring(token.raw.length);\n        tokens.push(token);\n        continue;\n      }\n      // lheading\n      if ((token = this.tokenizer.lheading(src))) {\n        src = src.substring(token.raw.length);\n        tokens.push(token);\n        continue;\n      }\n      // top-level paragraph\n      // prevent paragraph consuming extensions by clipping 'src' to extension start\n      cutSrc = src;\n      if (this.options.extensions && this.options.extensions.startBlock) {\n        let startIndex = Infinity;\n        const tempSrc = src.slice(1);\n        let tempStart;\n        this.options.extensions.startBlock.forEach((getStartIndex) => {\n          tempStart = getStartIndex.call({ lexer: this }, tempSrc);\n          if (typeof tempStart === \"number\" && tempStart >= 0) {\n            startIndex = Math.min(startIndex, tempStart);\n          }\n        });\n        if (startIndex < Infinity && startIndex >= 0) {\n          cutSrc = src.substring(0, startIndex + 1);\n        }\n      }\n      if (this.state.top && (token = this.tokenizer.paragraph(cutSrc))) {\n        lastToken = tokens[tokens.length - 1];\n        if (lastParagraphClipped && lastToken?.type === \"paragraph\") {\n          lastToken.raw += \"\\n\" + token.raw;\n          lastToken.text += \"\\n\" + token.text;\n          this.inlineQueue.pop();\n          this.inlineQueue[this.inlineQueue.length - 1].src = lastToken.text;\n        } else {\n          tokens.push(token);\n        }\n        lastParagraphClipped = cutSrc.length !== src.length;\n        src = src.substring(token.raw.length);\n        continue;\n      }\n      // text\n      if ((token = this.tokenizer.text(src))) {\n        src = src.substring(token.raw.length);\n        lastToken = tokens[tokens.length - 1];\n        if (lastToken && lastToken.type === \"text\") {\n          lastToken.raw += \"\\n\" + token.raw;\n          lastToken.text += \"\\n\" + token.text;\n          this.inlineQueue.pop();\n          this.inlineQueue[this.inlineQueue.length - 1].src = lastToken.text;\n        } else {\n          tokens.push(token);\n        }\n        continue;\n      }\n      if (src) {\n        const errMsg = \"Infinite loop on byte: \" + src.charCodeAt(0);\n        if (this.options.silent) {\n          console.error(errMsg);\n          break;\n        } else {\n          throw new Error(errMsg);\n        }\n      }\n    }\n    this.state.top = true;\n    return tokens;\n  }\n  inline(src, tokens = []) {\n    this.inlineQueue.push({ src, tokens });\n    return tokens;\n  }\n  /**\n   * Lexing/Compiling\n   */\n  inlineTokens(src, tokens = []) {\n    let token, lastToken, cutSrc;\n    // String with links masked to avoid interference with em and strong\n    let maskedSrc = src;\n    let match;\n    let keepPrevChar, prevChar;\n    // Mask out reflinks\n    if (this.tokens.links) {\n      const links = Object.keys(this.tokens.links);\n      if (links.length > 0) {\n        while (\n          (match = this.tokenizer.rules.inline.reflinkSearch.exec(maskedSrc)) !=\n          null\n        ) {\n          if (\n            links.includes(match[0].slice(match[0].lastIndexOf(\"[\") + 1, -1))\n          ) {\n            maskedSrc =\n              maskedSrc.slice(0, match.index) +\n              \"[\" +\n              \"a\".repeat(match[0].length - 2) +\n              \"]\" +\n              maskedSrc.slice(\n                this.tokenizer.rules.inline.reflinkSearch.lastIndex\n              );\n          }\n        }\n      }\n    }\n    // Mask out other blocks\n    while (\n      (match = this.tokenizer.rules.inline.blockSkip.exec(maskedSrc)) != null\n    ) {\n      maskedSrc =\n        maskedSrc.slice(0, match.index) +\n        \"[\" +\n        \"a\".repeat(match[0].length - 2) +\n        \"]\" +\n        maskedSrc.slice(this.tokenizer.rules.inline.blockSkip.lastIndex);\n    }\n    // Mask out escaped characters\n    while (\n      (match = this.tokenizer.rules.inline.anyPunctuation.exec(maskedSrc)) !=\n      null\n    ) {\n      maskedSrc =\n        maskedSrc.slice(0, match.index) +\n        \"++\" +\n        maskedSrc.slice(this.tokenizer.rules.inline.anyPunctuation.lastIndex);\n    }\n    while (src) {\n      if (!keepPrevChar) {\n        prevChar = \"\";\n      }\n      keepPrevChar = false;\n      // extensions\n      if (\n        this.options.extensions &&\n        this.options.extensions.inline &&\n        this.options.extensions.inline.some((extTokenizer) => {\n          if ((token = extTokenizer.call({ lexer: this }, src, tokens))) {\n            src = src.substring(token.raw.length);\n            tokens.push(token);\n            return true;\n          }\n          return false;\n        })\n      ) {\n        continue;\n      }\n      // escape\n      if ((token = this.tokenizer.escape(src))) {\n        src = src.substring(token.raw.length);\n        tokens.push(token);\n        continue;\n      }\n      // tag\n      if ((token = this.tokenizer.tag(src))) {\n        src = src.substring(token.raw.length);\n        lastToken = tokens[tokens.length - 1];\n        if (lastToken && token.type === \"text\" && lastToken.type === \"text\") {\n          lastToken.raw += token.raw;\n          lastToken.text += token.text;\n        } else {\n          tokens.push(token);\n        }\n        continue;\n      }\n      // link\n      if ((token = this.tokenizer.link(src))) {\n        src = src.substring(token.raw.length);\n        tokens.push(token);\n        continue;\n      }\n      // reflink, nolink\n      if ((token = this.tokenizer.reflink(src, this.tokens.links))) {\n        src = src.substring(token.raw.length);\n        lastToken = tokens[tokens.length - 1];\n        if (lastToken && token.type === \"text\" && lastToken.type === \"text\") {\n          lastToken.raw += token.raw;\n          lastToken.text += token.text;\n        } else {\n          tokens.push(token);\n        }\n        continue;\n      }\n      // em & strong\n      if ((token = this.tokenizer.emStrong(src, maskedSrc, prevChar))) {\n        src = src.substring(token.raw.length);\n        tokens.push(token);\n        continue;\n      }\n      // code\n      if ((token = this.tokenizer.codespan(src))) {\n        src = src.substring(token.raw.length);\n        tokens.push(token);\n        continue;\n      }\n      // br\n      if ((token = this.tokenizer.br(src))) {\n        src = src.substring(token.raw.length);\n        tokens.push(token);\n        continue;\n      }\n      // del (gfm)\n      if ((token = this.tokenizer.del(src))) {\n        src = src.substring(token.raw.length);\n        tokens.push(token);\n        continue;\n      }\n      // autolink\n      if ((token = this.tokenizer.autolink(src))) {\n        src = src.substring(token.raw.length);\n        tokens.push(token);\n        continue;\n      }\n      // url (gfm)\n      if (!this.state.inLink && (token = this.tokenizer.url(src))) {\n        src = src.substring(token.raw.length);\n        tokens.push(token);\n        continue;\n      }\n      // text\n      // prevent inlineText consuming extensions by clipping 'src' to extension start\n      cutSrc = src;\n      if (this.options.extensions && this.options.extensions.startInline) {\n        let startIndex = Infinity;\n        const tempSrc = src.slice(1);\n        let tempStart;\n        this.options.extensions.startInline.forEach((getStartIndex) => {\n          tempStart = getStartIndex.call({ lexer: this }, tempSrc);\n          if (typeof tempStart === \"number\" && tempStart >= 0) {\n            startIndex = Math.min(startIndex, tempStart);\n          }\n        });\n        if (startIndex < Infinity && startIndex >= 0) {\n          cutSrc = src.substring(0, startIndex + 1);\n        }\n      }\n      if ((token = this.tokenizer.inlineText(cutSrc))) {\n        src = src.substring(token.raw.length);\n        if (token.raw.slice(-1) !== \"_\") {\n          // Track prevChar before string of ____ started\n          prevChar = token.raw.slice(-1);\n        }\n        keepPrevChar = true;\n        lastToken = tokens[tokens.length - 1];\n        if (lastToken && lastToken.type === \"text\") {\n          lastToken.raw += token.raw;\n          lastToken.text += token.text;\n        } else {\n          tokens.push(token);\n        }\n        continue;\n      }\n      if (src) {\n        const errMsg = \"Infinite loop on byte: \" + src.charCodeAt(0);\n        if (this.options.silent) {\n          console.error(errMsg);\n          break;\n        } else {\n          throw new Error(errMsg);\n        }\n      }\n    }\n    return tokens;\n  }\n}\n\n/**\n * Renderer\n */\nclass _Renderer {\n  options;\n  parser; // set by the parser\n  constructor(options) {\n    this.options = options || _defaults;\n  }\n  space(token) {\n    return \"\";\n  }\n  code({ text, lang, escaped }) {\n    const langString = (lang || \"\").match(/^\\S*/)?.[0];\n    const code = text.replace(/\\n$/, \"\") + \"\\n\";\n    if (!langString) {\n      return (\n        \"<pre><code>\" +\n        (escaped ? code : escape$1(code, true)) +\n        \"</code></pre>\\n\"\n      );\n    }\n    return (\n      '<pre><code class=\"language-' +\n      escape$1(langString) +\n      '\">' +\n      (escaped ? code : escape$1(code, true)) +\n      \"</code></pre>\\n\"\n    );\n  }\n  blockquote({ tokens }) {\n    const body = this.parser.parse(tokens);\n    return `<blockquote>\\n${body}</blockquote>\\n`;\n  }\n  html({ text }) {\n    return text;\n  }\n  heading({ tokens, depth }) {\n    return `<h${depth}>${this.parser.parseInline(tokens)}</h${depth}>\\n`;\n  }\n  hr(token) {\n    return \"<hr>\\n\";\n  }\n  list(token) {\n    const ordered = token.ordered;\n    const start = token.start;\n    let body = \"\";\n    for (let j = 0; j < token.items.length; j++) {\n      const item = token.items[j];\n      body += this.listitem(item);\n    }\n    const type = ordered ? \"ol\" : \"ul\";\n    const startAttr = ordered && start !== 1 ? ' start=\"' + start + '\"' : \"\";\n    return \"<\" + type + startAttr + \">\\n\" + body + \"</\" + type + \">\\n\";\n  }\n  listitem(item) {\n    let itemBody = \"\";\n    if (item.task) {\n      const checkbox = this.checkbox({ checked: !!item.checked });\n      if (item.loose) {\n        if (item.tokens.length > 0 && item.tokens[0].type === \"paragraph\") {\n          item.tokens[0].text = checkbox + \" \" + item.tokens[0].text;\n          if (\n            item.tokens[0].tokens &&\n            item.tokens[0].tokens.length > 0 &&\n            item.tokens[0].tokens[0].type === \"text\"\n          ) {\n            item.tokens[0].tokens[0].text =\n              checkbox + \" \" + item.tokens[0].tokens[0].text;\n          }\n        } else {\n          item.tokens.unshift({\n            type: \"text\",\n            raw: checkbox + \" \",\n            text: checkbox + \" \",\n          });\n        }\n      } else {\n        itemBody += checkbox + \" \";\n      }\n    }\n    itemBody += this.parser.parse(item.tokens, !!item.loose);\n    return `<li>${itemBody}</li>\\n`;\n  }\n  checkbox({ checked }) {\n    return (\n      \"<input \" +\n      (checked ? 'checked=\"\" ' : \"\") +\n      'disabled=\"\" type=\"checkbox\">'\n    );\n  }\n  paragraph({ tokens }) {\n    return `<p>${this.parser.parseInline(tokens)}</p>\\n`;\n  }\n  table(token) {\n    let header = \"\";\n    // header\n    let cell = \"\";\n    for (let j = 0; j < token.header.length; j++) {\n      cell += this.tablecell(token.header[j]);\n    }\n    header += this.tablerow({ text: cell });\n    let body = \"\";\n    for (let j = 0; j < token.rows.length; j++) {\n      const row = token.rows[j];\n      cell = \"\";\n      for (let k = 0; k < row.length; k++) {\n        cell += this.tablecell(row[k]);\n      }\n      body += this.tablerow({ text: cell });\n    }\n    if (body) body = `<tbody>${body}</tbody>`;\n    return (\n      \"<table>\\n\" + \"<thead>\\n\" + header + \"</thead>\\n\" + body + \"</table>\\n\"\n    );\n  }\n  tablerow({ text }) {\n    return `<tr>\\n${text}</tr>\\n`;\n  }\n  tablecell(token) {\n    const content = this.parser.parseInline(token.tokens);\n    const type = token.header ? \"th\" : \"td\";\n    const tag = token.align ? `<${type} align=\"${token.align}\">` : `<${type}>`;\n    return tag + content + `</${type}>\\n`;\n  }\n  /**\n   * span level renderer\n   */\n  strong({ tokens }) {\n    return `<strong>${this.parser.parseInline(tokens)}</strong>`;\n  }\n  em({ tokens }) {\n    return `<em>${this.parser.parseInline(tokens)}</em>`;\n  }\n  codespan({ text }) {\n    return `<code>${text}</code>`;\n  }\n  br(token) {\n    return \"<br>\";\n  }\n  del({ tokens }) {\n    return `<del>${this.parser.parseInline(tokens)}</del>`;\n  }\n  link({ href, title, tokens }) {\n    const text = this.parser.parseInline(tokens);\n    const cleanHref = cleanUrl(href);\n    if (cleanHref === null) {\n      return text;\n    }\n    href = cleanHref;\n    let out = '<a href=\"' + href + '\"';\n    if (title) {\n      out += ' title=\"' + title + '\"';\n    }\n    out += \">\" + text + \"</a>\";\n    return out;\n  }\n  image({ href, title, text }) {\n    const cleanHref = cleanUrl(href);\n    if (cleanHref === null) {\n      return text;\n    }\n    href = cleanHref;\n    let out = `<img src=\"${href}\" alt=\"${text}\"`;\n    if (title) {\n      out += ` title=\"${title}\"`;\n    }\n    out += \">\";\n    return out;\n  }\n  text(token) {\n    return \"tokens\" in token && token.tokens\n      ? this.parser.parseInline(token.tokens)\n      : token.text;\n  }\n}\n\n/**\n * TextRenderer\n * returns only the textual part of the token\n */\nclass _TextRenderer {\n  // no need for block level renderers\n  strong({ text }) {\n    return text;\n  }\n  em({ text }) {\n    return text;\n  }\n  codespan({ text }) {\n    return text;\n  }\n  del({ text }) {\n    return text;\n  }\n  html({ text }) {\n    return text;\n  }\n  text({ text }) {\n    return text;\n  }\n  link({ text }) {\n    return \"\" + text;\n  }\n  image({ text }) {\n    return \"\" + text;\n  }\n  br() {\n    return \"\";\n  }\n}\n\n/**\n * Parsing & Compiling\n */\nclass _Parser {\n  options;\n  renderer;\n  textRenderer;\n  constructor(options) {\n    this.options = options || _defaults;\n    this.options.renderer = this.options.renderer || new _Renderer();\n    this.renderer = this.options.renderer;\n    this.renderer.options = this.options;\n    this.renderer.parser = this;\n    this.textRenderer = new _TextRenderer();\n  }\n  /**\n   * Static Parse Method\n   */\n  static parse(tokens, options) {\n    const parser = new _Parser(options);\n    return parser.parse(tokens);\n  }\n  /**\n   * Static Parse Inline Method\n   */\n  static parseInline(tokens, options) {\n    const parser = new _Parser(options);\n    return parser.parseInline(tokens);\n  }\n  /**\n   * Parse Loop\n   */\n  parse(tokens, top = true) {\n    let out = \"\";\n    for (let i = 0; i < tokens.length; i++) {\n      const anyToken = tokens[i];\n      // Run any renderer extensions\n      if (\n        this.options.extensions &&\n        this.options.extensions.renderers &&\n        this.options.extensions.renderers[anyToken.type]\n      ) {\n        const genericToken = anyToken;\n        const ret = this.options.extensions.renderers[genericToken.type].call(\n          { parser: this },\n          genericToken\n        );\n        if (\n          ret !== false ||\n          ![\n            \"space\",\n            \"hr\",\n            \"heading\",\n            \"code\",\n            \"table\",\n            \"blockquote\",\n            \"list\",\n            \"html\",\n            \"paragraph\",\n            \"text\",\n          ].includes(genericToken.type)\n        ) {\n          out += ret || \"\";\n          continue;\n        }\n      }\n      const token = anyToken;\n      switch (token.type) {\n        case \"space\": {\n          out += this.renderer.space(token);\n          continue;\n        }\n        case \"hr\": {\n          out += this.renderer.hr(token);\n          continue;\n        }\n        case \"heading\": {\n          out += this.renderer.heading(token);\n          continue;\n        }\n        case \"code\": {\n          out += this.renderer.code(token);\n          continue;\n        }\n        case \"table\": {\n          out += this.renderer.table(token);\n          continue;\n        }\n        case \"blockquote\": {\n          out += this.renderer.blockquote(token);\n          continue;\n        }\n        case \"list\": {\n          out += this.renderer.list(token);\n          continue;\n        }\n        case \"html\": {\n          out += this.renderer.html(token);\n          continue;\n        }\n        case \"paragraph\": {\n          out += this.renderer.paragraph(token);\n          continue;\n        }\n        case \"text\": {\n          let textToken = token;\n          let body = this.renderer.text(textToken);\n          while (i + 1 < tokens.length && tokens[i + 1].type === \"text\") {\n            textToken = tokens[++i];\n            body += \"\\n\" + this.renderer.text(textToken);\n          }\n          if (top) {\n            out += this.renderer.paragraph({\n              type: \"paragraph\",\n              raw: body,\n              text: body,\n              tokens: [{ type: \"text\", raw: body, text: body }],\n            });\n          } else {\n            out += body;\n          }\n          continue;\n        }\n        default: {\n          const errMsg = 'Token with \"' + token.type + '\" type was not found.';\n          if (this.options.silent) {\n            console.error(errMsg);\n            return \"\";\n          } else {\n            throw new Error(errMsg);\n          }\n        }\n      }\n    }\n    return out;\n  }\n  /**\n   * Parse Inline Tokens\n   */\n  parseInline(tokens, renderer) {\n    renderer = renderer || this.renderer;\n    let out = \"\";\n    for (let i = 0; i < tokens.length; i++) {\n      const anyToken = tokens[i];\n      // Run any renderer extensions\n      if (\n        this.options.extensions &&\n        this.options.extensions.renderers &&\n        this.options.extensions.renderers[anyToken.type]\n      ) {\n        const ret = this.options.extensions.renderers[anyToken.type].call(\n          { parser: this },\n          anyToken\n        );\n        if (\n          ret !== false ||\n          ![\n            \"escape\",\n            \"html\",\n            \"link\",\n            \"image\",\n            \"strong\",\n            \"em\",\n            \"codespan\",\n            \"br\",\n            \"del\",\n            \"text\",\n          ].includes(anyToken.type)\n        ) {\n          out += ret || \"\";\n          continue;\n        }\n      }\n      const token = anyToken;\n      switch (token.type) {\n        case \"escape\": {\n          out += renderer.text(token);\n          break;\n        }\n        case \"html\": {\n          out += renderer.html(token);\n          break;\n        }\n        case \"link\": {\n          out += renderer.link(token);\n          break;\n        }\n        case \"image\": {\n          out += renderer.image(token);\n          break;\n        }\n        case \"strong\": {\n          out += renderer.strong(token);\n          break;\n        }\n        case \"em\": {\n          out += renderer.em(token);\n          break;\n        }\n        case \"codespan\": {\n          out += renderer.codespan(token);\n          break;\n        }\n        case \"br\": {\n          out += renderer.br(token);\n          break;\n        }\n        case \"del\": {\n          out += renderer.del(token);\n          break;\n        }\n        case \"text\": {\n          out += renderer.text(token);\n          break;\n        }\n        default: {\n          const errMsg = 'Token with \"' + token.type + '\" type was not found.';\n          if (this.options.silent) {\n            console.error(errMsg);\n            return \"\";\n          } else {\n            throw new Error(errMsg);\n          }\n        }\n      }\n    }\n    return out;\n  }\n}\n\nclass _Hooks {\n  options;\n  block;\n  constructor(options) {\n    this.options = options || _defaults;\n  }\n  static passThroughHooks = new Set([\n    \"preprocess\",\n    \"postprocess\",\n    \"processAllTokens\",\n  ]);\n  /**\n   * Process markdown before marked\n   */\n  preprocess(markdown) {\n    return markdown;\n  }\n  /**\n   * Process HTML after marked is finished\n   */\n  postprocess(html) {\n    return html;\n  }\n  /**\n   * Process all tokens before walk tokens\n   */\n  processAllTokens(tokens) {\n    return tokens;\n  }\n  /**\n   * Provide function to tokenize markdown\n   */\n  provideLexer() {\n    return this.block ? _Lexer.lex : _Lexer.lexInline;\n  }\n  /**\n   * Provide function to parse tokens\n   */\n  provideParser() {\n    return this.block ? _Parser.parse : _Parser.parseInline;\n  }\n}\n\nclass Marked {\n  defaults = _getDefaults();\n  options = this.setOptions;\n  parse = this.parseMarkdown(true);\n  parseInline = this.parseMarkdown(false);\n  Parser = _Parser;\n  Renderer = _Renderer;\n  TextRenderer = _TextRenderer;\n  Lexer = _Lexer;\n  Tokenizer = _Tokenizer;\n  Hooks = _Hooks;\n  constructor(...args) {\n    this.use(...args);\n  }\n  /**\n   * Run callback for every token\n   */\n  walkTokens(tokens, callback) {\n    let values = [];\n    for (const token of tokens) {\n      values = values.concat(callback.call(this, token));\n      switch (token.type) {\n        case \"table\": {\n          const tableToken = token;\n          for (const cell of tableToken.header) {\n            values = values.concat(this.walkTokens(cell.tokens, callback));\n          }\n          for (const row of tableToken.rows) {\n            for (const cell of row) {\n              values = values.concat(this.walkTokens(cell.tokens, callback));\n            }\n          }\n          break;\n        }\n        case \"list\": {\n          const listToken = token;\n          values = values.concat(this.walkTokens(listToken.items, callback));\n          break;\n        }\n        default: {\n          const genericToken = token;\n          if (this.defaults.extensions?.childTokens?.[genericToken.type]) {\n            this.defaults.extensions.childTokens[genericToken.type].forEach(\n              (childTokens) => {\n                const tokens = genericToken[childTokens].flat(Infinity);\n                values = values.concat(this.walkTokens(tokens, callback));\n              }\n            );\n          } else if (genericToken.tokens) {\n            values = values.concat(\n              this.walkTokens(genericToken.tokens, callback)\n            );\n          }\n        }\n      }\n    }\n    return values;\n  }\n  use(...args) {\n    const extensions = this.defaults.extensions || {\n      renderers: {},\n      childTokens: {},\n    };\n    args.forEach((pack) => {\n      // copy options to new object\n      const opts = { ...pack };\n      // set async to true if it was set to true before\n      opts.async = this.defaults.async || opts.async || false;\n      // ==-- Parse \"addon\" extensions --== //\n      if (pack.extensions) {\n        pack.extensions.forEach((ext) => {\n          if (!ext.name) {\n            throw new Error(\"extension name required\");\n          }\n          if (\"renderer\" in ext) {\n            // Renderer extensions\n            const prevRenderer = extensions.renderers[ext.name];\n            if (prevRenderer) {\n              // Replace extension with func to run new extension but fall back if false\n              extensions.renderers[ext.name] = function (...args) {\n                let ret = ext.renderer.apply(this, args);\n                if (ret === false) {\n                  ret = prevRenderer.apply(this, args);\n                }\n                return ret;\n              };\n            } else {\n              extensions.renderers[ext.name] = ext.renderer;\n            }\n          }\n          if (\"tokenizer\" in ext) {\n            // Tokenizer Extensions\n            if (\n              !ext.level ||\n              (ext.level !== \"block\" && ext.level !== \"inline\")\n            ) {\n              throw new Error(\"extension level must be 'block' or 'inline'\");\n            }\n            const extLevel = extensions[ext.level];\n            if (extLevel) {\n              extLevel.unshift(ext.tokenizer);\n            } else {\n              extensions[ext.level] = [ext.tokenizer];\n            }\n            if (ext.start) {\n              // Function to check for start of token\n              if (ext.level === \"block\") {\n                if (extensions.startBlock) {\n                  extensions.startBlock.push(ext.start);\n                } else {\n                  extensions.startBlock = [ext.start];\n                }\n              } else if (ext.level === \"inline\") {\n                if (extensions.startInline) {\n                  extensions.startInline.push(ext.start);\n                } else {\n                  extensions.startInline = [ext.start];\n                }\n              }\n            }\n          }\n          if (\"childTokens\" in ext && ext.childTokens) {\n            // Child tokens to be visited by walkTokens\n            extensions.childTokens[ext.name] = ext.childTokens;\n          }\n        });\n        opts.extensions = extensions;\n      }\n      // ==-- Parse \"overwrite\" extensions --== //\n      if (pack.renderer) {\n        const renderer = this.defaults.renderer || new _Renderer(this.defaults);\n        for (const prop in pack.renderer) {\n          if (!(prop in renderer)) {\n            throw new Error(`renderer '${prop}' does not exist`);\n          }\n          if ([\"options\", \"parser\"].includes(prop)) {\n            // ignore options property\n            continue;\n          }\n          const rendererProp = prop;\n          const rendererFunc = pack.renderer[rendererProp];\n          const prevRenderer = renderer[rendererProp];\n          // Replace renderer with func to run extension, but fall back if false\n          renderer[rendererProp] = (...args) => {\n            let ret = rendererFunc.apply(renderer, args);\n            if (ret === false) {\n              ret = prevRenderer.apply(renderer, args);\n            }\n            return ret || \"\";\n          };\n        }\n        opts.renderer = renderer;\n      }\n      if (pack.tokenizer) {\n        const tokenizer =\n          this.defaults.tokenizer || new _Tokenizer(this.defaults);\n        for (const prop in pack.tokenizer) {\n          if (!(prop in tokenizer)) {\n            throw new Error(`tokenizer '${prop}' does not exist`);\n          }\n          if ([\"options\", \"rules\", \"lexer\"].includes(prop)) {\n            // ignore options, rules, and lexer properties\n            continue;\n          }\n          const tokenizerProp = prop;\n          const tokenizerFunc = pack.tokenizer[tokenizerProp];\n          const prevTokenizer = tokenizer[tokenizerProp];\n          // Replace tokenizer with func to run extension, but fall back if false\n          // @ts-expect-error cannot type tokenizer function dynamically\n          tokenizer[tokenizerProp] = (...args) => {\n            let ret = tokenizerFunc.apply(tokenizer, args);\n            if (ret === false) {\n              ret = prevTokenizer.apply(tokenizer, args);\n            }\n            return ret;\n          };\n        }\n        opts.tokenizer = tokenizer;\n      }\n      // ==-- Parse Hooks extensions --== //\n      if (pack.hooks) {\n        const hooks = this.defaults.hooks || new _Hooks();\n        for (const prop in pack.hooks) {\n          if (!(prop in hooks)) {\n            throw new Error(`hook '${prop}' does not exist`);\n          }\n          if ([\"options\", \"block\"].includes(prop)) {\n            // ignore options and block properties\n            continue;\n          }\n          const hooksProp = prop;\n          const hooksFunc = pack.hooks[hooksProp];\n          const prevHook = hooks[hooksProp];\n          if (_Hooks.passThroughHooks.has(prop)) {\n            // @ts-expect-error cannot type hook function dynamically\n            hooks[hooksProp] = (arg) => {\n              if (this.defaults.async) {\n                return Promise.resolve(hooksFunc.call(hooks, arg)).then(\n                  (ret) => {\n                    return prevHook.call(hooks, ret);\n                  }\n                );\n              }\n              const ret = hooksFunc.call(hooks, arg);\n              return prevHook.call(hooks, ret);\n            };\n          } else {\n            // @ts-expect-error cannot type hook function dynamically\n            hooks[hooksProp] = (...args) => {\n              let ret = hooksFunc.apply(hooks, args);\n              if (ret === false) {\n                ret = prevHook.apply(hooks, args);\n              }\n              return ret;\n            };\n          }\n        }\n        opts.hooks = hooks;\n      }\n      // ==-- Parse WalkTokens extensions --== //\n      if (pack.walkTokens) {\n        const walkTokens = this.defaults.walkTokens;\n        const packWalktokens = pack.walkTokens;\n        opts.walkTokens = function (token) {\n          let values = [];\n          values.push(packWalktokens.call(this, token));\n          if (walkTokens) {\n            values = values.concat(walkTokens.call(this, token));\n          }\n          return values;\n        };\n      }\n      this.defaults = { ...this.defaults, ...opts };\n    });\n    return this;\n  }\n  setOptions(opt) {\n    this.defaults = { ...this.defaults, ...opt };\n    return this;\n  }\n  lexer(src, options) {\n    return _Lexer.lex(src, options ?? this.defaults);\n  }\n  parser(tokens, options) {\n    return _Parser.parse(tokens, options ?? this.defaults);\n  }\n  parseMarkdown(blockType) {\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    const parse = (src, options) => {\n      const origOpt = { ...options };\n      const opt = { ...this.defaults, ...origOpt };\n      const throwError = this.onError(!!opt.silent, !!opt.async);\n      // throw error if an extension set async to true but parse was called with async: false\n      if (this.defaults.async === true && origOpt.async === false) {\n        return throwError(\n          new Error(\n            \"marked(): The async option was set to true by an extension. Remove async: false from the parse options object to return a Promise.\"\n          )\n        );\n      }\n      // throw error in case of non string input\n      if (typeof src === \"undefined\" || src === null) {\n        return throwError(\n          new Error(\"marked(): input parameter is undefined or null\")\n        );\n      }\n      if (typeof src !== \"string\") {\n        return throwError(\n          new Error(\n            \"marked(): input parameter is of type \" +\n              Object.prototype.toString.call(src) +\n              \", string expected\"\n          )\n        );\n      }\n      if (opt.hooks) {\n        opt.hooks.options = opt;\n        opt.hooks.block = blockType;\n      }\n      const lexer = opt.hooks\n        ? opt.hooks.provideLexer()\n        : blockType\n          ? _Lexer.lex\n          : _Lexer.lexInline;\n      const parser = opt.hooks\n        ? opt.hooks.provideParser()\n        : blockType\n          ? _Parser.parse\n          : _Parser.parseInline;\n      if (opt.async) {\n        return Promise.resolve(opt.hooks ? opt.hooks.preprocess(src) : src)\n          .then((src) => lexer(src, opt))\n          .then((tokens) =>\n            opt.hooks ? opt.hooks.processAllTokens(tokens) : tokens\n          )\n          .then((tokens) =>\n            opt.walkTokens\n              ? Promise.all(this.walkTokens(tokens, opt.walkTokens)).then(\n                  () => tokens\n                )\n              : tokens\n          )\n          .then((tokens) => parser(tokens, opt))\n          .then((html) => (opt.hooks ? opt.hooks.postprocess(html) : html))\n          .catch(throwError);\n      }\n      try {\n        if (opt.hooks) {\n          src = opt.hooks.preprocess(src);\n        }\n        let tokens = lexer(src, opt);\n        if (opt.hooks) {\n          tokens = opt.hooks.processAllTokens(tokens);\n        }\n        if (opt.walkTokens) {\n          this.walkTokens(tokens, opt.walkTokens);\n        }\n        let html = parser(tokens, opt);\n        if (opt.hooks) {\n          html = opt.hooks.postprocess(html);\n        }\n        return html;\n      } catch (e) {\n        return throwError(e);\n      }\n    };\n    return parse;\n  }\n  onError(silent, async) {\n    return (e) => {\n      e.message +=\n        \"\\nPlease report this to https://github.com/markedjs/marked.\";\n      if (silent) {\n        const msg =\n          \"<p>An error occurred:</p><pre>\" +\n          escape$1(e.message + \"\", true) +\n          \"</pre>\";\n        if (async) {\n          return Promise.resolve(msg);\n        }\n        return msg;\n      }\n      if (async) {\n        return Promise.reject(e);\n      }\n      throw e;\n    };\n  }\n}\n\nconst markedInstance = new Marked();\nfunction marked(src, opt) {\n  return markedInstance.parse(src, opt);\n}\n/**\n * Sets the default options.\n *\n * @param options Hash of options\n */\nmarked.options = marked.setOptions = function (options) {\n  markedInstance.setOptions(options);\n  marked.defaults = markedInstance.defaults;\n  changeDefaults(marked.defaults);\n  return marked;\n};\n/**\n * Gets the original marked default options.\n */\nmarked.getDefaults = _getDefaults;\nmarked.defaults = _defaults;\n/**\n * Use Extension\n */\nmarked.use = function (...args) {\n  markedInstance.use(...args);\n  marked.defaults = markedInstance.defaults;\n  changeDefaults(marked.defaults);\n  return marked;\n};\n/**\n * Run callback for every token\n */\nmarked.walkTokens = function (tokens, callback) {\n  return markedInstance.walkTokens(tokens, callback);\n};\n/**\n * Compiles markdown to HTML without enclosing `p` tag.\n *\n * @param src String of markdown source to be compiled\n * @param options Hash of options\n * @return String of compiled HTML\n */\nmarked.parseInline = markedInstance.parseInline;\n/**\n * Expose\n */\nmarked.Parser = _Parser;\nmarked.parser = _Parser.parse;\nmarked.Renderer = _Renderer;\nmarked.TextRenderer = _TextRenderer;\nmarked.Lexer = _Lexer;\nmarked.lexer = _Lexer.lex;\nmarked.Tokenizer = _Tokenizer;\nmarked.Hooks = _Hooks;\nmarked.parse = marked;\nconst options = marked.options;\nconst setOptions = marked.setOptions;\nconst use = marked.use;\nconst walkTokens = marked.walkTokens;\nconst parseInline = marked.parseInline;\nconst parse = marked;\nconst parser = _Parser.parse;\nconst lexer = _Lexer.lex;\n\nexport {\n  _Hooks as Hooks,\n  _Lexer as Lexer,\n  Marked,\n  _Parser as Parser,\n  _Renderer as Renderer,\n  _TextRenderer as TextRenderer,\n  _Tokenizer as Tokenizer,\n  _defaults as defaults,\n  _getDefaults as getDefaults,\n  lexer,\n  marked,\n  options,\n  parse,\n  parseInline,\n  parser,\n  setOptions,\n  use,\n  walkTokens,\n};\n//# sourceMappingURL=marked.esm.js.map\n",
      "metadata": {
        "description": "",
        "url": "marked.js",
        "runnable": false
      }
    },
    "unescape": {
      "code": "export { unescape };\n\nconst unescapeTest = /&(#(?:\\d+)|(?:#x[0-9A-Fa-f]+)|(?:\\w+));?/gi;\n\nconst namedEntities = {\n  colon: \":\",\n  quot: '\"',\n  amp: \"&\",\n  lt: \"<\",\n  gt: \">\",\n  apos: \"'\",\n  // add more as needed\n};\n\nfunction unescape(html) {\n  return html.replace(unescapeTest, (_, n) => {\n    n = n.toLowerCase();\n    if (n in namedEntities) return namedEntities[n];\n\n    if (n.charAt(0) === \"#\") {\n      const code =\n        n.charAt(1) === \"x\" ? parseInt(n.substring(2), 16) : +n.substring(1);\n\n      return code >= 0 && code <= 0x10ffff ? String.fromCodePoint(code) : \"\";\n    }\n    return \"\";\n  });\n}\n",
      "metadata": {
        "description": "",
        "url": "unescape.js",
        "runnable": false
      }
    }
  },
  "metadata": {
    "tags": [],
    "icon": "google-drive",
    "visual": {
      "presentation": {
        "themes": {
          "e29fcf24-f33a-4f54-a0b2-c406774d6f3c": {
            "themeColors": {
              "primaryColor": "#246db5",
              "secondaryColor": "#5cadff",
              "backgroundColor": "#ffffff",
              "textColor": "#1a1a1a",
              "primaryTextColor": "#ffffff"
            },
            "template": "basic",
            "splashScreen": {
              "storedData": {
                "handle": "/images/app/generic-flow.jpg",
                "mimeType": "image/jpeg"
              }
            }
          }
        },
        "theme": "e29fcf24-f33a-4f54-a0b2-c406774d6f3c"
      }
    }
  },
  "assets": {
    "@@thumbnail": {
      "metadata": {
        "title": "Thumbnail",
        "type": "file"
      },
      "data": "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjUwIiBoZWlnaHQ9IjIwMCIgdmlld0JveD0iMCAwIDI1MCAyMDAiIGZpbGw9Im5vbmUiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+CiAgICA8cmVjdCB4PSIxMC4wMCIKICAgICAgICAgICAgICAgICAgICB5PSI3OS40NCIKICAgICAgICAgICAgICAgICAgICB3aWR0aD0iNjEuMDgiCiAgICAgICAgICAgICAgICAgICAgaGVpZ2h0PSIyMy40OSIKICAgICAgICAgICAgICAgICAgICByeD0iMy41IgogICAgICAgICAgICAgICAgICAgIGZpbGw9IndoaXRlIgogICAgICAgICAgICAgICAgICAgIHN0cm9rZT0iIzIwYTIwMiIgLz4KPHJlY3QgeD0iMTc4LjkyIgogICAgICAgICAgICAgICAgICAgIHk9Ijk3LjA2IgogICAgICAgICAgICAgICAgICAgIHdpZHRoPSI2MS4wOCIKICAgICAgICAgICAgICAgICAgICBoZWlnaHQ9IjIzLjQ5IgogICAgICAgICAgICAgICAgICAgIHJ4PSIzLjUiCiAgICAgICAgICAgICAgICAgICAgZmlsbD0id2hpdGUiCiAgICAgICAgICAgICAgICAgICAgc3Ryb2tlPSIjMjBhMjAyIiAvPgo8cmVjdCB4PSI4MS44OSIKICAgICAgICAgICAgICAgICAgICB5PSI3OS42OCIKICAgICAgICAgICAgICAgICAgICB3aWR0aD0iNjEuMDgiCiAgICAgICAgICAgICAgICAgICAgaGVpZ2h0PSIyMy40OSIKICAgICAgICAgICAgICAgICAgICByeD0iMy41IgogICAgICAgICAgICAgICAgICAgIGZpbGw9IndoaXRlIgogICAgICAgICAgICAgICAgICAgIHN0cm9rZT0iIzJlOGJlOCIgLz4KICAgICAgCiAgICA8L3N2Zz4="
    }
  }
}